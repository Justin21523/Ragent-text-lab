{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db255586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (複製到每本 notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddff3591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import Dependencies and LLMAdapter\n",
    "import gradio as gr\n",
    "import threading\n",
    "import time\n",
    "import traceback\n",
    "from typing import Iterator, Optional, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import queue\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GenerationState:\n",
    "    \"\"\"Track generation state for UI updates\"\"\"\n",
    "\n",
    "    is_generating: bool = False\n",
    "    should_stop: bool = False\n",
    "    error_message: Optional[str] = None\n",
    "\n",
    "\n",
    "class LLMAdapter:\n",
    "    \"\"\"Lightweight streaming LLM adapter\"\"\"\n",
    "\n",
    "    def __init__(self, model_id: str = \"microsoft/DialoGPT-small\", **kwargs):\n",
    "        print(f\"Loading model: {model_id}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=\"auto\",\n",
    "            low_cpu_mem_usage=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "        print(f\"Model loaded on device: {self.model.device}\")\n",
    "\n",
    "    def stream_generate(\n",
    "        self,\n",
    "        messages: list,\n",
    "        max_new_tokens: int = 256,\n",
    "        temperature: float = 0.7,\n",
    "        stop_event: Optional[threading.Event] = None,\n",
    "    ) -> Iterator[str]:\n",
    "        \"\"\"Stream generation with cancellation support\"\"\"\n",
    "        try:\n",
    "            # Convert messages to prompt\n",
    "            prompt = self._messages_to_prompt(messages)\n",
    "\n",
    "            # Tokenize input\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "\n",
    "            # Track generated text\n",
    "            generated_text = \"\"\n",
    "\n",
    "            # Generate with streaming\n",
    "            with torch.no_grad():\n",
    "                for i in range(max_new_tokens):\n",
    "                    # Check for cancellation\n",
    "                    if stop_event and stop_event.is_set():\n",
    "                        yield generated_text + \" [CANCELLED]\"\n",
    "                        return\n",
    "\n",
    "                    # Generate next token\n",
    "                    outputs = self.model(**inputs)\n",
    "                    next_token_logits = outputs.logits[0, -1, :] / temperature\n",
    "\n",
    "                    # Sample next token\n",
    "                    probs = torch.softmax(next_token_logits, dim=-1)\n",
    "                    next_token = torch.multinomial(probs, 1)\n",
    "\n",
    "                    # Decode token\n",
    "                    token_text = self.tokenizer.decode(\n",
    "                        next_token, skip_special_tokens=True\n",
    "                    )\n",
    "\n",
    "                    # Update generated text\n",
    "                    generated_text += token_text\n",
    "\n",
    "                    # Yield current progress\n",
    "                    yield generated_text\n",
    "\n",
    "                    # Check for EOS\n",
    "                    if next_token.item() == self.tokenizer.eos_token_id:\n",
    "                        break\n",
    "\n",
    "                    # Update inputs for next iteration\n",
    "                    inputs[\"input_ids\"] = torch.cat(\n",
    "                        [inputs[\"input_ids\"], next_token.unsqueeze(0)], dim=-1\n",
    "                    )\n",
    "                    inputs[\"attention_mask\"] = torch.cat(\n",
    "                        [\n",
    "                            inputs[\"attention_mask\"],\n",
    "                            torch.ones((1, 1), device=self.model.device),\n",
    "                        ],\n",
    "                        dim=-1,\n",
    "                    )\n",
    "\n",
    "                    # Add small delay to make streaming visible\n",
    "                    time.sleep(0.05)\n",
    "\n",
    "        except Exception as e:\n",
    "            yield f\"Error during generation: {str(e)}\"\n",
    "\n",
    "    def _messages_to_prompt(self, messages: list) -> str:\n",
    "        \"\"\"Convert messages to simple prompt format\"\"\"\n",
    "        prompt_parts = []\n",
    "        for msg in messages:\n",
    "            role = msg.get(\"role\", \"user\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            if role == \"system\":\n",
    "                prompt_parts.append(f\"System: {content}\")\n",
    "            elif role == \"user\":\n",
    "                prompt_parts.append(f\"User: {content}\")\n",
    "            elif role == \"assistant\":\n",
    "                prompt_parts.append(f\"Assistant: {content}\")\n",
    "\n",
    "        prompt_parts.append(\"Assistant:\")\n",
    "        return \"\\n\".join(prompt_parts)\n",
    "\n",
    "\n",
    "# Initialize adapter with small model for testing\n",
    "llm_adapter = LLMAdapter(\"microsoft/DialoGPT-small\")\n",
    "generation_state = GenerationState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570ed16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Streaming Generator Implementation\n",
    "class StreamingChatbot:\n",
    "    \"\"\"Chatbot with streaming and retry capabilities\"\"\"\n",
    "\n",
    "    def __init__(self, llm_adapter: LLMAdapter):\n",
    "        self.llm_adapter = llm_adapter\n",
    "        self.stop_event = threading.Event()\n",
    "        self.generation_thread = None\n",
    "\n",
    "    def generate_streaming_response(\n",
    "        self, history: list, message: str\n",
    "    ) -> Iterator[tuple]:\n",
    "        \"\"\"Generate streaming response with proper history management\"\"\"\n",
    "        if not message.strip():\n",
    "            yield history, \"\"\n",
    "            return\n",
    "\n",
    "        # Add user message to history\n",
    "        history = history + [[message, \"\"]]\n",
    "\n",
    "        # Prepare messages for LLM\n",
    "        messages = self._history_to_messages(\n",
    "            history[:-1]\n",
    "        )  # Exclude current incomplete exchange\n",
    "        messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "        try:\n",
    "            generation_state.is_generating = True\n",
    "            generation_state.should_stop = False\n",
    "            generation_state.error_message = None\n",
    "\n",
    "            # Stream generation\n",
    "            for partial_response in self.llm_adapter.stream_generate(\n",
    "                messages,\n",
    "                max_new_tokens=200,\n",
    "                temperature=0.7,\n",
    "                stop_event=self.stop_event,\n",
    "            ):\n",
    "                if generation_state.should_stop:\n",
    "                    break\n",
    "\n",
    "                # Update last exchange in history\n",
    "                history[-1][1] = partial_response\n",
    "                yield history, \"\"\n",
    "\n",
    "        except Exception as e:\n",
    "            generation_state.error_message = f\"Generation failed: {str(e)}\"\n",
    "            history[-1][1] = f\"❌ Error: {str(e)}\"\n",
    "            yield history, \"\"\n",
    "        finally:\n",
    "            generation_state.is_generating = False\n",
    "\n",
    "    def _history_to_messages(self, history: list) -> list:\n",
    "        \"\"\"Convert chat history to messages format\"\"\"\n",
    "        messages = []\n",
    "        for user_msg, assistant_msg in history:\n",
    "            if user_msg:\n",
    "                messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            if assistant_msg:\n",
    "                messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "        return messages\n",
    "\n",
    "    def stop_generation(self):\n",
    "        \"\"\"Stop current generation\"\"\"\n",
    "        generation_state.should_stop = True\n",
    "        self.stop_event.set()\n",
    "\n",
    "    def retry_last_response(self, history: list) -> Iterator[tuple]:\n",
    "        \"\"\"Retry the last failed response\"\"\"\n",
    "        if not history:\n",
    "            return\n",
    "\n",
    "        # Get last user message\n",
    "        last_user_message = history[-1][0] if history else \"\"\n",
    "\n",
    "        # Remove last incomplete response\n",
    "        if history and history[-1][1]:\n",
    "            history = history[:-1]\n",
    "\n",
    "        # Regenerate response\n",
    "        yield from self.generate_streaming_response(history, last_user_message)\n",
    "\n",
    "\n",
    "# Initialize chatbot\n",
    "chatbot = StreamingChatbot(llm_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab03672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Gradio Interface with Event Handling\n",
    "def create_streaming_ui():\n",
    "    \"\"\"Create Gradio interface with streaming support\"\"\"\n",
    "\n",
    "    with gr.Blocks(title=\"Streaming Chat with Retry\", theme=gr.themes.Soft()) as demo:\n",
    "        gr.Markdown(\"## 🤖 串流對話機器人\")\n",
    "        gr.Markdown(\"支援即時串流輸出、錯誤重試與取消功能\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=4):\n",
    "                chatbot_ui = gr.Chatbot(\n",
    "                    label=\"對話記錄\",\n",
    "                    height=400,\n",
    "                    show_copy_button=True,\n",
    "                    bubble_full_width=False,\n",
    "                )\n",
    "\n",
    "                with gr.Row():\n",
    "                    msg_input = gr.Textbox(\n",
    "                        label=\"輸入訊息\",\n",
    "                        placeholder=\"在此輸入您的問題...\",\n",
    "                        scale=4,\n",
    "                        lines=2,\n",
    "                    )\n",
    "\n",
    "                with gr.Row():\n",
    "                    send_btn = gr.Button(\"發送\", variant=\"primary\", scale=1)\n",
    "                    stop_btn = gr.Button(\"停止\", variant=\"stop\", scale=1)\n",
    "                    retry_btn = gr.Button(\"重試\", variant=\"secondary\", scale=1)\n",
    "                    clear_btn = gr.Button(\"清除\", scale=1)\n",
    "\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\"### 狀態面板\")\n",
    "                status_text = gr.Textbox(\n",
    "                    label=\"生成狀態\", value=\"就緒\", interactive=False\n",
    "                )\n",
    "\n",
    "                gr.Markdown(\"### 設定\")\n",
    "                model_info = gr.Textbox(\n",
    "                    label=\"目前模型\",\n",
    "                    value=\"microsoft/DialoGPT-small\",\n",
    "                    interactive=False,\n",
    "                )\n",
    "\n",
    "                error_display = gr.Textbox(\n",
    "                    label=\"錯誤訊息\", value=\"\", visible=False, interactive=False\n",
    "                )\n",
    "\n",
    "        # Event handlers\n",
    "        def update_status(is_generating: bool, error: str = \"\"):\n",
    "            \"\"\"Update UI status\"\"\"\n",
    "            if error:\n",
    "                return \"❌ 錯誤\", True, error\n",
    "            elif is_generating:\n",
    "                return \"🔄 生成中...\", False, \"\"\n",
    "            else:\n",
    "                return \"✅ 就緒\", False, \"\"\n",
    "\n",
    "        def send_message(history, message):\n",
    "            \"\"\"Handle send message event\"\"\"\n",
    "            if not message.strip():\n",
    "                return history, \"\", \"請輸入有效訊息\", False, \"\"\n",
    "\n",
    "            # Start streaming generation\n",
    "            return chatbot.generate_streaming_response(history, message)\n",
    "\n",
    "        def stop_generation():\n",
    "            \"\"\"Handle stop generation event\"\"\"\n",
    "            chatbot.stop_generation()\n",
    "            return \"⏹️ 已停止\", False, \"\"\n",
    "\n",
    "        def retry_generation(history):\n",
    "            \"\"\"Handle retry event\"\"\"\n",
    "            return chatbot.retry_last_response(history)\n",
    "\n",
    "        def clear_chat():\n",
    "            \"\"\"Clear chat history\"\"\"\n",
    "            return [], \"\", \"✅ 已清除\", False, \"\"\n",
    "\n",
    "        # Wire up events\n",
    "        msg_input.submit(\n",
    "            send_message,\n",
    "            inputs=[chatbot_ui, msg_input],\n",
    "            outputs=[chatbot_ui, msg_input],\n",
    "        )\n",
    "\n",
    "        send_btn.click(\n",
    "            send_message,\n",
    "            inputs=[chatbot_ui, msg_input],\n",
    "            outputs=[chatbot_ui, msg_input],\n",
    "        )\n",
    "\n",
    "        stop_btn.click(\n",
    "            stop_generation, outputs=[status_text, error_display, error_display]\n",
    "        )\n",
    "\n",
    "        retry_btn.click(retry_generation, inputs=[chatbot_ui], outputs=[chatbot_ui])\n",
    "\n",
    "        clear_btn.click(\n",
    "            clear_chat,\n",
    "            outputs=[chatbot_ui, msg_input, status_text, error_display, error_display],\n",
    "        )\n",
    "\n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eb122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Error Handling and Retry Logic\n",
    "class RetryManager:\n",
    "    \"\"\"Manage retry attempts and error recovery\"\"\"\n",
    "\n",
    "    def __init__(self, max_retries: int = 3):\n",
    "        self.max_retries = max_retries\n",
    "        self.retry_count = 0\n",
    "        self.last_error = None\n",
    "\n",
    "    def attempt_generation(self, func, *args, **kwargs):\n",
    "        \"\"\"Attempt generation with retry logic\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                self.retry_count = attempt\n",
    "                result = func(*args, **kwargs)\n",
    "                self.last_error = None\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                self.last_error = str(e)\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    print(f\"Attempt {attempt + 1} failed: {e}. Retrying...\")\n",
    "                    time.sleep(1)  # Brief delay before retry\n",
    "                else:\n",
    "                    print(f\"All {self.max_retries} attempts failed. Last error: {e}\")\n",
    "                    raise e\n",
    "\n",
    "    def get_retry_status(self) -> dict:\n",
    "        \"\"\"Get current retry status\"\"\"\n",
    "        return {\n",
    "            \"retry_count\": self.retry_count,\n",
    "            \"max_retries\": self.max_retries,\n",
    "            \"last_error\": self.last_error,\n",
    "            \"has_failed\": self.last_error is not None,\n",
    "        }\n",
    "\n",
    "\n",
    "# Enhanced chatbot with retry management\n",
    "class RobustStreamingChatbot(StreamingChatbot):\n",
    "    \"\"\"Enhanced chatbot with robust error handling\"\"\"\n",
    "\n",
    "    def __init__(self, llm_adapter: LLMAdapter, max_retries: int = 2):\n",
    "        super().__init__(llm_adapter)\n",
    "        self.retry_manager = RetryManager(max_retries)\n",
    "\n",
    "    def generate_with_retry(self, history: list, message: str) -> Iterator[tuple]:\n",
    "        \"\"\"Generate response with automatic retry on failure\"\"\"\n",
    "        try:\n",
    "            yield from self.generate_streaming_response(history, message)\n",
    "        except Exception as e:\n",
    "            # Add error message to history\n",
    "            error_history = history + [\n",
    "                [message, f\"❌ 生成失敗: {str(e)}\\n💡 點擊「重試」按鈕重新生成\"]\n",
    "            ]\n",
    "            yield error_history, \"\"\n",
    "\n",
    "            # Update error state\n",
    "            generation_state.error_message = str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c57e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Cancellation Mechanism\n",
    "class CancellableGenerator:\n",
    "    \"\"\"Generator wrapper with cancellation support\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cancelled = False\n",
    "        self.cancel_event = threading.Event()\n",
    "\n",
    "    def cancel(self):\n",
    "        \"\"\"Cancel current operation\"\"\"\n",
    "        self.cancelled = True\n",
    "        self.cancel_event.set()\n",
    "\n",
    "    def is_cancelled(self) -> bool:\n",
    "        \"\"\"Check if operation was cancelled\"\"\"\n",
    "        return self.cancelled or self.cancel_event.is_set()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset cancellation state\"\"\"\n",
    "        self.cancelled = False\n",
    "        self.cancel_event.clear()\n",
    "\n",
    "\n",
    "# Global cancellation manager\n",
    "cancel_manager = CancellableGenerator()\n",
    "\n",
    "\n",
    "def create_enhanced_ui():\n",
    "    \"\"\"Create enhanced UI with proper cancellation\"\"\"\n",
    "\n",
    "    enhanced_chatbot = RobustStreamingChatbot(llm_adapter)\n",
    "\n",
    "    with gr.Blocks(title=\"Enhanced Streaming Chat\") as demo:\n",
    "        gr.Markdown(\"# 🚀 增強版串流對話\")\n",
    "\n",
    "        chatbot_ui = gr.Chatbot(height=450, show_copy_button=True)\n",
    "\n",
    "        with gr.Row():\n",
    "            msg_input = gr.Textbox(\n",
    "                placeholder=\"輸入您的問題 (支援中文)...\", scale=4, lines=3\n",
    "            )\n",
    "\n",
    "        with gr.Row():\n",
    "            send_btn = gr.Button(\"📤 發送\", variant=\"primary\")\n",
    "            stop_btn = gr.Button(\"⏹️ 停止\", variant=\"stop\")\n",
    "            retry_btn = gr.Button(\"🔄 重試\", variant=\"secondary\")\n",
    "            clear_btn = gr.Button(\"🗑️ 清除\")\n",
    "\n",
    "        with gr.Row():\n",
    "            status_display = gr.Textbox(\n",
    "                label=\"狀態\", value=\"✅ 就緒\", interactive=False, scale=2\n",
    "            )\n",
    "            retry_info = gr.Textbox(\n",
    "                label=\"重試資訊\", value=\"尚未重試\", interactive=False, scale=2\n",
    "            )\n",
    "\n",
    "        # Enhanced event handlers with proper state management\n",
    "        def enhanced_send(history, message):\n",
    "            \"\"\"Enhanced send with cancellation support\"\"\"\n",
    "            if not message.strip():\n",
    "                return history, \"\"\n",
    "\n",
    "            cancel_manager.reset()\n",
    "            return enhanced_chatbot.generate_with_retry(history, message)\n",
    "\n",
    "        def enhanced_stop():\n",
    "            \"\"\"Enhanced stop with immediate feedback\"\"\"\n",
    "            cancel_manager.cancel()\n",
    "            enhanced_chatbot.stop_generation()\n",
    "            return \"⏹️ 生成已停止\", \"停止於: \" + time.strftime(\"%H:%M:%S\")\n",
    "\n",
    "        def enhanced_retry(history):\n",
    "            \"\"\"Enhanced retry with status tracking\"\"\"\n",
    "            if not history:\n",
    "                return history\n",
    "\n",
    "            cancel_manager.reset()\n",
    "            status = enhanced_chatbot.retry_manager.get_retry_status()\n",
    "            retry_text = f\"重試次數: {status['retry_count']}/{status['max_retries']}\"\n",
    "\n",
    "            return (\n",
    "                enhanced_chatbot.retry_last_response(history),\n",
    "                \"🔄 重試中...\",\n",
    "                retry_text,\n",
    "            )\n",
    "\n",
    "        def enhanced_clear():\n",
    "            \"\"\"Enhanced clear with confirmation\"\"\"\n",
    "            cancel_manager.cancel()\n",
    "            return [], \"\", \"✅ 對話已清除\", \"已重置\"\n",
    "\n",
    "        # Wire events\n",
    "        send_btn.click(enhanced_send, [chatbot_ui, msg_input], [chatbot_ui, msg_input])\n",
    "        stop_btn.click(enhanced_stop, outputs=[status_display, retry_info])\n",
    "        retry_btn.click(\n",
    "            enhanced_retry, [chatbot_ui], [chatbot_ui, status_display, retry_info]\n",
    "        )\n",
    "        clear_btn.click(\n",
    "            enhanced_clear, outputs=[chatbot_ui, msg_input, status_display, retry_info]\n",
    "        )\n",
    "\n",
    "        msg_input.submit(\n",
    "            enhanced_send, [chatbot_ui, msg_input], [chatbot_ui, msg_input]\n",
    "        )\n",
    "\n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96996b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Complete UI Integration Test\n",
    "def run_comprehensive_test():\n",
    "    \"\"\"Test all streaming and retry features\"\"\"\n",
    "    print(\"=== 串流與重試功能綜合測試 ===\")\n",
    "\n",
    "    # Test 1: Basic streaming\n",
    "    print(\"\\n1. 測試基本串流生成...\")\n",
    "    test_messages = [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n",
    "\n",
    "    response_parts = []\n",
    "    for partial in llm_adapter.stream_generate(test_messages, max_new_tokens=50):\n",
    "        response_parts.append(partial)\n",
    "        if len(response_parts) % 5 == 0:  # Show progress every 5 tokens\n",
    "            print(f\"   部分回應: {partial[:50]}...\")\n",
    "\n",
    "    print(f\"   ✅ 完整回應: {response_parts[-1] if response_parts else 'No response'}\")\n",
    "\n",
    "    # Test 2: Cancellation\n",
    "    print(\"\\n2. 測試取消機制...\")\n",
    "    stop_event = threading.Event()\n",
    "\n",
    "    def delayed_cancel():\n",
    "        time.sleep(0.2)  # Cancel after 200ms\n",
    "        stop_event.set()\n",
    "        print(\"   ⏹️ 已發送取消信號\")\n",
    "\n",
    "    cancel_thread = threading.Thread(target=delayed_cancel)\n",
    "    cancel_thread.start()\n",
    "\n",
    "    cancelled_parts = []\n",
    "    for partial in llm_adapter.stream_generate(\n",
    "        test_messages, max_new_tokens=100, stop_event=stop_event\n",
    "    ):\n",
    "        cancelled_parts.append(partial)\n",
    "\n",
    "    cancel_thread.join()\n",
    "    final_response = cancelled_parts[-1] if cancelled_parts else \"\"\n",
    "    is_cancelled = \"[CANCELLED]\" in final_response\n",
    "    print(f\"   ✅ 取消測試: {'成功' if is_cancelled else '失敗'}\")\n",
    "\n",
    "    # Test 3: Error handling\n",
    "    print(\"\\n3. 測試錯誤處理...\")\n",
    "    try:\n",
    "        # Simulate error by passing invalid input\n",
    "        error_messages = [{\"role\": \"invalid\", \"content\": \"\"}]\n",
    "        list(llm_adapter.stream_generate(error_messages, max_new_tokens=10))\n",
    "        print(\"   ⚠️ 預期錯誤未發生\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ✅ 錯誤處理成功: {type(e).__name__}\")\n",
    "\n",
    "    # Test 4: Retry manager\n",
    "    print(\"\\n4. 測試重試管理器...\")\n",
    "    retry_mgr = RetryManager(max_retries=2)\n",
    "\n",
    "    def failing_function():\n",
    "        if retry_mgr.retry_count < 1:  # Fail on first attempt\n",
    "            raise ValueError(\"模擬失敗\")\n",
    "        return \"成功\"\n",
    "\n",
    "    try:\n",
    "        result = retry_mgr.attempt_generation(failing_function)\n",
    "        print(f\"   ✅ 重試成功: {result}\")\n",
    "    except:\n",
    "        print(\"   ❌ 重試失敗\")\n",
    "\n",
    "    status = retry_mgr.get_retry_status()\n",
    "    print(f\"   📊 重試狀態: {status['retry_count']}/{status['max_retries']} 次嘗試\")\n",
    "\n",
    "    print(\"\\n=== 測試完成 ===\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# Run comprehensive test\n",
    "test_result = run_comprehensive_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8917b81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Smoke Test - Launch UI\n",
    "def smoke_test():\n",
    "    \"\"\"Quick smoke test to verify everything works\"\"\"\n",
    "    print(\"🧪 Smoke Test: Gradio 串流介面\")\n",
    "\n",
    "    try:\n",
    "        # Create and launch demo\n",
    "        demo = create_enhanced_ui()\n",
    "        print(\"✅ UI 建立成功\")\n",
    "\n",
    "        # Test basic functionality without launching server\n",
    "        print(\"✅ 所有組件初始化完成\")\n",
    "        print(\"✅ 事件處理器綁定成功\")\n",
    "        print(\"✅ 錯誤處理機制就緒\")\n",
    "\n",
    "        print(\"\\n🚀 啟動 Gradio 界面...\")\n",
    "        print(\"📝 功能測試清單:\")\n",
    "        print(\"   - 輸入訊息並觀察串流輸出\")\n",
    "        print(\"   - 點擊「停止」按鈕測試取消功能\")\n",
    "        print(\"   - 點擊「重試」按鈕測試重新生成\")\n",
    "        print(\"   - 點擊「清除」按鈕清空對話\")\n",
    "\n",
    "        # Launch with specific config for testing\n",
    "        demo.launch(\n",
    "            server_name=\"0.0.0.0\",\n",
    "            server_port=7861,\n",
    "            share=False,\n",
    "            debug=True,\n",
    "            show_error=True,\n",
    "            quiet=False,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Smoke test 失敗: {str(e)}\")\n",
    "        print(f\"錯誤詳情: {traceback.format_exc()}\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "print(\"開始 Smoke Test...\")\n",
    "smoke_result = smoke_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b2b2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 低 VRAM 優化選項\n",
    "model_config:\n",
    "  device_map: \"auto\"           # 自動設備分配\n",
    "  torch_dtype: \"auto\"          # 自動精度選擇\n",
    "  low_cpu_mem_usage: true      # 減少 CPU 記憶體使用\n",
    "\n",
    "# 串流生成參數\n",
    "streaming_config:\n",
    "  max_new_tokens: 200          # 限制生成長度\n",
    "  temperature: 0.7             # 控制創造性\n",
    "  stream_delay: 0.05           # 串流延遲 (秒)\n",
    "\n",
    "# 重試機制設定\n",
    "retry_config:\n",
    "  max_retries: 2               # 最大重試次數\n",
    "  retry_delay: 1.0             # 重試間隔 (秒)\n",
    "\n",
    "# UI 設定\n",
    "ui_config:\n",
    "  chatbot_height: 450          # 對話區高度\n",
    "  show_copy_button: true       # 顯示複製按鈕\n",
    "  bubble_full_width: false     # 泡泡框寬度限制"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
