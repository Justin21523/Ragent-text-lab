{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db255586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (è¤‡è£½åˆ°æ¯æœ¬ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddff3591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import Dependencies and LLMAdapter\n",
    "import gradio as gr\n",
    "import threading\n",
    "import time\n",
    "import traceback\n",
    "from typing import Iterator, Optional, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import queue\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GenerationState:\n",
    "    \"\"\"Track generation state for UI updates\"\"\"\n",
    "\n",
    "    is_generating: bool = False\n",
    "    should_stop: bool = False\n",
    "    error_message: Optional[str] = None\n",
    "\n",
    "\n",
    "class LLMAdapter:\n",
    "    \"\"\"Lightweight streaming LLM adapter\"\"\"\n",
    "\n",
    "    def __init__(self, model_id: str = \"microsoft/DialoGPT-small\", **kwargs):\n",
    "        print(f\"Loading model: {model_id}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=\"auto\",\n",
    "            low_cpu_mem_usage=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "        print(f\"Model loaded on device: {self.model.device}\")\n",
    "\n",
    "    def stream_generate(\n",
    "        self,\n",
    "        messages: list,\n",
    "        max_new_tokens: int = 256,\n",
    "        temperature: float = 0.7,\n",
    "        stop_event: Optional[threading.Event] = None,\n",
    "    ) -> Iterator[str]:\n",
    "        \"\"\"Stream generation with cancellation support\"\"\"\n",
    "        try:\n",
    "            # Convert messages to prompt\n",
    "            prompt = self._messages_to_prompt(messages)\n",
    "\n",
    "            # Tokenize input\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "\n",
    "            # Track generated text\n",
    "            generated_text = \"\"\n",
    "\n",
    "            # Generate with streaming\n",
    "            with torch.no_grad():\n",
    "                for i in range(max_new_tokens):\n",
    "                    # Check for cancellation\n",
    "                    if stop_event and stop_event.is_set():\n",
    "                        yield generated_text + \" [CANCELLED]\"\n",
    "                        return\n",
    "\n",
    "                    # Generate next token\n",
    "                    outputs = self.model(**inputs)\n",
    "                    next_token_logits = outputs.logits[0, -1, :] / temperature\n",
    "\n",
    "                    # Sample next token\n",
    "                    probs = torch.softmax(next_token_logits, dim=-1)\n",
    "                    next_token = torch.multinomial(probs, 1)\n",
    "\n",
    "                    # Decode token\n",
    "                    token_text = self.tokenizer.decode(\n",
    "                        next_token, skip_special_tokens=True\n",
    "                    )\n",
    "\n",
    "                    # Update generated text\n",
    "                    generated_text += token_text\n",
    "\n",
    "                    # Yield current progress\n",
    "                    yield generated_text\n",
    "\n",
    "                    # Check for EOS\n",
    "                    if next_token.item() == self.tokenizer.eos_token_id:\n",
    "                        break\n",
    "\n",
    "                    # Update inputs for next iteration\n",
    "                    inputs[\"input_ids\"] = torch.cat(\n",
    "                        [inputs[\"input_ids\"], next_token.unsqueeze(0)], dim=-1\n",
    "                    )\n",
    "                    inputs[\"attention_mask\"] = torch.cat(\n",
    "                        [\n",
    "                            inputs[\"attention_mask\"],\n",
    "                            torch.ones((1, 1), device=self.model.device),\n",
    "                        ],\n",
    "                        dim=-1,\n",
    "                    )\n",
    "\n",
    "                    # Add small delay to make streaming visible\n",
    "                    time.sleep(0.05)\n",
    "\n",
    "        except Exception as e:\n",
    "            yield f\"Error during generation: {str(e)}\"\n",
    "\n",
    "    def _messages_to_prompt(self, messages: list) -> str:\n",
    "        \"\"\"Convert messages to simple prompt format\"\"\"\n",
    "        prompt_parts = []\n",
    "        for msg in messages:\n",
    "            role = msg.get(\"role\", \"user\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            if role == \"system\":\n",
    "                prompt_parts.append(f\"System: {content}\")\n",
    "            elif role == \"user\":\n",
    "                prompt_parts.append(f\"User: {content}\")\n",
    "            elif role == \"assistant\":\n",
    "                prompt_parts.append(f\"Assistant: {content}\")\n",
    "\n",
    "        prompt_parts.append(\"Assistant:\")\n",
    "        return \"\\n\".join(prompt_parts)\n",
    "\n",
    "\n",
    "# Initialize adapter with small model for testing\n",
    "llm_adapter = LLMAdapter(\"microsoft/DialoGPT-small\")\n",
    "generation_state = GenerationState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570ed16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Streaming Generator Implementation\n",
    "class StreamingChatbot:\n",
    "    \"\"\"Chatbot with streaming and retry capabilities\"\"\"\n",
    "\n",
    "    def __init__(self, llm_adapter: LLMAdapter):\n",
    "        self.llm_adapter = llm_adapter\n",
    "        self.stop_event = threading.Event()\n",
    "        self.generation_thread = None\n",
    "\n",
    "    def generate_streaming_response(\n",
    "        self, history: list, message: str\n",
    "    ) -> Iterator[tuple]:\n",
    "        \"\"\"Generate streaming response with proper history management\"\"\"\n",
    "        if not message.strip():\n",
    "            yield history, \"\"\n",
    "            return\n",
    "\n",
    "        # Add user message to history\n",
    "        history = history + [[message, \"\"]]\n",
    "\n",
    "        # Prepare messages for LLM\n",
    "        messages = self._history_to_messages(\n",
    "            history[:-1]\n",
    "        )  # Exclude current incomplete exchange\n",
    "        messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "        try:\n",
    "            generation_state.is_generating = True\n",
    "            generation_state.should_stop = False\n",
    "            generation_state.error_message = None\n",
    "\n",
    "            # Stream generation\n",
    "            for partial_response in self.llm_adapter.stream_generate(\n",
    "                messages,\n",
    "                max_new_tokens=200,\n",
    "                temperature=0.7,\n",
    "                stop_event=self.stop_event,\n",
    "            ):\n",
    "                if generation_state.should_stop:\n",
    "                    break\n",
    "\n",
    "                # Update last exchange in history\n",
    "                history[-1][1] = partial_response\n",
    "                yield history, \"\"\n",
    "\n",
    "        except Exception as e:\n",
    "            generation_state.error_message = f\"Generation failed: {str(e)}\"\n",
    "            history[-1][1] = f\"âŒ Error: {str(e)}\"\n",
    "            yield history, \"\"\n",
    "        finally:\n",
    "            generation_state.is_generating = False\n",
    "\n",
    "    def _history_to_messages(self, history: list) -> list:\n",
    "        \"\"\"Convert chat history to messages format\"\"\"\n",
    "        messages = []\n",
    "        for user_msg, assistant_msg in history:\n",
    "            if user_msg:\n",
    "                messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            if assistant_msg:\n",
    "                messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "        return messages\n",
    "\n",
    "    def stop_generation(self):\n",
    "        \"\"\"Stop current generation\"\"\"\n",
    "        generation_state.should_stop = True\n",
    "        self.stop_event.set()\n",
    "\n",
    "    def retry_last_response(self, history: list) -> Iterator[tuple]:\n",
    "        \"\"\"Retry the last failed response\"\"\"\n",
    "        if not history:\n",
    "            return\n",
    "\n",
    "        # Get last user message\n",
    "        last_user_message = history[-1][0] if history else \"\"\n",
    "\n",
    "        # Remove last incomplete response\n",
    "        if history and history[-1][1]:\n",
    "            history = history[:-1]\n",
    "\n",
    "        # Regenerate response\n",
    "        yield from self.generate_streaming_response(history, last_user_message)\n",
    "\n",
    "\n",
    "# Initialize chatbot\n",
    "chatbot = StreamingChatbot(llm_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab03672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Gradio Interface with Event Handling\n",
    "def create_streaming_ui():\n",
    "    \"\"\"Create Gradio interface with streaming support\"\"\"\n",
    "\n",
    "    with gr.Blocks(title=\"Streaming Chat with Retry\", theme=gr.themes.Soft()) as demo:\n",
    "        gr.Markdown(\"## ğŸ¤– ä¸²æµå°è©±æ©Ÿå™¨äºº\")\n",
    "        gr.Markdown(\"æ”¯æ´å³æ™‚ä¸²æµè¼¸å‡ºã€éŒ¯èª¤é‡è©¦èˆ‡å–æ¶ˆåŠŸèƒ½\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=4):\n",
    "                chatbot_ui = gr.Chatbot(\n",
    "                    label=\"å°è©±è¨˜éŒ„\",\n",
    "                    height=400,\n",
    "                    show_copy_button=True,\n",
    "                    bubble_full_width=False,\n",
    "                )\n",
    "\n",
    "                with gr.Row():\n",
    "                    msg_input = gr.Textbox(\n",
    "                        label=\"è¼¸å…¥è¨Šæ¯\",\n",
    "                        placeholder=\"åœ¨æ­¤è¼¸å…¥æ‚¨çš„å•é¡Œ...\",\n",
    "                        scale=4,\n",
    "                        lines=2,\n",
    "                    )\n",
    "\n",
    "                with gr.Row():\n",
    "                    send_btn = gr.Button(\"ç™¼é€\", variant=\"primary\", scale=1)\n",
    "                    stop_btn = gr.Button(\"åœæ­¢\", variant=\"stop\", scale=1)\n",
    "                    retry_btn = gr.Button(\"é‡è©¦\", variant=\"secondary\", scale=1)\n",
    "                    clear_btn = gr.Button(\"æ¸…é™¤\", scale=1)\n",
    "\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\"### ç‹€æ…‹é¢æ¿\")\n",
    "                status_text = gr.Textbox(\n",
    "                    label=\"ç”Ÿæˆç‹€æ…‹\", value=\"å°±ç·’\", interactive=False\n",
    "                )\n",
    "\n",
    "                gr.Markdown(\"### è¨­å®š\")\n",
    "                model_info = gr.Textbox(\n",
    "                    label=\"ç›®å‰æ¨¡å‹\",\n",
    "                    value=\"microsoft/DialoGPT-small\",\n",
    "                    interactive=False,\n",
    "                )\n",
    "\n",
    "                error_display = gr.Textbox(\n",
    "                    label=\"éŒ¯èª¤è¨Šæ¯\", value=\"\", visible=False, interactive=False\n",
    "                )\n",
    "\n",
    "        # Event handlers\n",
    "        def update_status(is_generating: bool, error: str = \"\"):\n",
    "            \"\"\"Update UI status\"\"\"\n",
    "            if error:\n",
    "                return \"âŒ éŒ¯èª¤\", True, error\n",
    "            elif is_generating:\n",
    "                return \"ğŸ”„ ç”Ÿæˆä¸­...\", False, \"\"\n",
    "            else:\n",
    "                return \"âœ… å°±ç·’\", False, \"\"\n",
    "\n",
    "        def send_message(history, message):\n",
    "            \"\"\"Handle send message event\"\"\"\n",
    "            if not message.strip():\n",
    "                return history, \"\", \"è«‹è¼¸å…¥æœ‰æ•ˆè¨Šæ¯\", False, \"\"\n",
    "\n",
    "            # Start streaming generation\n",
    "            return chatbot.generate_streaming_response(history, message)\n",
    "\n",
    "        def stop_generation():\n",
    "            \"\"\"Handle stop generation event\"\"\"\n",
    "            chatbot.stop_generation()\n",
    "            return \"â¹ï¸ å·²åœæ­¢\", False, \"\"\n",
    "\n",
    "        def retry_generation(history):\n",
    "            \"\"\"Handle retry event\"\"\"\n",
    "            return chatbot.retry_last_response(history)\n",
    "\n",
    "        def clear_chat():\n",
    "            \"\"\"Clear chat history\"\"\"\n",
    "            return [], \"\", \"âœ… å·²æ¸…é™¤\", False, \"\"\n",
    "\n",
    "        # Wire up events\n",
    "        msg_input.submit(\n",
    "            send_message,\n",
    "            inputs=[chatbot_ui, msg_input],\n",
    "            outputs=[chatbot_ui, msg_input],\n",
    "        )\n",
    "\n",
    "        send_btn.click(\n",
    "            send_message,\n",
    "            inputs=[chatbot_ui, msg_input],\n",
    "            outputs=[chatbot_ui, msg_input],\n",
    "        )\n",
    "\n",
    "        stop_btn.click(\n",
    "            stop_generation, outputs=[status_text, error_display, error_display]\n",
    "        )\n",
    "\n",
    "        retry_btn.click(retry_generation, inputs=[chatbot_ui], outputs=[chatbot_ui])\n",
    "\n",
    "        clear_btn.click(\n",
    "            clear_chat,\n",
    "            outputs=[chatbot_ui, msg_input, status_text, error_display, error_display],\n",
    "        )\n",
    "\n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eb122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Error Handling and Retry Logic\n",
    "class RetryManager:\n",
    "    \"\"\"Manage retry attempts and error recovery\"\"\"\n",
    "\n",
    "    def __init__(self, max_retries: int = 3):\n",
    "        self.max_retries = max_retries\n",
    "        self.retry_count = 0\n",
    "        self.last_error = None\n",
    "\n",
    "    def attempt_generation(self, func, *args, **kwargs):\n",
    "        \"\"\"Attempt generation with retry logic\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                self.retry_count = attempt\n",
    "                result = func(*args, **kwargs)\n",
    "                self.last_error = None\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                self.last_error = str(e)\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    print(f\"Attempt {attempt + 1} failed: {e}. Retrying...\")\n",
    "                    time.sleep(1)  # Brief delay before retry\n",
    "                else:\n",
    "                    print(f\"All {self.max_retries} attempts failed. Last error: {e}\")\n",
    "                    raise e\n",
    "\n",
    "    def get_retry_status(self) -> dict:\n",
    "        \"\"\"Get current retry status\"\"\"\n",
    "        return {\n",
    "            \"retry_count\": self.retry_count,\n",
    "            \"max_retries\": self.max_retries,\n",
    "            \"last_error\": self.last_error,\n",
    "            \"has_failed\": self.last_error is not None,\n",
    "        }\n",
    "\n",
    "\n",
    "# Enhanced chatbot with retry management\n",
    "class RobustStreamingChatbot(StreamingChatbot):\n",
    "    \"\"\"Enhanced chatbot with robust error handling\"\"\"\n",
    "\n",
    "    def __init__(self, llm_adapter: LLMAdapter, max_retries: int = 2):\n",
    "        super().__init__(llm_adapter)\n",
    "        self.retry_manager = RetryManager(max_retries)\n",
    "\n",
    "    def generate_with_retry(self, history: list, message: str) -> Iterator[tuple]:\n",
    "        \"\"\"Generate response with automatic retry on failure\"\"\"\n",
    "        try:\n",
    "            yield from self.generate_streaming_response(history, message)\n",
    "        except Exception as e:\n",
    "            # Add error message to history\n",
    "            error_history = history + [\n",
    "                [message, f\"âŒ ç”Ÿæˆå¤±æ•—: {str(e)}\\nğŸ’¡ é»æ“Šã€Œé‡è©¦ã€æŒ‰éˆ•é‡æ–°ç”Ÿæˆ\"]\n",
    "            ]\n",
    "            yield error_history, \"\"\n",
    "\n",
    "            # Update error state\n",
    "            generation_state.error_message = str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c57e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Cancellation Mechanism\n",
    "class CancellableGenerator:\n",
    "    \"\"\"Generator wrapper with cancellation support\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cancelled = False\n",
    "        self.cancel_event = threading.Event()\n",
    "\n",
    "    def cancel(self):\n",
    "        \"\"\"Cancel current operation\"\"\"\n",
    "        self.cancelled = True\n",
    "        self.cancel_event.set()\n",
    "\n",
    "    def is_cancelled(self) -> bool:\n",
    "        \"\"\"Check if operation was cancelled\"\"\"\n",
    "        return self.cancelled or self.cancel_event.is_set()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset cancellation state\"\"\"\n",
    "        self.cancelled = False\n",
    "        self.cancel_event.clear()\n",
    "\n",
    "\n",
    "# Global cancellation manager\n",
    "cancel_manager = CancellableGenerator()\n",
    "\n",
    "\n",
    "def create_enhanced_ui():\n",
    "    \"\"\"Create enhanced UI with proper cancellation\"\"\"\n",
    "\n",
    "    enhanced_chatbot = RobustStreamingChatbot(llm_adapter)\n",
    "\n",
    "    with gr.Blocks(title=\"Enhanced Streaming Chat\") as demo:\n",
    "        gr.Markdown(\"# ğŸš€ å¢å¼·ç‰ˆä¸²æµå°è©±\")\n",
    "\n",
    "        chatbot_ui = gr.Chatbot(height=450, show_copy_button=True)\n",
    "\n",
    "        with gr.Row():\n",
    "            msg_input = gr.Textbox(\n",
    "                placeholder=\"è¼¸å…¥æ‚¨çš„å•é¡Œ (æ”¯æ´ä¸­æ–‡)...\", scale=4, lines=3\n",
    "            )\n",
    "\n",
    "        with gr.Row():\n",
    "            send_btn = gr.Button(\"ğŸ“¤ ç™¼é€\", variant=\"primary\")\n",
    "            stop_btn = gr.Button(\"â¹ï¸ åœæ­¢\", variant=\"stop\")\n",
    "            retry_btn = gr.Button(\"ğŸ”„ é‡è©¦\", variant=\"secondary\")\n",
    "            clear_btn = gr.Button(\"ğŸ—‘ï¸ æ¸…é™¤\")\n",
    "\n",
    "        with gr.Row():\n",
    "            status_display = gr.Textbox(\n",
    "                label=\"ç‹€æ…‹\", value=\"âœ… å°±ç·’\", interactive=False, scale=2\n",
    "            )\n",
    "            retry_info = gr.Textbox(\n",
    "                label=\"é‡è©¦è³‡è¨Š\", value=\"å°šæœªé‡è©¦\", interactive=False, scale=2\n",
    "            )\n",
    "\n",
    "        # Enhanced event handlers with proper state management\n",
    "        def enhanced_send(history, message):\n",
    "            \"\"\"Enhanced send with cancellation support\"\"\"\n",
    "            if not message.strip():\n",
    "                return history, \"\"\n",
    "\n",
    "            cancel_manager.reset()\n",
    "            return enhanced_chatbot.generate_with_retry(history, message)\n",
    "\n",
    "        def enhanced_stop():\n",
    "            \"\"\"Enhanced stop with immediate feedback\"\"\"\n",
    "            cancel_manager.cancel()\n",
    "            enhanced_chatbot.stop_generation()\n",
    "            return \"â¹ï¸ ç”Ÿæˆå·²åœæ­¢\", \"åœæ­¢æ–¼: \" + time.strftime(\"%H:%M:%S\")\n",
    "\n",
    "        def enhanced_retry(history):\n",
    "            \"\"\"Enhanced retry with status tracking\"\"\"\n",
    "            if not history:\n",
    "                return history\n",
    "\n",
    "            cancel_manager.reset()\n",
    "            status = enhanced_chatbot.retry_manager.get_retry_status()\n",
    "            retry_text = f\"é‡è©¦æ¬¡æ•¸: {status['retry_count']}/{status['max_retries']}\"\n",
    "\n",
    "            return (\n",
    "                enhanced_chatbot.retry_last_response(history),\n",
    "                \"ğŸ”„ é‡è©¦ä¸­...\",\n",
    "                retry_text,\n",
    "            )\n",
    "\n",
    "        def enhanced_clear():\n",
    "            \"\"\"Enhanced clear with confirmation\"\"\"\n",
    "            cancel_manager.cancel()\n",
    "            return [], \"\", \"âœ… å°è©±å·²æ¸…é™¤\", \"å·²é‡ç½®\"\n",
    "\n",
    "        # Wire events\n",
    "        send_btn.click(enhanced_send, [chatbot_ui, msg_input], [chatbot_ui, msg_input])\n",
    "        stop_btn.click(enhanced_stop, outputs=[status_display, retry_info])\n",
    "        retry_btn.click(\n",
    "            enhanced_retry, [chatbot_ui], [chatbot_ui, status_display, retry_info]\n",
    "        )\n",
    "        clear_btn.click(\n",
    "            enhanced_clear, outputs=[chatbot_ui, msg_input, status_display, retry_info]\n",
    "        )\n",
    "\n",
    "        msg_input.submit(\n",
    "            enhanced_send, [chatbot_ui, msg_input], [chatbot_ui, msg_input]\n",
    "        )\n",
    "\n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96996b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Complete UI Integration Test\n",
    "def run_comprehensive_test():\n",
    "    \"\"\"Test all streaming and retry features\"\"\"\n",
    "    print(\"=== ä¸²æµèˆ‡é‡è©¦åŠŸèƒ½ç¶œåˆæ¸¬è©¦ ===\")\n",
    "\n",
    "    # Test 1: Basic streaming\n",
    "    print(\"\\n1. æ¸¬è©¦åŸºæœ¬ä¸²æµç”Ÿæˆ...\")\n",
    "    test_messages = [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n",
    "\n",
    "    response_parts = []\n",
    "    for partial in llm_adapter.stream_generate(test_messages, max_new_tokens=50):\n",
    "        response_parts.append(partial)\n",
    "        if len(response_parts) % 5 == 0:  # Show progress every 5 tokens\n",
    "            print(f\"   éƒ¨åˆ†å›æ‡‰: {partial[:50]}...\")\n",
    "\n",
    "    print(f\"   âœ… å®Œæ•´å›æ‡‰: {response_parts[-1] if response_parts else 'No response'}\")\n",
    "\n",
    "    # Test 2: Cancellation\n",
    "    print(\"\\n2. æ¸¬è©¦å–æ¶ˆæ©Ÿåˆ¶...\")\n",
    "    stop_event = threading.Event()\n",
    "\n",
    "    def delayed_cancel():\n",
    "        time.sleep(0.2)  # Cancel after 200ms\n",
    "        stop_event.set()\n",
    "        print(\"   â¹ï¸ å·²ç™¼é€å–æ¶ˆä¿¡è™Ÿ\")\n",
    "\n",
    "    cancel_thread = threading.Thread(target=delayed_cancel)\n",
    "    cancel_thread.start()\n",
    "\n",
    "    cancelled_parts = []\n",
    "    for partial in llm_adapter.stream_generate(\n",
    "        test_messages, max_new_tokens=100, stop_event=stop_event\n",
    "    ):\n",
    "        cancelled_parts.append(partial)\n",
    "\n",
    "    cancel_thread.join()\n",
    "    final_response = cancelled_parts[-1] if cancelled_parts else \"\"\n",
    "    is_cancelled = \"[CANCELLED]\" in final_response\n",
    "    print(f\"   âœ… å–æ¶ˆæ¸¬è©¦: {'æˆåŠŸ' if is_cancelled else 'å¤±æ•—'}\")\n",
    "\n",
    "    # Test 3: Error handling\n",
    "    print(\"\\n3. æ¸¬è©¦éŒ¯èª¤è™•ç†...\")\n",
    "    try:\n",
    "        # Simulate error by passing invalid input\n",
    "        error_messages = [{\"role\": \"invalid\", \"content\": \"\"}]\n",
    "        list(llm_adapter.stream_generate(error_messages, max_new_tokens=10))\n",
    "        print(\"   âš ï¸ é æœŸéŒ¯èª¤æœªç™¼ç”Ÿ\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ… éŒ¯èª¤è™•ç†æˆåŠŸ: {type(e).__name__}\")\n",
    "\n",
    "    # Test 4: Retry manager\n",
    "    print(\"\\n4. æ¸¬è©¦é‡è©¦ç®¡ç†å™¨...\")\n",
    "    retry_mgr = RetryManager(max_retries=2)\n",
    "\n",
    "    def failing_function():\n",
    "        if retry_mgr.retry_count < 1:  # Fail on first attempt\n",
    "            raise ValueError(\"æ¨¡æ“¬å¤±æ•—\")\n",
    "        return \"æˆåŠŸ\"\n",
    "\n",
    "    try:\n",
    "        result = retry_mgr.attempt_generation(failing_function)\n",
    "        print(f\"   âœ… é‡è©¦æˆåŠŸ: {result}\")\n",
    "    except:\n",
    "        print(\"   âŒ é‡è©¦å¤±æ•—\")\n",
    "\n",
    "    status = retry_mgr.get_retry_status()\n",
    "    print(f\"   ğŸ“Š é‡è©¦ç‹€æ…‹: {status['retry_count']}/{status['max_retries']} æ¬¡å˜—è©¦\")\n",
    "\n",
    "    print(\"\\n=== æ¸¬è©¦å®Œæˆ ===\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# Run comprehensive test\n",
    "test_result = run_comprehensive_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8917b81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Smoke Test - Launch UI\n",
    "def smoke_test():\n",
    "    \"\"\"Quick smoke test to verify everything works\"\"\"\n",
    "    print(\"ğŸ§ª Smoke Test: Gradio ä¸²æµä»‹é¢\")\n",
    "\n",
    "    try:\n",
    "        # Create and launch demo\n",
    "        demo = create_enhanced_ui()\n",
    "        print(\"âœ… UI å»ºç«‹æˆåŠŸ\")\n",
    "\n",
    "        # Test basic functionality without launching server\n",
    "        print(\"âœ… æ‰€æœ‰çµ„ä»¶åˆå§‹åŒ–å®Œæˆ\")\n",
    "        print(\"âœ… äº‹ä»¶è™•ç†å™¨ç¶å®šæˆåŠŸ\")\n",
    "        print(\"âœ… éŒ¯èª¤è™•ç†æ©Ÿåˆ¶å°±ç·’\")\n",
    "\n",
    "        print(\"\\nğŸš€ å•Ÿå‹• Gradio ç•Œé¢...\")\n",
    "        print(\"ğŸ“ åŠŸèƒ½æ¸¬è©¦æ¸…å–®:\")\n",
    "        print(\"   - è¼¸å…¥è¨Šæ¯ä¸¦è§€å¯Ÿä¸²æµè¼¸å‡º\")\n",
    "        print(\"   - é»æ“Šã€Œåœæ­¢ã€æŒ‰éˆ•æ¸¬è©¦å–æ¶ˆåŠŸèƒ½\")\n",
    "        print(\"   - é»æ“Šã€Œé‡è©¦ã€æŒ‰éˆ•æ¸¬è©¦é‡æ–°ç”Ÿæˆ\")\n",
    "        print(\"   - é»æ“Šã€Œæ¸…é™¤ã€æŒ‰éˆ•æ¸…ç©ºå°è©±\")\n",
    "\n",
    "        # Launch with specific config for testing\n",
    "        demo.launch(\n",
    "            server_name=\"0.0.0.0\",\n",
    "            server_port=7861,\n",
    "            share=False,\n",
    "            debug=True,\n",
    "            show_error=True,\n",
    "            quiet=False,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Smoke test å¤±æ•—: {str(e)}\")\n",
    "        print(f\"éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "print(\"é–‹å§‹ Smoke Test...\")\n",
    "smoke_result = smoke_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b2b2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½ VRAM å„ªåŒ–é¸é …\n",
    "model_config:\n",
    "  device_map: \"auto\"           # è‡ªå‹•è¨­å‚™åˆ†é…\n",
    "  torch_dtype: \"auto\"          # è‡ªå‹•ç²¾åº¦é¸æ“‡\n",
    "  low_cpu_mem_usage: true      # æ¸›å°‘ CPU è¨˜æ†¶é«”ä½¿ç”¨\n",
    "\n",
    "# ä¸²æµç”Ÿæˆåƒæ•¸\n",
    "streaming_config:\n",
    "  max_new_tokens: 200          # é™åˆ¶ç”Ÿæˆé•·åº¦\n",
    "  temperature: 0.7             # æ§åˆ¶å‰µé€ æ€§\n",
    "  stream_delay: 0.05           # ä¸²æµå»¶é² (ç§’)\n",
    "\n",
    "# é‡è©¦æ©Ÿåˆ¶è¨­å®š\n",
    "retry_config:\n",
    "  max_retries: 2               # æœ€å¤§é‡è©¦æ¬¡æ•¸\n",
    "  retry_delay: 1.0             # é‡è©¦é–“éš” (ç§’)\n",
    "\n",
    "# UI è¨­å®š\n",
    "ui_config:\n",
    "  chatbot_height: 450          # å°è©±å€é«˜åº¦\n",
    "  show_copy_button: true       # é¡¯ç¤ºè¤‡è£½æŒ‰éˆ•\n",
    "  bubble_full_width: false     # æ³¡æ³¡æ¡†å¯¬åº¦é™åˆ¶"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
