{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc97c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (Ë§áË£ΩÂà∞ÊØèÊú¨ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e4eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import & Dependencies\n",
    "import gradio as gr\n",
    "import json\n",
    "import time\n",
    "from typing import Optional, Dict, Any, List, Tuple\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath(\"../..\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "try:\n",
    "    from shared_utils.adapters.llm_adapter import LLMAdapter\n",
    "\n",
    "    print(\"‚úì LLMAdapter imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö† LLMAdapter import failed: {e}\")\n",
    "    print(\"Creating minimal fallback...\")\n",
    "\n",
    "    # Minimal fallback for demo\n",
    "    class LLMAdapter:\n",
    "        def __init__(self, model_id=\"demo\", backend=\"transformers\", **kwargs):\n",
    "            self.model_id = model_id\n",
    "            self.backend = backend\n",
    "            self.loaded = False\n",
    "\n",
    "        def generate(self, messages, max_new_tokens=256, temperature=0.7, **kwargs):\n",
    "            # Simple echo for demo\n",
    "            user_msg = messages[-1].get(\"content\", \"\") if messages else \"\"\n",
    "            return f\"[{self.backend}] Echo: {user_msg[:100]}...\"\n",
    "\n",
    "        def load_model(self):\n",
    "            self.loaded = True\n",
    "            return True\n",
    "\n",
    "\n",
    "# Default models for different backends\n",
    "DEFAULT_MODELS = {\n",
    "    \"transformers\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    \"llamacpp\": \"qwen2.5-7b-instruct-q4_k_m.gguf\",  # local file\n",
    "    \"ollama\": \"qwen2.5:7b\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e795fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: UI State Management\n",
    "class UIState:\n",
    "    def __init__(self):\n",
    "        self.current_adapter: Optional[LLMAdapter] = None\n",
    "        self.chat_history: List[List[str]] = []\n",
    "        self.mode = \"Chat\"  # Chat|RAG|Agents|Game\n",
    "        self.rag_enabled = False\n",
    "        self.tools_enabled = False\n",
    "\n",
    "    def clear_chat(self):\n",
    "        self.chat_history = []\n",
    "        return []\n",
    "\n",
    "\n",
    "# Global state\n",
    "ui_state = UIState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b314af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Model Loading Functions\n",
    "def load_model(\n",
    "    model_id: str, backend: str, quantization: str = \"fp16\", device_map: str = \"auto\"\n",
    ") -> Tuple[bool, str]:\n",
    "    \"\"\"Load model with specified backend and quantization\"\"\"\n",
    "    try:\n",
    "        # Quantization settings for low-VRAM\n",
    "        load_kwargs = {\"device_map\": device_map}\n",
    "\n",
    "        if backend == \"transformers\":\n",
    "            if quantization == \"int4\":\n",
    "                load_kwargs.update(\n",
    "                    {\n",
    "                        \"load_in_4bit\": True,\n",
    "                        \"bnb_4bit_compute_dtype\": torch.float16,\n",
    "                        \"bnb_4bit_use_double_quant\": True,\n",
    "                    }\n",
    "                )\n",
    "            elif quantization == \"int8\":\n",
    "                load_kwargs[\"load_in_8bit\"] = True\n",
    "            elif quantization == \"fp16\":\n",
    "                load_kwargs[\"torch_dtype\"] = torch.float16\n",
    "\n",
    "        # Create adapter\n",
    "        adapter = LLMAdapter(model_id=model_id, backend=backend, **load_kwargs)\n",
    "\n",
    "        # Test load\n",
    "        if hasattr(adapter, \"load_model\"):\n",
    "            success = adapter.load_model()\n",
    "        else:\n",
    "            success = True\n",
    "\n",
    "        if success:\n",
    "            ui_state.current_adapter = adapter\n",
    "            return True, f\"‚úì Loaded {model_id} ({backend}, {quantization})\"\n",
    "        else:\n",
    "            return False, f\"‚úó Failed to load {model_id}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return False, f\"‚úó Error: {str(e)[:100]}\"\n",
    "\n",
    "\n",
    "def get_model_info() -> str:\n",
    "    \"\"\"Get current model information\"\"\"\n",
    "    if ui_state.current_adapter:\n",
    "        return f\"Model: {ui_state.current_adapter.model_id} | Backend: {ui_state.current_adapter.backend}\"\n",
    "    return \"No model loaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470a1c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Chat Function (MVP)\n",
    "def chat_fn(\n",
    "    message: str,\n",
    "    history: List[List[str]],\n",
    "    mode: str,\n",
    "    rag_enabled: bool,\n",
    "    tools_enabled: bool,\n",
    ") -> Tuple[List[List[str]], str]:\n",
    "    \"\"\"Main chat function supporting different modes\"\"\"\n",
    "\n",
    "    if not message.strip():\n",
    "        return history, \"\"\n",
    "\n",
    "    if not ui_state.current_adapter:\n",
    "        return (\n",
    "            history + [[message, \"‚ö† No model loaded. Please load a model first.\"]],\n",
    "            \"\",\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        # Prepare messages in standard format\n",
    "        messages = []\n",
    "\n",
    "        # Add system message based on mode\n",
    "        if mode == \"Chat\":\n",
    "            system_msg = \"You are a helpful AI assistant. Respond in Traditional Chinese when appropriate.\"\n",
    "        elif mode == \"RAG\":\n",
    "            system_msg = \"You are a RAG-enabled assistant. Use provided context to answer questions with citations.\"\n",
    "        elif mode == \"Agents\":\n",
    "            system_msg = \"You are part of a multi-agent system. Collaborate to solve complex tasks.\"\n",
    "        elif mode == \"Game\":\n",
    "            system_msg = (\n",
    "                \"You are a text adventure game master. Create engaging narratives.\"\n",
    "            )\n",
    "        else:\n",
    "            system_msg = \"You are a helpful assistant.\"\n",
    "\n",
    "        messages.append({\"role\": \"system\", \"content\": system_msg})\n",
    "\n",
    "        # Add conversation history (last 5 exchanges to save tokens)\n",
    "        for h in history[-5:]:\n",
    "            if len(h) >= 2:\n",
    "                messages.append({\"role\": \"user\", \"content\": h[0]})\n",
    "                messages.append({\"role\": \"assistant\", \"content\": h[1]})\n",
    "\n",
    "        # Add current message\n",
    "        messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "        # Generate response\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Mode-specific modifications\n",
    "        generation_kwargs = {\"max_new_tokens\": 512, \"temperature\": 0.7}\n",
    "\n",
    "        if mode == \"RAG\" and rag_enabled:\n",
    "            # TODO: Integrate RAG retrieval here\n",
    "            generation_kwargs[\"temperature\"] = 0.3  # More focused for factual responses\n",
    "\n",
    "        if mode == \"Agents\" and tools_enabled:\n",
    "            # TODO: Add tool calling logic\n",
    "            generation_kwargs[\"max_new_tokens\"] = 256  # Shorter for tool planning\n",
    "\n",
    "        response = ui_state.current_adapter.generate(messages, **generation_kwargs)\n",
    "\n",
    "        # Extract just the assistant's response (remove the full conversation)\n",
    "        if isinstance(response, str):\n",
    "            # Simple extraction - get content after last \"assistant:\"\n",
    "            if \"assistant:\" in response.lower():\n",
    "                response = response.split(\"assistant:\")[-1].strip()\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        # Add timing info\n",
    "        timing_info = f\"\\n\\n*[Generated in {elapsed:.1f}s]*\"\n",
    "        response_with_timing = response + timing_info\n",
    "\n",
    "        # Update history\n",
    "        new_history = history + [[message, response_with_timing]]\n",
    "\n",
    "        return new_history, \"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"‚ö† Generation error: {str(e)[:200]}\"\n",
    "        return history + [[message, error_msg]], \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09de3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Control Panel Callbacks\n",
    "def on_model_change(model_id: str, backend: str, quantization: str) -> str:\n",
    "    \"\"\"Handle model selection change\"\"\"\n",
    "    if not model_id.strip():\n",
    "        return \"Please enter a model ID\"\n",
    "\n",
    "    success, message = load_model(model_id, backend, quantization)\n",
    "    return message\n",
    "\n",
    "\n",
    "def on_mode_change(mode: str) -> Dict[str, Any]:\n",
    "    \"\"\"Handle mode change\"\"\"\n",
    "    ui_state.mode = mode\n",
    "\n",
    "    # Mode-specific UI updates\n",
    "    updates = {}\n",
    "\n",
    "    if mode == \"RAG\":\n",
    "        updates[\"rag_enabled\"] = gr.update(visible=True)\n",
    "        updates[\"tools_enabled\"] = gr.update(visible=False)\n",
    "    elif mode == \"Agents\":\n",
    "        updates[\"rag_enabled\"] = gr.update(visible=True)\n",
    "        updates[\"tools_enabled\"] = gr.update(visible=True)\n",
    "    elif mode == \"Game\":\n",
    "        updates[\"rag_enabled\"] = gr.update(visible=True)\n",
    "        updates[\"tools_enabled\"] = gr.update(visible=False)\n",
    "    else:  # Chat\n",
    "        updates[\"rag_enabled\"] = gr.update(visible=False)\n",
    "        updates[\"tools_enabled\"] = gr.update(visible=False)\n",
    "\n",
    "    return updates\n",
    "\n",
    "\n",
    "def clear_chat_history() -> List[List[str]]:\n",
    "    \"\"\"Clear chat history\"\"\"\n",
    "    return ui_state.clear_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fbacb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Gradio Interface Assembly\n",
    "def create_gradio_interface():\n",
    "    \"\"\"Create the main Gradio interface\"\"\"\n",
    "\n",
    "    with gr.Blocks(title=\"ragent-text-lab\", theme=gr.themes.Soft()) as demo:\n",
    "        gr.Markdown(\"# ü§ñ ragent-text-lab | Multi-Mode AI Assistant\")\n",
    "\n",
    "        with gr.Row():\n",
    "            # Left Column: Chat Interface\n",
    "            with gr.Column(scale=2):\n",
    "                gr.Markdown(\"## üí¨ Chat Interface\")\n",
    "\n",
    "                chatbot = gr.Chatbot(\n",
    "                    value=[], height=400, show_copy_button=True, bubble_full_width=False\n",
    "                )\n",
    "\n",
    "                with gr.Row():\n",
    "                    msg_input = gr.Textbox(\n",
    "                        placeholder=\"Ëº∏ÂÖ•ÊÇ®ÁöÑÂïèÈ°å...\",\n",
    "                        scale=4,\n",
    "                        show_label=False,\n",
    "                        container=False,\n",
    "                    )\n",
    "                    send_btn = gr.Button(\"ÁôºÈÄÅ\", variant=\"primary\")\n",
    "                    clear_btn = gr.Button(\"Ê∏ÖÈô§\", variant=\"secondary\")\n",
    "\n",
    "            # Right Column: Control Panel\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\"## ‚öôÔ∏è Control Panel\")\n",
    "\n",
    "                # Model Selection\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\"### üéØ Model Selection\")\n",
    "\n",
    "                    model_id = gr.Textbox(\n",
    "                        value=DEFAULT_MODELS[\"transformers\"],\n",
    "                        label=\"Model ID\",\n",
    "                        placeholder=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "                    )\n",
    "\n",
    "                    backend = gr.Dropdown(\n",
    "                        choices=[\"transformers\", \"llamacpp\", \"ollama\"],\n",
    "                        value=\"transformers\",\n",
    "                        label=\"Backend\",\n",
    "                    )\n",
    "\n",
    "                    quantization = gr.Dropdown(\n",
    "                        choices=[\"fp16\", \"int8\", \"int4\"],\n",
    "                        value=\"int4\",  # Default to int4 for low VRAM\n",
    "                        label=\"Quantization\",\n",
    "                    )\n",
    "\n",
    "                    load_model_btn = gr.Button(\"Load Model\", variant=\"primary\")\n",
    "                    model_status = gr.Textbox(\n",
    "                        value=\"No model loaded\", label=\"Status\", interactive=False\n",
    "                    )\n",
    "\n",
    "                # Mode Selection\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\"### üéÆ Mode Selection\")\n",
    "\n",
    "                    mode = gr.Radio(\n",
    "                        choices=[\"Chat\", \"RAG\", \"Agents\", \"Game\"],\n",
    "                        value=\"Chat\",\n",
    "                        label=\"Mode\",\n",
    "                    )\n",
    "\n",
    "                    # Conditional controls\n",
    "                    rag_enabled = gr.Checkbox(\n",
    "                        label=\"Enable RAG\", value=False, visible=False\n",
    "                    )\n",
    "\n",
    "                    tools_enabled = gr.Checkbox(\n",
    "                        label=\"Enable Tools\", value=False, visible=False\n",
    "                    )\n",
    "\n",
    "                # Advanced Settings\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\"### üîß Advanced Settings\")\n",
    "\n",
    "                    max_tokens = gr.Slider(\n",
    "                        minimum=64, maximum=2048, value=512, step=64, label=\"Max Tokens\"\n",
    "                    )\n",
    "\n",
    "                    temperature = gr.Slider(\n",
    "                        minimum=0.1,\n",
    "                        maximum=1.0,\n",
    "                        value=0.7,\n",
    "                        step=0.1,\n",
    "                        label=\"Temperature\",\n",
    "                    )\n",
    "\n",
    "                # Info Panel\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\"### üìä System Info\")\n",
    "\n",
    "                    system_info = gr.Textbox(\n",
    "                        value=get_model_info(), label=\"Current Model\", interactive=False\n",
    "                    )\n",
    "\n",
    "        # Event Handlers\n",
    "\n",
    "        # Model loading\n",
    "        load_model_btn.click(\n",
    "            fn=on_model_change,\n",
    "            inputs=[model_id, backend, quantization],\n",
    "            outputs=[model_status],\n",
    "        ).then(fn=get_model_info, outputs=[system_info])\n",
    "\n",
    "        # Auto-fill model ID when backend changes\n",
    "        def update_model_id(backend_choice):\n",
    "            return DEFAULT_MODELS.get(backend_choice, \"\")\n",
    "\n",
    "        backend.change(fn=update_model_id, inputs=[backend], outputs=[model_id])\n",
    "\n",
    "        # Mode change\n",
    "        mode.change(\n",
    "            fn=on_mode_change, inputs=[mode], outputs=[rag_enabled, tools_enabled]\n",
    "        )\n",
    "\n",
    "        # Chat functionality\n",
    "        def handle_send(message, history, current_mode, rag_on, tools_on):\n",
    "            return chat_fn(message, history, current_mode, rag_on, tools_on)\n",
    "\n",
    "        # Send message\n",
    "        send_btn.click(\n",
    "            fn=handle_send,\n",
    "            inputs=[msg_input, chatbot, mode, rag_enabled, tools_enabled],\n",
    "            outputs=[chatbot, msg_input],\n",
    "        )\n",
    "\n",
    "        # Enter key\n",
    "        msg_input.submit(\n",
    "            fn=handle_send,\n",
    "            inputs=[msg_input, chatbot, mode, rag_enabled, tools_enabled],\n",
    "            outputs=[chatbot, msg_input],\n",
    "        )\n",
    "\n",
    "        # Clear chat\n",
    "        clear_btn.click(fn=clear_chat_history, outputs=[chatbot])\n",
    "\n",
    "    return demo\n",
    "\n",
    "\n",
    "# Create interface\n",
    "demo = create_gradio_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3977de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Smoke Test\n",
    "print(\"üß™ Starting Gradio interface smoke test...\")\n",
    "\n",
    "# Test model loading function\n",
    "print(\"\\n1. Testing model loading...\")\n",
    "try:\n",
    "    success, msg = load_model(\"demo-model\", \"transformers\", \"fp16\")\n",
    "    print(f\"   Load test: {msg}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Load test failed: {e}\")\n",
    "\n",
    "# Test chat function\n",
    "print(\"\\n2. Testing chat function...\")\n",
    "try:\n",
    "    test_history = []\n",
    "    new_history, _ = chat_fn(\"Hello\", test_history, \"Chat\", False, False)\n",
    "    print(f\"   Chat test: Generated {len(new_history)} exchanges\")\n",
    "except Exception as e:\n",
    "    print(f\"   Chat test failed: {e}\")\n",
    "\n",
    "print(\"\\n3. Interface ready! Run demo.launch() to start.\")\n",
    "print(\"   - Left panel: Chat interface with history\")\n",
    "print(\"   - Right panel: Model/mode controls\")\n",
    "print(\"   - Default: int4 quantization for low VRAM\")\n",
    "\n",
    "# To launch (uncomment when ready):\n",
    "# demo.launch(\n",
    "#     server_name=\"0.0.0.0\",\n",
    "#     server_port=7860,\n",
    "#     share=False,\n",
    "#     inbrowser=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb5c983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Key Parameters & Low-VRAM Configuration\n",
    "print(\"\\nüìã Key Parameters for Low-VRAM Setup:\")\n",
    "print(\n",
    "    \"\"\"\n",
    "üîß Quantization Options:\n",
    "- int4: ~2-3GB VRAM (recommend for 8GB cards)\n",
    "- int8: ~4-5GB VRAM\n",
    "- fp16: ~6-8GB VRAM\n",
    "\n",
    "üéØ Model Recommendations by VRAM:\n",
    "- 8GB:  Qwen2.5-7B (int4) or smaller models\n",
    "- 12GB: Qwen2.5-7B (int8/fp16)\n",
    "- 16GB+: Qwen2.5-14B (int4/int8)\n",
    "\n",
    "‚öôÔ∏è Backend Trade-offs:\n",
    "- transformers: Full features, higher VRAM\n",
    "- llamacpp: CPU/GPU hybrid, GGUF quantization\n",
    "- ollama: Easy management, good for inference\n",
    "\n",
    "üöÄ Next Extensions:\n",
    "- Streaming responses (nb51)\n",
    "- RAG integration from Stage 2\n",
    "- Agent orchestrator from Stage 4\n",
    "- File upload & indexing (nb53)\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106bd720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test of the interface\n",
    "print(\"üß™ Interface Components Test:\")\n",
    "print(\"‚úì Gradio layout created\")\n",
    "print(\"‚úì Model loading functions defined\")\n",
    "print(\"‚úì Chat pipeline ready\")\n",
    "print(\"‚úì Mode switching prepared\")\n",
    "print(\"‚úì Low-VRAM defaults set (int4)\")\n",
    "\n",
    "# Test the actual launch (comment out for notebook)\n",
    "# demo.launch(debug=True, share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
