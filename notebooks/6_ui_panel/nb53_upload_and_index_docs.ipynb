{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6734cf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (複製到每本 notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef1ec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Dependencies and Imports\n",
    "import gradio as gr\n",
    "import tempfile\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "# RAG dependencies\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import trafilatura\n",
    "import PyPDF2\n",
    "import re\n",
    "import opencc\n",
    "\n",
    "print(\"✓ Dependencies loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4573673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Document Processing Utils\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Process multiple document formats for RAG indexing\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cc = opencc.OpenCC(\"t2s\")  # Traditional to Simplified Chinese\n",
    "        self.splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\n",
    "                \"\\n### \",\n",
    "                \"\\n## \",\n",
    "                \"\\n# \",\n",
    "                \"。\",\n",
    "                \"！\",\n",
    "                \"？\",\n",
    "                \"；\",\n",
    "                \"…\",\n",
    "                \"\\n\\n\",\n",
    "                \"\\n\",\n",
    "                \" \",\n",
    "            ],\n",
    "            chunk_size=800,\n",
    "            chunk_overlap=80,\n",
    "        )\n",
    "\n",
    "    def extract_text(self, file_path: str, file_type: str) -> str:\n",
    "        \"\"\"Extract text from different file formats\"\"\"\n",
    "        try:\n",
    "            if file_type == \"txt\":\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    return f.read()\n",
    "\n",
    "            elif file_type == \"md\":\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    return f.read()\n",
    "\n",
    "            elif file_type == \"html\":\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    html_content = f.read()\n",
    "                return trafilatura.extract(html_content) or \"\"\n",
    "\n",
    "            elif file_type == \"pdf\":\n",
    "                text = \"\"\n",
    "                with open(file_path, \"rb\") as f:\n",
    "                    reader = PyPDF2.PdfReader(f)\n",
    "                    for page in reader.pages:\n",
    "                        text += page.extract_text() + \"\\n\"\n",
    "                return text\n",
    "\n",
    "            else:\n",
    "                return f\"Unsupported format: {file_type}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error processing {file_type}: {str(e)}\"\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize Chinese text\"\"\"\n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        # Convert traditional to simplified (optional)\n",
    "        # text = self.cc.convert(text)\n",
    "\n",
    "        # Remove special characters but keep Chinese punctuation\n",
    "        text = re.sub(\n",
    "            r'[^\\w\\s\\u4e00-\\u9fff。！？；，、\"\"' \"（）【】《》〈〉]\", \"\", text\n",
    "        )\n",
    "\n",
    "        return text\n",
    "\n",
    "    def chunk_document(self, text: str, filename: str) -> List[dict]:\n",
    "        \"\"\"Split document into chunks with metadata\"\"\"\n",
    "        clean_text = self.clean_text(text)\n",
    "\n",
    "        if len(clean_text) < 50:  # Skip very short documents\n",
    "            return []\n",
    "\n",
    "        chunks = self.splitter.create_documents([clean_text])\n",
    "\n",
    "        result = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            result.append(\n",
    "                {\n",
    "                    \"id\": f\"{filename}_{i}\",\n",
    "                    \"text\": chunk.page_content,\n",
    "                    \"meta\": {\n",
    "                        \"source\": filename,\n",
    "                        \"chunk_id\": i,\n",
    "                        \"length\": len(chunk.page_content),\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d25acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: RAG Index Builder\n",
    "class RAGIndexBuilder:\n",
    "    \"\"\"Build and manage FAISS index for uploaded documents\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model: str = \"BAAI/bge-m3\"):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.encoder = None\n",
    "        self.index = None\n",
    "        self.chunks = []\n",
    "        self.index_built = False\n",
    "\n",
    "    def load_encoder(self):\n",
    "        \"\"\"Load embedding model (lazy loading)\"\"\"\n",
    "        if self.encoder is None:\n",
    "            print(f\"Loading embedding model: {self.embedding_model}\")\n",
    "            self.encoder = SentenceTransformer(self.embedding_model)\n",
    "        return self.encoder\n",
    "\n",
    "    def build_index(self, all_chunks: List[dict]) -> Tuple[bool, str]:\n",
    "        \"\"\"Build FAISS index from document chunks\"\"\"\n",
    "        try:\n",
    "            if not all_chunks:\n",
    "                return False, \"No chunks to index\"\n",
    "\n",
    "            # Load model\n",
    "            encoder = self.load_encoder()\n",
    "\n",
    "            # Extract texts and encode\n",
    "            texts = [chunk[\"text\"] for chunk in all_chunks]\n",
    "            print(f\"Encoding {len(texts)} chunks...\")\n",
    "\n",
    "            embeddings = encoder.encode(\n",
    "                texts, normalize_embeddings=True, batch_size=32, show_progress_bar=True\n",
    "            ).astype(\"float32\")\n",
    "\n",
    "            # Build FAISS index\n",
    "            dim = embeddings.shape[1]\n",
    "            self.index = faiss.IndexFlatIP(dim)  # Inner product for normalized vectors\n",
    "            self.index.add(embeddings)\n",
    "\n",
    "            # Store chunks for retrieval\n",
    "            self.chunks = all_chunks\n",
    "            self.index_built = True\n",
    "\n",
    "            return True, f\"✓ Index built: {len(texts)} chunks, {dim}D vectors\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return False, f\"Error building index: {str(e)}\"\n",
    "\n",
    "    def search(self, query: str, top_k: int = 5) -> List[Tuple[str, dict, float]]:\n",
    "        \"\"\"Search similar chunks\"\"\"\n",
    "        if not self.index_built:\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            # Encode query\n",
    "            query_vec = self.encoder.encode([query], normalize_embeddings=True).astype(\n",
    "                \"float32\"\n",
    "            )\n",
    "\n",
    "            # Search\n",
    "            scores, indices = self.index.search(query_vec, top_k)\n",
    "\n",
    "            results = []\n",
    "            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "                if idx < len(self.chunks):\n",
    "                    chunk = self.chunks[idx]\n",
    "                    results.append((chunk[\"text\"], chunk[\"meta\"], float(score)))\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Search error: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f30d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Gradio Interface with Upload and Index\n",
    "def create_upload_index_interface():\n",
    "    \"\"\"Create Gradio interface for document upload and indexing\"\"\"\n",
    "\n",
    "    processor = DocumentProcessor()\n",
    "    index_builder = RAGIndexBuilder()\n",
    "\n",
    "    def process_files(files) -> Tuple[str, str]:\n",
    "        \"\"\"Process uploaded files and build index\"\"\"\n",
    "        if not files:\n",
    "            return \"No files uploaded\", \"\"\n",
    "\n",
    "        all_chunks = []\n",
    "        processing_log = []\n",
    "\n",
    "        for file in files:\n",
    "            try:\n",
    "                # Get file info\n",
    "                file_path = file.name\n",
    "                filename = Path(file_path).name\n",
    "                file_ext = Path(file_path).suffix.lower().lstrip(\".\")\n",
    "\n",
    "                processing_log.append(f\"📄 Processing: {filename}\")\n",
    "\n",
    "                # Extract text\n",
    "                text = processor.extract_text(file_path, file_ext)\n",
    "\n",
    "                if text.startswith(\"Error\") or text.startswith(\"Unsupported\"):\n",
    "                    processing_log.append(f\"❌ {text}\")\n",
    "                    continue\n",
    "\n",
    "                # Chunk document\n",
    "                chunks = processor.chunk_document(text, filename)\n",
    "                all_chunks.extend(chunks)\n",
    "\n",
    "                processing_log.append(f\"✓ {filename}: {len(chunks)} chunks\")\n",
    "\n",
    "            except Exception as e:\n",
    "                processing_log.append(f\"❌ Error processing {filename}: {str(e)}\")\n",
    "\n",
    "        # Build index\n",
    "        if all_chunks:\n",
    "            processing_log.append(\n",
    "                f\"\\n🔧 Building index for {len(all_chunks)} total chunks...\"\n",
    "            )\n",
    "            success, message = index_builder.build_index(all_chunks)\n",
    "            processing_log.append(message)\n",
    "\n",
    "            if success:\n",
    "                index_status = (\n",
    "                    f\"✅ Index ready: {len(all_chunks)} chunks from {len(files)} files\"\n",
    "                )\n",
    "            else:\n",
    "                index_status = f\"❌ Index failed: {message}\"\n",
    "        else:\n",
    "            index_status = \"❌ No valid chunks to index\"\n",
    "\n",
    "        return \"\\n\".join(processing_log), index_status\n",
    "\n",
    "    def query_index(query: str, top_k: int = 3) -> str:\n",
    "        \"\"\"Query the built index\"\"\"\n",
    "        if not query.strip():\n",
    "            return \"Please enter a query\"\n",
    "\n",
    "        if not index_builder.index_built:\n",
    "            return \"❌ No index available. Please upload and process documents first.\"\n",
    "\n",
    "        try:\n",
    "            results = index_builder.search(query, top_k)\n",
    "\n",
    "            if not results:\n",
    "                return \"No relevant results found\"\n",
    "\n",
    "            response = f\"🔍 Query: {query}\\n\\n\"\n",
    "\n",
    "            for i, (text, meta, score) in enumerate(results, 1):\n",
    "                response += f\"[{i}] Score: {score:.3f} | Source: {meta['source']}\\n\"\n",
    "                response += f\"Text: {text[:200]}{'...' if len(text) > 200 else ''}\\n\\n\"\n",
    "\n",
    "            return response\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"❌ Search error: {str(e)}\"\n",
    "\n",
    "    # Build interface\n",
    "    with gr.Blocks(title=\"Document Upload & RAG Index\") as demo:\n",
    "        gr.Markdown(\"## 📚 Document Upload & RAG Index Builder\")\n",
    "        gr.Markdown(\"Upload documents (PDF/TXT/MD/HTML) → Build index → Query\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                file_upload = gr.File(\n",
    "                    label=\"Upload Documents\",\n",
    "                    file_count=\"multiple\",\n",
    "                    file_types=[\".pdf\", \".txt\", \".md\", \".html\"],\n",
    "                )\n",
    "\n",
    "                process_btn = gr.Button(\"🔧 Process & Build Index\", variant=\"primary\")\n",
    "\n",
    "                index_status = gr.Textbox(\n",
    "                    label=\"Index Status\", value=\"No index built yet\", interactive=False\n",
    "                )\n",
    "\n",
    "            with gr.Column(scale=1):\n",
    "                query_input = gr.Textbox(\n",
    "                    label=\"Query\", placeholder=\"Enter your question...\", lines=2\n",
    "                )\n",
    "\n",
    "                top_k_slider = gr.Slider(\n",
    "                    minimum=1, maximum=10, value=3, step=1, label=\"Number of results\"\n",
    "                )\n",
    "\n",
    "                search_btn = gr.Button(\"🔍 Search\", variant=\"secondary\")\n",
    "\n",
    "        processing_output = gr.Textbox(\n",
    "            label=\"Processing Log\", lines=8, interactive=False\n",
    "        )\n",
    "\n",
    "        search_output = gr.Textbox(label=\"Search Results\", lines=10, interactive=False)\n",
    "\n",
    "        # Event handlers\n",
    "        process_btn.click(\n",
    "            process_files,\n",
    "            inputs=[file_upload],\n",
    "            outputs=[processing_output, index_status],\n",
    "        )\n",
    "\n",
    "        search_btn.click(\n",
    "            query_index, inputs=[query_input, top_k_slider], outputs=[search_output]\n",
    "        )\n",
    "\n",
    "        # Allow Enter key for search\n",
    "        query_input.submit(\n",
    "            query_index, inputs=[query_input, top_k_slider], outputs=[search_output]\n",
    "        )\n",
    "\n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4163e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Launch Interface\n",
    "if __name__ == \"__main__\":\n",
    "    demo = create_upload_index_interface()\n",
    "    demo.launch(server_name=\"127.0.0.1\", server_port=7860, share=False, show_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b5cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Smoke Test\n",
    "def smoke_test():\n",
    "    \"\"\"Test document processing and indexing functionality\"\"\"\n",
    "    print(\"🧪 Running smoke test...\")\n",
    "\n",
    "    # Create test document\n",
    "    test_content = \"\"\"\n",
    "    # Test Document\n",
    "\n",
    "    這是一個測試文檔。我們要測試中文RAG系統的文檔處理能力。\n",
    "\n",
    "    ## Section 1\n",
    "    RAG (Retrieval-Augmented Generation) 是一種結合檢索和生成的技術。\n",
    "\n",
    "    ## Section 2\n",
    "    FAISS 是一個高效的向量搜索庫，支援大規模相似性搜索。\n",
    "    \"\"\"\n",
    "\n",
    "    # Test processor\n",
    "    processor = DocumentProcessor()\n",
    "    chunks = processor.chunk_document(test_content, \"test.md\")\n",
    "\n",
    "    print(f\"✓ Document chunking: {len(chunks)} chunks\")\n",
    "\n",
    "    # Test index builder\n",
    "    builder = RAGIndexBuilder()\n",
    "    success, message = builder.build_index(chunks)\n",
    "\n",
    "    print(f\"✓ Index building: {message}\")\n",
    "\n",
    "    if success:\n",
    "        # Test search\n",
    "        results = builder.search(\"什麼是RAG？\", top_k=2)\n",
    "        print(f\"✓ Search test: {len(results)} results\")\n",
    "\n",
    "        if results:\n",
    "            print(f\"  Top result score: {results[0][2]:.3f}\")\n",
    "\n",
    "    print(\"🎉 Smoke test completed!\")\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2a6d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Key Parameters & Tips\n",
    "print(\n",
    "    \"\"\"\n",
    "🔧 Key Parameters:\n",
    "- chunk_size=800, chunk_overlap=80 (中文友好)\n",
    "- embedding_model=\"BAAI/bge-m3\" (多語言支援)\n",
    "- FAISS IndexFlatIP (normalized vectors)\n",
    "- Support: PDF, TXT, MD, HTML\n",
    "\n",
    "💡 Low-VRAM Options:\n",
    "- Use bge-small-zh-v1.5 for lighter embedding\n",
    "- Process documents in smaller batches\n",
    "- Set batch_size=16 for encoding\n",
    "\n",
    "⚠️ Important Notes:\n",
    "- Large files may take time to process\n",
    "- Index is stored in memory (not persisted)\n",
    "- Check file encoding for proper Chinese display\n",
    "- PDF extraction quality depends on source\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Cell 9: When to Use This\n",
    "print(\n",
    "    \"\"\"\n",
    "📋 When to use this notebook:\n",
    "- Need real-time document indexing in UI\n",
    "- Want to test RAG with custom documents\n",
    "- Building document Q&A applications\n",
    "- Prototyping knowledge base systems\n",
    "\n",
    "🔄 Integration points:\n",
    "- Combine with chat interface (nb52)\n",
    "- Add persistent storage for indices\n",
    "- Integrate with agent systems (Stage 4)\n",
    "- Scale with multi-domain routing (nb19)\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef53cfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification that upload and indexing works\n",
    "def quick_smoke_test():\n",
    "    # Test with minimal content\n",
    "    processor = DocumentProcessor()\n",
    "    test_text = \"RAG是檢索增強生成技術。FAISS是向量搜索庫。\"\n",
    "    chunks = processor.chunk_document(test_text, \"test.txt\")\n",
    "\n",
    "    builder = RAGIndexBuilder()\n",
    "    success, msg = builder.build_index(chunks)\n",
    "\n",
    "    if success:\n",
    "        results = builder.search(\"什麼是RAG\", top_k=1)\n",
    "        print(f\"✅ Upload & Index 功能正常: {len(results)} 個結果\")\n",
    "    else:\n",
    "        print(f\"❌ 測試失敗: {msg}\")\n",
    "\n",
    "\n",
    "quick_smoke_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
