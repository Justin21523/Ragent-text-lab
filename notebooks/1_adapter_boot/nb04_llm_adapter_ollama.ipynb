{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca10b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (複製到每本 notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba7ebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Check Ollama Service & Available Models\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Optional, Iterator, Union\n",
    "\n",
    "\n",
    "def check_ollama_service(base_url: str = \"http://localhost:11434\") -> bool:\n",
    "    \"\"\"Check if Ollama service is running\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{base_url}/api/tags\", timeout=5)\n",
    "        return response.status_code == 200\n",
    "    except requests.exceptions.RequestException:\n",
    "        return False\n",
    "\n",
    "\n",
    "def list_ollama_models(base_url: str = \"http://localhost:11434\") -> List[str]:\n",
    "    \"\"\"List available Ollama models\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{base_url}/api/tags\", timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return [model[\"name\"] for model in data.get(\"models\", [])]\n",
    "    except requests.exceptions.RequestException:\n",
    "        pass\n",
    "    return []\n",
    "\n",
    "\n",
    "# Check service status\n",
    "ollama_running = check_ollama_service()\n",
    "print(f\"Ollama service running: {ollama_running}\")\n",
    "\n",
    "if ollama_running:\n",
    "    models = list_ollama_models()\n",
    "    print(f\"Available models: {models}\")\n",
    "    if not models:\n",
    "        print(\"No models found. Please run: ollama pull qwen2.5:7b\")\n",
    "else:\n",
    "    print(\"Ollama not running. Please start with: ollama serve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2756930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: OllamaAdapter Implementation\n",
    "class OllamaAdapter:\n",
    "    \"\"\"Ollama HTTP backend adapter with unified messages interface\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model_name: str = \"qwen2.5:7b\", base_url: str = \"http://localhost:11434\"\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = base_url\n",
    "        self.session = requests.Session()\n",
    "\n",
    "        # Verify service and model availability\n",
    "        if not check_ollama_service(base_url):\n",
    "            raise ConnectionError(f\"Ollama service not available at {base_url}\")\n",
    "\n",
    "        available_models = list_ollama_models(base_url)\n",
    "        if model_name not in available_models:\n",
    "            print(f\"Warning: Model {model_name} not found in {available_models}\")\n",
    "            print(f\"Attempting to pull model automatically...\")\n",
    "            self._pull_model(model_name)\n",
    "\n",
    "    def _pull_model(self, model_name: str) -> bool:\n",
    "        \"\"\"Pull model if not available\"\"\"\n",
    "        try:\n",
    "            url = f\"{self.base_url}/api/pull\"\n",
    "            payload = {\"name\": model_name}\n",
    "            response = self.session.post(url, json=payload, timeout=300, stream=True)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                print(f\"Successfully pulled model: {model_name}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Failed to pull model: {response.text}\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"Error pulling model: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _messages_to_prompt(self, messages: List[Dict[str, str]]) -> str:\n",
    "        \"\"\"Convert messages format to simple prompt\"\"\"\n",
    "        prompt_parts = []\n",
    "        for msg in messages:\n",
    "            role = msg[\"role\"]\n",
    "            content = msg[\"content\"]\n",
    "            if role == \"system\":\n",
    "                prompt_parts.append(f\"System: {content}\")\n",
    "            elif role == \"user\":\n",
    "                prompt_parts.append(f\"User: {content}\")\n",
    "            elif role == \"assistant\":\n",
    "                prompt_parts.append(f\"Assistant: {content}\")\n",
    "\n",
    "        prompt_parts.append(\"Assistant:\")\n",
    "        return \"\\n\".join(prompt_parts)\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        max_new_tokens: int = 256,\n",
    "        temperature: float = 0.7,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        stream: bool = False,\n",
    "        timeout: int = 60,\n",
    "    ) -> Union[str, Iterator[str]]:\n",
    "        \"\"\"Generate response with unified interface\"\"\"\n",
    "\n",
    "        prompt = self._messages_to_prompt(messages)\n",
    "\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": stream,\n",
    "            \"options\": {\n",
    "                \"temperature\": temperature,\n",
    "                \"num_predict\": max_new_tokens,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        if stop:\n",
    "            payload[\"options\"][\"stop\"] = stop\n",
    "\n",
    "        try:\n",
    "            response = self.session.post(\n",
    "                f\"{self.base_url}/api/generate\",\n",
    "                json=payload,\n",
    "                timeout=timeout,\n",
    "                stream=stream,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "\n",
    "            if stream:\n",
    "                return self._stream_response(response)\n",
    "            else:\n",
    "                return self._get_complete_response(response)\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            raise TimeoutError(f\"Request timed out after {timeout} seconds\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise RuntimeError(f\"Ollama API error: {e}\")\n",
    "\n",
    "    def _stream_response(self, response) -> Iterator[str]:\n",
    "        \"\"\"Handle streaming response\"\"\"\n",
    "        for line in response.iter_lines(decode_unicode=True):\n",
    "            if line:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    if \"response\" in data:\n",
    "                        yield data[\"response\"]\n",
    "                    if data.get(\"done\", False):\n",
    "                        break\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "    def _get_complete_response(self, response) -> str:\n",
    "        \"\"\"Handle non-streaming response\"\"\"\n",
    "        full_response = \"\"\n",
    "        for line in response.iter_lines(decode_unicode=True):\n",
    "            if line:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    if \"response\" in data:\n",
    "                        full_response += data[\"response\"]\n",
    "                    if data.get(\"done\", False):\n",
    "                        break\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        return full_response\n",
    "\n",
    "    def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:\n",
    "        \"\"\"Simple chat interface (non-streaming)\"\"\"\n",
    "        return self.generate(messages, stream=False, **kwargs)\n",
    "\n",
    "\n",
    "print(\"OllamaAdapter class defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2562861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Message Template & Unified Interface\n",
    "def create_chat_messages(system_prompt: str, user_query: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Create standardized messages format\"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_query},\n",
    "    ]\n",
    "\n",
    "\n",
    "# Test message templates\n",
    "system_prompt = (\n",
    "    \"You are a helpful AI assistant. Respond concisely in Traditional Chinese.\"\n",
    ")\n",
    "user_query = \"請用3句話解釋什麼是大語言模型？\"\n",
    "\n",
    "test_messages = create_chat_messages(system_prompt, user_query)\n",
    "print(\"Test messages:\", test_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f058fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Streaming Output & Error Handling\n",
    "def demo_streaming_generation(adapter: OllamaAdapter, messages: List[Dict[str, str]]):\n",
    "    \"\"\"Demonstrate streaming generation with error handling\"\"\"\n",
    "    print(\"=== Streaming Generation ===\")\n",
    "    print(\"Query:\", messages[-1][\"content\"])\n",
    "    print(\"Response:\", end=\" \")\n",
    "\n",
    "    try:\n",
    "        full_response = \"\"\n",
    "        for chunk in adapter.generate(\n",
    "            messages, max_new_tokens=100, temperature=0.7, stream=True\n",
    "        ):\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "            full_response += chunk\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        return full_response\n",
    "\n",
    "    except TimeoutError as e:\n",
    "        print(f\"\\nTimeout error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nGeneration error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def demo_non_streaming(adapter: OllamaAdapter, messages: List[Dict[str, str]]):\n",
    "    \"\"\"Demonstrate non-streaming generation\"\"\"\n",
    "    print(\"=== Non-Streaming Generation ===\")\n",
    "    print(\"Query:\", messages[-1][\"content\"])\n",
    "\n",
    "    try:\n",
    "        response = adapter.generate(\n",
    "            messages, max_new_tokens=100, temperature=0.7, stream=False\n",
    "        )\n",
    "        print(\"Response:\", response)\n",
    "        print(\"=\" * 50)\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Generation error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b6db93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Smoke Test - Generate Conversation\n",
    "# Only run if Ollama is available\n",
    "if ollama_running and models:\n",
    "    try:\n",
    "        # Initialize adapter with first available model\n",
    "        model_to_use = models[0] if \"qwen2.5:7b\" not in models else \"qwen2.5:7b\"\n",
    "        print(f\"Using model: {model_to_use}\")\n",
    "\n",
    "        adapter = OllamaAdapter(model_name=model_to_use)\n",
    "\n",
    "        # Test messages\n",
    "        messages = create_chat_messages(\n",
    "            \"You are a helpful assistant. Be concise.\",\n",
    "            \"What are the key benefits of using Ollama?\",\n",
    "        )\n",
    "\n",
    "        # Test non-streaming\n",
    "        response1 = demo_non_streaming(adapter, messages)\n",
    "\n",
    "        # Test streaming\n",
    "        time.sleep(1)  # Brief pause\n",
    "        response2 = demo_streaming_generation(adapter, messages)\n",
    "\n",
    "        print(\n",
    "            f\"✅ Smoke test passed! Generated {len(response1 or '')} + {len(response2 or '')} characters\" # type: ignore\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Smoke test failed: {e}\")\n",
    "        print(\"Make sure Ollama is running with: ollama serve\")\n",
    "        print(\"And pull a model with: ollama pull qwen2.5:7b\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Skipping smoke test - Ollama service or models not available\")\n",
    "    print(\"To run test:\")\n",
    "    print(\"1. Start Ollama: ollama serve\")\n",
    "    print(\"2. Pull model: ollama pull qwen2.5:7b\")\n",
    "    print(\"3. Re-run this notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40a6eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Interface Comparison with TransformersAdapter\n",
    "print(\"=== Interface Comparison ===\")\n",
    "print(\"Both adapters should support:\")\n",
    "print(\"1. generate(messages, max_new_tokens, temperature, stream)\")\n",
    "print(\"2. chat(messages) - simple wrapper\")\n",
    "print(\"3. Unified messages format: [{'role': 'user', 'content': '...'}]\")\n",
    "print(\"4. Error handling for timeouts and model issues\")\n",
    "print(\"\")\n",
    "\n",
    "# Mock comparison table\n",
    "comparison_data = [\n",
    "    [\"Feature\", \"TransformersAdapter\", \"OllamaAdapter\"],\n",
    "    [\"Model Loading\", \"Direct GPU/CPU\", \"HTTP Service\"],\n",
    "    [\"Memory Usage\", \"High (model in RAM)\", \"Low (service handles)\"],\n",
    "    [\"Latency\", \"Fast (local)\", \"Medium (HTTP overhead)\"],\n",
    "    [\"Streaming\", \"✅ Token-by-token\", \"✅ Chunk-by-chunk\"],\n",
    "    [\"Quantization\", \"bitsandbytes/GPTQ\", \"Built-in GGUF\"],\n",
    "    [\"Model Switch\", \"Reload required\", \"API call only\"],\n",
    "    [\"Offline Usage\", \"✅ Full offline\", \"❌ Needs Ollama service\"],\n",
    "]\n",
    "\n",
    "for row in comparison_data:\n",
    "    print(f\"{row[0]:<15} | {row[1]:<20} | {row[2]:<20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd10bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Performance Baseline & VRAM Usage\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "\n",
    "def benchmark_ollama_performance(adapter: OllamaAdapter, num_tests: int = 3):\n",
    "    \"\"\"Simple performance benchmark\"\"\"\n",
    "    print(\"=== Performance Benchmark ===\")\n",
    "\n",
    "    test_messages = create_chat_messages(\n",
    "        \"You are a helpful assistant.\", \"Explain quantum computing in 2 sentences.\"\n",
    "    )\n",
    "\n",
    "    latencies = []\n",
    "    token_counts = []\n",
    "\n",
    "    for i in range(num_tests):\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            response = adapter.generate(\n",
    "                test_messages, max_new_tokens=50, temperature=0.7, stream=False\n",
    "            )\n",
    "\n",
    "            end_time = time.time()\n",
    "            latency = end_time - start_time\n",
    "            token_count = len(response.split()) if response else 0\n",
    "\n",
    "            latencies.append(latency)\n",
    "            token_counts.append(token_count)\n",
    "\n",
    "            print(f\"Test {i+1}: {latency:.2f}s, ~{token_count} tokens\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Test {i+1} failed: {e}\")\n",
    "\n",
    "    if latencies:\n",
    "        avg_latency = sum(latencies) / len(latencies)\n",
    "        avg_tokens = sum(token_counts) / len(token_counts)\n",
    "        tokens_per_sec = avg_tokens / avg_latency if avg_latency > 0 else 0\n",
    "\n",
    "        print(f\"\\nAverage latency: {avg_latency:.2f}s\")\n",
    "        print(f\"Average tokens: {avg_tokens:.1f}\")\n",
    "        print(f\"Tokens/sec: {tokens_per_sec:.1f}\")\n",
    "\n",
    "    # System resource usage\n",
    "    memory_usage = psutil.virtual_memory().percent\n",
    "    print(f\"System RAM usage: {memory_usage:.1f}%\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"GPU memory: N/A (Ollama manages GPU separately)\")\n",
    "    else:\n",
    "        print(\"Running on CPU\")\n",
    "\n",
    "\n",
    "# Run benchmark if adapter is available\n",
    "if ollama_running and models and \"adapter\" in locals():\n",
    "    try:\n",
    "        benchmark_ollama_performance(adapter)\n",
    "    except Exception as e:\n",
    "        print(f\"Benchmark failed: {e}\")\n",
    "else:\n",
    "    print(\"⚠️ Skipping benchmark - adapter not initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d62ec6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== What We Built ===\n",
      "✅ OllamaAdapter with HTTP backend\n",
      "✅ Unified messages interface\n",
      "✅ Streaming and non-streaming support\n",
      "✅ Error handling and timeouts\n",
      "✅ Model availability checking\n",
      "✅ Performance baseline\n",
      "\n",
      "=== Key Parameters ===\n",
      "• model_name: Ollama model tag (e.g., 'qwen2.5:7b')\n",
      "• base_url: Ollama service URL (default: http://localhost:11434)\n",
      "• temperature: 0.1-1.5 (creativity level)\n",
      "• max_new_tokens: Response length limit\n",
      "• timeout: Request timeout in seconds\n",
      "• stream: True for real-time output\n",
      "\n",
      "=== Pitfalls & Solutions ===\n",
      "• Service not running → Check 'ollama serve'\n",
      "• Model not found → Auto-pull or manual 'ollama pull'\n",
      "• Slow responses → Reduce max_new_tokens or use GPU\n",
      "• Memory issues → Ollama handles quantization automatically\n",
      "• Network timeouts → Increase timeout parameter\n",
      "\n",
      "=== When to Use This ===\n",
      "✅ Low VRAM environments (2-4GB)\n",
      "✅ Quick model switching without reloading\n",
      "✅ CPU-only inference\n",
      "✅ Development/prototyping with multiple models\n",
      "✅ Shared model serving across applications\n",
      "❌ Offline deployment\n",
      "❌ Maximum inference speed (use TransformersAdapter)\n",
      "❌ Custom model modifications\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: What We Built & Key Takeaways\n",
    "print(\"=== What We Built ===\")\n",
    "print(\"✅ OllamaAdapter with HTTP backend\")\n",
    "print(\"✅ Unified messages interface\")\n",
    "print(\"✅ Streaming and non-streaming support\")\n",
    "print(\"✅ Error handling and timeouts\")\n",
    "print(\"✅ Model availability checking\")\n",
    "print(\"✅ Performance baseline\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"=== Key Parameters ===\")\n",
    "print(\"• model_name: Ollama model tag (e.g., 'qwen2.5:7b')\")\n",
    "print(\"• base_url: Ollama service URL (default: http://localhost:11434)\")\n",
    "print(\"• temperature: 0.1-1.5 (creativity level)\")\n",
    "print(\"• max_new_tokens: Response length limit\")\n",
    "print(\"• timeout: Request timeout in seconds\")\n",
    "print(\"• stream: True for real-time output\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"=== Pitfalls & Solutions ===\")\n",
    "print(\"• Service not running → Check 'ollama serve'\")\n",
    "print(\"• Model not found → Auto-pull or manual 'ollama pull'\")\n",
    "print(\"• Slow responses → Reduce max_new_tokens or use GPU\")\n",
    "print(\"• Memory issues → Ollama handles quantization automatically\")\n",
    "print(\"• Network timeouts → Increase timeout parameter\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"=== When to Use This ===\")\n",
    "print(\"✅ Low VRAM environments (2-4GB)\")\n",
    "print(\"✅ Quick model switching without reloading\")\n",
    "print(\"✅ CPU-only inference\")\n",
    "print(\"✅ Development/prototyping with multiple models\")\n",
    "print(\"✅ Shared model serving across applications\")\n",
    "print(\"❌ Offline deployment\")\n",
    "print(\"❌ Maximum inference speed (use TransformersAdapter)\")\n",
    "print(\"❌ Custom model modifications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecd019c",
   "metadata": {},
   "source": [
    "低RAM優勢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c03a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama 自動處理量化，無需手動設定\n",
    "adapter = OllamaAdapter(\"qwen2.5:7b\")  # 自動 GGUF 量化\n",
    "\n",
    "# 支援 CPU 推理\n",
    "adapter = OllamaAdapter(\"qwen2.5:7b\")  # 無需 device_map 設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202d61d7",
   "metadata": {},
   "source": [
    "Smoke Test測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582127c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最小驗證：服務連通性 + 模型回應\n",
    "messages = [{\"role\": \"user\",\n",
    "             \"content\": \"Hi, test response in 5 words.\"}]\n",
    "response = adapter.generate(messages, max_new_tokens=20)\n",
    "assert len(response) > 0, \"No response generated\"\n",
    "print(f\"✅ Success: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
