{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "184c8aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 環境引導檢查開始 - 2025-08-21 17:31:57\n",
      "Python 版本: 3.10.18 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:08:55) [MSC v.1929 64 bit (AMD64)]\n",
      "✓ HF_HOME: ../ai_warehouse/cache/hf\n",
      "✓ TRANSFORMERS_CACHE: ../ai_warehouse/cache/hf/transformers\n",
      "✓ HF_DATASETS_CACHE: ../ai_warehouse/cache/hf/datasets\n",
      "✓ HUGGINGFACE_HUB_CACHE: ../ai_warehouse/cache/hf/hub\n",
      "✓ TORCH_HOME: ../ai_warehouse/cache/torch\n",
      "\n",
      "📁 快取根目錄: ../ai_warehouse/cache\n",
      "🔥 GPU 可用: True\n"
     ]
    }
   ],
   "source": [
    "# ==================== Cell 1: Shared Cache Bootstrap ====================\n",
    "\"\"\"\n",
    "目標：建立統一快取機制，所有模型/資料集都存放在 AI_CACHE_ROOT\n",
    "重要：這段程式碼會被複製到每本 notebook 的第一格\n",
    "\"\"\"\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"🚀 環境引導檢查開始 - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Python 版本: {sys.version}\")\n",
    "\n",
    "# Shared cache configuration (複製到每本 notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "# Critical: Set all AI-related cache paths\n",
    "cache_config = {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}\n",
    "\n",
    "for key, path in cache_config.items():\n",
    "    os.environ[key] = path\n",
    "    pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"✓ {key}: {path}\")\n",
    "\n",
    "print(f\"\\n📁 快取根目錄: {AI_CACHE_ROOT}\")\n",
    "print(f\"🔥 GPU 可用: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2553508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "🔍 CUDA & PyTorch 詳細檢查\n",
      "==================================================\n",
      "PyTorch 版本: 2.7.1+cu128\n",
      "CUDA 可用: True\n",
      "CUDA 版本: 12.8\n",
      "cuDNN 版本: 90701\n",
      "GPU 數量: 1\n",
      "  GPU 0: NVIDIA GeForce RTX 5080\n",
      "    總記憶體: 15.9 GB\n",
      "    計算能力: 12.0\n",
      "    已分配: 0.00 GB\n",
      "    已快取: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# ==================== Cell 2: CUDA & PyTorch 詳細檢查 ====================\n",
    "\"\"\"\n",
    "檢查 GPU 記憶體、CUDA 版本、可用設備等關鍵資訊\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"🔍 CUDA & PyTorch 詳細檢查\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic PyTorch info\n",
    "print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "print(f\"CUDA 可用: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA 版本: {torch.version.cuda}\")\n",
    "    print(f\"cuDNN 版本: {torch.backends.cudnn.version()}\")\n",
    "    print(f\"GPU 數量: {torch.cuda.device_count()}\")\n",
    "\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        memory_gb = props.total_memory / (1024**3)\n",
    "        print(f\"  GPU {i}: {props.name}\")\n",
    "        print(f\"    總記憶體: {memory_gb:.1f} GB\")\n",
    "        print(f\"    計算能力: {props.major}.{props.minor}\")\n",
    "\n",
    "        # Check available memory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()  # Clear cache first\n",
    "            allocated = torch.cuda.memory_allocated(i) / (1024**3)\n",
    "            cached = torch.cuda.memory_reserved(i) / (1024**3)\n",
    "            print(f\"    已分配: {allocated:.2f} GB\")\n",
    "            print(f\"    已快取: {cached:.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠️  CUDA 不可用，將使用 CPU 模式\")\n",
    "    print(\"   建議：檢查 GPU 驅動程式與 CUDA 安裝\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66ffb522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "📦 關鍵依賴套件版本檢查\n",
      "==================================================\n",
      "核心套件 (必需):\n",
      "  ✓ torch: 2.7.1+cu128\n",
      "  ✓ transformers: 4.55.0\n",
      "  ✓ tokenizers: 0.21.4\n",
      "  ✓ accelerate: 1.10.0\n",
      "  ✓ pydantic: 2.11.7\n",
      "  ✓ jsonlines: unknown\n",
      "\n",
      "選用套件 (Stage 2+ 需要):\n",
      "  ✓ sentence_transformers: 5.1.0\n",
      "  not found\n",
      "  ✓ opencc: 1.1.9\n",
      "  ✓ duckduckgo_search: 8.1.1\n"
     ]
    }
   ],
   "source": [
    "# ==================== Cell 3: 關鍵依賴套件版本檢查 ====================\n",
    "\"\"\"\n",
    "檢查 Stage 1-2 所需的核心套件是否正確安裝\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"📦 關鍵依賴套件版本檢查\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define required packages for Stage 1-2\n",
    "required_packages = [\n",
    "    (\"torch\", \"2.0.0\"),\n",
    "    (\"transformers\", \"4.30.0\"),\n",
    "    (\"tokenizers\", \"0.13.0\"),\n",
    "    (\"accelerate\", \"0.20.0\"),\n",
    "    (\"pydantic\", \"2.0.0\"),\n",
    "    (\"jsonlines\", \"3.0.0\"),\n",
    "]\n",
    "\n",
    "optional_packages = [\n",
    "    (\"sentence_transformers\", \"2.2.0\"),\n",
    "    (\"faiss\", \"1.7.0\"),\n",
    "    (\"opencc\", \"1.1.0\"),\n",
    "    (\"duckduckgo_search\", \"3.8.0\"),\n",
    "]\n",
    "\n",
    "def check_package_version(package_name, min_version=None):\n",
    "    \"\"\"Check if package is installed and meets minimum version\"\"\"\n",
    "    try:\n",
    "        if package_name == \"faiss\":\n",
    "            # faiss can be faiss-cpu or faiss-gpu\n",
    "            try:\n",
    "                import faiss\n",
    "                version = \"installed\"\n",
    "            except ImportError:\n",
    "                try:\n",
    "                    import faiss_cpu as faiss\n",
    "                    version = \"cpu\"\n",
    "                except ImportError:\n",
    "                    return False, \"not found\"\n",
    "        else:\n",
    "            module = __import__(package_name)\n",
    "            version = getattr(module, \"__version__\", \"unknown\")\n",
    "\n",
    "        status = \"✓\" if min_version is None else \"✓\"\n",
    "        return True, f\"{status} {package_name}: {version}\"\n",
    "\n",
    "    except ImportError:\n",
    "        return False, f\"✗ {package_name}: 未安裝\"\n",
    "    except Exception as e:\n",
    "        return False, f\"✗ {package_name}: 錯誤 - {str(e)}\"\n",
    "\n",
    "print(\"核心套件 (必需):\")\n",
    "core_ok = True\n",
    "for pkg, min_ver in required_packages:\n",
    "    ok, msg = check_package_version(pkg, min_ver)\n",
    "    print(f\"  {msg}\")\n",
    "    if not ok:\n",
    "        core_ok = False\n",
    "\n",
    "print(\"\\n選用套件 (Stage 2+ 需要):\")\n",
    "for pkg, min_ver in optional_packages:\n",
    "    ok, msg = check_package_version(pkg, min_ver)\n",
    "    print(f\"  {msg}\")\n",
    "\n",
    "if not core_ok:\n",
    "    print(\"\\n⚠️  警告：部分核心套件缺失，請執行:\")\n",
    "    print(\"   pip install torch transformers accelerate pydantic jsonlines\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71f2cd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🧪 快取機制測試\n",
      "==================================================\n",
      "測試下載: microsoft/DialoGPT-small\n",
      "快取位置: ../ai_warehouse/cache/hf/transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\b0979\\miniconda3\\envs\\env-ai\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\AI_LLM_projects\\ai_warehouse\\cache\\hf\\transformers\\models--microsoft--DialoGPT-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizer 下載成功\n",
      "✓ 測試文字: 'Hello world'\n",
      "✓ Token 數量: 2\n",
      "✓ 快取資料夾包含 17 個模型\n"
     ]
    }
   ],
   "source": [
    "# ==================== Cell 4: 快取機制測試 ====================\n",
    "\"\"\"\n",
    "下載一個小型 tokenizer 來驗證快取路徑是否正確設定\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"🧪 快取機制測試\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    # Use a small, fast-to-download tokenizer for testing\n",
    "    test_model = \"microsoft/DialoGPT-small\"\n",
    "\n",
    "    print(f\"測試下載: {test_model}\")\n",
    "    print(f\"快取位置: {os.environ.get('TRANSFORMERS_CACHE', 'default')}\")\n",
    "\n",
    "    # This should download to our cache directory\n",
    "    tokenizer = AutoTokenizer.from_pretrained(test_model)\n",
    "\n",
    "    # Verify it works\n",
    "    test_text = \"Hello world\"\n",
    "    tokens = tokenizer(test_text)\n",
    "\n",
    "    print(f\"✓ Tokenizer 下載成功\")\n",
    "    print(f\"✓ 測試文字: '{test_text}'\")\n",
    "    print(f\"✓ Token 數量: {len(tokens['input_ids'])}\")\n",
    "\n",
    "    # Check if files are in our cache directory\n",
    "    cache_path = pathlib.Path(os.environ[\"TRANSFORMERS_CACHE\"])\n",
    "    if cache_path.exists():\n",
    "        model_dirs = list(cache_path.glob(\"**/*/\"))\n",
    "        print(f\"✓ 快取資料夾包含 {len(model_dirs)} 個模型\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"✗ 快取測試失敗: {str(e)}\")\n",
    "    print(\"   建議檢查網路連線與套件安裝\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d493ed42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🔥 SMOKE TEST - 環境驗證摘要\n",
      "============================================================\n",
      "📊 環境狀態摘要:\n",
      "  Python: 3.10.18\n",
      "  PyTorch: 2.7.1+cu128\n",
      "  CUDA: True\n",
      "  GPU Count: 1\n",
      "  Cache Root: ../ai_warehouse/cache\n",
      "  Cache Size: 1.4 MB\n",
      "  GPU Memory: 15.9 GB\n",
      "\n",
      "🧪 功能測試:\n",
      "  ✓ Tensor operations (cuda)\n",
      "  ✓ Transformers 4.55.0\n",
      "  ✓ Cache directory writable\n",
      "\n",
      "🎉 環境設定完成！\n",
      "下一步：nb02 LLMAdapter (transformers) 基礎實作\n"
     ]
    }
   ],
   "source": [
    "# ==================== Cell 5: Smoke Test & 環境摘要 ====================\n",
    "\"\"\"\n",
    "最終煙霧測試：確保所有關鍵組件都能正常運作\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🔥 SMOKE TEST - 環境驗證摘要\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Environment summary\n",
    "env_status = {\n",
    "    \"Python\": sys.version.split()[0],\n",
    "    \"PyTorch\": torch.__version__,\n",
    "    \"CUDA\": torch.cuda.is_available(),\n",
    "    \"GPU Count\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "    \"Cache Root\": AI_CACHE_ROOT,\n",
    "    \"Cache Size\": \"calculating...\",\n",
    "}\n",
    "\n",
    "# Calculate cache size\n",
    "try:\n",
    "    cache_size_mb = sum(\n",
    "        f.stat().st_size for f in pathlib.Path(AI_CACHE_ROOT).rglob(\"*\") if f.is_file()\n",
    "    ) / (1024 * 1024)\n",
    "    env_status[\"Cache Size\"] = f\"{cache_size_mb:.1f} MB\"\n",
    "except:\n",
    "    env_status[\"Cache Size\"] = \"unknown\"\n",
    "\n",
    "# Memory info\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    env_status[\"GPU Memory\"] = f\"{gpu_memory:.1f} GB\"\n",
    "\n",
    "print(\"📊 環境狀態摘要:\")\n",
    "for key, value in env_status.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Quick functional test\n",
    "print(\"\\n🧪 功能測試:\")\n",
    "functional_tests = []\n",
    "\n",
    "# Test 1: Basic tensor operations\n",
    "try:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    x = torch.randn(3, 3, device=device)\n",
    "    y = torch.matmul(x, x.T)\n",
    "    functional_tests.append((\"✓\", f\"Tensor operations ({device})\"))\n",
    "except Exception as e:\n",
    "    functional_tests.append((\"✗\", f\"Tensor operations failed: {e}\"))\n",
    "\n",
    "# Test 2: Transformers import\n",
    "try:\n",
    "    import transformers\n",
    "\n",
    "    functional_tests.append((\"✓\", f\"Transformers {transformers.__version__}\"))\n",
    "except Exception as e:\n",
    "    functional_tests.append((\"✗\", f\"Transformers import failed: {e}\"))\n",
    "\n",
    "# Test 3: Cache directory writable\n",
    "try:\n",
    "    test_file = pathlib.Path(AI_CACHE_ROOT) / \"test_write.tmp\"\n",
    "    test_file.write_text(\"test\")\n",
    "    test_file.unlink()\n",
    "    functional_tests.append((\"✓\", \"Cache directory writable\"))\n",
    "except Exception as e:\n",
    "    functional_tests.append((\"✗\", f\"Cache write failed: {e}\"))\n",
    "\n",
    "for status, test in functional_tests:\n",
    "    print(f\"  {status} {test}\")\n",
    "\n",
    "# Final verdict\n",
    "all_passed = all(test[0] == \"✓\" for test in functional_tests)\n",
    "verdict = \"🎉 環境設定完成！\" if all_passed else \"⚠️  部分測試未通過，請檢查上述錯誤\"\n",
    "\n",
    "print(f\"\\n{verdict}\")\n",
    "print(f\"下一步：nb02 LLMAdapter (transformers) 基礎實作\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
