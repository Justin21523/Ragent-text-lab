{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "184c8aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ç’°å¢ƒå¼•å°æª¢æŸ¥é–‹å§‹ - 2025-08-21 17:31:57\n",
      "Python ç‰ˆæœ¬: 3.10.18 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:08:55) [MSC v.1929 64 bit (AMD64)]\n",
      "âœ“ HF_HOME: ../ai_warehouse/cache/hf\n",
      "âœ“ TRANSFORMERS_CACHE: ../ai_warehouse/cache/hf/transformers\n",
      "âœ“ HF_DATASETS_CACHE: ../ai_warehouse/cache/hf/datasets\n",
      "âœ“ HUGGINGFACE_HUB_CACHE: ../ai_warehouse/cache/hf/hub\n",
      "âœ“ TORCH_HOME: ../ai_warehouse/cache/torch\n",
      "\n",
      "ğŸ“ å¿«å–æ ¹ç›®éŒ„: ../ai_warehouse/cache\n",
      "ğŸ”¥ GPU å¯ç”¨: True\n"
     ]
    }
   ],
   "source": [
    "# ==================== Cell 1: Shared Cache Bootstrap ====================\n",
    "\"\"\"\n",
    "ç›®æ¨™ï¼šå»ºç«‹çµ±ä¸€å¿«å–æ©Ÿåˆ¶ï¼Œæ‰€æœ‰æ¨¡å‹/è³‡æ–™é›†éƒ½å­˜æ”¾åœ¨ AI_CACHE_ROOT\n",
    "é‡è¦ï¼šé€™æ®µç¨‹å¼ç¢¼æœƒè¢«è¤‡è£½åˆ°æ¯æœ¬ notebook çš„ç¬¬ä¸€æ ¼\n",
    "\"\"\"\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"ğŸš€ ç’°å¢ƒå¼•å°æª¢æŸ¥é–‹å§‹ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Python ç‰ˆæœ¬: {sys.version}\")\n",
    "\n",
    "# Shared cache configuration (è¤‡è£½åˆ°æ¯æœ¬ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "# Critical: Set all AI-related cache paths\n",
    "cache_config = {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}\n",
    "\n",
    "for key, path in cache_config.items():\n",
    "    os.environ[key] = path\n",
    "    pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"âœ“ {key}: {path}\")\n",
    "\n",
    "print(f\"\\nğŸ“ å¿«å–æ ¹ç›®éŒ„: {AI_CACHE_ROOT}\")\n",
    "print(f\"ğŸ”¥ GPU å¯ç”¨: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2553508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ğŸ” CUDA & PyTorch è©³ç´°æª¢æŸ¥\n",
      "==================================================\n",
      "PyTorch ç‰ˆæœ¬: 2.7.1+cu128\n",
      "CUDA å¯ç”¨: True\n",
      "CUDA ç‰ˆæœ¬: 12.8\n",
      "cuDNN ç‰ˆæœ¬: 90701\n",
      "GPU æ•¸é‡: 1\n",
      "  GPU 0: NVIDIA GeForce RTX 5080\n",
      "    ç¸½è¨˜æ†¶é«”: 15.9 GB\n",
      "    è¨ˆç®—èƒ½åŠ›: 12.0\n",
      "    å·²åˆ†é…: 0.00 GB\n",
      "    å·²å¿«å–: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# ==================== Cell 2: CUDA & PyTorch è©³ç´°æª¢æŸ¥ ====================\n",
    "\"\"\"\n",
    "æª¢æŸ¥ GPU è¨˜æ†¶é«”ã€CUDA ç‰ˆæœ¬ã€å¯ç”¨è¨­å‚™ç­‰é—œéµè³‡è¨Š\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ” CUDA & PyTorch è©³ç´°æª¢æŸ¥\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic PyTorch info\n",
    "print(f\"PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDA å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA ç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "    print(f\"cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}\")\n",
    "    print(f\"GPU æ•¸é‡: {torch.cuda.device_count()}\")\n",
    "\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        memory_gb = props.total_memory / (1024**3)\n",
    "        print(f\"  GPU {i}: {props.name}\")\n",
    "        print(f\"    ç¸½è¨˜æ†¶é«”: {memory_gb:.1f} GB\")\n",
    "        print(f\"    è¨ˆç®—èƒ½åŠ›: {props.major}.{props.minor}\")\n",
    "\n",
    "        # Check available memory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()  # Clear cache first\n",
    "            allocated = torch.cuda.memory_allocated(i) / (1024**3)\n",
    "            cached = torch.cuda.memory_reserved(i) / (1024**3)\n",
    "            print(f\"    å·²åˆ†é…: {allocated:.2f} GB\")\n",
    "            print(f\"    å·²å¿«å–: {cached:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸  CUDA ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨ CPU æ¨¡å¼\")\n",
    "    print(\"   å»ºè­°ï¼šæª¢æŸ¥ GPU é©…å‹•ç¨‹å¼èˆ‡ CUDA å®‰è£\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66ffb522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ“¦ é—œéµä¾è³´å¥—ä»¶ç‰ˆæœ¬æª¢æŸ¥\n",
      "==================================================\n",
      "æ ¸å¿ƒå¥—ä»¶ (å¿…éœ€):\n",
      "  âœ“ torch: 2.7.1+cu128\n",
      "  âœ“ transformers: 4.55.0\n",
      "  âœ“ tokenizers: 0.21.4\n",
      "  âœ“ accelerate: 1.10.0\n",
      "  âœ“ pydantic: 2.11.7\n",
      "  âœ“ jsonlines: unknown\n",
      "\n",
      "é¸ç”¨å¥—ä»¶ (Stage 2+ éœ€è¦):\n",
      "  âœ“ sentence_transformers: 5.1.0\n",
      "  not found\n",
      "  âœ“ opencc: 1.1.9\n",
      "  âœ“ duckduckgo_search: 8.1.1\n"
     ]
    }
   ],
   "source": [
    "# ==================== Cell 3: é—œéµä¾è³´å¥—ä»¶ç‰ˆæœ¬æª¢æŸ¥ ====================\n",
    "\"\"\"\n",
    "æª¢æŸ¥ Stage 1-2 æ‰€éœ€çš„æ ¸å¿ƒå¥—ä»¶æ˜¯å¦æ­£ç¢ºå®‰è£\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ“¦ é—œéµä¾è³´å¥—ä»¶ç‰ˆæœ¬æª¢æŸ¥\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define required packages for Stage 1-2\n",
    "required_packages = [\n",
    "    (\"torch\", \"2.0.0\"),\n",
    "    (\"transformers\", \"4.30.0\"),\n",
    "    (\"tokenizers\", \"0.13.0\"),\n",
    "    (\"accelerate\", \"0.20.0\"),\n",
    "    (\"pydantic\", \"2.0.0\"),\n",
    "    (\"jsonlines\", \"3.0.0\"),\n",
    "]\n",
    "\n",
    "optional_packages = [\n",
    "    (\"sentence_transformers\", \"2.2.0\"),\n",
    "    (\"faiss\", \"1.7.0\"),\n",
    "    (\"opencc\", \"1.1.0\"),\n",
    "    (\"duckduckgo_search\", \"3.8.0\"),\n",
    "]\n",
    "\n",
    "def check_package_version(package_name, min_version=None):\n",
    "    \"\"\"Check if package is installed and meets minimum version\"\"\"\n",
    "    try:\n",
    "        if package_name == \"faiss\":\n",
    "            # faiss can be faiss-cpu or faiss-gpu\n",
    "            try:\n",
    "                import faiss\n",
    "                version = \"installed\"\n",
    "            except ImportError:\n",
    "                try:\n",
    "                    import faiss_cpu as faiss\n",
    "                    version = \"cpu\"\n",
    "                except ImportError:\n",
    "                    return False, \"not found\"\n",
    "        else:\n",
    "            module = __import__(package_name)\n",
    "            version = getattr(module, \"__version__\", \"unknown\")\n",
    "\n",
    "        status = \"âœ“\" if min_version is None else \"âœ“\"\n",
    "        return True, f\"{status} {package_name}: {version}\"\n",
    "\n",
    "    except ImportError:\n",
    "        return False, f\"âœ— {package_name}: æœªå®‰è£\"\n",
    "    except Exception as e:\n",
    "        return False, f\"âœ— {package_name}: éŒ¯èª¤ - {str(e)}\"\n",
    "\n",
    "print(\"æ ¸å¿ƒå¥—ä»¶ (å¿…éœ€):\")\n",
    "core_ok = True\n",
    "for pkg, min_ver in required_packages:\n",
    "    ok, msg = check_package_version(pkg, min_ver)\n",
    "    print(f\"  {msg}\")\n",
    "    if not ok:\n",
    "        core_ok = False\n",
    "\n",
    "print(\"\\né¸ç”¨å¥—ä»¶ (Stage 2+ éœ€è¦):\")\n",
    "for pkg, min_ver in optional_packages:\n",
    "    ok, msg = check_package_version(pkg, min_ver)\n",
    "    print(f\"  {msg}\")\n",
    "\n",
    "if not core_ok:\n",
    "    print(\"\\nâš ï¸  è­¦å‘Šï¼šéƒ¨åˆ†æ ¸å¿ƒå¥—ä»¶ç¼ºå¤±ï¼Œè«‹åŸ·è¡Œ:\")\n",
    "    print(\"   pip install torch transformers accelerate pydantic jsonlines\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71f2cd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ§ª å¿«å–æ©Ÿåˆ¶æ¸¬è©¦\n",
      "==================================================\n",
      "æ¸¬è©¦ä¸‹è¼‰: microsoft/DialoGPT-small\n",
      "å¿«å–ä½ç½®: ../ai_warehouse/cache/hf/transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\b0979\\miniconda3\\envs\\env-ai\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\AI_LLM_projects\\ai_warehouse\\cache\\hf\\transformers\\models--microsoft--DialoGPT-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Tokenizer ä¸‹è¼‰æˆåŠŸ\n",
      "âœ“ æ¸¬è©¦æ–‡å­—: 'Hello world'\n",
      "âœ“ Token æ•¸é‡: 2\n",
      "âœ“ å¿«å–è³‡æ–™å¤¾åŒ…å« 17 å€‹æ¨¡å‹\n"
     ]
    }
   ],
   "source": [
    "# ==================== Cell 4: å¿«å–æ©Ÿåˆ¶æ¸¬è©¦ ====================\n",
    "\"\"\"\n",
    "ä¸‹è¼‰ä¸€å€‹å°å‹ tokenizer ä¾†é©—è­‰å¿«å–è·¯å¾‘æ˜¯å¦æ­£ç¢ºè¨­å®š\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ§ª å¿«å–æ©Ÿåˆ¶æ¸¬è©¦\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    # Use a small, fast-to-download tokenizer for testing\n",
    "    test_model = \"microsoft/DialoGPT-small\"\n",
    "\n",
    "    print(f\"æ¸¬è©¦ä¸‹è¼‰: {test_model}\")\n",
    "    print(f\"å¿«å–ä½ç½®: {os.environ.get('TRANSFORMERS_CACHE', 'default')}\")\n",
    "\n",
    "    # This should download to our cache directory\n",
    "    tokenizer = AutoTokenizer.from_pretrained(test_model)\n",
    "\n",
    "    # Verify it works\n",
    "    test_text = \"Hello world\"\n",
    "    tokens = tokenizer(test_text)\n",
    "\n",
    "    print(f\"âœ“ Tokenizer ä¸‹è¼‰æˆåŠŸ\")\n",
    "    print(f\"âœ“ æ¸¬è©¦æ–‡å­—: '{test_text}'\")\n",
    "    print(f\"âœ“ Token æ•¸é‡: {len(tokens['input_ids'])}\")\n",
    "\n",
    "    # Check if files are in our cache directory\n",
    "    cache_path = pathlib.Path(os.environ[\"TRANSFORMERS_CACHE\"])\n",
    "    if cache_path.exists():\n",
    "        model_dirs = list(cache_path.glob(\"**/*/\"))\n",
    "        print(f\"âœ“ å¿«å–è³‡æ–™å¤¾åŒ…å« {len(model_dirs)} å€‹æ¨¡å‹\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âœ— å¿«å–æ¸¬è©¦å¤±æ•—: {str(e)}\")\n",
    "    print(\"   å»ºè­°æª¢æŸ¥ç¶²è·¯é€£ç·šèˆ‡å¥—ä»¶å®‰è£\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d493ed42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ”¥ SMOKE TEST - ç’°å¢ƒé©—è­‰æ‘˜è¦\n",
      "============================================================\n",
      "ğŸ“Š ç’°å¢ƒç‹€æ…‹æ‘˜è¦:\n",
      "  Python: 3.10.18\n",
      "  PyTorch: 2.7.1+cu128\n",
      "  CUDA: True\n",
      "  GPU Count: 1\n",
      "  Cache Root: ../ai_warehouse/cache\n",
      "  Cache Size: 1.4 MB\n",
      "  GPU Memory: 15.9 GB\n",
      "\n",
      "ğŸ§ª åŠŸèƒ½æ¸¬è©¦:\n",
      "  âœ“ Tensor operations (cuda)\n",
      "  âœ“ Transformers 4.55.0\n",
      "  âœ“ Cache directory writable\n",
      "\n",
      "ğŸ‰ ç’°å¢ƒè¨­å®šå®Œæˆï¼\n",
      "ä¸‹ä¸€æ­¥ï¼šnb02 LLMAdapter (transformers) åŸºç¤å¯¦ä½œ\n"
     ]
    }
   ],
   "source": [
    "# ==================== Cell 5: Smoke Test & ç’°å¢ƒæ‘˜è¦ ====================\n",
    "\"\"\"\n",
    "æœ€çµ‚ç…™éœ§æ¸¬è©¦ï¼šç¢ºä¿æ‰€æœ‰é—œéµçµ„ä»¶éƒ½èƒ½æ­£å¸¸é‹ä½œ\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ”¥ SMOKE TEST - ç’°å¢ƒé©—è­‰æ‘˜è¦\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Environment summary\n",
    "env_status = {\n",
    "    \"Python\": sys.version.split()[0],\n",
    "    \"PyTorch\": torch.__version__,\n",
    "    \"CUDA\": torch.cuda.is_available(),\n",
    "    \"GPU Count\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "    \"Cache Root\": AI_CACHE_ROOT,\n",
    "    \"Cache Size\": \"calculating...\",\n",
    "}\n",
    "\n",
    "# Calculate cache size\n",
    "try:\n",
    "    cache_size_mb = sum(\n",
    "        f.stat().st_size for f in pathlib.Path(AI_CACHE_ROOT).rglob(\"*\") if f.is_file()\n",
    "    ) / (1024 * 1024)\n",
    "    env_status[\"Cache Size\"] = f\"{cache_size_mb:.1f} MB\"\n",
    "except:\n",
    "    env_status[\"Cache Size\"] = \"unknown\"\n",
    "\n",
    "# Memory info\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    env_status[\"GPU Memory\"] = f\"{gpu_memory:.1f} GB\"\n",
    "\n",
    "print(\"ğŸ“Š ç’°å¢ƒç‹€æ…‹æ‘˜è¦:\")\n",
    "for key, value in env_status.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Quick functional test\n",
    "print(\"\\nğŸ§ª åŠŸèƒ½æ¸¬è©¦:\")\n",
    "functional_tests = []\n",
    "\n",
    "# Test 1: Basic tensor operations\n",
    "try:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    x = torch.randn(3, 3, device=device)\n",
    "    y = torch.matmul(x, x.T)\n",
    "    functional_tests.append((\"âœ“\", f\"Tensor operations ({device})\"))\n",
    "except Exception as e:\n",
    "    functional_tests.append((\"âœ—\", f\"Tensor operations failed: {e}\"))\n",
    "\n",
    "# Test 2: Transformers import\n",
    "try:\n",
    "    import transformers\n",
    "\n",
    "    functional_tests.append((\"âœ“\", f\"Transformers {transformers.__version__}\"))\n",
    "except Exception as e:\n",
    "    functional_tests.append((\"âœ—\", f\"Transformers import failed: {e}\"))\n",
    "\n",
    "# Test 3: Cache directory writable\n",
    "try:\n",
    "    test_file = pathlib.Path(AI_CACHE_ROOT) / \"test_write.tmp\"\n",
    "    test_file.write_text(\"test\")\n",
    "    test_file.unlink()\n",
    "    functional_tests.append((\"âœ“\", \"Cache directory writable\"))\n",
    "except Exception as e:\n",
    "    functional_tests.append((\"âœ—\", f\"Cache write failed: {e}\"))\n",
    "\n",
    "for status, test in functional_tests:\n",
    "    print(f\"  {status} {test}\")\n",
    "\n",
    "# Final verdict\n",
    "all_passed = all(test[0] == \"âœ“\" for test in functional_tests)\n",
    "verdict = \"ğŸ‰ ç’°å¢ƒè¨­å®šå®Œæˆï¼\" if all_passed else \"âš ï¸  éƒ¨åˆ†æ¸¬è©¦æœªé€šéï¼Œè«‹æª¢æŸ¥ä¸Šè¿°éŒ¯èª¤\"\n",
    "\n",
    "print(f\"\\n{verdict}\")\n",
    "print(f\"ä¸‹ä¸€æ­¥ï¼šnb02 LLMAdapter (transformers) åŸºç¤å¯¦ä½œ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
