{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00e5ab0e",
   "metadata": {},
   "source": [
    "\n",
    "## Stage 1 - nb07: Dataset JSONL Messages 格式與校驗\n",
    "\n",
    "**目標**：建立標準化的對話資料格式、Pydantic 驗證器與實用工具\n",
    " \n",
    " **前提**：Stage 1 基礎環境已設定完成\n",
    " \n",
    " **主要功能**：\n",
    " - 定義 OpenAI 風格的 messages 格式\n",
    " - Pydantic 模型驗證與型別安全\n",
    " - JSONL 檔案處理與錯誤容忍\n",
    " - 資料統計與品質檢查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575b2b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (複製到每本 notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ba04ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell 2: Dependencies and Setup\n",
    "import json\n",
    "import jsonlines\n",
    "from typing import List, Dict, Any, Optional, Union, Literal\n",
    "from pathlib import Path\n",
    "from pydantic import BaseModel, Field, validator, field_validator, ValidationError\n",
    "from datetime import datetime\n",
    "import tiktoken\n",
    "from collections import Counter\n",
    "\n",
    "# Create data directories\n",
    "Path(\"data\").mkdir(exist_ok=True)\n",
    "Path(\"outs\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defdbcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell 3: Messages Schema Definition with Pydantic\n",
    "class Message(BaseModel):\n",
    "    \"\"\"Single message in conversation\"\"\"\n",
    "\n",
    "    role: Literal[\"system\", \"user\", \"assistant\"] = Field(\n",
    "        ..., description=\"Message role\"\n",
    "    )\n",
    "    content: str = Field(..., min_length=1, description=\"Message content\")\n",
    "\n",
    "    @field_validator(\"content\", mode='after')\n",
    "    def content_not_empty(cls, v):\n",
    "        if not v.strip():\n",
    "            raise ValueError(\"Content cannot be empty or whitespace only\")\n",
    "        return v.strip()\n",
    "\n",
    "\n",
    "class Conversation(BaseModel):\n",
    "    \"\"\"Complete conversation with metadata\"\"\"\n",
    "\n",
    "    messages: List[Message] = Field(..., min_items=1, description=\"List of messages\") # type: ignore\n",
    "    conversation_id: Optional[str] = Field(None, description=\"Unique conversation ID\")\n",
    "    metadata: Optional[Dict[str, Any]] = Field(\n",
    "        default_factory=dict, description=\"Additional metadata\"\n",
    "    )\n",
    "\n",
    "    @field_validator(\"messages\", mode='after')\n",
    "    def validate_conversation_flow(cls, v):\n",
    "        \"\"\"Ensure conversation has proper flow\"\"\"\n",
    "        if not v:\n",
    "            raise ValueError(\"Conversation must have at least one message\")\n",
    "\n",
    "        # Check if conversation ends with assistant (good practice)\n",
    "        if len(v) > 1 and v[-1].role != \"assistant\":\n",
    "            print(f\"Warning: Conversation doesn't end with assistant response\")\n",
    "\n",
    "        return v\n",
    "\n",
    "    def token_count(self, model_name: str = \"gpt-3.5-turbo\") -> int:\n",
    "        \"\"\"Estimate token count using tiktoken\"\"\"\n",
    "        try:\n",
    "            enc = tiktoken.encoding_for_model(model_name)\n",
    "        except KeyError:\n",
    "            enc = tiktoken.get_encoding(\"cl100k_base\")  # fallback\n",
    "\n",
    "        total_tokens = 0\n",
    "        for msg in self.messages:\n",
    "            # Rough estimate: role + content + formatting tokens\n",
    "            total_tokens += len(enc.encode(f\"{msg.role}: {msg.content}\"))\n",
    "\n",
    "        return total_tokens\n",
    "\n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get conversation statistics\"\"\"\n",
    "        roles = [msg.role for msg in self.messages]\n",
    "        role_counts = Counter(roles)\n",
    "\n",
    "        content_lengths = [len(msg.content) for msg in self.messages]\n",
    "\n",
    "        return {\n",
    "            \"message_count\": len(self.messages),\n",
    "            \"role_distribution\": dict(role_counts),\n",
    "            \"total_chars\": sum(content_lengths),\n",
    "            \"avg_message_length\": sum(content_lengths) / len(content_lengths),\n",
    "            \"token_estimate\": self.token_count(),\n",
    "            \"conversation_id\": self.conversation_id,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0695605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell 4: JSONL File Processing Tools\n",
    "class DatasetProcessor:\n",
    "    \"\"\"Handle JSONL dataset processing with error tolerance\"\"\"\n",
    "\n",
    "    def __init__(self, error_tolerance: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            error_tolerance: Maximum fraction of invalid records to tolerate\n",
    "        \"\"\"\n",
    "        self.error_tolerance = error_tolerance\n",
    "        self.validation_errors = []\n",
    "\n",
    "    def load_jsonl(self, file_path: Union[str, Path]) -> List[Conversation]:\n",
    "        \"\"\"Load and validate JSONL file\"\"\"\n",
    "        file_path = Path(file_path)\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "        conversations = []\n",
    "        errors = []\n",
    "        total_lines = 0\n",
    "\n",
    "        print(f\"Loading JSONL from: {file_path}\")\n",
    "\n",
    "        with jsonlines.open(file_path, \"r\") as reader:\n",
    "            for line_num, data in enumerate(reader, 1):\n",
    "                total_lines += 1\n",
    "                try:\n",
    "                    if \"messages\" in data:\n",
    "                        # Direct format\n",
    "                        conv = Conversation(**data)\n",
    "                    else:\n",
    "                        # Legacy format conversion\n",
    "                        conv = self._convert_legacy_format(data, line_num)\n",
    "\n",
    "                    conversations.append(conv)\n",
    "\n",
    "                except ValidationError as e:\n",
    "                    error_msg = f\"Line {line_num}: {str(e)}\"\n",
    "                    errors.append(error_msg)\n",
    "                    self.validation_errors.append(\n",
    "                        {\"line\": line_num, \"data\": data, \"error\": str(e)}\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Line {line_num}: Unexpected error - {str(e)}\"\n",
    "                    errors.append(error_msg)\n",
    "\n",
    "        # Check error tolerance\n",
    "        error_rate = len(errors) / total_lines if total_lines > 0 else 0\n",
    "\n",
    "        print(f\"Loaded {len(conversations)}/{total_lines} conversations\")\n",
    "        print(f\"Error rate: {error_rate:.1%}\")\n",
    "\n",
    "        if error_rate > self.error_tolerance:\n",
    "            print(\n",
    "                f\"WARNING: Error rate {error_rate:.1%} exceeds tolerance {self.error_tolerance:.1%}\"\n",
    "            )\n",
    "            print(\"First 3 errors:\")\n",
    "            for err in errors[:3]:\n",
    "                print(f\"  - {err}\")\n",
    "\n",
    "        return conversations\n",
    "\n",
    "    def save_jsonl(\n",
    "        self, conversations: List[Conversation], file_path: Union[str, Path]\n",
    "    ):\n",
    "        \"\"\"Save conversations to JSONL file\"\"\"\n",
    "        file_path = Path(file_path)\n",
    "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with jsonlines.open(file_path, \"w\") as writer:\n",
    "            for conv in conversations:\n",
    "                writer.write(conv.dict())\n",
    "\n",
    "        print(f\"Saved {len(conversations)} conversations to: {file_path}\")\n",
    "\n",
    "    def _convert_legacy_format(self, data: Dict, line_num: int) -> Conversation:\n",
    "        \"\"\"Convert legacy formats to standard format\"\"\"\n",
    "\n",
    "        # Handle various legacy formats\n",
    "        if \"instruction\" in data and \"output\" in data:\n",
    "            # Alpaca-style format\n",
    "            messages = [\n",
    "                Message(role=\"user\", content=data[\"instruction\"]),\n",
    "                Message(role=\"assistant\", content=data[\"output\"]),\n",
    "            ]\n",
    "            if \"input\" in data and data[\"input\"].strip():\n",
    "                messages[0].content = f\"{data['instruction']}\\n\\nInput: {data['input']}\"\n",
    "\n",
    "        elif \"prompt\" in data and \"response\" in data:\n",
    "            # Simple prompt-response format\n",
    "            messages = [\n",
    "                Message(role=\"user\", content=data[\"prompt\"]),\n",
    "                Message(role=\"assistant\", content=data[\"response\"]),\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown format in line {line_num}\")\n",
    "\n",
    "        return Conversation(\n",
    "            messages=messages,\n",
    "            conversation_id=data.get(\"id\", f\"conv_{line_num}\"),\n",
    "            metadata=data.get(\"metadata\", {}),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfb4874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell 5: Dataset Validation and Statistics\n",
    "class DatasetAnalyzer:\n",
    "    \"\"\"Analyze and validate datasets\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def analyze_dataset(conversations: List[Conversation]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate comprehensive dataset statistics\"\"\"\n",
    "\n",
    "        if not conversations:\n",
    "            return {\"error\": \"No conversations to analyze\"}\n",
    "\n",
    "        # Aggregate statistics\n",
    "        all_stats = [conv.get_stats() for conv in conversations]\n",
    "\n",
    "        # Role distribution across all conversations\n",
    "        role_counts = Counter()\n",
    "        for stats in all_stats:\n",
    "            for role, count in stats[\"role_distribution\"].items():\n",
    "                role_counts[role] += count\n",
    "\n",
    "        # Message and token statistics\n",
    "        message_counts = [stats[\"message_count\"] for stats in all_stats]\n",
    "        token_counts = [stats[\"token_estimate\"] for stats in all_stats]\n",
    "        char_counts = [stats[\"total_chars\"] for stats in all_stats]\n",
    "\n",
    "        analysis = {\n",
    "            \"dataset_size\": len(conversations),\n",
    "            \"total_messages\": sum(message_counts),\n",
    "            \"total_estimated_tokens\": sum(token_counts),\n",
    "            \"total_characters\": sum(char_counts),\n",
    "            \"role_distribution\": dict(role_counts),\n",
    "            \"conversation_length\": {\n",
    "                \"min\": min(message_counts),\n",
    "                \"max\": max(message_counts),\n",
    "                \"avg\": sum(message_counts) / len(message_counts),\n",
    "                \"median\": sorted(message_counts)[len(message_counts) // 2],\n",
    "            },\n",
    "            \"token_distribution\": {\n",
    "                \"min\": min(token_counts),\n",
    "                \"max\": max(token_counts),\n",
    "                \"avg\": sum(token_counts) / len(token_counts),\n",
    "                \"median\": sorted(token_counts)[len(token_counts) // 2],\n",
    "            },\n",
    "            \"quality_checks\": DatasetAnalyzer._quality_checks(conversations),\n",
    "        }\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    @staticmethod\n",
    "    def _quality_checks(conversations: List[Conversation]) -> Dict[str, Any]:\n",
    "        \"\"\"Run quality checks on dataset\"\"\"\n",
    "        checks = {\n",
    "            \"empty_messages\": 0,\n",
    "            \"very_short_messages\": 0,  # < 10 chars\n",
    "            \"very_long_messages\": 0,  # > 2000 chars\n",
    "            \"single_message_convs\": 0,\n",
    "            \"incomplete_convs\": 0,  # don't end with assistant\n",
    "        }\n",
    "\n",
    "        for conv in conversations:\n",
    "            if len(conv.messages) == 1:\n",
    "                checks[\"single_message_convs\"] += 1\n",
    "\n",
    "            if conv.messages[-1].role != \"assistant\":\n",
    "                checks[\"incomplete_convs\"] += 1\n",
    "\n",
    "            for msg in conv.messages:\n",
    "                if len(msg.content.strip()) == 0:\n",
    "                    checks[\"empty_messages\"] += 1\n",
    "                elif len(msg.content) < 10:\n",
    "                    checks[\"very_short_messages\"] += 1\n",
    "                elif len(msg.content) > 2000:\n",
    "                    checks[\"very_long_messages\"] += 1\n",
    "\n",
    "        return checks\n",
    "\n",
    "    @staticmethod\n",
    "    def print_analysis(analysis: Dict[str, Any]):\n",
    "        \"\"\"Pretty print analysis results\"\"\"\n",
    "        print(\"=== Dataset Analysis ===\")\n",
    "        print(f\"Dataset size: {analysis['dataset_size']} conversations\")\n",
    "        print(f\"Total messages: {analysis['total_messages']}\")\n",
    "        print(f\"Estimated tokens: {analysis['total_estimated_tokens']:,}\")\n",
    "\n",
    "        print(\"\\n--- Role Distribution ---\")\n",
    "        for role, count in analysis[\"role_distribution\"].items():\n",
    "            percentage = count / analysis[\"total_messages\"] * 100\n",
    "            print(f\"{role}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "        print(\"\\n--- Conversation Length ---\")\n",
    "        conv_len = analysis[\"conversation_length\"]\n",
    "        print(\n",
    "            f\"Min: {conv_len['min']}, Max: {conv_len['max']}, Avg: {conv_len['avg']:.1f}\"\n",
    "        )\n",
    "\n",
    "        print(\"\\n--- Token Distribution ---\")\n",
    "        token_dist = analysis[\"token_distribution\"]\n",
    "        print(\n",
    "            f\"Min: {token_dist['min']}, Max: {token_dist['max']}, Avg: {token_dist['avg']:.1f}\"\n",
    "        )\n",
    "\n",
    "        print(\"\\n--- Quality Issues ---\")\n",
    "        quality = analysis[\"quality_checks\"]\n",
    "        for issue, count in quality.items():\n",
    "            if count > 0:\n",
    "                print(f\"{issue}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb75ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell 6: Format Conversion Tools\n",
    "class FormatConverter:\n",
    "    \"\"\"Convert between different conversation formats\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def to_alpaca_format(conversations: List[Conversation]) -> List[Dict[str, str]]:\n",
    "        \"\"\"Convert to Alpaca instruction format\"\"\"\n",
    "        alpaca_data = []\n",
    "\n",
    "        for conv in conversations:\n",
    "            if (\n",
    "                len(conv.messages) >= 2\n",
    "                and conv.messages[0].role == \"user\"\n",
    "                and conv.messages[1].role == \"assistant\"\n",
    "            ):\n",
    "                alpaca_item = {\n",
    "                    \"instruction\": conv.messages[0].content,\n",
    "                    \"input\": \"\",\n",
    "                    \"output\": conv.messages[1].content,\n",
    "                }\n",
    "                alpaca_data.append(alpaca_item)\n",
    "\n",
    "        print(f\"Converted {len(alpaca_data)} conversations to Alpaca format\")\n",
    "        return alpaca_data\n",
    "\n",
    "    @staticmethod\n",
    "    def to_chat_format(conversations: List[Conversation]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Convert to pure chat format (list of messages)\"\"\"\n",
    "        chat_data = []\n",
    "\n",
    "        for conv in conversations:\n",
    "            chat_item = {\n",
    "                \"messages\": [\n",
    "                    {\"role\": msg.role, \"content\": msg.content} for msg in conv.messages\n",
    "                ],\n",
    "                \"id\": conv.conversation_id,\n",
    "            }\n",
    "            if conv.metadata:\n",
    "                chat_item[\"metadata\"] = conv.metadata\n",
    "            chat_data.append(chat_item)\n",
    "\n",
    "        return chat_data\n",
    "\n",
    "    @staticmethod\n",
    "    def to_prompt_response_format(\n",
    "        conversations: List[Conversation],\n",
    "    ) -> List[Dict[str, str]]:\n",
    "        \"\"\"Convert to simple prompt-response format\"\"\"\n",
    "        prompt_data = []\n",
    "\n",
    "        for conv in conversations:\n",
    "            # Combine all user messages as prompt, last assistant as response\n",
    "            user_messages = [msg.content for msg in conv.messages if msg.role == \"user\"]\n",
    "            assistant_messages = [\n",
    "                msg.content for msg in conv.messages if msg.role == \"assistant\"\n",
    "            ]\n",
    "\n",
    "            if user_messages and assistant_messages:\n",
    "                prompt_data.append(\n",
    "                    {\n",
    "                        \"prompt\": \"\\n\".join(user_messages),\n",
    "                        \"response\": assistant_messages[-1],  # Last assistant response\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        print(f\"Converted {len(prompt_data)} conversations to prompt-response format\")\n",
    "        return prompt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1326990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell 7: Create Sample Data and Smoke Test\n",
    "def create_sample_data():\n",
    "    \"\"\"Create sample conversations for testing\"\"\"\n",
    "\n",
    "    sample_conversations = [\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful AI assistant specializing in programming.\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": \"What is Python list comprehension?\"},\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"Python list comprehension is a concise way to create lists. It follows the syntax: [expression for item in iterable if condition]. For example: [x**2 for x in range(10) if x%2==0] creates a list of squares of even numbers.\",\n",
    "                },\n",
    "            ],\n",
    "            \"conversation_id\": \"sample_001\",\n",
    "            \"metadata\": {\"topic\": \"programming\", \"difficulty\": \"beginner\"},\n",
    "        },\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": \"Explain RAG in AI\"},\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"RAG (Retrieval-Augmented Generation) is an AI technique that combines information retrieval with text generation. It first retrieves relevant documents from a knowledge base, then uses that context to generate more accurate and informed responses.\",\n",
    "                },\n",
    "            ],\n",
    "            \"conversation_id\": \"sample_002\",\n",
    "            \"metadata\": {\"topic\": \"AI\", \"difficulty\": \"intermediate\"},\n",
    "        },\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a cooking assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"How to make scrambled eggs?\"},\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"To make scrambled eggs: 1) Crack 2-3 eggs into a bowl, 2) Add a pinch of salt and pepper, 3) Whisk well, 4) Heat butter in a non-stick pan over medium-low heat, 5) Pour in eggs and gently stir continuously until just set. Don't overcook!\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": \"What if I want them creamier?\"},\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"For creamier scrambled eggs: 1) Add 1-2 tablespoons of cream or milk when whisking, 2) Cook on lower heat, 3) Remove from heat while still slightly wet as they'll continue cooking, 4) You can also add a pat of cold butter at the end and stir it in.\",\n",
    "                },\n",
    "            ],\n",
    "            \"conversation_id\": \"sample_003\",\n",
    "            \"metadata\": {\"topic\": \"cooking\", \"difficulty\": \"beginner\"},\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    return sample_conversations\n",
    "\n",
    "\n",
    "# Smoke Test\n",
    "print(\"=== Smoke Test: JSONL Messages Processing ===\")\n",
    "\n",
    "# Create sample data\n",
    "sample_data = create_sample_data()\n",
    "processor = DatasetProcessor(error_tolerance=0.2)\n",
    "\n",
    "# Save sample data to file\n",
    "sample_file = Path(\"data/sample_conversations.jsonl\")\n",
    "with jsonlines.open(sample_file, \"w\") as writer:\n",
    "    for item in sample_data:\n",
    "        writer.write(item)\n",
    "\n",
    "print(f\"✓ Created sample file: {sample_file}\")\n",
    "\n",
    "# Test loading and validation\n",
    "conversations = processor.load_jsonl(sample_file)\n",
    "print(f\"✓ Loaded {len(conversations)} conversations\")\n",
    "\n",
    "# Test analysis\n",
    "analyzer = DatasetAnalyzer()\n",
    "analysis = analyzer.analyze_dataset(conversations)\n",
    "analyzer.print_analysis(analysis)\n",
    "\n",
    "# Test format conversion\n",
    "converter = FormatConverter()\n",
    "alpaca_format = converter.to_alpaca_format(conversations)\n",
    "chat_format = converter.to_chat_format(conversations)\n",
    "\n",
    "print(f\"\\n✓ Format conversions successful:\")\n",
    "print(f\"  - Alpaca format: {len(alpaca_format)} items\")\n",
    "print(f\"  - Chat format: {len(chat_format)} items\")\n",
    "\n",
    "# Test individual conversation\n",
    "conv = conversations[0]\n",
    "print(f\"\\n✓ Sample conversation stats:\")\n",
    "print(f\"  - Messages: {len(conv.messages)}\")\n",
    "print(f\"  - Tokens: {conv.token_count()}\")\n",
    "print(f\"  - Characters: {len(conv.dict()['messages'][1]['content'])}\")\n",
    "\n",
    "print(\"\\n🎉 All tests passed! JSONL messages processing is ready.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9079a9",
   "metadata": {},
   "source": [
    "\n",
    " ## 重要參數與設定\n",
    "\n",
    " ### 低顯存選項\n",
    " - 此 notebook 主要處理文字資料，不使用 GPU\n",
    " - 記憶體使用：主要受資料集大小影響，建議分批處理大型資料集\n",
    "\n",
    " ### 關鍵參數\n",
    " - `error_tolerance`: 容錯率設定（預設 10%）\n",
    " - `min_length=1`: 訊息最小長度\n",
    " - Token 估算：使用 tiktoken，支援多種模型\n",
    "\n",
    " ### 安全考量\n",
    " - 輸入驗證：防止空白或惡意內容\n",
    " - 檔案路徑檢查：防止路徑遍歷攻擊\n",
    " - 錯誤容忍：避免單一錯誤影響整個處理流程\n",
    "\n",
    " ## 使用時機\n",
    "\n",
    " **適用情況**：\n",
    " - 準備訓練資料集\n",
    " - 驗證對話格式正確性\n",
    " - 資料集品質檢查與統計\n",
    " - 格式轉換與標準化\n",
    "\n",
    " **不適用情況**：\n",
    " - 即時對話處理（太重）\n",
    " - 小量資料（過度工程化）\n",
    " - 非結構化文字處理"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
