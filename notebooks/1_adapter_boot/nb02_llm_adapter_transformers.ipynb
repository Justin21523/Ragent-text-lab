{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "395b5cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cache] ../ai_warehouse/cache | GPU: True\n"
     ]
    }
   ],
   "source": [
    "# ==================== Cell 1: Shared Cache Bootstrap ====================\n",
    "\"\"\"\n",
    "ç›®æ¨™ï¼šå»ºç«‹çµ±ä¸€å¿«å–æ©Ÿåˆ¶ï¼Œæ‰€æœ‰æ¨¡å‹/è³‡æ–™é›†éƒ½å­˜æ”¾åœ¨ AI_CACHE_ROOT\n",
    "é‡è¦ï¼šé€™æ®µç¨‹å¼ç¢¼æœƒè¢«è¤‡è£½åˆ°æ¯æœ¬ notebook çš„ç¬¬ä¸€æ ¼\n",
    "\"\"\"\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (è¤‡è£½åˆ°æ¯æœ¬ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a512176e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\b0979\\miniconda3\\envs\\env-ai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\b0979\\miniconda3\\envs\\env-ai\\lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dependencies loaded\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 2: Import Dependencies\n",
    "# ===========================================\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TextIteratorStreamer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from threading import Thread\n",
    "import time\n",
    "from typing import List, Dict, Generator, Optional, Union\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "print(\"âœ“ Dependencies loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "697c22db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ LLMAdapter base class defined\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 3: LLMAdapter Base Class Design\n",
    "# ===========================================\n",
    "\n",
    "\n",
    "class LLMAdapter:\n",
    "    \"\"\"\n",
    "    Unified interface for different LLM backends.\n",
    "    Supports: transformers, llama.cpp, ollama\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str,\n",
    "        backend: str = \"transformers\",\n",
    "        quantization: Optional[str] = None,  # \"4bit\", \"8bit\", None\n",
    "        device_map: str = \"auto\",\n",
    "        torch_dtype: str = \"auto\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        self.model_id = model_id\n",
    "        self.backend = backend\n",
    "        self.quantization = quantization\n",
    "        self.device_map = device_map\n",
    "        self.torch_dtype = torch_dtype\n",
    "\n",
    "        # Initialize backend\n",
    "        if backend == \"transformers\":\n",
    "            self._init_transformers(**kwargs)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Backend {backend} not implemented yet\")\n",
    "\n",
    "    def _init_transformers(self, **kwargs):\n",
    "        \"\"\"Initialize Transformers backend with optional quantization\"\"\"\n",
    "        print(f\"ğŸ”„ Loading {self.model_id} (transformers backend)\")\n",
    "\n",
    "        # Setup quantization config\n",
    "        quant_config = None\n",
    "        if self.quantization == \"4bit\":\n",
    "            quant_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "            )\n",
    "            print(\"  â””â”€ Using 4-bit quantization\")\n",
    "        elif self.quantization == \"8bit\":\n",
    "            quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            print(\"  â””â”€ Using 8-bit quantization\")\n",
    "\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_id, use_fast=True, trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        # Ensure pad token exists\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # Load model\n",
    "        model_kwargs = {\n",
    "            \"device_map\": self.device_map,\n",
    "            \"quantization_config\": quant_config,\n",
    "            \"trust_remote_code\": True,\n",
    "            **kwargs,\n",
    "        }\n",
    "\n",
    "        # Handle torch_dtype\n",
    "        if self.torch_dtype == \"auto\":\n",
    "            model_kwargs[\"torch_dtype\"] = (\n",
    "                torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "            )\n",
    "        elif self.torch_dtype:\n",
    "            model_kwargs[\"torch_dtype\"] = getattr(torch, self.torch_dtype)\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.model_id, **model_kwargs)\n",
    "\n",
    "        print(f\"âœ“ Model loaded on {self.model.device}\")\n",
    "        if hasattr(self.model, \"get_memory_footprint\"):\n",
    "            print(f\"  â””â”€ Memory: {self.model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        max_new_tokens: int = 256,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.9,\n",
    "        do_sample: bool = True,\n",
    "        stream: bool = False,\n",
    "    ) -> Union[str, Generator[str, None, None]]:\n",
    "        \"\"\"\n",
    "        Generate response from messages.\n",
    "\n",
    "        Args:\n",
    "            messages: List of {\"role\": \"user/assistant/system\", \"content\": \"...\"}\n",
    "            stream: If True, return generator for streaming\n",
    "        \"\"\"\n",
    "\n",
    "        if self.backend == \"transformers\":\n",
    "            return self._generate_transformers(\n",
    "                messages, max_new_tokens, temperature, top_p, do_sample, stream\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Backend {self.backend} not implemented\")\n",
    "\n",
    "\n",
    "print(\"âœ“ LLMAdapter base class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a0901f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Message formatting function defined\n",
      "Sample messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'ä»€éº¼æ˜¯ RAGï¼Ÿè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚'}]\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 4: Messages Template Processing\n",
    "# ===========================================\n",
    "\n",
    "\n",
    "def format_messages_for_model(messages: List[Dict[str, str]], tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Convert messages to model-specific chat template.\n",
    "    Falls back to simple format if chat_template not available.\n",
    "    \"\"\"\n",
    "\n",
    "    # Try using the model's chat template first\n",
    "    if hasattr(tokenizer, \"chat_template\") and tokenizer.chat_template:\n",
    "        try:\n",
    "            formatted = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            return formatted\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Chat template failed: {e}\")\n",
    "\n",
    "    # Fallback to simple format\n",
    "    formatted_parts = []\n",
    "    for msg in messages:\n",
    "        role = msg[\"role\"]\n",
    "        content = msg[\"content\"]\n",
    "\n",
    "        if role == \"system\":\n",
    "            formatted_parts.append(f\"System: {content}\")\n",
    "        elif role == \"user\":\n",
    "            formatted_parts.append(f\"User: {content}\")\n",
    "        elif role == \"assistant\":\n",
    "            formatted_parts.append(f\"Assistant: {content}\")\n",
    "\n",
    "    # Add generation prompt\n",
    "    formatted_parts.append(\"Assistant:\")\n",
    "\n",
    "    return \"\\n\".join(formatted_parts)\n",
    "\n",
    "\n",
    "# Test message formatting\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ä»€éº¼æ˜¯ RAGï¼Ÿè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚\"},\n",
    "]\n",
    "\n",
    "print(\"âœ“ Message formatting function defined\")\n",
    "print(\"Sample messages:\", test_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4147515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Transformers backend implementation added\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 5: Transformers Backend Implementation\n",
    "# ===========================================\n",
    "\n",
    "\n",
    "def _generate_transformers(\n",
    "    self, messages, max_new_tokens, temperature, top_p, do_sample, stream\n",
    "):\n",
    "    \"\"\"Transformers backend generation with streaming support\"\"\"\n",
    "\n",
    "    # Format messages\n",
    "    prompt = format_messages_for_model(messages, self.tokenizer)\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = self.tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=4096,  # Prevent overly long prompts\n",
    "    ).to(self.model.device)\n",
    "\n",
    "    # Generation parameters\n",
    "    gen_kwargs = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"do_sample\": do_sample,\n",
    "        \"pad_token_id\": self.tokenizer.pad_token_id,\n",
    "        \"eos_token_id\": self.tokenizer.eos_token_id,\n",
    "    }\n",
    "\n",
    "    if stream:\n",
    "        # Streaming generation\n",
    "        streamer = TextIteratorStreamer(\n",
    "            self.tokenizer, timeout=30.0, skip_prompt=True, skip_special_tokens=True\n",
    "        )\n",
    "        gen_kwargs[\"streamer\"] = streamer\n",
    "\n",
    "        # Start generation in separate thread\n",
    "        thread = Thread(target=self.model.generate, kwargs={**inputs, **gen_kwargs})\n",
    "        thread.start()\n",
    "\n",
    "        # Yield tokens as they come\n",
    "        for token in streamer:\n",
    "            yield token\n",
    "\n",
    "    else:\n",
    "        # Non-streaming generation\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "        # Decode response (skip input tokens)\n",
    "        response = self.tokenizer.decode(\n",
    "            outputs[0][inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n",
    "        )\n",
    "        return response.strip()\n",
    "\n",
    "\n",
    "# Monkey patch the method to LLMAdapter\n",
    "LLMAdapter._generate_transformers = _generate_transformers\n",
    "\n",
    "print(\"âœ“ Transformers backend implementation added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9e0b015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Available VRAM: 17.1 GB\n",
      "ğŸ’¡ Recommended model: Qwen/Qwen2.5-7B-Instruct\n",
      "\n",
      "ğŸ”§ Low-VRAM strategies:\n",
      "1. 4-bit quantization: quantization='4bit'\n",
      "2. CPU offloading: device_map='auto'\n",
      "3. Smaller models: 3B/7B instead of 14B+\n",
      "4. Gradient checkpointing (for training)\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 6: Low-VRAM Model Loading Examples\n",
    "# ===========================================\n",
    "\n",
    "# Model recommendations by VRAM\n",
    "VRAM_RECOMMENDATIONS = {\n",
    "    \"4GB\": [\"microsoft/DialoGPT-small\", \"distilgpt2\"],\n",
    "    \"8GB\": [\"Qwen/Qwen2.5-3B-Instruct\", \"microsoft/DialoGPT-medium\"],\n",
    "    \"12GB\": [\"Qwen/Qwen2.5-7B-Instruct\", \"meta-llama/Llama-3.2-3B-Instruct\"],\n",
    "    \"16GB+\": [\"Qwen/Qwen2.5-14B-Instruct\", \"meta-llama/Llama-3.1-8B-Instruct\"],\n",
    "}\n",
    "\n",
    "\n",
    "def suggest_model_for_vram():\n",
    "    \"\"\"Suggest appropriate model based on available VRAM\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return \"microsoft/DialoGPT-small\"  # CPU fallback\n",
    "\n",
    "    try:\n",
    "        vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"ğŸ“Š Available VRAM: {vram_gb:.1f} GB\")\n",
    "\n",
    "        if vram_gb >= 16:\n",
    "            return \"Qwen/Qwen2.5-7B-Instruct\"  # Will use 4-bit\n",
    "        elif vram_gb >= 12:\n",
    "            return \"Qwen/Qwen2.5-7B-Instruct\"  # Definitely need 4-bit\n",
    "        elif vram_gb >= 8:\n",
    "            return \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "        else:\n",
    "            return \"microsoft/DialoGPT-medium\"\n",
    "\n",
    "    except Exception:\n",
    "        return \"microsoft/DialoGPT-small\"\n",
    "\n",
    "\n",
    "recommended_model = suggest_model_for_vram()\n",
    "print(f\"ğŸ’¡ Recommended model: {recommended_model}\")\n",
    "\n",
    "# Low-VRAM loading strategies\n",
    "print(\"\\nğŸ”§ Low-VRAM strategies:\")\n",
    "print(\"1. 4-bit quantization: quantization='4bit'\")\n",
    "print(\"2. CPU offloading: device_map='auto'\")\n",
    "print(\"3. Smaller models: 3B/7B instead of 14B+\")\n",
    "print(\"4. Gradient checkpointing (for training)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6296cf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Starting Smoke Test...\n",
      "ğŸ”„ Loading Qwen/Qwen2.5-7B-Instruct (transformers backend)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [23:24<00:00, 351.03s/it]   \n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.07s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model loaded on cuda:0\n",
      "  â””â”€ Memory: 15.23 GB\n",
      "\n",
      "ğŸ“ Testing non-streaming generation...\n",
      "âœ“ Response: <generator object _generate_transformers at 0x00000195383EE180>\n",
      "â±ï¸  Time: 0.01s\n",
      "\n",
      "ğŸŒŠ Testing streaming generation...\n",
      "RAG æ˜¯ Retrieval-Augmented Generationï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰çš„ç¸®å¯«ï¼Œæ˜¯ä¸€ç¨®åˆ©ç”¨èªéŸ³æ¨¡å‹é€²è¡Œç”Ÿæˆä»»å‹™çš„æ–¹æ³•ã€‚å®ƒçµåˆ\n",
      "âœ“ Streaming complete. Total: 70 chars\n",
      "\n",
      "ğŸ‰ Smoke test PASSED!\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 7: Smoke Test - Initialize and Test\n",
    "# ===========================================\n",
    "\n",
    "print(\"ğŸ§ª Starting Smoke Test...\")\n",
    "\n",
    "# Use a small model for testing (adjust if needed)\n",
    "test_model = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "try:\n",
    "    # Initialize adapter\n",
    "    adapter = LLMAdapter(\n",
    "        model_id=test_model,\n",
    "        backend=\"transformers\",\n",
    "        quantization=\"8-bit\",  # No quantization for small model\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    # Test messages\n",
    "    test_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"è«‹ä»‹ç´¹ç”šéº¼æ˜¯RAG å®ƒçš„æ‡‰ç”¨æœ‰å“ªä¸€äº›\"},\n",
    "    ]\n",
    "\n",
    "    print(\"\\nğŸ“ Testing non-streaming generation...\")\n",
    "    start_time = time.time()\n",
    "    response = adapter.generate(\n",
    "        messages=test_messages, max_new_tokens=50, temperature=0.7, stream=False\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"âœ“ Response: {response}\")\n",
    "    print(f\"â±ï¸  Time: {end_time - start_time:.2f}s\")\n",
    "\n",
    "    print(\"\\nğŸŒŠ Testing streaming generation...\")\n",
    "    stream_response = \"\"\n",
    "    for token in adapter.generate(\n",
    "        messages=test_messages, max_new_tokens=30, temperature=0.7, stream=True\n",
    "    ):\n",
    "        print(token, end=\"\", flush=True)\n",
    "        stream_response += token\n",
    "\n",
    "    print(f\"\\nâœ“ Streaming complete. Total: {len(stream_response)} chars\")\n",
    "\n",
    "    print(\"\\nğŸ‰ Smoke test PASSED!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Smoke test FAILED: {e}\")\n",
    "    print(\"ğŸ’¡ Try adjusting the model or check your environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9866376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Model Information:\n",
      "  model_id: Qwen/Qwen2.5-7B-Instruct\n",
      "  backend: transformers\n",
      "  quantization: 8-bit\n",
      "  device: cuda:0\n",
      "  vram_allocated: 12.29 GB\n",
      "  vram_cached: 13.58 GB\n",
      "  parameters: 7615.6M\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 8: Performance Monitoring\n",
    "# ===========================================\n",
    "\n",
    "\n",
    "def get_model_info(adapter):\n",
    "    \"\"\"Get basic model information and memory usage\"\"\"\n",
    "    info = {\n",
    "        \"model_id\": adapter.model_id,\n",
    "        \"backend\": adapter.backend,\n",
    "        \"quantization\": adapter.quantization,\n",
    "        \"device\": str(adapter.model.device) if hasattr(adapter, \"model\") else \"unknown\",\n",
    "    }\n",
    "\n",
    "    # Memory info\n",
    "    if torch.cuda.is_available() and hasattr(adapter, \"model\"):\n",
    "        try:\n",
    "            info[\"vram_allocated\"] = f\"{torch.cuda.memory_allocated() / 1e9:.2f} GB\"\n",
    "            info[\"vram_cached\"] = f\"{torch.cuda.memory_reserved() / 1e9:.2f} GB\"\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Model parameters\n",
    "    if hasattr(adapter, \"model\"):\n",
    "        try:\n",
    "            total_params = sum(p.numel() for p in adapter.model.parameters())\n",
    "            info[\"parameters\"] = f\"{total_params / 1e6:.1f}M\"\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return info\n",
    "\n",
    "\n",
    "# Test performance monitoring (if adapter was created successfully)\n",
    "try:\n",
    "    model_info = get_model_info(adapter)\n",
    "    print(\"ğŸ“Š Model Information:\")\n",
    "    for key, value in model_info.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "except NameError:\n",
    "    print(\"âš ï¸  No adapter to analyze (smoke test may have failed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d905333c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ›ï¸  Key Parameters for LLMAdapter:\n",
      "\n",
      "ğŸ“¦ Model Selection:\n",
      "  â€¢ model_id: HuggingFace model identifier\n",
      "  â€¢ quantization: '4bit', '8bit', or None\n",
      "  â€¢ device_map: 'auto', 'cpu', or specific device\n",
      "\n",
      "ğŸ”§ Generation Parameters:\n",
      "  â€¢ max_new_tokens: Output length limit (default: 256)\n",
      "  â€¢ temperature: Randomness (0.1=conservative, 1.0=creative)\n",
      "  â€¢ top_p: Nucleus sampling (0.9 recommended)\n",
      "  â€¢ stream: True for real-time output\n",
      "\n",
      "ğŸ’¾ Memory Optimization:\n",
      "  â€¢ 4-bit quantization: ~75% memory reduction\n",
      "  â€¢ device_map='auto': Automatic CPU/GPU distribution\n",
      "  â€¢ torch_dtype='float16': Half precision (GPU only)\n",
      "\n",
      "ğŸ“‹ Preset Configurations:\n",
      "  high_quality: {'quantization': None, 'torch_dtype': 'float16', 'temperature': 0.3, 'top_p': 0.9}\n",
      "  low_vram: {'quantization': '4bit', 'device_map': 'auto', 'temperature': 0.7, 'max_new_tokens': 128}\n",
      "  creative_writing: {'quantization': '8bit', 'temperature': 0.8, 'top_p': 0.95, 'max_new_tokens': 512}\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 9: Key Parameters and Configuration\n",
    "# ===========================================\n",
    "\n",
    "print(\"ğŸ›ï¸  Key Parameters for LLMAdapter:\")\n",
    "print()\n",
    "print(\"ğŸ“¦ Model Selection:\")\n",
    "print(\"  â€¢ model_id: HuggingFace model identifier\")\n",
    "print(\"  â€¢ quantization: '4bit', '8bit', or None\")\n",
    "print(\"  â€¢ device_map: 'auto', 'cpu', or specific device\")\n",
    "print()\n",
    "print(\"ğŸ”§ Generation Parameters:\")\n",
    "print(\"  â€¢ max_new_tokens: Output length limit (default: 256)\")\n",
    "print(\"  â€¢ temperature: Randomness (0.1=conservative, 1.0=creative)\")\n",
    "print(\"  â€¢ top_p: Nucleus sampling (0.9 recommended)\")\n",
    "print(\"  â€¢ stream: True for real-time output\")\n",
    "print()\n",
    "print(\"ğŸ’¾ Memory Optimization:\")\n",
    "print(\"  â€¢ 4-bit quantization: ~75% memory reduction\")\n",
    "print(\"  â€¢ device_map='auto': Automatic CPU/GPU distribution\")\n",
    "print(\"  â€¢ torch_dtype='float16': Half precision (GPU only)\")\n",
    "\n",
    "# Configuration examples for different scenarios\n",
    "configs = {\n",
    "    \"high_quality\": {\n",
    "        \"quantization\": None,\n",
    "        \"torch_dtype\": \"float16\",\n",
    "        \"temperature\": 0.3,\n",
    "        \"top_p\": 0.9,\n",
    "    },\n",
    "    \"low_vram\": {\n",
    "        \"quantization\": \"4bit\",\n",
    "        \"device_map\": \"auto\",\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_new_tokens\": 128,\n",
    "    },\n",
    "    \"creative_writing\": {\n",
    "        \"quantization\": \"8bit\",\n",
    "        \"temperature\": 0.8,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_new_tokens\": 512,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ“‹ Preset Configurations:\")\n",
    "for name, config in configs.items():\n",
    "    print(f\"  {name}: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d4cc069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—ï¸  What We Built in nb02:\n",
      "âœ“ LLMAdapter çµ±ä¸€ä»‹é¢è¨­è¨ˆ\n",
      "âœ“ Transformers å¾Œç«¯å¯¦ç¾ (with 4/8-bit quantization)\n",
      "âœ“ Messages æ ¼å¼è™•ç†èˆ‡æ¨¡æ¿ç³»çµ±\n",
      "âœ“ ä¸²æµè¼¸å‡ºæ”¯æ´ (TextIteratorStreamer)\n",
      "âœ“ ä½ VRAM å‹å–„è¨­è¨ˆ (device_map, quantization)\n",
      "âœ“ æ•ˆèƒ½ç›£æ§èˆ‡è¨˜æ†¶é«”è¿½è¹¤\n",
      "\n",
      "âš ï¸  Key Pitfalls:\n",
      "â€¢ æ¨¡å‹å¿«å–è·¯å¾‘è¨­å®šéŒ¯èª¤æœƒé‡è¤‡ä¸‹è¼‰\n",
      "â€¢ 4-bit é‡åŒ–å¯èƒ½å½±éŸ¿è¼¸å‡ºå“è³ª\n",
      "â€¢ ä¸²æµè¼¸å‡ºéœ€è¦é¡å¤–çš„ threading è™•ç†\n",
      "â€¢ Chat template ä¸ç›¸å®¹æ™‚éœ€è¦ fallback\n",
      "\n",
      "ğŸ¯ Next Steps (nb03-nb08):\n",
      "â€¢ nb03: llama.cpp å¾Œç«¯ (GGUF æ”¯æ´)\n",
      "â€¢ nb04: Ollama HTTP å¾Œç«¯\n",
      "â€¢ nb05: æ›´ç²¾ç´°çš„ chat template è™•ç†\n",
      "â€¢ nb06: å»¶é²ã€throughput åŸºæº–æ¸¬è©¦\n",
      "â€¢ nb07: JSONL è³‡æ–™æ ¼å¼èˆ‡é©—è­‰\n",
      "â€¢ nb08: è¼¸å…¥/è¼¸å‡ºå®‰å…¨æª¢æŸ¥\n",
      "\n",
      "ğŸ”„ Reproducibility Tips:\n",
      "â€¢ è¨­å®š AI_CACHE_ROOT ç’°å¢ƒè®Šæ•¸é¿å…é‡è¤‡ä¸‹è¼‰\n",
      "â€¢ ä½¿ç”¨ torch.manual_seed() ä¿è­‰å¯é‡ç¾è¼¸å‡º\n",
      "â€¢ è¨˜éŒ„ VRAM ä½¿ç”¨é‡ä»¥ä¾¿é™¤éŒ¯\n",
      "â€¢ ä¿å­˜ model_info ä»¥ä¾¿ç‰ˆæœ¬è¿½è¹¤\n",
      "\n",
      "âœ… Ready for Stage 1 integration!\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 10: What We Built & Next Steps\n",
    "# ===========================================\n",
    "\n",
    "print(\"ğŸ—ï¸  What We Built in nb02:\")\n",
    "print(\"âœ“ LLMAdapter çµ±ä¸€ä»‹é¢è¨­è¨ˆ\")\n",
    "print(\"âœ“ Transformers å¾Œç«¯å¯¦ç¾ (with 4/8-bit quantization)\")\n",
    "print(\"âœ“ Messages æ ¼å¼è™•ç†èˆ‡æ¨¡æ¿ç³»çµ±\")\n",
    "print(\"âœ“ ä¸²æµè¼¸å‡ºæ”¯æ´ (TextIteratorStreamer)\")\n",
    "print(\"âœ“ ä½ VRAM å‹å–„è¨­è¨ˆ (device_map, quantization)\")\n",
    "print(\"âœ“ æ•ˆèƒ½ç›£æ§èˆ‡è¨˜æ†¶é«”è¿½è¹¤\")\n",
    "\n",
    "print(\"\\nâš ï¸  Key Pitfalls:\")\n",
    "print(\"â€¢ æ¨¡å‹å¿«å–è·¯å¾‘è¨­å®šéŒ¯èª¤æœƒé‡è¤‡ä¸‹è¼‰\")\n",
    "print(\"â€¢ 4-bit é‡åŒ–å¯èƒ½å½±éŸ¿è¼¸å‡ºå“è³ª\")\n",
    "print(\"â€¢ ä¸²æµè¼¸å‡ºéœ€è¦é¡å¤–çš„ threading è™•ç†\")\n",
    "print(\"â€¢ Chat template ä¸ç›¸å®¹æ™‚éœ€è¦ fallback\")\n",
    "\n",
    "print(\"\\nğŸ¯ Next Steps (nb03-nb08):\")\n",
    "print(\"â€¢ nb03: llama.cpp å¾Œç«¯ (GGUF æ”¯æ´)\")\n",
    "print(\"â€¢ nb04: Ollama HTTP å¾Œç«¯\")\n",
    "print(\"â€¢ nb05: æ›´ç²¾ç´°çš„ chat template è™•ç†\")\n",
    "print(\"â€¢ nb06: å»¶é²ã€throughput åŸºæº–æ¸¬è©¦\")\n",
    "print(\"â€¢ nb07: JSONL è³‡æ–™æ ¼å¼èˆ‡é©—è­‰\")\n",
    "print(\"â€¢ nb08: è¼¸å…¥/è¼¸å‡ºå®‰å…¨æª¢æŸ¥\")\n",
    "\n",
    "print(\"\\nğŸ”„ Reproducibility Tips:\")\n",
    "print(\"â€¢ è¨­å®š AI_CACHE_ROOT ç’°å¢ƒè®Šæ•¸é¿å…é‡è¤‡ä¸‹è¼‰\")\n",
    "print(\"â€¢ ä½¿ç”¨ torch.manual_seed() ä¿è­‰å¯é‡ç¾è¼¸å‡º\")\n",
    "print(\"â€¢ è¨˜éŒ„ VRAM ä½¿ç”¨é‡ä»¥ä¾¿é™¤éŒ¯\")\n",
    "print(\"â€¢ ä¿å­˜ model_info ä»¥ä¾¿ç‰ˆæœ¬è¿½è¹¤\")\n",
    "\n",
    "print(\"\\nâœ… Ready for Stage 1 integration!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3afbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 11: Save Adapter to Shared Utils (Optional)\n",
    "# ===========================================\n",
    "\n",
    "# Create shared_utils directory structure\n",
    "import os\n",
    "\n",
    "os.makedirs(\"../shared_utils/adapters\", exist_ok=True)\n",
    "\n",
    "# Save the LLMAdapter class for reuse\n",
    "adapter_code = '''\n",
    "\"\"\"\n",
    "LLMAdapter for ragent-text-lab\n",
    "Unified interface for transformers/llama.cpp/ollama backends\n",
    "\"\"\"\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TextIteratorStreamer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from threading import Thread\n",
    "import torch\n",
    "from typing import List, Dict, Generator, Optional, Union\n",
    "\n",
    "class LLMAdapter:\n",
    "    \"\"\"Unified LLM interface supporting multiple backends\"\"\"\n",
    "\n",
    "    def __init__(self, model_id: str, backend: str = \"transformers\", **kwargs):\n",
    "        self.model_id = model_id\n",
    "        self.backend = backend\n",
    "\n",
    "        if backend == \"transformers\":\n",
    "            self._init_transformers(**kwargs)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Backend {backend} not implemented yet\")\n",
    "\n",
    "    def _init_transformers(self, quantization=None, device_map=\"auto\", torch_dtype=\"auto\", **kwargs):\n",
    "        # Implementation from notebook...\n",
    "        pass\n",
    "\n",
    "    def generate(self, messages: List[Dict[str, str]], **kwargs):\n",
    "        # Implementation from notebook...\n",
    "        pass\n",
    "'''\n",
    "\n",
    "with open(\"../shared_utils/adapters/llm_adapter.py\", \"w\") as f:\n",
    "    f.write(adapter_code)\n",
    "\n",
    "print(\"ğŸ’¾ LLMAdapter saved to shared_utils/adapters/llm_adapter.py\")\n",
    "print(\n",
    "    \"ğŸ“ Ready to import in future notebooks: from shared_utils.adapters import LLMAdapter\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
