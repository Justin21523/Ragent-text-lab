{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "395b5cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cache] ../ai_warehouse/cache | GPU: True\n"
     ]
    }
   ],
   "source": [
    "# ==================== Cell 1: Shared Cache Bootstrap ====================\n",
    "\"\"\"\n",
    "ÁõÆÊ®ôÔºöÂª∫Á´ãÁµ±‰∏ÄÂø´ÂèñÊ©üÂà∂ÔºåÊâÄÊúâÊ®°Âûã/Ë≥áÊñôÈõÜÈÉΩÂ≠òÊîæÂú® AI_CACHE_ROOT\n",
    "ÈáçË¶ÅÔºöÈÄôÊÆµÁ®ãÂºèÁ¢ºÊúÉË¢´Ë§áË£ΩÂà∞ÊØèÊú¨ notebook ÁöÑÁ¨¨‰∏ÄÊ†º\n",
    "\"\"\"\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (Ë§áË£ΩÂà∞ÊØèÊú¨ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a512176e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\b0979\\miniconda3\\envs\\env-ai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\b0979\\miniconda3\\envs\\env-ai\\lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dependencies loaded\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 2: Import Dependencies\n",
    "# ===========================================\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TextIteratorStreamer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from threading import Thread\n",
    "import time\n",
    "from typing import List, Dict, Generator, Optional, Union\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "print(\"‚úì Dependencies loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "697c22db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LLMAdapter base class defined\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 3: LLMAdapter Base Class Design\n",
    "# ===========================================\n",
    "\n",
    "\n",
    "class LLMAdapter:\n",
    "    \"\"\"\n",
    "    Unified interface for different LLM backends.\n",
    "    Supports: transformers, llama.cpp, ollama\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str,\n",
    "        backend: str = \"transformers\",\n",
    "        quantization: Optional[str] = None,  # \"4bit\", \"8bit\", None\n",
    "        device_map: str = \"auto\",\n",
    "        torch_dtype: str = \"auto\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        self.model_id = model_id\n",
    "        self.backend = backend\n",
    "        self.quantization = quantization\n",
    "        self.device_map = device_map\n",
    "        self.torch_dtype = torch_dtype\n",
    "\n",
    "        # Initialize backend\n",
    "        if backend == \"transformers\":\n",
    "            self._init_transformers(**kwargs)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Backend {backend} not implemented yet\")\n",
    "\n",
    "    def _init_transformers(self, **kwargs):\n",
    "        \"\"\"Initialize Transformers backend with optional quantization\"\"\"\n",
    "        print(f\"üîÑ Loading {self.model_id} (transformers backend)\")\n",
    "\n",
    "        # Setup quantization config\n",
    "        quant_config = None\n",
    "        if self.quantization == \"4bit\":\n",
    "            quant_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "            )\n",
    "            print(\"  ‚îî‚îÄ Using 4-bit quantization\")\n",
    "        elif self.quantization == \"8bit\":\n",
    "            quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            print(\"  ‚îî‚îÄ Using 8-bit quantization\")\n",
    "\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_id, use_fast=True, trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        # Ensure pad token exists\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # Load model\n",
    "        model_kwargs = {\n",
    "            \"device_map\": self.device_map,\n",
    "            \"quantization_config\": quant_config,\n",
    "            \"trust_remote_code\": True,\n",
    "            **kwargs,\n",
    "        }\n",
    "\n",
    "        # Handle torch_dtype\n",
    "        if self.torch_dtype == \"auto\":\n",
    "            model_kwargs[\"torch_dtype\"] = (\n",
    "                torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "            )\n",
    "        elif self.torch_dtype:\n",
    "            model_kwargs[\"torch_dtype\"] = getattr(torch, self.torch_dtype)\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.model_id, **model_kwargs)\n",
    "\n",
    "        print(f\"‚úì Model loaded on {self.model.device}\")\n",
    "        if hasattr(self.model, \"get_memory_footprint\"):\n",
    "            print(f\"  ‚îî‚îÄ Memory: {self.model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        max_new_tokens: int = 256,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.9,\n",
    "        do_sample: bool = True,\n",
    "        stream: bool = False,\n",
    "    ) -> Union[str, Generator[str, None, None]]:\n",
    "        \"\"\"\n",
    "        Generate response from messages.\n",
    "\n",
    "        Args:\n",
    "            messages: List of {\"role\": \"user/assistant/system\", \"content\": \"...\"}\n",
    "            stream: If True, return generator for streaming\n",
    "        \"\"\"\n",
    "\n",
    "        if self.backend == \"transformers\":\n",
    "            return self._generate_transformers(\n",
    "                messages, max_new_tokens, temperature, top_p, do_sample, stream\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Backend {self.backend} not implemented\")\n",
    "\n",
    "\n",
    "print(\"‚úì LLMAdapter base class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a0901f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Message formatting function defined\n",
      "Sample messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '‰ªÄÈ∫ºÊòØ RAGÔºüË´ãÁî®ÁπÅÈ´î‰∏≠ÊñáÂõûÁ≠î„ÄÇ'}]\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 4: Messages Template Processing\n",
    "# ===========================================\n",
    "\n",
    "\n",
    "def format_messages_for_model(messages: List[Dict[str, str]], tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Convert messages to model-specific chat template.\n",
    "    Falls back to simple format if chat_template not available.\n",
    "    \"\"\"\n",
    "\n",
    "    # Try using the model's chat template first\n",
    "    if hasattr(tokenizer, \"chat_template\") and tokenizer.chat_template:\n",
    "        try:\n",
    "            formatted = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            return formatted\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Chat template failed: {e}\")\n",
    "\n",
    "    # Fallback to simple format\n",
    "    formatted_parts = []\n",
    "    for msg in messages:\n",
    "        role = msg[\"role\"]\n",
    "        content = msg[\"content\"]\n",
    "\n",
    "        if role == \"system\":\n",
    "            formatted_parts.append(f\"System: {content}\")\n",
    "        elif role == \"user\":\n",
    "            formatted_parts.append(f\"User: {content}\")\n",
    "        elif role == \"assistant\":\n",
    "            formatted_parts.append(f\"Assistant: {content}\")\n",
    "\n",
    "    # Add generation prompt\n",
    "    formatted_parts.append(\"Assistant:\")\n",
    "\n",
    "    return \"\\n\".join(formatted_parts)\n",
    "\n",
    "\n",
    "# Test message formatting\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"‰ªÄÈ∫ºÊòØ RAGÔºüË´ãÁî®ÁπÅÈ´î‰∏≠ÊñáÂõûÁ≠î„ÄÇ\"},\n",
    "]\n",
    "\n",
    "print(\"‚úì Message formatting function defined\")\n",
    "print(\"Sample messages:\", test_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4147515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Transformers backend implementation added\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 5: Transformers Backend Implementation\n",
    "# ===========================================\n",
    "\n",
    "\n",
    "def _generate_transformers(\n",
    "    self, messages, max_new_tokens, temperature, top_p, do_sample, stream\n",
    "):\n",
    "    \"\"\"Transformers backend generation with streaming support\"\"\"\n",
    "\n",
    "    # Format messages\n",
    "    prompt = format_messages_for_model(messages, self.tokenizer)\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = self.tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=4096,  # Prevent overly long prompts\n",
    "    ).to(self.model.device)\n",
    "\n",
    "    # Generation parameters\n",
    "    gen_kwargs = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"do_sample\": do_sample,\n",
    "        \"pad_token_id\": self.tokenizer.pad_token_id,\n",
    "        \"eos_token_id\": self.tokenizer.eos_token_id,\n",
    "    }\n",
    "\n",
    "    if stream:\n",
    "        # Streaming generation\n",
    "        streamer = TextIteratorStreamer(\n",
    "            self.tokenizer, timeout=30.0, skip_prompt=True, skip_special_tokens=True\n",
    "        )\n",
    "        gen_kwargs[\"streamer\"] = streamer\n",
    "\n",
    "        # Start generation in separate thread\n",
    "        thread = Thread(target=self.model.generate, kwargs={**inputs, **gen_kwargs})\n",
    "        thread.start()\n",
    "\n",
    "        # Yield tokens as they come\n",
    "        for token in streamer:\n",
    "            yield token\n",
    "\n",
    "    else:\n",
    "        # Non-streaming generation\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "        # Decode response (skip input tokens)\n",
    "        response = self.tokenizer.decode(\n",
    "            outputs[0][inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n",
    "        )\n",
    "        return response.strip()\n",
    "\n",
    "\n",
    "# Monkey patch the method to LLMAdapter\n",
    "LLMAdapter._generate_transformers = _generate_transformers\n",
    "\n",
    "print(\"‚úì Transformers backend implementation added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9e0b015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Available VRAM: 17.1 GB\n",
      "üí° Recommended model: Qwen/Qwen2.5-7B-Instruct\n",
      "\n",
      "üîß Low-VRAM strategies:\n",
      "1. 4-bit quantization: quantization='4bit'\n",
      "2. CPU offloading: device_map='auto'\n",
      "3. Smaller models: 3B/7B instead of 14B+\n",
      "4. Gradient checkpointing (for training)\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 6: Low-VRAM Model Loading Examples\n",
    "# ===========================================\n",
    "\n",
    "# Model recommendations by VRAM\n",
    "VRAM_RECOMMENDATIONS = {\n",
    "    \"4GB\": [\"microsoft/DialoGPT-small\", \"distilgpt2\"],\n",
    "    \"8GB\": [\"Qwen/Qwen2.5-3B-Instruct\", \"microsoft/DialoGPT-medium\"],\n",
    "    \"12GB\": [\"Qwen/Qwen2.5-7B-Instruct\", \"meta-llama/Llama-3.2-3B-Instruct\"],\n",
    "    \"16GB+\": [\"Qwen/Qwen2.5-14B-Instruct\", \"meta-llama/Llama-3.1-8B-Instruct\"],\n",
    "}\n",
    "\n",
    "\n",
    "def suggest_model_for_vram():\n",
    "    \"\"\"Suggest appropriate model based on available VRAM\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return \"microsoft/DialoGPT-small\"  # CPU fallback\n",
    "\n",
    "    try:\n",
    "        vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"üìä Available VRAM: {vram_gb:.1f} GB\")\n",
    "\n",
    "        if vram_gb >= 16:\n",
    "            return \"Qwen/Qwen2.5-7B-Instruct\"  # Will use 4-bit\n",
    "        elif vram_gb >= 12:\n",
    "            return \"Qwen/Qwen2.5-7B-Instruct\"  # Definitely need 4-bit\n",
    "        elif vram_gb >= 8:\n",
    "            return \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "        else:\n",
    "            return \"microsoft/DialoGPT-medium\"\n",
    "\n",
    "    except Exception:\n",
    "        return \"microsoft/DialoGPT-small\"\n",
    "\n",
    "\n",
    "recommended_model = suggest_model_for_vram()\n",
    "print(f\"üí° Recommended model: {recommended_model}\")\n",
    "\n",
    "# Low-VRAM loading strategies\n",
    "print(\"\\nüîß Low-VRAM strategies:\")\n",
    "print(\"1. 4-bit quantization: quantization='4bit'\")\n",
    "print(\"2. CPU offloading: device_map='auto'\")\n",
    "print(\"3. Smaller models: 3B/7B instead of 14B+\")\n",
    "print(\"4. Gradient checkpointing (for training)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6296cf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Starting Smoke Test...\n",
      "üîÑ Loading Qwen/Qwen2.5-7B-Instruct (transformers backend)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [23:24<00:00, 351.03s/it]   \n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.07s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model loaded on cuda:0\n",
      "  ‚îî‚îÄ Memory: 15.23 GB\n",
      "\n",
      "üìù Testing non-streaming generation...\n",
      "‚úì Response: <generator object _generate_transformers at 0x00000195383EE180>\n",
      "‚è±Ô∏è  Time: 0.01s\n",
      "\n",
      "üåä Testing streaming generation...\n",
      "RAG ÊòØ Retrieval-Augmented GenerationÔºàÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºâÁöÑÁ∏ÆÂØ´ÔºåÊòØ‰∏ÄÁ®ÆÂà©Áî®Ë™ûÈü≥Ê®°ÂûãÈÄ≤Ë°åÁîüÊàê‰ªªÂãôÁöÑÊñπÊ≥ï„ÄÇÂÆÉÁµêÂêà\n",
      "‚úì Streaming complete. Total: 70 chars\n",
      "\n",
      "üéâ Smoke test PASSED!\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 7: Smoke Test - Initialize and Test\n",
    "# ===========================================\n",
    "\n",
    "print(\"üß™ Starting Smoke Test...\")\n",
    "\n",
    "# Use a small model for testing (adjust if needed)\n",
    "test_model = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "try:\n",
    "    # Initialize adapter\n",
    "    adapter = LLMAdapter(\n",
    "        model_id=test_model,\n",
    "        backend=\"transformers\",\n",
    "        quantization=\"8-bit\",  # No quantization for small model\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    # Test messages\n",
    "    test_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Ë´ã‰ªãÁ¥πÁîöÈ∫ºÊòØRAG ÂÆÉÁöÑÊáâÁî®ÊúâÂì™‰∏Ä‰∫õ\"},\n",
    "    ]\n",
    "\n",
    "    print(\"\\nüìù Testing non-streaming generation...\")\n",
    "    start_time = time.time()\n",
    "    response = adapter.generate(\n",
    "        messages=test_messages, max_new_tokens=50, temperature=0.7, stream=False\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"‚úì Response: {response}\")\n",
    "    print(f\"‚è±Ô∏è  Time: {end_time - start_time:.2f}s\")\n",
    "\n",
    "    print(\"\\nüåä Testing streaming generation...\")\n",
    "    stream_response = \"\"\n",
    "    for token in adapter.generate(\n",
    "        messages=test_messages, max_new_tokens=30, temperature=0.7, stream=True\n",
    "    ):\n",
    "        print(token, end=\"\", flush=True)\n",
    "        stream_response += token\n",
    "\n",
    "    print(f\"\\n‚úì Streaming complete. Total: {len(stream_response)} chars\")\n",
    "\n",
    "    print(\"\\nüéâ Smoke test PASSED!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Smoke test FAILED: {e}\")\n",
    "    print(\"üí° Try adjusting the model or check your environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9866376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Model Information:\n",
      "  model_id: Qwen/Qwen2.5-7B-Instruct\n",
      "  backend: transformers\n",
      "  quantization: 8-bit\n",
      "  device: cuda:0\n",
      "  vram_allocated: 12.29 GB\n",
      "  vram_cached: 13.58 GB\n",
      "  parameters: 7615.6M\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 8: Performance Monitoring\n",
    "# ===========================================\n",
    "\n",
    "\n",
    "def get_model_info(adapter):\n",
    "    \"\"\"Get basic model information and memory usage\"\"\"\n",
    "    info = {\n",
    "        \"model_id\": adapter.model_id,\n",
    "        \"backend\": adapter.backend,\n",
    "        \"quantization\": adapter.quantization,\n",
    "        \"device\": str(adapter.model.device) if hasattr(adapter, \"model\") else \"unknown\",\n",
    "    }\n",
    "\n",
    "    # Memory info\n",
    "    if torch.cuda.is_available() and hasattr(adapter, \"model\"):\n",
    "        try:\n",
    "            info[\"vram_allocated\"] = f\"{torch.cuda.memory_allocated() / 1e9:.2f} GB\"\n",
    "            info[\"vram_cached\"] = f\"{torch.cuda.memory_reserved() / 1e9:.2f} GB\"\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Model parameters\n",
    "    if hasattr(adapter, \"model\"):\n",
    "        try:\n",
    "            total_params = sum(p.numel() for p in adapter.model.parameters())\n",
    "            info[\"parameters\"] = f\"{total_params / 1e6:.1f}M\"\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return info\n",
    "\n",
    "\n",
    "# Test performance monitoring (if adapter was created successfully)\n",
    "try:\n",
    "    model_info = get_model_info(adapter)\n",
    "    print(\"üìä Model Information:\")\n",
    "    for key, value in model_info.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "except NameError:\n",
    "    print(\"‚ö†Ô∏è  No adapter to analyze (smoke test may have failed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d905333c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéõÔ∏è  Key Parameters for LLMAdapter:\n",
      "\n",
      "üì¶ Model Selection:\n",
      "  ‚Ä¢ model_id: HuggingFace model identifier\n",
      "  ‚Ä¢ quantization: '4bit', '8bit', or None\n",
      "  ‚Ä¢ device_map: 'auto', 'cpu', or specific device\n",
      "\n",
      "üîß Generation Parameters:\n",
      "  ‚Ä¢ max_new_tokens: Output length limit (default: 256)\n",
      "  ‚Ä¢ temperature: Randomness (0.1=conservative, 1.0=creative)\n",
      "  ‚Ä¢ top_p: Nucleus sampling (0.9 recommended)\n",
      "  ‚Ä¢ stream: True for real-time output\n",
      "\n",
      "üíæ Memory Optimization:\n",
      "  ‚Ä¢ 4-bit quantization: ~75% memory reduction\n",
      "  ‚Ä¢ device_map='auto': Automatic CPU/GPU distribution\n",
      "  ‚Ä¢ torch_dtype='float16': Half precision (GPU only)\n",
      "\n",
      "üìã Preset Configurations:\n",
      "  high_quality: {'quantization': None, 'torch_dtype': 'float16', 'temperature': 0.3, 'top_p': 0.9}\n",
      "  low_vram: {'quantization': '4bit', 'device_map': 'auto', 'temperature': 0.7, 'max_new_tokens': 128}\n",
      "  creative_writing: {'quantization': '8bit', 'temperature': 0.8, 'top_p': 0.95, 'max_new_tokens': 512}\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 9: Key Parameters and Configuration\n",
    "# ===========================================\n",
    "\n",
    "print(\"üéõÔ∏è  Key Parameters for LLMAdapter:\")\n",
    "print()\n",
    "print(\"üì¶ Model Selection:\")\n",
    "print(\"  ‚Ä¢ model_id: HuggingFace model identifier\")\n",
    "print(\"  ‚Ä¢ quantization: '4bit', '8bit', or None\")\n",
    "print(\"  ‚Ä¢ device_map: 'auto', 'cpu', or specific device\")\n",
    "print()\n",
    "print(\"üîß Generation Parameters:\")\n",
    "print(\"  ‚Ä¢ max_new_tokens: Output length limit (default: 256)\")\n",
    "print(\"  ‚Ä¢ temperature: Randomness (0.1=conservative, 1.0=creative)\")\n",
    "print(\"  ‚Ä¢ top_p: Nucleus sampling (0.9 recommended)\")\n",
    "print(\"  ‚Ä¢ stream: True for real-time output\")\n",
    "print()\n",
    "print(\"üíæ Memory Optimization:\")\n",
    "print(\"  ‚Ä¢ 4-bit quantization: ~75% memory reduction\")\n",
    "print(\"  ‚Ä¢ device_map='auto': Automatic CPU/GPU distribution\")\n",
    "print(\"  ‚Ä¢ torch_dtype='float16': Half precision (GPU only)\")\n",
    "\n",
    "# Configuration examples for different scenarios\n",
    "configs = {\n",
    "    \"high_quality\": {\n",
    "        \"quantization\": None,\n",
    "        \"torch_dtype\": \"float16\",\n",
    "        \"temperature\": 0.3,\n",
    "        \"top_p\": 0.9,\n",
    "    },\n",
    "    \"low_vram\": {\n",
    "        \"quantization\": \"4bit\",\n",
    "        \"device_map\": \"auto\",\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_new_tokens\": 128,\n",
    "    },\n",
    "    \"creative_writing\": {\n",
    "        \"quantization\": \"8bit\",\n",
    "        \"temperature\": 0.8,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_new_tokens\": 512,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"\\nüìã Preset Configurations:\")\n",
    "for name, config in configs.items():\n",
    "    print(f\"  {name}: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d4cc069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è  What We Built in nb02:\n",
      "‚úì LLMAdapter Áµ±‰∏Ä‰ªãÈù¢Ë®≠Ë®à\n",
      "‚úì Transformers ÂæåÁ´ØÂØ¶Áèæ (with 4/8-bit quantization)\n",
      "‚úì Messages Ê†ºÂºèËôïÁêÜËàáÊ®°ÊùøÁ≥ªÁµ±\n",
      "‚úì ‰∏≤ÊµÅËº∏Âá∫ÊîØÊè¥ (TextIteratorStreamer)\n",
      "‚úì ‰Ωé VRAM ÂèãÂñÑË®≠Ë®à (device_map, quantization)\n",
      "‚úì ÊïàËÉΩÁõ£ÊéßËàáË®òÊÜ∂È´îËøΩËπ§\n",
      "\n",
      "‚ö†Ô∏è  Key Pitfalls:\n",
      "‚Ä¢ Ê®°ÂûãÂø´ÂèñË∑ØÂæëË®≠ÂÆöÈåØË™§ÊúÉÈáçË§á‰∏ãËºâ\n",
      "‚Ä¢ 4-bit ÈáèÂåñÂèØËÉΩÂΩ±ÈüøËº∏Âá∫ÂìÅË≥™\n",
      "‚Ä¢ ‰∏≤ÊµÅËº∏Âá∫ÈúÄË¶ÅÈ°çÂ§ñÁöÑ threading ËôïÁêÜ\n",
      "‚Ä¢ Chat template ‰∏çÁõ∏ÂÆπÊôÇÈúÄË¶Å fallback\n",
      "\n",
      "üéØ Next Steps (nb03-nb08):\n",
      "‚Ä¢ nb03: llama.cpp ÂæåÁ´Ø (GGUF ÊîØÊè¥)\n",
      "‚Ä¢ nb04: Ollama HTTP ÂæåÁ´Ø\n",
      "‚Ä¢ nb05: Êõ¥Á≤æÁ¥∞ÁöÑ chat template ËôïÁêÜ\n",
      "‚Ä¢ nb06: Âª∂ÈÅ≤„ÄÅthroughput Âü∫Ê∫ñÊ∏¨Ë©¶\n",
      "‚Ä¢ nb07: JSONL Ë≥áÊñôÊ†ºÂºèËàáÈ©óË≠â\n",
      "‚Ä¢ nb08: Ëº∏ÂÖ•/Ëº∏Âá∫ÂÆâÂÖ®Ê™¢Êü•\n",
      "\n",
      "üîÑ Reproducibility Tips:\n",
      "‚Ä¢ Ë®≠ÂÆö AI_CACHE_ROOT Áí∞Â¢ÉËÆäÊï∏ÈÅøÂÖçÈáçË§á‰∏ãËºâ\n",
      "‚Ä¢ ‰ΩøÁî® torch.manual_seed() ‰øùË≠âÂèØÈáçÁèæËº∏Âá∫\n",
      "‚Ä¢ Ë®òÈåÑ VRAM ‰ΩøÁî®Èáè‰ª•‰æøÈô§ÈåØ\n",
      "‚Ä¢ ‰øùÂ≠ò model_info ‰ª•‰æøÁâàÊú¨ËøΩËπ§\n",
      "\n",
      "‚úÖ Ready for Stage 1 integration!\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 10: What We Built & Next Steps\n",
    "# ===========================================\n",
    "\n",
    "print(\"üèóÔ∏è  What We Built in nb02:\")\n",
    "print(\"‚úì LLMAdapter Áµ±‰∏Ä‰ªãÈù¢Ë®≠Ë®à\")\n",
    "print(\"‚úì Transformers ÂæåÁ´ØÂØ¶Áèæ (with 4/8-bit quantization)\")\n",
    "print(\"‚úì Messages Ê†ºÂºèËôïÁêÜËàáÊ®°ÊùøÁ≥ªÁµ±\")\n",
    "print(\"‚úì ‰∏≤ÊµÅËº∏Âá∫ÊîØÊè¥ (TextIteratorStreamer)\")\n",
    "print(\"‚úì ‰Ωé VRAM ÂèãÂñÑË®≠Ë®à (device_map, quantization)\")\n",
    "print(\"‚úì ÊïàËÉΩÁõ£ÊéßËàáË®òÊÜ∂È´îËøΩËπ§\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Key Pitfalls:\")\n",
    "print(\"‚Ä¢ Ê®°ÂûãÂø´ÂèñË∑ØÂæëË®≠ÂÆöÈåØË™§ÊúÉÈáçË§á‰∏ãËºâ\")\n",
    "print(\"‚Ä¢ 4-bit ÈáèÂåñÂèØËÉΩÂΩ±ÈüøËº∏Âá∫ÂìÅË≥™\")\n",
    "print(\"‚Ä¢ ‰∏≤ÊµÅËº∏Âá∫ÈúÄË¶ÅÈ°çÂ§ñÁöÑ threading ËôïÁêÜ\")\n",
    "print(\"‚Ä¢ Chat template ‰∏çÁõ∏ÂÆπÊôÇÈúÄË¶Å fallback\")\n",
    "\n",
    "print(\"\\nüéØ Next Steps (nb03-nb08):\")\n",
    "print(\"‚Ä¢ nb03: llama.cpp ÂæåÁ´Ø (GGUF ÊîØÊè¥)\")\n",
    "print(\"‚Ä¢ nb04: Ollama HTTP ÂæåÁ´Ø\")\n",
    "print(\"‚Ä¢ nb05: Êõ¥Á≤æÁ¥∞ÁöÑ chat template ËôïÁêÜ\")\n",
    "print(\"‚Ä¢ nb06: Âª∂ÈÅ≤„ÄÅthroughput Âü∫Ê∫ñÊ∏¨Ë©¶\")\n",
    "print(\"‚Ä¢ nb07: JSONL Ë≥áÊñôÊ†ºÂºèËàáÈ©óË≠â\")\n",
    "print(\"‚Ä¢ nb08: Ëº∏ÂÖ•/Ëº∏Âá∫ÂÆâÂÖ®Ê™¢Êü•\")\n",
    "\n",
    "print(\"\\nüîÑ Reproducibility Tips:\")\n",
    "print(\"‚Ä¢ Ë®≠ÂÆö AI_CACHE_ROOT Áí∞Â¢ÉËÆäÊï∏ÈÅøÂÖçÈáçË§á‰∏ãËºâ\")\n",
    "print(\"‚Ä¢ ‰ΩøÁî® torch.manual_seed() ‰øùË≠âÂèØÈáçÁèæËº∏Âá∫\")\n",
    "print(\"‚Ä¢ Ë®òÈåÑ VRAM ‰ΩøÁî®Èáè‰ª•‰æøÈô§ÈåØ\")\n",
    "print(\"‚Ä¢ ‰øùÂ≠ò model_info ‰ª•‰æøÁâàÊú¨ËøΩËπ§\")\n",
    "\n",
    "print(\"\\n‚úÖ Ready for Stage 1 integration!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3afbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 11: Save Adapter to Shared Utils (Optional)\n",
    "# ===========================================\n",
    "\n",
    "# Create shared_utils directory structure\n",
    "import os\n",
    "\n",
    "os.makedirs(\"../shared_utils/adapters\", exist_ok=True)\n",
    "\n",
    "# Save the LLMAdapter class for reuse\n",
    "adapter_code = '''\n",
    "\"\"\"\n",
    "LLMAdapter for ragent-text-lab\n",
    "Unified interface for transformers/llama.cpp/ollama backends\n",
    "\"\"\"\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TextIteratorStreamer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from threading import Thread\n",
    "import torch\n",
    "from typing import List, Dict, Generator, Optional, Union\n",
    "\n",
    "class LLMAdapter:\n",
    "    \"\"\"Unified LLM interface supporting multiple backends\"\"\"\n",
    "\n",
    "    def __init__(self, model_id: str, backend: str = \"transformers\", **kwargs):\n",
    "        self.model_id = model_id\n",
    "        self.backend = backend\n",
    "\n",
    "        if backend == \"transformers\":\n",
    "            self._init_transformers(**kwargs)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Backend {backend} not implemented yet\")\n",
    "\n",
    "    def _init_transformers(self, quantization=None, device_map=\"auto\", torch_dtype=\"auto\", **kwargs):\n",
    "        # Implementation from notebook...\n",
    "        pass\n",
    "\n",
    "    def generate(self, messages: List[Dict[str, str]], **kwargs):\n",
    "        # Implementation from notebook...\n",
    "        pass\n",
    "'''\n",
    "\n",
    "with open(\"../shared_utils/adapters/llm_adapter.py\", \"w\") as f:\n",
    "    f.write(adapter_code)\n",
    "\n",
    "print(\"üíæ LLMAdapter saved to shared_utils/adapters/llm_adapter.py\")\n",
    "print(\n",
    "    \"üìù Ready to import in future notebooks: from shared_utils.adapters import LLMAdapter\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
