{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "595e60ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cache] ../ai_warehouse/cache | GPU: True\n"
     ]
    }
   ],
   "source": [
    "# nb03_llm_adapter_llamacpp.ipynb\n",
    "# Stage 1: LLM Adapter - llama.cpp Backend with GGUF\n",
    "\n",
    "# %% [1] Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (Ë§áË£ΩÂà∞ÊØèÊú¨ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6db94c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Missing llama-cpp-python. Install with:\n",
      "pip install llama-cpp-python[server]\n",
      "For GPU support: CMAKE_ARGS='-DLLAMA_CUBLAS=on' pip install llama-cpp-python --force-reinstall --no-cache-dir\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_cpp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpip install llama-cpp-python[server]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor GPU support: CMAKE_ARGS=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-DLLAMA_CUBLAS=on\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m pip install llama-cpp-python --force-reinstall --no-cache-dir\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m     )\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# %% [2] Install and verify llama-cpp-python\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllama_cpp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Llama, LlamaGrammar\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úì llama-cpp-python imported successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'"
     ]
    }
   ],
   "source": [
    "# %% [2] Install and verify llama-cpp-python\n",
    "try:\n",
    "    from llama_cpp import Llama, LlamaGrammar\n",
    "\n",
    "    print(\"‚úì llama-cpp-python imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(\"‚ùå Missing llama-cpp-python. Install with:\")\n",
    "    print(\"pip install llama-cpp-python[server]\")\n",
    "    print(\n",
    "        \"For GPU support: CMAKE_ARGS='-DLLAMA_CUBLAS=on' pip install llama-cpp-python --force-reinstall --no-cache-dir\"\n",
    "    )\n",
    "    raise e\n",
    "\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional, Iterator\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4eb099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [3] GGUF Model Download and Path Management\n",
    "# Setup model cache directory\n",
    "GGUF_CACHE = Path(AI_CACHE_ROOT) / \"gguf_models\"\n",
    "GGUF_CACHE.mkdir(exist_ok=True)\n",
    "\n",
    "# Default model info (modify as needed)\n",
    "DEFAULT_MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct-GGUF\"\n",
    "DEFAULT_GGUF_FILE = \"qwen2.5-7b-instruct-q4_k_m.gguf\"\n",
    "\n",
    "\n",
    "def download_gguf_model(model_id: str, filename: str) -> Path:\n",
    "    \"\"\"Download GGUF model from HuggingFace Hub\"\"\"\n",
    "    from huggingface_hub import hf_hub_download\n",
    "\n",
    "    local_path = GGUF_CACHE / filename\n",
    "    if local_path.exists():\n",
    "        print(f\"‚úì Found cached GGUF: {local_path}\")\n",
    "        return local_path\n",
    "\n",
    "    print(f\"üì• Downloading {model_id}/{filename}...\")\n",
    "    try:\n",
    "        downloaded_path = hf_hub_download(\n",
    "            repo_id=model_id,\n",
    "            filename=filename,\n",
    "            cache_dir=str(GGUF_CACHE),\n",
    "            local_dir=str(GGUF_CACHE),\n",
    "            local_dir_use_symlinks=False,\n",
    "        )\n",
    "        print(f\"‚úì Downloaded to: {downloaded_path}\")\n",
    "        return Path(downloaded_path)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Download failed: {e}\")\n",
    "        print(\"Manual alternatives:\")\n",
    "        print(f\"1. Visit: https://huggingface.co/{model_id}\")\n",
    "        print(f\"2. Download {filename} to {GGUF_CACHE}/\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Auto-download if needed\n",
    "try:\n",
    "    model_path = download_gguf_model(DEFAULT_MODEL_ID, DEFAULT_GGUF_FILE)\n",
    "except:\n",
    "    # Fallback to manual path specification\n",
    "    model_path = GGUF_CACHE / DEFAULT_GGUF_FILE\n",
    "    if not model_path.exists():\n",
    "        print(f\"‚ùå Model not found at {model_path}\")\n",
    "        print(\"Please download manually or check model_id/filename\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79345b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [4] LlamaCppBackend Implementation\n",
    "class LlamaCppBackend:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        n_ctx: int = 4096,\n",
    "        n_gpu_layers: int = -1,  # -1 = all layers on GPU if available\n",
    "        n_threads: Optional[int] = None,\n",
    "        verbose: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Initialize llama.cpp backend with GGUF model\n",
    "\n",
    "        Args:\n",
    "            model_path: Path to GGUF file\n",
    "            n_ctx: Context window size\n",
    "            n_gpu_layers: Number of layers to offload to GPU (-1 = all)\n",
    "            n_threads: CPU threads (None = auto-detect)\n",
    "            verbose: Enable llama.cpp logging\n",
    "        \"\"\"\n",
    "        self.model_path = Path(model_path)\n",
    "        if not self.model_path.exists():\n",
    "            raise FileNotFoundError(f\"GGUF model not found: {model_path}\")\n",
    "\n",
    "        # Auto-detect threads if not specified\n",
    "        if n_threads is None:\n",
    "            import os\n",
    "\n",
    "            n_threads = min(8, os.cpu_count() or 4)\n",
    "\n",
    "        print(f\"ü¶ô Loading GGUF: {self.model_path.name}\")\n",
    "        print(f\"   Context: {n_ctx}, GPU layers: {n_gpu_layers}, Threads: {n_threads}\")\n",
    "\n",
    "        self.llama = Llama(\n",
    "            model_path=str(self.model_path),\n",
    "            n_ctx=n_ctx,\n",
    "            n_gpu_layers=n_gpu_layers,\n",
    "            n_threads=n_threads,\n",
    "            verbose=verbose,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # Store generation defaults\n",
    "        self.default_params = {\n",
    "            \"max_tokens\": 256,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.95,\n",
    "            \"top_k\": 40,\n",
    "            \"repeat_penalty\": 1.1,\n",
    "            \"stop\": [\"<|im_end|>\", \"<|endoftext|>\"],\n",
    "        }\n",
    "\n",
    "        print(\"‚úì llama.cpp model loaded successfully\")\n",
    "\n",
    "    def format_messages(self, messages: List[Dict[str, str]]) -> str:\n",
    "        \"\"\"Convert messages to chat template (Qwen2.5 format)\"\"\"\n",
    "        formatted = \"\"\n",
    "        for msg in messages:\n",
    "            role = msg.get(\"role\", \"user\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "\n",
    "            if role == \"system\":\n",
    "                formatted += f\"<|im_start|>system\\n{content}<|im_end|>\\n\"\n",
    "            elif role == \"user\":\n",
    "                formatted += f\"<|im_start|>user\\n{content}<|im_end|>\\n\"\n",
    "            elif role == \"assistant\":\n",
    "                formatted += f\"<|im_start|>assistant\\n{content}<|im_end|>\\n\"\n",
    "\n",
    "        # Add assistant start token for generation\n",
    "        formatted += \"<|im_start|>assistant\\n\"\n",
    "        return formatted\n",
    "\n",
    "    def generate(\n",
    "        self, messages: List[Dict[str, str]], stream: bool = False, **kwargs\n",
    "    ) -> str:\n",
    "        \"\"\"Generate response from messages\"\"\"\n",
    "        # Merge params\n",
    "        params = {**self.default_params, **kwargs}\n",
    "\n",
    "        # Format prompt\n",
    "        prompt = self.format_messages(messages)\n",
    "\n",
    "        if stream:\n",
    "            return self._generate_stream(prompt, **params)\n",
    "        else:\n",
    "            return self._generate_sync(prompt, **params)\n",
    "\n",
    "    def _generate_sync(self, prompt: str, **params) -> str:\n",
    "        \"\"\"Synchronous generation\"\"\"\n",
    "        output = self.llama(prompt, **params)\n",
    "        return output[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    def _generate_stream(self, prompt: str, **params) -> Iterator[str]:\n",
    "        \"\"\"Streaming generation\"\"\"\n",
    "        stream = self.llama(prompt, stream=True, **params)\n",
    "        for chunk in stream:\n",
    "            if \"choices\" in chunk and len(chunk[\"choices\"]) > 0:\n",
    "                delta = chunk[\"choices\"][0].get(\"text\", \"\")\n",
    "                if delta:\n",
    "                    yield delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fadbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [5] Extended LLMAdapter with llama.cpp Support\n",
    "class LLMAdapter:\n",
    "    \"\"\"Unified LLM interface supporting multiple backends\"\"\"\n",
    "\n",
    "    def __init__(self, model_id: str, backend: str = \"transformers\", **kwargs):\n",
    "        self.model_id = model_id\n",
    "        self.backend = backend\n",
    "        self.model = None\n",
    "\n",
    "        if backend == \"transformers\":\n",
    "            self._init_transformers(**kwargs)\n",
    "        elif backend == \"llama_cpp\":\n",
    "            self._init_llamacpp(**kwargs)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backend: {backend}\")\n",
    "\n",
    "    def _init_transformers(self, **kwargs):\n",
    "        \"\"\"Initialize transformers backend (from nb02)\"\"\"\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id, use_fast=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_id, device_map=\"auto\", torch_dtype=\"auto\", **kwargs\n",
    "        )\n",
    "        print(f\"‚úì Transformers model loaded: {self.model_id}\")\n",
    "\n",
    "    def _init_llamacpp(self, model_path: Optional[str] = None, **kwargs):\n",
    "        \"\"\"Initialize llama.cpp backend\"\"\"\n",
    "        if model_path is None:\n",
    "            # Use the globally downloaded model\n",
    "            model_path = str(model_path) if \"model_path\" in globals() else None\n",
    "\n",
    "        if model_path is None:\n",
    "            raise ValueError(\"model_path required for llama_cpp backend\")\n",
    "\n",
    "        self.model = LlamaCppBackend(model_path, **kwargs)\n",
    "        print(f\"‚úì llama.cpp model loaded: {model_path}\")\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        max_new_tokens: int = 256,\n",
    "        temperature: float = 0.7,\n",
    "        stream: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> str:\n",
    "        \"\"\"Generate response with unified interface\"\"\"\n",
    "\n",
    "        if self.backend == \"transformers\":\n",
    "            return self._generate_transformers(\n",
    "                messages, max_new_tokens, temperature, stream, **kwargs\n",
    "            )\n",
    "        elif self.backend == \"llama_cpp\":\n",
    "            return self._generate_llamacpp(\n",
    "                messages, max_new_tokens, temperature, stream, **kwargs\n",
    "            )\n",
    "\n",
    "    def _generate_transformers(\n",
    "        self, messages, max_new_tokens, temperature, stream, **kwargs\n",
    "    ):\n",
    "        \"\"\"Transformers generation\"\"\"\n",
    "        # Convert messages to prompt (simplified)\n",
    "        prompt = \"\\n\".join(f\"{m['role']}: {m['content']}\" for m in messages)\n",
    "\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        # Decode only the new tokens\n",
    "        new_tokens = outputs[0][inputs.input_ids.shape[1] :]\n",
    "        return self.tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "    def _generate_llamacpp(\n",
    "        self, messages, max_new_tokens, temperature, stream, **kwargs\n",
    "    ):\n",
    "        \"\"\"llama.cpp generation\"\"\"\n",
    "        llama_params = {\n",
    "            \"max_tokens\": max_new_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            **kwargs,\n",
    "        }\n",
    "\n",
    "        if stream:\n",
    "            # Return generator for streaming\n",
    "            return self.model.generate(messages, stream=True, **llama_params)\n",
    "        else:\n",
    "            return self.model.generate(messages, stream=False, **llama_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81d7d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [6] Memory Usage Comparison\n",
    "def measure_memory_usage():\n",
    "    \"\"\"Compare memory usage between backends\"\"\"\n",
    "    import psutil\n",
    "    import torch.cuda as cuda\n",
    "\n",
    "    def get_memory_info():\n",
    "        cpu_mb = psutil.virtual_memory().used / 1024**2\n",
    "        gpu_mb = cuda.memory_allocated() / 1024**2 if cuda.is_available() else 0\n",
    "        return cpu_mb, gpu_mb\n",
    "\n",
    "    print(\"=== Memory Usage Comparison ===\")\n",
    "\n",
    "    # Baseline\n",
    "    cpu_base, gpu_base = get_memory_info()\n",
    "    print(f\"Baseline - CPU: {cpu_base:.0f}MB, GPU: {gpu_base:.0f}MB\")\n",
    "\n",
    "    # Test llama.cpp\n",
    "    if model_path.exists():\n",
    "        print(\"\\nü¶ô Testing llama.cpp (GGUF)...\")\n",
    "        adapter_cpp = LLMAdapter(\n",
    "            model_id=\"\",\n",
    "            backend=\"llama_cpp\",\n",
    "            model_path=str(model_path),\n",
    "            n_gpu_layers=10,  # Limit GPU layers for comparison\n",
    "        )\n",
    "\n",
    "        cpu_cpp, gpu_cpp = get_memory_info()\n",
    "        print(\n",
    "            f\"llama.cpp - CPU: {cpu_cpp:.0f}MB (+{cpu_cpp-cpu_base:.0f}), GPU: {gpu_cpp:.0f}MB (+{gpu_cpp-gpu_base:.0f})\"\n",
    "        )\n",
    "\n",
    "        # Clean up\n",
    "        del adapter_cpp\n",
    "\n",
    "    print(f\"\\nüí° llama.cpp typically uses less VRAM due to quantization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2831c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [7] Streaming vs Non-Streaming Demo\n",
    "def demo_streaming_vs_sync():\n",
    "    \"\"\"Compare streaming vs synchronous generation\"\"\"\n",
    "    if not model_path.exists():\n",
    "        print(\"‚ùå GGUF model not available for demo\")\n",
    "        return\n",
    "\n",
    "    # Initialize adapter\n",
    "    adapter = LLMAdapter(\n",
    "        model_id=\"\",\n",
    "        backend=\"llama_cpp\",\n",
    "        model_path=str(model_path),\n",
    "        n_gpu_layers=-1,  # Use all GPU if available\n",
    "    )\n",
    "\n",
    "    test_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain quantum computing in 2 sentences.\"},\n",
    "    ]\n",
    "\n",
    "    print(\"=== Generation Comparison ===\")\n",
    "\n",
    "    # Synchronous generation\n",
    "    print(\"\\nüîÑ Synchronous generation:\")\n",
    "    start_time = time.time()\n",
    "    sync_response = adapter.generate(\n",
    "        test_messages, max_new_tokens=100, temperature=0.7, stream=False\n",
    "    )\n",
    "    sync_time = time.time() - start_time\n",
    "    print(f\"Response: {sync_response}\")\n",
    "    print(f\"Time: {sync_time:.2f}s\")\n",
    "\n",
    "    # Streaming generation\n",
    "    print(\"\\nüì° Streaming generation:\")\n",
    "    start_time = time.time()\n",
    "    stream_response = \"\"\n",
    "\n",
    "    stream_gen = adapter.generate(\n",
    "        test_messages, max_new_tokens=100, temperature=0.7, stream=True\n",
    "    )\n",
    "\n",
    "    if hasattr(stream_gen, \"__iter__\"):\n",
    "        print(\"Response: \", end=\"\", flush=True)\n",
    "        for chunk in stream_gen:\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "            stream_response += chunk\n",
    "        print()  # New line\n",
    "    else:\n",
    "        stream_response = stream_gen\n",
    "        print(f\"Response: {stream_response}\")\n",
    "\n",
    "    stream_time = time.time() - start_time\n",
    "    print(f\"Time: {stream_time:.2f}s\")\n",
    "\n",
    "    print(f\"\\nüìä Streaming overhead: {stream_time - sync_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a6a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [8] Backend Performance Benchmark\n",
    "def benchmark_backends():\n",
    "    \"\"\"Simple performance comparison\"\"\"\n",
    "    if not model_path.exists():\n",
    "        print(\"‚ùå GGUF model required for benchmark\")\n",
    "        return\n",
    "\n",
    "    test_messages = [{\"role\": \"user\", \"content\": \"What is machine learning?\"}]\n",
    "\n",
    "    print(\"=== Backend Benchmark ===\")\n",
    "\n",
    "    # llama.cpp benchmark\n",
    "    print(\"\\nü¶ô llama.cpp (GGUF Q4_K_M):\")\n",
    "    try:\n",
    "        adapter_cpp = LLMAdapter(\n",
    "            model_id=\"\",\n",
    "            backend=\"llama_cpp\",\n",
    "            model_path=str(model_path),\n",
    "            n_gpu_layers=-1,\n",
    "        )\n",
    "\n",
    "        start = time.time()\n",
    "        response = adapter_cpp.generate(\n",
    "            test_messages, max_new_tokens=50, temperature=0.7\n",
    "        )\n",
    "        latency = time.time() - start\n",
    "\n",
    "        token_count = len(response.split())\n",
    "        tokens_per_sec = token_count / latency if latency > 0 else 0\n",
    "\n",
    "        print(f\"  Latency: {latency:.2f}s\")\n",
    "        print(f\"  Tokens/sec: {tokens_per_sec:.1f}\")\n",
    "        print(f\"  Response: {response[:100]}...\")\n",
    "\n",
    "        del adapter_cpp\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8256b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [9] Smoke Test\n",
    "def smoke_test():\n",
    "    \"\"\"Minimal test to verify llama.cpp adapter works\"\"\"\n",
    "    print(\"=== Smoke Test ===\")\n",
    "\n",
    "    if not model_path.exists():\n",
    "        print(\"‚ùå GGUF model not found - downloading may be required\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        # Initialize adapter\n",
    "        adapter = LLMAdapter(\n",
    "            model_id=\"\",\n",
    "            backend=\"llama_cpp\",\n",
    "            model_path=str(model_path),\n",
    "            n_gpu_layers=5,  # Conservative GPU usage\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        # Test generation\n",
    "        messages = [{\"role\": \"user\", \"content\": \"Say hello!\"}]\n",
    "        response = adapter.generate(messages, max_new_tokens=20, temperature=0.7)\n",
    "\n",
    "        print(f\"‚úì Generation successful: '{response[:50]}...'\")\n",
    "        print(f\"‚úì Backend: {adapter.backend}\")\n",
    "        print(f\"‚úì Model path: {adapter.model.model_path.name}\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Smoke test failed: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b5e899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [10] Run All Tests\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Running nb03 llama.cpp adapter tests...\\n\")\n",
    "\n",
    "    # Core functionality\n",
    "    measure_memory_usage()\n",
    "    demo_streaming_vs_sync()\n",
    "    benchmark_backends()\n",
    "\n",
    "    # Final verification\n",
    "    success = smoke_test()\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"üìã nb03 Status: {'‚úÖ PASSED' if success else '‚ùå FAILED'}\")\n",
    "    print(f\"üìã Backend: llama.cpp with GGUF quantization\")\n",
    "    print(f\"üìã Model: {model_path.name if model_path.exists() else 'Not found'}\")\n",
    "    print(f\"üìã Ready for: nb04 (Ollama backend)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
