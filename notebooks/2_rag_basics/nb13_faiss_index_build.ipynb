{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c26e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb13_faiss_index_build.ipynb\n",
    "# Stage 2: ä¸­æ–‡ RAG åŸºç¤Ž - FAISS ç´¢å¼•å»ºç«‹èˆ‡ç®¡ç†\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (è¤‡è£½åˆ°æ¯æœ¬ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5a7917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 2: Prerequisites & Imports ===\n",
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import time\n",
    "\n",
    "# Check FAISS installation\n",
    "print(f\"FAISS version: {faiss.__version__}\")\n",
    "print(f\"FAISS has GPU support: {hasattr(faiss, 'StandardGpuResources')}\")\n",
    "\n",
    "# Create output directories\n",
    "Path(\"indices\").mkdir(exist_ok=True)\n",
    "Path(\"outs\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8282a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3: Load Demo Data (If Previous Notebooks Not Available) ===\n",
    "def create_demo_chunks_and_embeddings():\n",
    "    \"\"\"Create demo chunks for testing if nb11/nb12 outputs not available\"\"\"\n",
    "    demo_texts = [\n",
    "        \"äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è¨ˆç®—æ©Ÿç§‘å­¸çš„ä¸€å€‹åˆ†æ”¯ï¼Œæ—¨åœ¨å‰µå»ºèƒ½å¤ åŸ·è¡Œé€šå¸¸éœ€è¦äººé¡žæ™ºèƒ½çš„ä»»å‹™çš„æ©Ÿå™¨ã€‚\",\n",
    "        \"æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€å€‹å­é›†ï¼Œå®ƒä½¿ç”¨çµ±è¨ˆæŠ€è¡“ä½¿è¨ˆç®—æ©Ÿèƒ½å¤ å¾žæ•¸æ“šä¸­å­¸ç¿’ã€‚\",\n",
    "        \"æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’çš„ä¸€å€‹åˆ†æ”¯ï¼Œå®ƒä½¿ç”¨å¤šå±¤ç¥žç¶“ç¶²çµ¡ä¾†å»ºæ¨¡å’Œç†è§£è¤‡é›œçš„æ¨¡å¼ã€‚\",\n",
    "        \"è‡ªç„¶èªžè¨€è™•ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€å€‹é ˜åŸŸï¼Œå°ˆæ³¨æ–¼è¨ˆç®—æ©Ÿèˆ‡äººé¡žèªžè¨€ä¹‹é–“çš„äº¤äº’ã€‚\",\n",
    "        \"è¨ˆç®—æ©Ÿè¦–è¦ºæ˜¯ä¸€å€‹è·¨å­¸ç§‘ç§‘å­¸é ˜åŸŸï¼Œæ¶‰åŠå¦‚ä½•ä½¿è¨ˆç®—æ©Ÿå¾žæ•¸å­—åœ–åƒæˆ–è¦–é »ä¸­ç²å¾—é«˜ç´šç†è§£ã€‚\",\n",
    "        \"æª¢ç´¢å¢žå¼·ç”Ÿæˆï¼ˆRAGï¼‰æ˜¯ä¸€ç¨®çµåˆä¿¡æ¯æª¢ç´¢å’Œæ–‡æœ¬ç”Ÿæˆçš„æŠ€è¡“ã€‚\",\n",
    "        \"å‘é‡æ•¸æ“šåº«ç”¨æ–¼å­˜å„²å’Œæª¢ç´¢é«˜ç¶­å‘é‡ï¼Œå¸¸ç”¨æ–¼ç›¸ä¼¼æ€§æœç´¢ã€‚\",\n",
    "        \"FAISSæ˜¯Facebooké–‹ç™¼çš„é«˜æ•ˆç›¸ä¼¼æ€§æœç´¢å’Œèšé¡žåº«ã€‚\",\n",
    "    ]\n",
    "\n",
    "    chunks = []\n",
    "    for i, text in enumerate(demo_texts):\n",
    "        chunks.append(\n",
    "            {\n",
    "                \"id\": i,\n",
    "                \"text\": text,\n",
    "                \"meta\": {\n",
    "                    \"source_id\": f\"demo_doc_{i//3}\",\n",
    "                    \"section\": f\"section_{i%3}\",\n",
    "                    \"page\": i // 4 + 1,\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Try to load from previous notebooks, fallback to demo data\n",
    "try:\n",
    "    with open(\"outs/chunks_with_embeddings.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        chunks_data = json.load(f)\n",
    "    print(f\"âœ“ Loaded {len(chunks_data)} chunks from previous notebooks\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âš  Previous notebook outputs not found, creating demo data...\")\n",
    "    chunks_data = create_demo_chunks_and_embeddings()\n",
    "\n",
    "print(f\"Working with {len(chunks_data)} text chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c43de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4: Load Embedding Model ===\n",
    "print(\"Loading bge-m3 embedding model...\")\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "embedding_model.max_seq_length = 512  # Optimize for memory\n",
    "\n",
    "# Generate embeddings if not available\n",
    "if \"embedding\" not in chunks_data[0]:\n",
    "    print(\"Generating embeddings for chunks...\")\n",
    "    texts = [chunk[\"text\"] for chunk in chunks_data]\n",
    "\n",
    "    # Batch processing for memory efficiency\n",
    "    batch_size = 8\n",
    "    embeddings = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "        batch_embeddings = embedding_model.encode(\n",
    "            batch_texts, normalize_embeddings=True, show_progress_bar=True\n",
    "        )\n",
    "        embeddings.extend(batch_embeddings)\n",
    "\n",
    "    # Add embeddings to chunks\n",
    "    for i, chunk in enumerate(chunks_data):\n",
    "        chunk[\"embedding\"] = embeddings[i].tolist()\n",
    "\n",
    "    # Save updated chunks\n",
    "    with open(\"outs/chunks_with_embeddings.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunks_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ“ Embeddings ready, dimension: {len(chunks_data[0]['embedding'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba80c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5: FAISS Index Creation ===\n",
    "class FAISSIndexManager:\n",
    "    \"\"\"Manages FAISS index creation, saving, loading, and searching\"\"\"\n",
    "\n",
    "    def __init__(self, dimension: int, metric: str = \"ip\"):\n",
    "        self.dimension = dimension\n",
    "        self.metric = metric.lower()\n",
    "        self.index = None\n",
    "        self.id_to_chunk = {}\n",
    "\n",
    "    def create_index(self, embeddings: np.ndarray, chunk_ids: List[int]):\n",
    "        \"\"\"Create FAISS index from embeddings\"\"\"\n",
    "        if self.metric == \"ip\":\n",
    "            # Inner Product (for normalized embeddings)\n",
    "            self.index = faiss.IndexFlatIP(self.dimension)\n",
    "        elif self.metric == \"l2\":\n",
    "            # L2 distance\n",
    "            self.index = faiss.IndexFlatL2(self.dimension)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric: {self.metric}\")\n",
    "\n",
    "        # Add embeddings to index\n",
    "        self.index.add(embeddings.astype(np.float32))\n",
    "        print(f\"âœ“ Created FAISS index with {self.index.ntotal} vectors\")\n",
    "\n",
    "        # Store chunk ID mapping\n",
    "        for i, chunk_id in enumerate(chunk_ids):\n",
    "            self.id_to_chunk[i] = chunk_id\n",
    "\n",
    "    def save_index(self, index_path: str, mapping_path: str):\n",
    "        \"\"\"Save index and ID mapping to disk\"\"\"\n",
    "        faiss.write_index(self.index, index_path)\n",
    "        with open(mapping_path, \"wb\") as f:\n",
    "            pickle.dump(self.id_to_chunk, f)\n",
    "        print(f\"âœ“ Saved index to {index_path} and mapping to {mapping_path}\")\n",
    "\n",
    "    def load_index(self, index_path: str, mapping_path: str):\n",
    "        \"\"\"Load index and ID mapping from disk\"\"\"\n",
    "        self.index = faiss.read_index(index_path)\n",
    "        with open(mapping_path, \"rb\") as f:\n",
    "            self.id_to_chunk = pickle.load(f)\n",
    "        print(f\"âœ“ Loaded index with {self.index.ntotal} vectors\")\n",
    "\n",
    "    def search(\n",
    "        self, query_embeddings: np.ndarray, k: int = 5\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Search for similar vectors\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not created or loaded\")\n",
    "\n",
    "        distances, indices = self.index.search(query_embeddings.astype(np.float32), k)\n",
    "        return distances, indices\n",
    "\n",
    "    def add_vectors(self, new_embeddings: np.ndarray, new_chunk_ids: List[int]):\n",
    "        \"\"\"Add new vectors to existing index (incremental update)\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not created or loaded\")\n",
    "\n",
    "        start_idx = self.index.ntotal\n",
    "        self.index.add(new_embeddings.astype(np.float32))\n",
    "\n",
    "        # Update ID mapping\n",
    "        for i, chunk_id in enumerate(new_chunk_ids):\n",
    "            self.id_to_chunk[start_idx + i] = chunk_id\n",
    "\n",
    "        print(f\"âœ“ Added {len(new_chunk_ids)} vectors, total: {self.index.ntotal}\")\n",
    "\n",
    "\n",
    "# Create embeddings matrix\n",
    "embeddings_matrix = np.array([chunk[\"embedding\"] for chunk in chunks_data])\n",
    "chunk_ids = [chunk[\"id\"] for chunk in chunks_data]\n",
    "\n",
    "print(f\"Embeddings matrix shape: {embeddings_matrix.shape}\")\n",
    "print(f\"Embedding dimension: {embeddings_matrix.shape[1]}\")\n",
    "\n",
    "# Initialize and create index\n",
    "index_manager = FAISSIndexManager(dimension=embeddings_matrix.shape[1], metric=\"ip\")\n",
    "index_manager.create_index(embeddings_matrix, chunk_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044433b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 6: Save Index and Chunks Metadata ===\n",
    "# Save FAISS index and mappings\n",
    "index_path = \"indices/general.faiss\"\n",
    "mapping_path = \"indices/id_mapping.pkl\"\n",
    "chunks_path = \"indices/chunks.jsonl\"\n",
    "\n",
    "index_manager.save_index(index_path, mapping_path)\n",
    "\n",
    "# Save chunks as JSONL for easy access\n",
    "with open(chunks_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for chunk in chunks_data:\n",
    "        # Remove embedding from JSONL to save space\n",
    "        chunk_no_embed = {k: v for k, v in chunk.items() if k != \"embedding\"}\n",
    "        f.write(json.dumps(chunk_no_embed, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"âœ“ Saved {len(chunks_data)} chunks metadata to {chunks_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9f661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 7: Load and Validate Index ===\n",
    "def load_chunks_from_jsonl(jsonl_path: str) -> Dict[int, Dict]:\n",
    "    \"\"\"Load chunks from JSONL file\"\"\"\n",
    "    chunks = {}\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            chunk = json.loads(line.strip())\n",
    "            chunks[chunk[\"id\"]] = chunk\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Test loading\n",
    "test_index_manager = FAISSIndexManager(dimension=embeddings_matrix.shape[1])\n",
    "test_index_manager.load_index(index_path, mapping_path)\n",
    "\n",
    "# Load chunks metadata\n",
    "chunks_lookup = load_chunks_from_jsonl(chunks_path)\n",
    "print(f\"âœ“ Loaded index with {test_index_manager.index.ntotal} vectors\")\n",
    "print(f\"âœ“ Loaded {len(chunks_lookup)} chunks metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21cf312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 8: Search Functionality ===\n",
    "def search_similar_chunks(query: str, k: int = 3) -> List[Dict]:\n",
    "    \"\"\"Search for similar chunks given a text query\"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = embedding_model.encode([query], normalize_embeddings=True)\n",
    "\n",
    "    # Search in index\n",
    "    distances, indices = test_index_manager.search(query_embedding, k)\n",
    "\n",
    "    results = []\n",
    "    for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "        if idx == -1:  # No more results\n",
    "            break\n",
    "\n",
    "        chunk_id = test_index_manager.id_to_chunk[idx]\n",
    "        chunk = chunks_lookup[chunk_id]\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"rank\": i + 1,\n",
    "                \"score\": float(dist),\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"text\": chunk[\"text\"],\n",
    "                \"meta\": chunk[\"meta\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Test search functionality\n",
    "test_query = \"ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ\"\n",
    "results = search_similar_chunks(test_query, k=3)\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(\"-\" * 50)\n",
    "for result in results:\n",
    "    print(f\"Rank {result['rank']} (Score: {result['score']:.4f})\")\n",
    "    print(f\"Text: {result['text'][:100]}...\")\n",
    "    print(f\"Source: {result['meta']['source_id']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351f0b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 9: Incremental Update Demo ===\n",
    "def add_new_chunks_to_index(new_texts: List[str], start_id: int = None):\n",
    "    \"\"\"Add new chunks to existing index\"\"\"\n",
    "    if start_id is None:\n",
    "        start_id = max(chunks_lookup.keys()) + 1\n",
    "\n",
    "    # Create new chunks\n",
    "    new_chunks = []\n",
    "    for i, text in enumerate(new_texts):\n",
    "        chunk_id = start_id + i\n",
    "        new_chunks.append(\n",
    "            {\n",
    "                \"id\": chunk_id,\n",
    "                \"text\": text,\n",
    "                \"meta\": {\"source_id\": f\"new_doc_{i}\", \"section\": \"intro\", \"page\": 1},\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Generate embeddings for new chunks\n",
    "    new_embeddings = embedding_model.encode(new_texts, normalize_embeddings=True)\n",
    "\n",
    "    # Add to index\n",
    "    new_chunk_ids = [chunk[\"id\"] for chunk in new_chunks]\n",
    "    test_index_manager.add_vectors(new_embeddings, new_chunk_ids)\n",
    "\n",
    "    # Update chunks lookup\n",
    "    for chunk in new_chunks:\n",
    "        chunks_lookup[chunk[\"id\"]] = chunk\n",
    "\n",
    "    # Append to JSONL file\n",
    "    with open(chunks_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for chunk in new_chunks:\n",
    "            f.write(json.dumps(chunk, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    return new_chunks\n",
    "\n",
    "\n",
    "# Demo incremental update\n",
    "new_texts = [\n",
    "    \"å¤§èªžè¨€æ¨¡åž‹ï¼ˆLLMï¼‰æ˜¯ä½¿ç”¨æ·±åº¦å­¸ç¿’æŠ€è¡“è¨“ç·´çš„å¤§è¦æ¨¡ç¥žç¶“ç¶²çµ¡æ¨¡åž‹ã€‚\",\n",
    "    \"RAG ç³»çµ±çµåˆäº†æª¢ç´¢å’Œç”Ÿæˆï¼Œèƒ½å¤ æä¾›æ›´æº–ç¢ºçš„ç­”æ¡ˆã€‚\",\n",
    "]\n",
    "\n",
    "new_chunks = add_new_chunks_to_index(new_texts)\n",
    "print(f\"âœ“ Added {len(new_chunks)} new chunks to index\")\n",
    "\n",
    "# Test search with new chunks\n",
    "results = search_similar_chunks(\"å¤§èªžè¨€æ¨¡åž‹æ˜¯ä»€éº¼ï¼Ÿ\", k=3)\n",
    "print(\"\\nSearch results after incremental update:\")\n",
    "for result in results:\n",
    "    print(\n",
    "        f\"Rank {result['rank']}: {result['text'][:80]}... (Score: {result['score']:.4f})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858bd5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 10: Performance Analysis ===\n",
    "def analyze_index_performance():\n",
    "    \"\"\"Analyze index size and search performance\"\"\"\n",
    "    import psutil\n",
    "    import os\n",
    "\n",
    "    # Index file size\n",
    "    index_size_mb = os.path.getsize(index_path) / (1024 * 1024)\n",
    "\n",
    "    # Memory usage\n",
    "    process = psutil.Process()\n",
    "    memory_mb = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "    print(\"=== Index Performance Analysis ===\")\n",
    "    print(f\"Index file size: {index_size_mb:.2f} MB\")\n",
    "    print(f\"Current memory usage: {memory_mb:.2f} MB\")\n",
    "    print(f\"Vectors in index: {test_index_manager.index.ntotal}\")\n",
    "    print(f\"Vector dimension: {test_index_manager.dimension}\")\n",
    "\n",
    "    # Search latency test\n",
    "    test_queries = [\n",
    "        \"äººå·¥æ™ºèƒ½çš„æ‡‰ç”¨\",\n",
    "        \"æ©Ÿå™¨å­¸ç¿’ç®—æ³•\",\n",
    "        \"æ·±åº¦å­¸ç¿’ç¶²çµ¡\",\n",
    "        \"è‡ªç„¶èªžè¨€è™•ç†\",\n",
    "        \"å‘é‡æ•¸æ“šåº«\",\n",
    "    ]\n",
    "\n",
    "    latencies = []\n",
    "    for query in test_queries:\n",
    "        start_time = time.time()\n",
    "        _ = search_similar_chunks(query, k=5)\n",
    "        latency = (time.time() - start_time) * 1000  # ms\n",
    "        latencies.append(latency)\n",
    "\n",
    "    avg_latency = np.mean(latencies)\n",
    "    p95_latency = np.percentile(latencies, 95)\n",
    "\n",
    "    print(f\"\\nSearch Performance:\")\n",
    "    print(f\"Average latency: {avg_latency:.2f} ms\")\n",
    "    print(f\"P95 latency: {p95_latency:.2f} ms\")\n",
    "    print(f\"Queries per second: {1000/avg_latency:.1f}\")\n",
    "\n",
    "    return {\n",
    "        \"index_size_mb\": index_size_mb,\n",
    "        \"memory_mb\": memory_mb,\n",
    "        \"avg_latency_ms\": avg_latency,\n",
    "        \"p95_latency_ms\": p95_latency,\n",
    "    }\n",
    "\n",
    "\n",
    "perf_metrics = analyze_index_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0da76df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 11: Advanced Index Configuration ===\n",
    "def create_optimized_index(embeddings: np.ndarray, index_type: str = \"flat\"):\n",
    "    \"\"\"Create different types of FAISS indexes for comparison\"\"\"\n",
    "    dimension = embeddings.shape[1]\n",
    "\n",
    "    if index_type == \"flat\":\n",
    "        # Standard flat index (exact search)\n",
    "        index = faiss.IndexFlatIP(dimension)\n",
    "    elif index_type == \"ivf\":\n",
    "        # IVF index for faster approximate search\n",
    "        nlist = min(100, embeddings.shape[0] // 10)  # Number of clusters\n",
    "        quantizer = faiss.IndexFlatIP(dimension)\n",
    "        index = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n",
    "\n",
    "        # Train the index\n",
    "        index.train(embeddings.astype(np.float32))\n",
    "        index.nprobe = min(10, nlist)  # Search in top 10 clusters\n",
    "    elif index_type == \"hnsw\":\n",
    "        # HNSW index for very fast approximate search\n",
    "        M = 16  # Number of connections\n",
    "        index = faiss.IndexHNSWFlat(dimension, M)\n",
    "        index.hnsw.efConstruction = 200\n",
    "        index.hnsw.efSearch = 100\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported index type: {index_type}\")\n",
    "\n",
    "    index.add(embeddings.astype(np.float32))\n",
    "    return index\n",
    "\n",
    "\n",
    "# Compare different index types\n",
    "print(\"=== Index Type Comparison ===\")\n",
    "for idx_type in [\"flat\", \"ivf\"]:  # Skip HNSW for small datasets\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        test_index = create_optimized_index(embeddings_matrix, idx_type)\n",
    "        creation_time = time.time() - start_time\n",
    "\n",
    "        # Test search speed\n",
    "        query_emb = embedding_model.encode([\"æ¸¬è©¦æŸ¥è©¢\"], normalize_embeddings=True)\n",
    "        start_time = time.time()\n",
    "        _, _ = test_index.search(query_emb.astype(np.float32), 5)\n",
    "        search_time = (time.time() - start_time) * 1000\n",
    "\n",
    "        print(\n",
    "            f\"{idx_type.upper()}: Creation={creation_time:.3f}s, Search={search_time:.2f}ms\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"{idx_type.upper()}: Failed - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a1ac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 12: Smoke Test ===\n",
    "def smoke_test():\n",
    "    \"\"\"Comprehensive smoke test for FAISS index functionality\"\"\"\n",
    "    print(\"=== FAISS Index Smoke Test ===\")\n",
    "\n",
    "    tests_passed = 0\n",
    "    total_tests = 6\n",
    "\n",
    "    # Test 1: Index creation\n",
    "    try:\n",
    "        test_mgr = FAISSIndexManager(dimension=1024)\n",
    "        dummy_embeddings = np.random.rand(10, 1024).astype(np.float32)\n",
    "        test_mgr.create_index(dummy_embeddings, list(range(10)))\n",
    "        assert test_mgr.index.ntotal == 10\n",
    "        print(\"âœ“ Test 1: Index creation - PASSED\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Test 1: Index creation - FAILED ({e})\")\n",
    "\n",
    "    # Test 2: Save/Load\n",
    "    try:\n",
    "        test_path = \"outs/test_index.faiss\"\n",
    "        test_mapping = \"outs/test_mapping.pkl\"\n",
    "        test_mgr.save_index(test_path, test_mapping)\n",
    "\n",
    "        load_mgr = FAISSIndexManager(dimension=1024)\n",
    "        load_mgr.load_index(test_path, test_mapping)\n",
    "        assert load_mgr.index.ntotal == 10\n",
    "        print(\"âœ“ Test 2: Save/Load - PASSED\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Test 2: Save/Load - FAILED ({e})\")\n",
    "\n",
    "    # Test 3: Search functionality\n",
    "    try:\n",
    "        query = np.random.rand(1, 1024).astype(np.float32)\n",
    "        distances, indices = load_mgr.search(query, k=3)\n",
    "        assert len(distances[0]) == 3\n",
    "        assert len(indices[0]) == 3\n",
    "        print(\"âœ“ Test 3: Search - PASSED\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Test 3: Search - FAILED ({e})\")\n",
    "\n",
    "    # Test 4: Incremental update\n",
    "    try:\n",
    "        new_embeddings = np.random.rand(3, 1024).astype(np.float32)\n",
    "        load_mgr.add_vectors(new_embeddings, [10, 11, 12])\n",
    "        assert load_mgr.index.ntotal == 13\n",
    "        print(\"âœ“ Test 4: Incremental update - PASSED\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Test 4: Incremental update - FAILED ({e})\")\n",
    "\n",
    "    # Test 5: Real embedding search\n",
    "    try:\n",
    "        results = search_similar_chunks(\"æ¸¬è©¦æŸ¥è©¢\", k=2)\n",
    "        assert len(results) >= 1\n",
    "        assert \"text\" in results[0]\n",
    "        assert \"score\" in results[0]\n",
    "        print(\"âœ“ Test 5: Real embedding search - PASSED\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Test 5: Real embedding search - FAILED ({e})\")\n",
    "\n",
    "    # Test 6: Index file exists and is readable\n",
    "    try:\n",
    "        assert os.path.exists(index_path)\n",
    "        test_index = faiss.read_index(index_path)\n",
    "        assert test_index.ntotal > 0\n",
    "        print(\"âœ“ Test 6: Index file validation - PASSED\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Test 6: Index file validation - FAILED ({e})\")\n",
    "\n",
    "    print(f\"\\n=== Smoke Test Results: {tests_passed}/{total_tests} PASSED ===\")\n",
    "\n",
    "    if tests_passed == total_tests:\n",
    "        print(\"ðŸŽ‰ All tests passed! FAISS index is working correctly.\")\n",
    "    else:\n",
    "        print(\"âš  Some tests failed. Check the implementation.\")\n",
    "\n",
    "    return tests_passed == total_tests\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_passed = smoke_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc936dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 13: Summary and Next Steps ===\n",
    "print(\"=== nb13 Summary: FAISS Index Build ===\")\n",
    "print(\"\\nâœ… Completed:\")\n",
    "print(\"- FAISS IndexFlatIP creation with normalized embeddings\")\n",
    "print(\"- Index persistence (save/load) with ID mapping\")\n",
    "print(\"- Incremental vector addition capability\")\n",
    "print(\"- Search functionality with similarity scoring\")\n",
    "print(\"- Performance analysis and optimization options\")\n",
    "print(\"- Comprehensive smoke testing\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Key Metrics:\")\n",
    "print(f\"- Index size: {perf_metrics['index_size_mb']:.2f} MB\")\n",
    "print(f\"- Average search latency: {perf_metrics['avg_latency_ms']:.2f} ms\")\n",
    "print(f\"- Total vectors: {test_index_manager.index.ntotal}\")\n",
    "\n",
    "print(\"\\nðŸ”§ Key Parameters (Low-VRAM friendly):\")\n",
    "print(\"- batch_size=8 for embedding generation\")\n",
    "print(\"- normalize_embeddings=True for IP metric\")\n",
    "print(\"- max_seq_length=512 for memory efficiency\")\n",
    "\n",
    "print(\"\\nâš  Pitfalls:\")\n",
    "print(\"- Always normalize embeddings when using Inner Product\")\n",
    "print(\"- Index ID mapping is separate from chunk metadata\")\n",
    "print(\"- Incremental updates require careful ID management\")\n",
    "print(\"- Large indexes may need IVF/HNSW for speed\")\n",
    "\n",
    "print(\"\\nðŸš€ Next Steps:\")\n",
    "print(\"- nb14: Query processing and citation formatting\")\n",
    "print(\"- nb15: BGE reranker integration\")\n",
    "print(\"- nb16: Context optimization and token budgeting\")\n",
    "\n",
    "print(f\"\\nðŸ“ Generated Files:\")\n",
    "print(f\"- {index_path} (FAISS index)\")\n",
    "print(f\"- {mapping_path} (ID mapping)\")\n",
    "print(f\"- {chunks_path} (chunks metadata)\")\n",
    "print(\"- outs/chunks_with_embeddings.json (full chunks)\")\n",
    "\n",
    "# Final validation\n",
    "if smoke_test_passed:\n",
    "    print(\"\\nðŸŽ¯ Ready for next notebook: nb14_query_and_citations.ipynb\")\n",
    "else:\n",
    "    print(\"\\nâŒ Please fix failing tests before proceeding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5478f16",
   "metadata": {},
   "source": [
    "Smoke Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d7870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick smoke test - run this to verify everything works\n",
    "query_test = \"äººå·¥æ™ºèƒ½æŠ€è¡“\"\n",
    "results = search_similar_chunks(query_test, k=2)\n",
    "assert len(results) >= 1, \"Search should return results\"\n",
    "assert results[0][\"score\"] > 0, \"Similarity score should be positive\"\n",
    "assert test_index_manager.index.ntotal >= len(\n",
    "    chunks_data\n",
    "), \"Index should contain all chunks\"\n",
    "print(f\"âœ… Smoke test passed! Found {len(results)} results for '{query_test}'\")\n",
    "print(f\"ðŸ“Š Index contains {test_index_manager.index.ntotal} vectors\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
