{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47851b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context Optimization for RAG Systems\n",
    "# Stage 2, Notebook 16: 上下文優化 - 去重/MMR/Token Budget\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (複製到每本 notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c46d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 2: Load Dependencies and Existing Index\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "from rapidfuzz import fuzz\n",
    "import tiktoken\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load embedding model\n",
    "print(\"Loading BGE-M3 model...\")\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-m3\",\n",
    "                                     cache_folder=f\"{AI_CACHE_ROOT}/hf\")\n",
    "embed_dim = embedding_model.get_sentence_embedding_dimension()\n",
    "print(f\"Embedding dimension: {embed_dim}\")\n",
    "\n",
    "# Load or create sample index and chunks\n",
    "index_path = \"indices/general.faiss\"\n",
    "chunks_path = \"indices/chunks.jsonl\"\n",
    "\n",
    "if Path(index_path).exists() and Path(chunks_path).exists():\n",
    "    # Load existing index\n",
    "    index = faiss.read_index(index_path)\n",
    "    chunks = []\n",
    "    with open(chunks_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            chunks.append(json.loads(line))\n",
    "    print(f\"Loaded {len(chunks)} chunks and FAISS index\")\n",
    "else:\n",
    "    # Create sample data for demonstration\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    sample_texts = [\n",
    "        \"人工智慧是電腦科學的一個分支，致力於創建能夠執行通常需要人類智慧的任務的系統。\",\n",
    "        \"機器學習是人工智慧的子領域，使用演算法和統計模型來使電腦系統能夠改善其性能。\",\n",
    "        \"深度學習是機器學習的一種方法，使用具有多層的人工神經網路。\",\n",
    "        \"自然語言處理（NLP）是人工智慧的一個領域，專注於電腦與人類語言之間的互動。\",\n",
    "        \"電腦視覺是人工智慧的另一個重要分支，使機器能夠解釋和理解視覺世界。\",\n",
    "        \"人工智慧技術廣泛應用於醫療、金融、交通等各個領域。\",\n",
    "        \"機器學習演算法包括監督學習、無監督學習和強化學習等不同類型。\",\n",
    "        \"深度學習網路包含輸入層、隱藏層和輸出層，能夠學習複雜的模式。\",\n",
    "        \"自然語言處理技術包括文本分類、情感分析、機器翻譯等應用。\",\n",
    "        \"電腦視覺應用包括圖像識別、物體檢測、人臉識別等技術。\"\n",
    "    ]\n",
    "\n",
    "    # Create embeddings\n",
    "    embeddings = embedding_model.encode(sample_texts, normalize_embeddings=True)\n",
    "    embeddings = embeddings.astype(np.float32)\n",
    "\n",
    "    # Create FAISS index\n",
    "    index = faiss.IndexFlatIP(embed_dim)\n",
    "    index.add(embeddings)\n",
    "\n",
    "    # Create chunks data\n",
    "    chunks = []\n",
    "    for i, text in enumerate(sample_texts):\n",
    "        chunks.append({\n",
    "            \"id\": i,\n",
    "            \"text\": text,\n",
    "            \"meta\": {\"source_id\": f\"sample_{i}\", \"page\": 1}\n",
    "        })\n",
    "\n",
    "    # Save for reuse\n",
    "    Path(\"indices\").mkdir(exist_ok=True)\n",
    "    faiss.write_index(index, index_path)\n",
    "    with open(chunks_path, 'w', encoding='utf-8') as f:\n",
    "        for chunk in chunks:\n",
    "            f.write(json.dumps(chunk, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\"Created {len(chunks)} sample chunks\")\n",
    "\n",
    "# Initialize tokenizer for token counting\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # GPT-3.5/4 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bc9106",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 3: Duplicate Removal Implementation\n",
    "@dataclass\n",
    "class RetrievalResult:\n",
    "    \"\"\"檢索結果數據結構\"\"\"\n",
    "    text: str\n",
    "    meta: Dict[str, Any]\n",
    "    score: float\n",
    "    chunk_id: int\n",
    "\n",
    "def remove_duplicates(results: List[RetrievalResult],\n",
    "                     similarity_threshold: float = 0.85) -> List[RetrievalResult]:\n",
    "    \"\"\"\n",
    "    Remove duplicate or highly similar chunks using rapidfuzz\n",
    "\n",
    "    Args:\n",
    "        results: List of retrieval results\n",
    "        similarity_threshold: Similarity threshold (0-1), above which chunks are considered duplicates\n",
    "\n",
    "    Returns:\n",
    "        Deduplicated list of results\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return results\n",
    "\n",
    "    deduplicated = []\n",
    "\n",
    "    for current in results:\n",
    "        is_duplicate = False\n",
    "\n",
    "        # Check against already accepted chunks\n",
    "        for accepted in deduplicated:\n",
    "            # Use token_sort_ratio for better handling of word order differences\n",
    "            similarity = fuzz.token_sort_ratio(current.text, accepted.text) / 100.0\n",
    "\n",
    "            if similarity > similarity_threshold:\n",
    "                is_duplicate = True\n",
    "                # Keep the one with higher score\n",
    "                if current.score > accepted.score:\n",
    "                    deduplicated.remove(accepted)\n",
    "                    deduplicated.append(current)\n",
    "                break\n",
    "\n",
    "        if not is_duplicate:\n",
    "            deduplicated.append(current)\n",
    "\n",
    "    print(f\"Deduplication: {len(results)} -> {len(deduplicated)} chunks \"\n",
    "          f\"(removed {len(results) - len(deduplicated)} duplicates)\")\n",
    "\n",
    "    return deduplicated\n",
    "\n",
    "# Test deduplication\n",
    "def test_deduplication():\n",
    "    \"\"\"Test the deduplication function\"\"\"\n",
    "    test_results = [\n",
    "        RetrievalResult(\"人工智慧是電腦科學的分支\", {\"source\": \"doc1\"}, 0.9, 0),\n",
    "        RetrievalResult(\"人工智能是計算機科學的分支\", {\"source\": \"doc2\"}, 0.8, 1),  # Similar\n",
    "        RetrievalResult(\"機器學習是AI的子領域\", {\"source\": \"doc3\"}, 0.7, 2),\n",
    "        RetrievalResult(\"深度學習使用神經網路\", {\"source\": \"doc4\"}, 0.6, 3),\n",
    "    ]\n",
    "\n",
    "    print(\"Original results:\")\n",
    "    for i, result in enumerate(test_results):\n",
    "        print(f\"  {i}: {result.text[:30]}... (score: {result.score})\")\n",
    "\n",
    "    deduplicated = remove_duplicates(test_results, similarity_threshold=0.85)\n",
    "\n",
    "    print(\"\\nAfter deduplication:\")\n",
    "    for i, result in enumerate(deduplicated):\n",
    "        print(f\"  {i}: {result.text[:30]}... (score: {result.score})\")\n",
    "\n",
    "test_deduplication()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a5d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 4: MMR (Maximal Marginal Relevance) Implementation\n",
    "def compute_mmr(query_embedding: np.ndarray,\n",
    "                doc_embeddings: np.ndarray,\n",
    "                selected_indices: List[int],\n",
    "                lambda_param: float = 0.5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute MMR scores for remaining documents\n",
    "\n",
    "    Args:\n",
    "        query_embedding: Query vector (1, dim)\n",
    "        doc_embeddings: Document vectors (n, dim)\n",
    "        selected_indices: Already selected document indices\n",
    "        lambda_param: Balance between relevance (1.0) and diversity (0.0)\n",
    "\n",
    "    Returns:\n",
    "        MMR scores for all documents\n",
    "    \"\"\"\n",
    "    # Relevance scores (cosine similarity with query)\n",
    "    relevance_scores = np.dot(doc_embeddings, query_embedding.T).flatten()\n",
    "\n",
    "    if not selected_indices:\n",
    "        # If no documents selected yet, return pure relevance\n",
    "        return relevance_scores\n",
    "\n",
    "    # Diversity scores (max similarity with already selected documents)\n",
    "    selected_embeddings = doc_embeddings[selected_indices]\n",
    "    similarity_matrix = np.dot(doc_embeddings, selected_embeddings.T)\n",
    "    max_similarity = np.max(similarity_matrix, axis=1)\n",
    "\n",
    "    # MMR formula: λ * relevance - (1-λ) * max_similarity\n",
    "    mmr_scores = lambda_param * relevance_scores - (1 - lambda_param) * max_similarity\n",
    "\n",
    "    return mmr_scores\n",
    "\n",
    "def mmr_rerank(results: List[RetrievalResult],\n",
    "               query: str,\n",
    "               k: int = 5,\n",
    "               lambda_param: float = 0.5) -> List[RetrievalResult]:\n",
    "    \"\"\"\n",
    "    Rerank results using MMR to balance relevance and diversity\n",
    "\n",
    "    Args:\n",
    "        results: Initial retrieval results\n",
    "        query: Original query text\n",
    "        k: Number of results to return\n",
    "        lambda_param: Balance parameter (1.0=pure relevance, 0.0=pure diversity)\n",
    "\n",
    "    Returns:\n",
    "        MMR-reranked results\n",
    "    \"\"\"\n",
    "    if len(results) <= k:\n",
    "        return results\n",
    "\n",
    "    # Get embeddings for query and all documents\n",
    "    query_embedding = embedding_model.encode([query], normalize_embeddings=True)\n",
    "    doc_texts = [r.text for r in results]\n",
    "    doc_embeddings = embedding_model.encode(doc_texts, normalize_embeddings=True)\n",
    "\n",
    "    selected_indices = []\n",
    "    remaining_indices = list(range(len(results)))\n",
    "\n",
    "    for _ in range(min(k, len(results))):\n",
    "        if not remaining_indices:\n",
    "            break\n",
    "\n",
    "        # Compute MMR scores for remaining documents\n",
    "        mmr_scores = compute_mmr(query_embedding, doc_embeddings,\n",
    "                               selected_indices, lambda_param)\n",
    "\n",
    "        # Find best remaining document\n",
    "        best_idx = None\n",
    "        best_score = float('-inf')\n",
    "\n",
    "        for idx in remaining_indices:\n",
    "            if mmr_scores[idx] > best_score:\n",
    "                best_score = mmr_scores[idx]\n",
    "                best_idx = idx\n",
    "\n",
    "        # Move from remaining to selected\n",
    "        selected_indices.append(best_idx)\n",
    "        remaining_indices.remove(best_idx)\n",
    "\n",
    "    # Return reranked results\n",
    "    mmr_results = [results[i] for i in selected_indices]\n",
    "\n",
    "    print(f\"MMR reranking: {len(results)} -> {len(mmr_results)} chunks \"\n",
    "          f\"(λ={lambda_param})\")\n",
    "\n",
    "    return mmr_results\n",
    "\n",
    "# Test MMR\n",
    "def test_mmr():\n",
    "    \"\"\"Test MMR functionality\"\"\"\n",
    "    # Create test query and results\n",
    "    query = \"人工智慧機器學習\"\n",
    "\n",
    "    # Simulate retrieval results\n",
    "    test_results = []\n",
    "    for i, chunk in enumerate(chunks[:8]):  # Use first 8 chunks\n",
    "        # Simulate similarity scores\n",
    "        score = 0.9 - i * 0.1  # Decreasing scores\n",
    "        test_results.append(RetrievalResult(\n",
    "            text=chunk[\"text\"],\n",
    "            meta=chunk[\"meta\"],\n",
    "            score=score,\n",
    "            chunk_id=chunk[\"id\"]\n",
    "        ))\n",
    "\n",
    "    print(\"Original results (by score):\")\n",
    "    for i, result in enumerate(test_results):\n",
    "        print(f\"  {i}: {result.text[:50]}... (score: {result.score:.2f})\")\n",
    "\n",
    "    # Apply MMR with different lambda values\n",
    "    for lambda_val in [0.3, 0.5, 0.8]:\n",
    "        print(f\"\\nMMR results (λ={lambda_val}):\")\n",
    "        mmr_results = mmr_rerank(test_results, query, k=5, lambda_param=lambda_val)\n",
    "        for i, result in enumerate(mmr_results):\n",
    "            print(f\"  {i}: {result.text[:50]}... (score: {result.score:.2f})\")\n",
    "\n",
    "test_mmr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd116ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 5: Token Budget Management\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Count tokens in text using tiktoken\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def fit_token_budget(results: List[RetrievalResult],\n",
    "                    max_tokens: int = 3000,\n",
    "                    reserve_tokens: int = 500) -> List[RetrievalResult]:\n",
    "    \"\"\"\n",
    "    Trim results to fit within token budget\n",
    "\n",
    "    Args:\n",
    "        results: Retrieval results to trim\n",
    "        max_tokens: Maximum total tokens for context\n",
    "        reserve_tokens: Tokens to reserve for system/user messages\n",
    "\n",
    "    Returns:\n",
    "        Trimmed results that fit within budget\n",
    "    \"\"\"\n",
    "    available_tokens = max_tokens - reserve_tokens\n",
    "    current_tokens = 0\n",
    "    fitted_results = []\n",
    "\n",
    "    for result in results:\n",
    "        result_tokens = count_tokens(result.text)\n",
    "\n",
    "        if current_tokens + result_tokens <= available_tokens:\n",
    "            fitted_results.append(result)\n",
    "            current_tokens += result_tokens\n",
    "        else:\n",
    "            # Try to fit partial text if it's the first result\n",
    "            if not fitted_results and result_tokens > available_tokens:\n",
    "                # Truncate the text to fit\n",
    "                truncated_text = truncate_to_tokens(result.text, available_tokens)\n",
    "                if truncated_text:\n",
    "                    truncated_result = RetrievalResult(\n",
    "                        text=truncated_text,\n",
    "                        meta=result.meta,\n",
    "                        score=result.score,\n",
    "                        chunk_id=result.chunk_id\n",
    "                    )\n",
    "                    fitted_results.append(truncated_result)\n",
    "                    current_tokens = count_tokens(truncated_text)\n",
    "            break\n",
    "\n",
    "    print(f\"Token budget: {current_tokens}/{available_tokens} tokens used \"\n",
    "          f\"({len(fitted_results)}/{len(results)} chunks)\")\n",
    "\n",
    "    return fitted_results\n",
    "\n",
    "def truncate_to_tokens(text: str, max_tokens: int) -> str:\n",
    "    \"\"\"\n",
    "    Truncate text to fit within token limit\n",
    "\n",
    "    Args:\n",
    "        text: Text to truncate\n",
    "        max_tokens: Maximum tokens allowed\n",
    "\n",
    "    Returns:\n",
    "        Truncated text\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return text\n",
    "\n",
    "    # Truncate and decode, try to end at sentence boundary\n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    truncated_text = tokenizer.decode(truncated_tokens)\n",
    "\n",
    "    # Try to end at a sentence boundary (Chinese punctuation)\n",
    "    for punct in ['。', '！', '？', '；']:\n",
    "        last_punct = truncated_text.rfind(punct)\n",
    "        if last_punct > len(truncated_text) * 0.7:  # Don't cut too much\n",
    "            return truncated_text[:last_punct + 1]\n",
    "\n",
    "    return truncated_text\n",
    "\n",
    "# Test token budget management\n",
    "def test_token_budget():\n",
    "    \"\"\"Test token budget functionality\"\"\"\n",
    "    # Create test results with known token counts\n",
    "    test_results = []\n",
    "    for i, chunk in enumerate(chunks[:6]):\n",
    "        test_results.append(RetrievalResult(\n",
    "            text=chunk[\"text\"],\n",
    "            meta=chunk[\"meta\"],\n",
    "            score=0.9 - i * 0.1,\n",
    "            chunk_id=chunk[\"id\"]\n",
    "        ))\n",
    "\n",
    "    print(\"Original results:\")\n",
    "    total_tokens = 0\n",
    "    for i, result in enumerate(test_results):\n",
    "        tokens = count_tokens(result.text)\n",
    "        total_tokens += tokens\n",
    "        print(f\"  {i}: {tokens} tokens - {result.text[:50]}...\")\n",
    "    print(f\"Total: {total_tokens} tokens\")\n",
    "\n",
    "    # Test different budget limits\n",
    "    for budget in [500, 300, 150]:\n",
    "        print(f\"\\nBudget limit: {budget} tokens\")\n",
    "        fitted = fit_token_budget(test_results, max_tokens=budget, reserve_tokens=50)\n",
    "        print(f\"Fitted {len(fitted)} chunks\")\n",
    "\n",
    "test_token_budget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd31206",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 6: Score Threshold Filtering\n",
    "def filter_by_score(results: List[RetrievalResult],\n",
    "                   min_score: float = 0.3) -> List[RetrievalResult]:\n",
    "    \"\"\"\n",
    "    Filter results by minimum relevance score\n",
    "\n",
    "    Args:\n",
    "        results: Retrieval results to filter\n",
    "        min_score: Minimum score threshold\n",
    "\n",
    "    Returns:\n",
    "        Filtered results above threshold\n",
    "    \"\"\"\n",
    "    filtered = [r for r in results if r.score >= min_score]\n",
    "\n",
    "    print(f\"Score filtering: {len(results)} -> {len(filtered)} chunks \"\n",
    "          f\"(threshold: {min_score})\")\n",
    "\n",
    "    return filtered\n",
    "\n",
    "def adaptive_threshold(results: List[RetrievalResult],\n",
    "                      min_results: int = 3,\n",
    "                      score_gap_threshold: float = 0.2) -> float:\n",
    "    \"\"\"\n",
    "    Compute adaptive score threshold based on score distribution\n",
    "\n",
    "    Args:\n",
    "        results: Retrieval results (should be sorted by score desc)\n",
    "        min_results: Minimum number of results to keep\n",
    "        score_gap_threshold: Minimum gap to consider a natural cut-off\n",
    "\n",
    "    Returns:\n",
    "        Adaptive threshold score\n",
    "    \"\"\"\n",
    "    if len(results) <= min_results:\n",
    "        return 0.0\n",
    "\n",
    "    scores = [r.score for r in results]\n",
    "\n",
    "    # Look for natural score gaps after min_results\n",
    "    for i in range(min_results, len(scores) - 1):\n",
    "        score_gap = scores[i] - scores[i + 1]\n",
    "        if score_gap >= score_gap_threshold:\n",
    "            threshold = scores[i + 1] + score_gap / 2\n",
    "            print(f\"Adaptive threshold: {threshold:.3f} (gap at position {i})\")\n",
    "            return threshold\n",
    "\n",
    "    # Fallback: use median score if no clear gap\n",
    "    median_idx = len(scores) // 2\n",
    "    threshold = scores[median_idx]\n",
    "    print(f\"Adaptive threshold: {threshold:.3f} (median fallback)\")\n",
    "    return threshold\n",
    "\n",
    "# Test score filtering\n",
    "def test_score_filtering():\n",
    "    \"\"\"Test score filtering functionality\"\"\"\n",
    "    # Create test results with varying scores\n",
    "    test_scores = [0.95, 0.87, 0.76, 0.45, 0.32, 0.18, 0.12, 0.05]\n",
    "    test_results = []\n",
    "\n",
    "    for i, score in enumerate(test_scores):\n",
    "        test_results.append(RetrievalResult(\n",
    "            text=f\"測試文本 {i+1}: \" + chunks[i % len(chunks)][\"text\"],\n",
    "            meta={\"source\": f\"test_{i}\"},\n",
    "            score=score,\n",
    "            chunk_id=i\n",
    "        ))\n",
    "\n",
    "    print(\"Original results:\")\n",
    "    for i, result in enumerate(test_results):\n",
    "        print(f\"  {i}: score={result.score:.3f} - {result.text[:40]}...\")\n",
    "\n",
    "    # Test fixed threshold\n",
    "    print(f\"\\nFixed threshold (0.4):\")\n",
    "    filtered_fixed = filter_by_score(test_results, min_score=0.4)\n",
    "\n",
    "    # Test adaptive threshold\n",
    "    print(f\"\\nAdaptive threshold:\")\n",
    "    adaptive_thresh = adaptive_threshold(test_results)\n",
    "    filtered_adaptive = filter_by_score(test_results, min_score=adaptive_thresh)\n",
    "\n",
    "test_score_filtering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57cd64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 7: Integrated Context Optimizer\n",
    "class ContextOptimizer:\n",
    "    \"\"\"\n",
    "    Complete context optimization pipeline\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 similarity_threshold: float = 0.85,\n",
    "                 mmr_lambda: float = 0.5,\n",
    "                 max_tokens: int = 3000,\n",
    "                 reserve_tokens: int = 500,\n",
    "                 min_score: float = 0.3,\n",
    "                 use_adaptive_threshold: bool = True):\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.mmr_lambda = mmr_lambda\n",
    "        self.max_tokens = max_tokens\n",
    "        self.reserve_tokens = reserve_tokens\n",
    "        self.min_score = min_score\n",
    "        self.use_adaptive_threshold = use_adaptive_threshold\n",
    "\n",
    "    def optimize(self,\n",
    "                results: List[RetrievalResult],\n",
    "                query: str,\n",
    "                target_k: int = 5) -> List[RetrievalResult]:\n",
    "        \"\"\"\n",
    "        Apply complete optimization pipeline\n",
    "\n",
    "        Args:\n",
    "            results: Raw retrieval results\n",
    "            query: Original query\n",
    "            target_k: Target number of results\n",
    "\n",
    "        Returns:\n",
    "            Optimized results\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== Context Optimization Pipeline ===\")\n",
    "        print(f\"Input: {len(results)} chunks\")\n",
    "\n",
    "        # Step 1: Score filtering\n",
    "        if self.use_adaptive_threshold:\n",
    "            threshold = adaptive_threshold(results)\n",
    "            filtered = filter_by_score(results, min_score=threshold)\n",
    "        else:\n",
    "            filtered = filter_by_score(results, min_score=self.min_score)\n",
    "\n",
    "        if not filtered:\n",
    "            print(\"Warning: No results after score filtering!\")\n",
    "            return []\n",
    "\n",
    "        # Step 2: Deduplication\n",
    "        deduplicated = remove_duplicates(filtered, self.similarity_threshold)\n",
    "\n",
    "        # Step 3: MMR reranking for diversity\n",
    "        mmr_results = mmr_rerank(deduplicated, query,\n",
    "                               k=min(target_k * 2, len(deduplicated)),\n",
    "                               lambda_param=self.mmr_lambda)\n",
    "\n",
    "        # Step 4: Token budget fitting\n",
    "        final_results = fit_token_budget(mmr_results,\n",
    "                                       self.max_tokens,\n",
    "                                       self.reserve_tokens)\n",
    "\n",
    "        print(f\"Final output: {len(final_results)} chunks\")\n",
    "\n",
    "        return final_results\n",
    "\n",
    "    def optimize_with_stats(self,\n",
    "                          results: List[RetrievalResult],\n",
    "                          query: str,\n",
    "                          target_k: int = 5) -> Tuple[List[RetrievalResult], Dict]:\n",
    "        \"\"\"\n",
    "        Optimize with detailed statistics\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (optimized_results, stats_dict)\n",
    "        \"\"\"\n",
    "        stats = {\n",
    "            \"original_count\": len(results),\n",
    "            \"original_tokens\": sum(count_tokens(r.text) for r in results),\n",
    "            \"original_avg_score\": np.mean([r.score for r in results]) if results else 0,\n",
    "        }\n",
    "\n",
    "        optimized = self.optimize(results, query, target_k)\n",
    "\n",
    "        stats.update({\n",
    "            \"final_count\": len(optimized),\n",
    "            \"final_tokens\": sum(count_tokens(r.text) for r in optimized),\n",
    "            \"final_avg_score\": np.mean([r.score for r in optimized]) if optimized else 0,\n",
    "            \"compression_ratio\": len(optimized) / max(1, len(results)),\n",
    "            \"token_reduction\": 1 - (sum(count_tokens(r.text) for r in optimized) /\n",
    "                                  max(1, sum(count_tokens(r.text) for r in results)))\n",
    "        })\n",
    "\n",
    "        return optimized, stats\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = ContextOptimizer(\n",
    "    similarity_threshold=0.8,\n",
    "    mmr_lambda=0.6,  # Slightly favor diversity\n",
    "    max_tokens=2000,\n",
    "    reserve_tokens=500,\n",
    "    use_adaptive_threshold=True\n",
    ")\n",
    "\n",
    "print(\"Context Optimizer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cdea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 8: Performance Comparison\n",
    "def simulate_retrieval(query: str, k: int = 10) -> List[RetrievalResult]:\n",
    "    \"\"\"\n",
    "    Simulate retrieval results for testing\n",
    "    \"\"\"\n",
    "    # Encode query\n",
    "    query_embedding = embedding_model.encode([query], normalize_embeddings=True)\n",
    "\n",
    "    # Search in index\n",
    "    scores, indices = index.search(query_embedding.astype(np.float32), k)\n",
    "\n",
    "    # Convert to RetrievalResult objects\n",
    "    results = []\n",
    "    for i, (idx, score) in enumerate(zip(indices[0], scores[0])):\n",
    "        if idx >= 0 and idx < len(chunks):  # Valid index\n",
    "            results.append(RetrievalResult(\n",
    "                text=chunks[idx][\"text\"],\n",
    "                meta=chunks[idx][\"meta\"],\n",
    "                score=float(score),\n",
    "                chunk_id=chunks[idx][\"id\"]\n",
    "            ))\n",
    "\n",
    "    return results\n",
    "\n",
    "def compare_optimization_strategies(query: str):\n",
    "    \"\"\"\n",
    "    Compare different optimization strategies\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Optimization Strategy Comparison ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "\n",
    "    # Get raw retrieval results\n",
    "    raw_results = simulate_retrieval(query, k=8)\n",
    "\n",
    "    if not raw_results:\n",
    "        print(\"No retrieval results found!\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nRaw retrieval: {len(raw_results)} chunks\")\n",
    "    for i, result in enumerate(raw_results):\n",
    "        tokens = count_tokens(result.text)\n",
    "        print(f\"  {i}: score={result.score:.3f}, tokens={tokens} - {result.text[:50]}...\")\n",
    "\n",
    "    # Strategy 1: No optimization (just top-k)\n",
    "    baseline = raw_results[:5]\n",
    "    baseline_tokens = sum(count_tokens(r.text) for r in baseline)\n",
    "    print(f\"\\nBaseline (top-5): {len(baseline)} chunks, {baseline_tokens} tokens\")\n",
    "\n",
    "    # Strategy 2: Score filtering only\n",
    "    score_filtered = filter_by_score(raw_results, min_score=0.3)[:5]\n",
    "    score_tokens = sum(count_tokens(r.text) for r in score_filtered)\n",
    "    print(f\"Score filtering: {len(score_filtered)} chunks, {score_tokens} tokens\")\n",
    "\n",
    "    # Strategy 3: Deduplication only\n",
    "    dedup_only = remove_duplicates(raw_results, similarity_threshold=0.8)[:5]\n",
    "    dedup_tokens = sum(count_tokens(r.text) for r in dedup_only)\n",
    "    print(f\"Deduplication: {len(dedup_only)} chunks, {dedup_tokens} tokens\")\n",
    "\n",
    "    # Strategy 4: Full optimization\n",
    "    optimized, stats = optimizer.optimize_with_stats(raw_results, query, target_k=5)\n",
    "\n",
    "    print(f\"\\n=== Optimization Statistics ===\")\n",
    "    for key, value in stats.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"{key}: {value:.3f}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "    return {\n",
    "        \"baseline\": baseline,\n",
    "        \"score_filtered\": score_filtered,\n",
    "        \"dedup_only\": dedup_only,\n",
    "        \"optimized\": optimized,\n",
    "        \"stats\": stats\n",
    "    }\n",
    "\n",
    "# Test with different queries\n",
    "test_queries = [\n",
    "    \"人工智慧和機器學習的關係\",\n",
    "    \"深度學習神經網路\",\n",
    "    \"自然語言處理應用\"\n",
    "]\n",
    "\n",
    "comparison_results = {}\n",
    "for query in test_queries:\n",
    "    comparison_results[query] = compare_optimization_strategies(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4de12b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 9: Smoke Test - End-to-End Optimization\n",
    "def smoke_test_optimization():\n",
    "    \"\"\"\n",
    "    Comprehensive smoke test for context optimization\n",
    "    \"\"\"\n",
    "    print(\"=== Context Optimization Smoke Test ===\")\n",
    "\n",
    "    # Test query\n",
    "    test_query = \"人工智慧技術應用\"\n",
    "\n",
    "    # Get retrieval results\n",
    "    raw_results = simulate_retrieval(test_query, k=8)\n",
    "\n",
    "    if not raw_results:\n",
    "        print(\"❌ No retrieval results - check index and chunks\")\n",
    "        return False\n",
    "\n",
    "    print(f\"✓ Retrieved {len(raw_results)} raw results\")\n",
    "\n",
    "    # Test optimization\n",
    "    try:\n",
    "        optimized_results, stats = optimizer.optimize_with_stats(\n",
    "            raw_results, test_query, target_k=5\n",
    "        )\n",
    "\n",
    "        print(f\"✓ Optimization completed: {stats['original_count']} -> {stats['final_count']} chunks\")\n",
    "        print(f\"✓ Token reduction: {stats['token_reduction']:.1%}\")\n",
    "        print(f\"✓ Average score: {stats['original_avg_score']:.3f} -> {stats['final_avg_score']:.3f}\")\n",
    "\n",
    "        # Verify results are valid\n",
    "        if not optimized_results:\n",
    "            print(\"❌ No optimized results returned\")\n",
    "            return False\n",
    "\n",
    "        # Check for token budget compliance\n",
    "        total_tokens = sum(count_tokens(r.text) for r in optimized_results)\n",
    "        max_allowed = optimizer.max_tokens - optimizer.reserve_tokens\n",
    "\n",
    "        if total_tokens > max_allowed:\n",
    "            print(f\"❌ Token budget exceeded: {total_tokens} > {max_allowed}\")\n",
    "            return False\n",
    "\n",
    "        print(f\"✓ Token budget respected: {total_tokens}/{max_allowed} tokens\")\n",
    "\n",
    "        # Check for duplicate removal\n",
    "        unique_texts = set(r.text for r in optimized_results)\n",
    "        if len(unique_texts) != len(optimized_results):\n",
    "            print(\"❌ Duplicates found in optimized results\")\n",
    "            return False\n",
    "\n",
    "        print(f\"✓ No duplicates in final results\")\n",
    "\n",
    "        # Display final optimized results\n",
    "        print(f\"\\n=== Final Optimized Results ===\")\n",
    "        for i, result in enumerate(optimized_results):\n",
    "            tokens = count_tokens(result.text)\n",
    "            print(f\"{i+1}. Score: {result.score:.3f}, Tokens: {tokens}\")\n",
    "            print(f\"   Text: {result.text[:80]}...\")\n",
    "            print(f\"   Source: {result.meta.get('source_id', 'N/A')}\")\n",
    "\n",
    "        print(\"\\n✅ All smoke tests passed!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Optimization failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Run smoke test\n",
    "success = smoke_test_optimization()\n",
    "\n",
    "if success:\n",
    "    print(\"\\n🎉 Context optimization is working correctly!\")\n",
    "    print(\"\\nKey features verified:\")\n",
    "    print(\"- ✓ Duplicate removal with configurable similarity threshold\")\n",
    "    print(\"- ✓ MMR reranking for relevance-diversity balance\")\n",
    "    print(\"- ✓ Token budget management with smart truncation\")\n",
    "    print(\"- ✓ Adaptive score threshold filtering\")\n",
    "    print(\"- ✓ End-to-end optimization pipeline\")\n",
    "else:\n",
    "    print(\"\\n⚠️  Some issues detected - check implementation\")\n",
    "\n",
    "# Save configuration for reuse\n",
    "config = {\n",
    "    \"similarity_threshold\": optimizer.similarity_threshold,\n",
    "    \"mmr_lambda\": optimizer.mmr_lambda,\n",
    "    \"max_tokens\": optimizer.max_tokens,\n",
    "    \"reserve_tokens\": optimizer.reserve_tokens,\n",
    "    \"min_score\": optimizer.min_score,\n",
    "    \"use_adaptive_threshold\": optimizer.use_adaptive_threshold\n",
    "}\n",
    "\n",
    "print(f\"\\n📝 Current optimizer configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883154ad",
   "metadata": {},
   "source": [
    "## 🎯 What We Built\n",
    "\n",
    "1. **去重演算法**: 使用 `rapidfuzz` 檢測文本相似度，移除冗餘段落\n",
    "2. **MMR 重排**: 平衡相關性與多樣性，避免結果過於相似\n",
    "3. **Token Budget 管理**: 動態截斷，確保不超過 context window 限制\n",
    "4. **自適應分數閾值**: 根據分數分布自動決定過濾門檻\n",
    "5. **完整優化流水線**: 整合所有策略的 `ContextOptimizer` 類別\n",
    "\n",
    "## ⚠️ Pitfalls\n",
    "\n",
    "- **過度去重**: 相似度閾值設太低會移除相關但不完全重複的內容\n",
    "- **MMR λ 參數**: 需要根據具體應用調整相關性與多樣性的平衡\n",
    "- **Token 計算**: 不同 tokenizer 結果差異大，需要留足夠 buffer\n",
    "- **分數標準化**: 不同檢索方法的分數範圍可能不同，需要統一處理\n",
    "\n",
    "## 🔄 Next Steps\n",
    "\n",
    "1. **實驗不同參數組合**: 針對你的資料集調優 threshold, λ, token budget\n",
    "2. **整合到檢索流水線**: 修改 `nb14` 的檢索函數加入優化步驟\n",
    "3. **添加評估指標**: 測量優化前後的 Recall@k 和 relevance 變化\n",
    "4. **支援更多策略**: 如 cluster-based 去重、semantic chunking 等"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
