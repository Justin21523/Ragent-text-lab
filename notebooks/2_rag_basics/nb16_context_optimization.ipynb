{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47851b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context Optimization for RAG Systems\n",
    "# Stage 2, Notebook 16: ä¸Šä¸‹æ–‡å„ªåŒ– - å»é‡/MMR/Token Budget\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (è¤‡è£½åˆ°æ¯æœ¬ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c46d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 2: Load Dependencies and Existing Index\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "from rapidfuzz import fuzz\n",
    "import tiktoken\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load embedding model\n",
    "print(\"Loading BGE-M3 model...\")\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-m3\",\n",
    "                                     cache_folder=f\"{AI_CACHE_ROOT}/hf\")\n",
    "embed_dim = embedding_model.get_sentence_embedding_dimension()\n",
    "print(f\"Embedding dimension: {embed_dim}\")\n",
    "\n",
    "# Load or create sample index and chunks\n",
    "index_path = \"indices/general.faiss\"\n",
    "chunks_path = \"indices/chunks.jsonl\"\n",
    "\n",
    "if Path(index_path).exists() and Path(chunks_path).exists():\n",
    "    # Load existing index\n",
    "    index = faiss.read_index(index_path)\n",
    "    chunks = []\n",
    "    with open(chunks_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            chunks.append(json.loads(line))\n",
    "    print(f\"Loaded {len(chunks)} chunks and FAISS index\")\n",
    "else:\n",
    "    # Create sample data for demonstration\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    sample_texts = [\n",
    "        \"äººå·¥æ™ºæ…§æ˜¯é›»è…¦ç§‘å­¸çš„ä¸€å€‹åˆ†æ”¯ï¼Œè‡´åŠ›æ–¼å‰µå»ºèƒ½å¤ åŸ·è¡Œé€šå¸¸éœ€è¦äººé¡æ™ºæ…§çš„ä»»å‹™çš„ç³»çµ±ã€‚\",\n",
    "        \"æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§çš„å­é ˜åŸŸï¼Œä½¿ç”¨æ¼”ç®—æ³•å’Œçµ±è¨ˆæ¨¡å‹ä¾†ä½¿é›»è…¦ç³»çµ±èƒ½å¤ æ”¹å–„å…¶æ€§èƒ½ã€‚\",\n",
    "        \"æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’çš„ä¸€ç¨®æ–¹æ³•ï¼Œä½¿ç”¨å…·æœ‰å¤šå±¤çš„äººå·¥ç¥ç¶“ç¶²è·¯ã€‚\",\n",
    "        \"è‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºæ…§çš„ä¸€å€‹é ˜åŸŸï¼Œå°ˆæ³¨æ–¼é›»è…¦èˆ‡äººé¡èªè¨€ä¹‹é–“çš„äº’å‹•ã€‚\",\n",
    "        \"é›»è…¦è¦–è¦ºæ˜¯äººå·¥æ™ºæ…§çš„å¦ä¸€å€‹é‡è¦åˆ†æ”¯ï¼Œä½¿æ©Ÿå™¨èƒ½å¤ è§£é‡‹å’Œç†è§£è¦–è¦ºä¸–ç•Œã€‚\",\n",
    "        \"äººå·¥æ™ºæ…§æŠ€è¡“å»£æ³›æ‡‰ç”¨æ–¼é†«ç™‚ã€é‡‘èã€äº¤é€šç­‰å„å€‹é ˜åŸŸã€‚\",\n",
    "        \"æ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³•åŒ…æ‹¬ç›£ç£å­¸ç¿’ã€ç„¡ç›£ç£å­¸ç¿’å’Œå¼·åŒ–å­¸ç¿’ç­‰ä¸åŒé¡å‹ã€‚\",\n",
    "        \"æ·±åº¦å­¸ç¿’ç¶²è·¯åŒ…å«è¼¸å…¥å±¤ã€éš±è—å±¤å’Œè¼¸å‡ºå±¤ï¼Œèƒ½å¤ å­¸ç¿’è¤‡é›œçš„æ¨¡å¼ã€‚\",\n",
    "        \"è‡ªç„¶èªè¨€è™•ç†æŠ€è¡“åŒ…æ‹¬æ–‡æœ¬åˆ†é¡ã€æƒ…æ„Ÿåˆ†æã€æ©Ÿå™¨ç¿»è­¯ç­‰æ‡‰ç”¨ã€‚\",\n",
    "        \"é›»è…¦è¦–è¦ºæ‡‰ç”¨åŒ…æ‹¬åœ–åƒè­˜åˆ¥ã€ç‰©é«”æª¢æ¸¬ã€äººè‡‰è­˜åˆ¥ç­‰æŠ€è¡“ã€‚\"\n",
    "    ]\n",
    "\n",
    "    # Create embeddings\n",
    "    embeddings = embedding_model.encode(sample_texts, normalize_embeddings=True)\n",
    "    embeddings = embeddings.astype(np.float32)\n",
    "\n",
    "    # Create FAISS index\n",
    "    index = faiss.IndexFlatIP(embed_dim)\n",
    "    index.add(embeddings)\n",
    "\n",
    "    # Create chunks data\n",
    "    chunks = []\n",
    "    for i, text in enumerate(sample_texts):\n",
    "        chunks.append({\n",
    "            \"id\": i,\n",
    "            \"text\": text,\n",
    "            \"meta\": {\"source_id\": f\"sample_{i}\", \"page\": 1}\n",
    "        })\n",
    "\n",
    "    # Save for reuse\n",
    "    Path(\"indices\").mkdir(exist_ok=True)\n",
    "    faiss.write_index(index, index_path)\n",
    "    with open(chunks_path, 'w', encoding='utf-8') as f:\n",
    "        for chunk in chunks:\n",
    "            f.write(json.dumps(chunk, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\"Created {len(chunks)} sample chunks\")\n",
    "\n",
    "# Initialize tokenizer for token counting\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # GPT-3.5/4 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bc9106",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 3: Duplicate Removal Implementation\n",
    "@dataclass\n",
    "class RetrievalResult:\n",
    "    \"\"\"æª¢ç´¢çµæœæ•¸æ“šçµæ§‹\"\"\"\n",
    "    text: str\n",
    "    meta: Dict[str, Any]\n",
    "    score: float\n",
    "    chunk_id: int\n",
    "\n",
    "def remove_duplicates(results: List[RetrievalResult],\n",
    "                     similarity_threshold: float = 0.85) -> List[RetrievalResult]:\n",
    "    \"\"\"\n",
    "    Remove duplicate or highly similar chunks using rapidfuzz\n",
    "\n",
    "    Args:\n",
    "        results: List of retrieval results\n",
    "        similarity_threshold: Similarity threshold (0-1), above which chunks are considered duplicates\n",
    "\n",
    "    Returns:\n",
    "        Deduplicated list of results\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return results\n",
    "\n",
    "    deduplicated = []\n",
    "\n",
    "    for current in results:\n",
    "        is_duplicate = False\n",
    "\n",
    "        # Check against already accepted chunks\n",
    "        for accepted in deduplicated:\n",
    "            # Use token_sort_ratio for better handling of word order differences\n",
    "            similarity = fuzz.token_sort_ratio(current.text, accepted.text) / 100.0\n",
    "\n",
    "            if similarity > similarity_threshold:\n",
    "                is_duplicate = True\n",
    "                # Keep the one with higher score\n",
    "                if current.score > accepted.score:\n",
    "                    deduplicated.remove(accepted)\n",
    "                    deduplicated.append(current)\n",
    "                break\n",
    "\n",
    "        if not is_duplicate:\n",
    "            deduplicated.append(current)\n",
    "\n",
    "    print(f\"Deduplication: {len(results)} -> {len(deduplicated)} chunks \"\n",
    "          f\"(removed {len(results) - len(deduplicated)} duplicates)\")\n",
    "\n",
    "    return deduplicated\n",
    "\n",
    "# Test deduplication\n",
    "def test_deduplication():\n",
    "    \"\"\"Test the deduplication function\"\"\"\n",
    "    test_results = [\n",
    "        RetrievalResult(\"äººå·¥æ™ºæ…§æ˜¯é›»è…¦ç§‘å­¸çš„åˆ†æ”¯\", {\"source\": \"doc1\"}, 0.9, 0),\n",
    "        RetrievalResult(\"äººå·¥æ™ºèƒ½æ˜¯è¨ˆç®—æ©Ÿç§‘å­¸çš„åˆ†æ”¯\", {\"source\": \"doc2\"}, 0.8, 1),  # Similar\n",
    "        RetrievalResult(\"æ©Ÿå™¨å­¸ç¿’æ˜¯AIçš„å­é ˜åŸŸ\", {\"source\": \"doc3\"}, 0.7, 2),\n",
    "        RetrievalResult(\"æ·±åº¦å­¸ç¿’ä½¿ç”¨ç¥ç¶“ç¶²è·¯\", {\"source\": \"doc4\"}, 0.6, 3),\n",
    "    ]\n",
    "\n",
    "    print(\"Original results:\")\n",
    "    for i, result in enumerate(test_results):\n",
    "        print(f\"  {i}: {result.text[:30]}... (score: {result.score})\")\n",
    "\n",
    "    deduplicated = remove_duplicates(test_results, similarity_threshold=0.85)\n",
    "\n",
    "    print(\"\\nAfter deduplication:\")\n",
    "    for i, result in enumerate(deduplicated):\n",
    "        print(f\"  {i}: {result.text[:30]}... (score: {result.score})\")\n",
    "\n",
    "test_deduplication()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a5d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 4: MMR (Maximal Marginal Relevance) Implementation\n",
    "def compute_mmr(query_embedding: np.ndarray,\n",
    "                doc_embeddings: np.ndarray,\n",
    "                selected_indices: List[int],\n",
    "                lambda_param: float = 0.5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute MMR scores for remaining documents\n",
    "\n",
    "    Args:\n",
    "        query_embedding: Query vector (1, dim)\n",
    "        doc_embeddings: Document vectors (n, dim)\n",
    "        selected_indices: Already selected document indices\n",
    "        lambda_param: Balance between relevance (1.0) and diversity (0.0)\n",
    "\n",
    "    Returns:\n",
    "        MMR scores for all documents\n",
    "    \"\"\"\n",
    "    # Relevance scores (cosine similarity with query)\n",
    "    relevance_scores = np.dot(doc_embeddings, query_embedding.T).flatten()\n",
    "\n",
    "    if not selected_indices:\n",
    "        # If no documents selected yet, return pure relevance\n",
    "        return relevance_scores\n",
    "\n",
    "    # Diversity scores (max similarity with already selected documents)\n",
    "    selected_embeddings = doc_embeddings[selected_indices]\n",
    "    similarity_matrix = np.dot(doc_embeddings, selected_embeddings.T)\n",
    "    max_similarity = np.max(similarity_matrix, axis=1)\n",
    "\n",
    "    # MMR formula: Î» * relevance - (1-Î») * max_similarity\n",
    "    mmr_scores = lambda_param * relevance_scores - (1 - lambda_param) * max_similarity\n",
    "\n",
    "    return mmr_scores\n",
    "\n",
    "def mmr_rerank(results: List[RetrievalResult],\n",
    "               query: str,\n",
    "               k: int = 5,\n",
    "               lambda_param: float = 0.5) -> List[RetrievalResult]:\n",
    "    \"\"\"\n",
    "    Rerank results using MMR to balance relevance and diversity\n",
    "\n",
    "    Args:\n",
    "        results: Initial retrieval results\n",
    "        query: Original query text\n",
    "        k: Number of results to return\n",
    "        lambda_param: Balance parameter (1.0=pure relevance, 0.0=pure diversity)\n",
    "\n",
    "    Returns:\n",
    "        MMR-reranked results\n",
    "    \"\"\"\n",
    "    if len(results) <= k:\n",
    "        return results\n",
    "\n",
    "    # Get embeddings for query and all documents\n",
    "    query_embedding = embedding_model.encode([query], normalize_embeddings=True)\n",
    "    doc_texts = [r.text for r in results]\n",
    "    doc_embeddings = embedding_model.encode(doc_texts, normalize_embeddings=True)\n",
    "\n",
    "    selected_indices = []\n",
    "    remaining_indices = list(range(len(results)))\n",
    "\n",
    "    for _ in range(min(k, len(results))):\n",
    "        if not remaining_indices:\n",
    "            break\n",
    "\n",
    "        # Compute MMR scores for remaining documents\n",
    "        mmr_scores = compute_mmr(query_embedding, doc_embeddings,\n",
    "                               selected_indices, lambda_param)\n",
    "\n",
    "        # Find best remaining document\n",
    "        best_idx = None\n",
    "        best_score = float('-inf')\n",
    "\n",
    "        for idx in remaining_indices:\n",
    "            if mmr_scores[idx] > best_score:\n",
    "                best_score = mmr_scores[idx]\n",
    "                best_idx = idx\n",
    "\n",
    "        # Move from remaining to selected\n",
    "        selected_indices.append(best_idx)\n",
    "        remaining_indices.remove(best_idx)\n",
    "\n",
    "    # Return reranked results\n",
    "    mmr_results = [results[i] for i in selected_indices]\n",
    "\n",
    "    print(f\"MMR reranking: {len(results)} -> {len(mmr_results)} chunks \"\n",
    "          f\"(Î»={lambda_param})\")\n",
    "\n",
    "    return mmr_results\n",
    "\n",
    "# Test MMR\n",
    "def test_mmr():\n",
    "    \"\"\"Test MMR functionality\"\"\"\n",
    "    # Create test query and results\n",
    "    query = \"äººå·¥æ™ºæ…§æ©Ÿå™¨å­¸ç¿’\"\n",
    "\n",
    "    # Simulate retrieval results\n",
    "    test_results = []\n",
    "    for i, chunk in enumerate(chunks[:8]):  # Use first 8 chunks\n",
    "        # Simulate similarity scores\n",
    "        score = 0.9 - i * 0.1  # Decreasing scores\n",
    "        test_results.append(RetrievalResult(\n",
    "            text=chunk[\"text\"],\n",
    "            meta=chunk[\"meta\"],\n",
    "            score=score,\n",
    "            chunk_id=chunk[\"id\"]\n",
    "        ))\n",
    "\n",
    "    print(\"Original results (by score):\")\n",
    "    for i, result in enumerate(test_results):\n",
    "        print(f\"  {i}: {result.text[:50]}... (score: {result.score:.2f})\")\n",
    "\n",
    "    # Apply MMR with different lambda values\n",
    "    for lambda_val in [0.3, 0.5, 0.8]:\n",
    "        print(f\"\\nMMR results (Î»={lambda_val}):\")\n",
    "        mmr_results = mmr_rerank(test_results, query, k=5, lambda_param=lambda_val)\n",
    "        for i, result in enumerate(mmr_results):\n",
    "            print(f\"  {i}: {result.text[:50]}... (score: {result.score:.2f})\")\n",
    "\n",
    "test_mmr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd116ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 5: Token Budget Management\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Count tokens in text using tiktoken\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def fit_token_budget(results: List[RetrievalResult],\n",
    "                    max_tokens: int = 3000,\n",
    "                    reserve_tokens: int = 500) -> List[RetrievalResult]:\n",
    "    \"\"\"\n",
    "    Trim results to fit within token budget\n",
    "\n",
    "    Args:\n",
    "        results: Retrieval results to trim\n",
    "        max_tokens: Maximum total tokens for context\n",
    "        reserve_tokens: Tokens to reserve for system/user messages\n",
    "\n",
    "    Returns:\n",
    "        Trimmed results that fit within budget\n",
    "    \"\"\"\n",
    "    available_tokens = max_tokens - reserve_tokens\n",
    "    current_tokens = 0\n",
    "    fitted_results = []\n",
    "\n",
    "    for result in results:\n",
    "        result_tokens = count_tokens(result.text)\n",
    "\n",
    "        if current_tokens + result_tokens <= available_tokens:\n",
    "            fitted_results.append(result)\n",
    "            current_tokens += result_tokens\n",
    "        else:\n",
    "            # Try to fit partial text if it's the first result\n",
    "            if not fitted_results and result_tokens > available_tokens:\n",
    "                # Truncate the text to fit\n",
    "                truncated_text = truncate_to_tokens(result.text, available_tokens)\n",
    "                if truncated_text:\n",
    "                    truncated_result = RetrievalResult(\n",
    "                        text=truncated_text,\n",
    "                        meta=result.meta,\n",
    "                        score=result.score,\n",
    "                        chunk_id=result.chunk_id\n",
    "                    )\n",
    "                    fitted_results.append(truncated_result)\n",
    "                    current_tokens = count_tokens(truncated_text)\n",
    "            break\n",
    "\n",
    "    print(f\"Token budget: {current_tokens}/{available_tokens} tokens used \"\n",
    "          f\"({len(fitted_results)}/{len(results)} chunks)\")\n",
    "\n",
    "    return fitted_results\n",
    "\n",
    "def truncate_to_tokens(text: str, max_tokens: int) -> str:\n",
    "    \"\"\"\n",
    "    Truncate text to fit within token limit\n",
    "\n",
    "    Args:\n",
    "        text: Text to truncate\n",
    "        max_tokens: Maximum tokens allowed\n",
    "\n",
    "    Returns:\n",
    "        Truncated text\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return text\n",
    "\n",
    "    # Truncate and decode, try to end at sentence boundary\n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    truncated_text = tokenizer.decode(truncated_tokens)\n",
    "\n",
    "    # Try to end at a sentence boundary (Chinese punctuation)\n",
    "    for punct in ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›']:\n",
    "        last_punct = truncated_text.rfind(punct)\n",
    "        if last_punct > len(truncated_text) * 0.7:  # Don't cut too much\n",
    "            return truncated_text[:last_punct + 1]\n",
    "\n",
    "    return truncated_text\n",
    "\n",
    "# Test token budget management\n",
    "def test_token_budget():\n",
    "    \"\"\"Test token budget functionality\"\"\"\n",
    "    # Create test results with known token counts\n",
    "    test_results = []\n",
    "    for i, chunk in enumerate(chunks[:6]):\n",
    "        test_results.append(RetrievalResult(\n",
    "            text=chunk[\"text\"],\n",
    "            meta=chunk[\"meta\"],\n",
    "            score=0.9 - i * 0.1,\n",
    "            chunk_id=chunk[\"id\"]\n",
    "        ))\n",
    "\n",
    "    print(\"Original results:\")\n",
    "    total_tokens = 0\n",
    "    for i, result in enumerate(test_results):\n",
    "        tokens = count_tokens(result.text)\n",
    "        total_tokens += tokens\n",
    "        print(f\"  {i}: {tokens} tokens - {result.text[:50]}...\")\n",
    "    print(f\"Total: {total_tokens} tokens\")\n",
    "\n",
    "    # Test different budget limits\n",
    "    for budget in [500, 300, 150]:\n",
    "        print(f\"\\nBudget limit: {budget} tokens\")\n",
    "        fitted = fit_token_budget(test_results, max_tokens=budget, reserve_tokens=50)\n",
    "        print(f\"Fitted {len(fitted)} chunks\")\n",
    "\n",
    "test_token_budget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd31206",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 6: Score Threshold Filtering\n",
    "def filter_by_score(results: List[RetrievalResult],\n",
    "                   min_score: float = 0.3) -> List[RetrievalResult]:\n",
    "    \"\"\"\n",
    "    Filter results by minimum relevance score\n",
    "\n",
    "    Args:\n",
    "        results: Retrieval results to filter\n",
    "        min_score: Minimum score threshold\n",
    "\n",
    "    Returns:\n",
    "        Filtered results above threshold\n",
    "    \"\"\"\n",
    "    filtered = [r for r in results if r.score >= min_score]\n",
    "\n",
    "    print(f\"Score filtering: {len(results)} -> {len(filtered)} chunks \"\n",
    "          f\"(threshold: {min_score})\")\n",
    "\n",
    "    return filtered\n",
    "\n",
    "def adaptive_threshold(results: List[RetrievalResult],\n",
    "                      min_results: int = 3,\n",
    "                      score_gap_threshold: float = 0.2) -> float:\n",
    "    \"\"\"\n",
    "    Compute adaptive score threshold based on score distribution\n",
    "\n",
    "    Args:\n",
    "        results: Retrieval results (should be sorted by score desc)\n",
    "        min_results: Minimum number of results to keep\n",
    "        score_gap_threshold: Minimum gap to consider a natural cut-off\n",
    "\n",
    "    Returns:\n",
    "        Adaptive threshold score\n",
    "    \"\"\"\n",
    "    if len(results) <= min_results:\n",
    "        return 0.0\n",
    "\n",
    "    scores = [r.score for r in results]\n",
    "\n",
    "    # Look for natural score gaps after min_results\n",
    "    for i in range(min_results, len(scores) - 1):\n",
    "        score_gap = scores[i] - scores[i + 1]\n",
    "        if score_gap >= score_gap_threshold:\n",
    "            threshold = scores[i + 1] + score_gap / 2\n",
    "            print(f\"Adaptive threshold: {threshold:.3f} (gap at position {i})\")\n",
    "            return threshold\n",
    "\n",
    "    # Fallback: use median score if no clear gap\n",
    "    median_idx = len(scores) // 2\n",
    "    threshold = scores[median_idx]\n",
    "    print(f\"Adaptive threshold: {threshold:.3f} (median fallback)\")\n",
    "    return threshold\n",
    "\n",
    "# Test score filtering\n",
    "def test_score_filtering():\n",
    "    \"\"\"Test score filtering functionality\"\"\"\n",
    "    # Create test results with varying scores\n",
    "    test_scores = [0.95, 0.87, 0.76, 0.45, 0.32, 0.18, 0.12, 0.05]\n",
    "    test_results = []\n",
    "\n",
    "    for i, score in enumerate(test_scores):\n",
    "        test_results.append(RetrievalResult(\n",
    "            text=f\"æ¸¬è©¦æ–‡æœ¬ {i+1}: \" + chunks[i % len(chunks)][\"text\"],\n",
    "            meta={\"source\": f\"test_{i}\"},\n",
    "            score=score,\n",
    "            chunk_id=i\n",
    "        ))\n",
    "\n",
    "    print(\"Original results:\")\n",
    "    for i, result in enumerate(test_results):\n",
    "        print(f\"  {i}: score={result.score:.3f} - {result.text[:40]}...\")\n",
    "\n",
    "    # Test fixed threshold\n",
    "    print(f\"\\nFixed threshold (0.4):\")\n",
    "    filtered_fixed = filter_by_score(test_results, min_score=0.4)\n",
    "\n",
    "    # Test adaptive threshold\n",
    "    print(f\"\\nAdaptive threshold:\")\n",
    "    adaptive_thresh = adaptive_threshold(test_results)\n",
    "    filtered_adaptive = filter_by_score(test_results, min_score=adaptive_thresh)\n",
    "\n",
    "test_score_filtering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57cd64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 7: Integrated Context Optimizer\n",
    "class ContextOptimizer:\n",
    "    \"\"\"\n",
    "    Complete context optimization pipeline\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 similarity_threshold: float = 0.85,\n",
    "                 mmr_lambda: float = 0.5,\n",
    "                 max_tokens: int = 3000,\n",
    "                 reserve_tokens: int = 500,\n",
    "                 min_score: float = 0.3,\n",
    "                 use_adaptive_threshold: bool = True):\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.mmr_lambda = mmr_lambda\n",
    "        self.max_tokens = max_tokens\n",
    "        self.reserve_tokens = reserve_tokens\n",
    "        self.min_score = min_score\n",
    "        self.use_adaptive_threshold = use_adaptive_threshold\n",
    "\n",
    "    def optimize(self,\n",
    "                results: List[RetrievalResult],\n",
    "                query: str,\n",
    "                target_k: int = 5) -> List[RetrievalResult]:\n",
    "        \"\"\"\n",
    "        Apply complete optimization pipeline\n",
    "\n",
    "        Args:\n",
    "            results: Raw retrieval results\n",
    "            query: Original query\n",
    "            target_k: Target number of results\n",
    "\n",
    "        Returns:\n",
    "            Optimized results\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== Context Optimization Pipeline ===\")\n",
    "        print(f\"Input: {len(results)} chunks\")\n",
    "\n",
    "        # Step 1: Score filtering\n",
    "        if self.use_adaptive_threshold:\n",
    "            threshold = adaptive_threshold(results)\n",
    "            filtered = filter_by_score(results, min_score=threshold)\n",
    "        else:\n",
    "            filtered = filter_by_score(results, min_score=self.min_score)\n",
    "\n",
    "        if not filtered:\n",
    "            print(\"Warning: No results after score filtering!\")\n",
    "            return []\n",
    "\n",
    "        # Step 2: Deduplication\n",
    "        deduplicated = remove_duplicates(filtered, self.similarity_threshold)\n",
    "\n",
    "        # Step 3: MMR reranking for diversity\n",
    "        mmr_results = mmr_rerank(deduplicated, query,\n",
    "                               k=min(target_k * 2, len(deduplicated)),\n",
    "                               lambda_param=self.mmr_lambda)\n",
    "\n",
    "        # Step 4: Token budget fitting\n",
    "        final_results = fit_token_budget(mmr_results,\n",
    "                                       self.max_tokens,\n",
    "                                       self.reserve_tokens)\n",
    "\n",
    "        print(f\"Final output: {len(final_results)} chunks\")\n",
    "\n",
    "        return final_results\n",
    "\n",
    "    def optimize_with_stats(self,\n",
    "                          results: List[RetrievalResult],\n",
    "                          query: str,\n",
    "                          target_k: int = 5) -> Tuple[List[RetrievalResult], Dict]:\n",
    "        \"\"\"\n",
    "        Optimize with detailed statistics\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (optimized_results, stats_dict)\n",
    "        \"\"\"\n",
    "        stats = {\n",
    "            \"original_count\": len(results),\n",
    "            \"original_tokens\": sum(count_tokens(r.text) for r in results),\n",
    "            \"original_avg_score\": np.mean([r.score for r in results]) if results else 0,\n",
    "        }\n",
    "\n",
    "        optimized = self.optimize(results, query, target_k)\n",
    "\n",
    "        stats.update({\n",
    "            \"final_count\": len(optimized),\n",
    "            \"final_tokens\": sum(count_tokens(r.text) for r in optimized),\n",
    "            \"final_avg_score\": np.mean([r.score for r in optimized]) if optimized else 0,\n",
    "            \"compression_ratio\": len(optimized) / max(1, len(results)),\n",
    "            \"token_reduction\": 1 - (sum(count_tokens(r.text) for r in optimized) /\n",
    "                                  max(1, sum(count_tokens(r.text) for r in results)))\n",
    "        })\n",
    "\n",
    "        return optimized, stats\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = ContextOptimizer(\n",
    "    similarity_threshold=0.8,\n",
    "    mmr_lambda=0.6,  # Slightly favor diversity\n",
    "    max_tokens=2000,\n",
    "    reserve_tokens=500,\n",
    "    use_adaptive_threshold=True\n",
    ")\n",
    "\n",
    "print(\"Context Optimizer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cdea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 8: Performance Comparison\n",
    "def simulate_retrieval(query: str, k: int = 10) -> List[RetrievalResult]:\n",
    "    \"\"\"\n",
    "    Simulate retrieval results for testing\n",
    "    \"\"\"\n",
    "    # Encode query\n",
    "    query_embedding = embedding_model.encode([query], normalize_embeddings=True)\n",
    "\n",
    "    # Search in index\n",
    "    scores, indices = index.search(query_embedding.astype(np.float32), k)\n",
    "\n",
    "    # Convert to RetrievalResult objects\n",
    "    results = []\n",
    "    for i, (idx, score) in enumerate(zip(indices[0], scores[0])):\n",
    "        if idx >= 0 and idx < len(chunks):  # Valid index\n",
    "            results.append(RetrievalResult(\n",
    "                text=chunks[idx][\"text\"],\n",
    "                meta=chunks[idx][\"meta\"],\n",
    "                score=float(score),\n",
    "                chunk_id=chunks[idx][\"id\"]\n",
    "            ))\n",
    "\n",
    "    return results\n",
    "\n",
    "def compare_optimization_strategies(query: str):\n",
    "    \"\"\"\n",
    "    Compare different optimization strategies\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Optimization Strategy Comparison ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "\n",
    "    # Get raw retrieval results\n",
    "    raw_results = simulate_retrieval(query, k=8)\n",
    "\n",
    "    if not raw_results:\n",
    "        print(\"No retrieval results found!\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nRaw retrieval: {len(raw_results)} chunks\")\n",
    "    for i, result in enumerate(raw_results):\n",
    "        tokens = count_tokens(result.text)\n",
    "        print(f\"  {i}: score={result.score:.3f}, tokens={tokens} - {result.text[:50]}...\")\n",
    "\n",
    "    # Strategy 1: No optimization (just top-k)\n",
    "    baseline = raw_results[:5]\n",
    "    baseline_tokens = sum(count_tokens(r.text) for r in baseline)\n",
    "    print(f\"\\nBaseline (top-5): {len(baseline)} chunks, {baseline_tokens} tokens\")\n",
    "\n",
    "    # Strategy 2: Score filtering only\n",
    "    score_filtered = filter_by_score(raw_results, min_score=0.3)[:5]\n",
    "    score_tokens = sum(count_tokens(r.text) for r in score_filtered)\n",
    "    print(f\"Score filtering: {len(score_filtered)} chunks, {score_tokens} tokens\")\n",
    "\n",
    "    # Strategy 3: Deduplication only\n",
    "    dedup_only = remove_duplicates(raw_results, similarity_threshold=0.8)[:5]\n",
    "    dedup_tokens = sum(count_tokens(r.text) for r in dedup_only)\n",
    "    print(f\"Deduplication: {len(dedup_only)} chunks, {dedup_tokens} tokens\")\n",
    "\n",
    "    # Strategy 4: Full optimization\n",
    "    optimized, stats = optimizer.optimize_with_stats(raw_results, query, target_k=5)\n",
    "\n",
    "    print(f\"\\n=== Optimization Statistics ===\")\n",
    "    for key, value in stats.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"{key}: {value:.3f}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "    return {\n",
    "        \"baseline\": baseline,\n",
    "        \"score_filtered\": score_filtered,\n",
    "        \"dedup_only\": dedup_only,\n",
    "        \"optimized\": optimized,\n",
    "        \"stats\": stats\n",
    "    }\n",
    "\n",
    "# Test with different queries\n",
    "test_queries = [\n",
    "    \"äººå·¥æ™ºæ…§å’Œæ©Ÿå™¨å­¸ç¿’çš„é—œä¿‚\",\n",
    "    \"æ·±åº¦å­¸ç¿’ç¥ç¶“ç¶²è·¯\",\n",
    "    \"è‡ªç„¶èªè¨€è™•ç†æ‡‰ç”¨\"\n",
    "]\n",
    "\n",
    "comparison_results = {}\n",
    "for query in test_queries:\n",
    "    comparison_results[query] = compare_optimization_strategies(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4de12b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 9: Smoke Test - End-to-End Optimization\n",
    "def smoke_test_optimization():\n",
    "    \"\"\"\n",
    "    Comprehensive smoke test for context optimization\n",
    "    \"\"\"\n",
    "    print(\"=== Context Optimization Smoke Test ===\")\n",
    "\n",
    "    # Test query\n",
    "    test_query = \"äººå·¥æ™ºæ…§æŠ€è¡“æ‡‰ç”¨\"\n",
    "\n",
    "    # Get retrieval results\n",
    "    raw_results = simulate_retrieval(test_query, k=8)\n",
    "\n",
    "    if not raw_results:\n",
    "        print(\"âŒ No retrieval results - check index and chunks\")\n",
    "        return False\n",
    "\n",
    "    print(f\"âœ“ Retrieved {len(raw_results)} raw results\")\n",
    "\n",
    "    # Test optimization\n",
    "    try:\n",
    "        optimized_results, stats = optimizer.optimize_with_stats(\n",
    "            raw_results, test_query, target_k=5\n",
    "        )\n",
    "\n",
    "        print(f\"âœ“ Optimization completed: {stats['original_count']} -> {stats['final_count']} chunks\")\n",
    "        print(f\"âœ“ Token reduction: {stats['token_reduction']:.1%}\")\n",
    "        print(f\"âœ“ Average score: {stats['original_avg_score']:.3f} -> {stats['final_avg_score']:.3f}\")\n",
    "\n",
    "        # Verify results are valid\n",
    "        if not optimized_results:\n",
    "            print(\"âŒ No optimized results returned\")\n",
    "            return False\n",
    "\n",
    "        # Check for token budget compliance\n",
    "        total_tokens = sum(count_tokens(r.text) for r in optimized_results)\n",
    "        max_allowed = optimizer.max_tokens - optimizer.reserve_tokens\n",
    "\n",
    "        if total_tokens > max_allowed:\n",
    "            print(f\"âŒ Token budget exceeded: {total_tokens} > {max_allowed}\")\n",
    "            return False\n",
    "\n",
    "        print(f\"âœ“ Token budget respected: {total_tokens}/{max_allowed} tokens\")\n",
    "\n",
    "        # Check for duplicate removal\n",
    "        unique_texts = set(r.text for r in optimized_results)\n",
    "        if len(unique_texts) != len(optimized_results):\n",
    "            print(\"âŒ Duplicates found in optimized results\")\n",
    "            return False\n",
    "\n",
    "        print(f\"âœ“ No duplicates in final results\")\n",
    "\n",
    "        # Display final optimized results\n",
    "        print(f\"\\n=== Final Optimized Results ===\")\n",
    "        for i, result in enumerate(optimized_results):\n",
    "            tokens = count_tokens(result.text)\n",
    "            print(f\"{i+1}. Score: {result.score:.3f}, Tokens: {tokens}\")\n",
    "            print(f\"   Text: {result.text[:80]}...\")\n",
    "            print(f\"   Source: {result.meta.get('source_id', 'N/A')}\")\n",
    "\n",
    "        print(\"\\nâœ… All smoke tests passed!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Optimization failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Run smoke test\n",
    "success = smoke_test_optimization()\n",
    "\n",
    "if success:\n",
    "    print(\"\\nğŸ‰ Context optimization is working correctly!\")\n",
    "    print(\"\\nKey features verified:\")\n",
    "    print(\"- âœ“ Duplicate removal with configurable similarity threshold\")\n",
    "    print(\"- âœ“ MMR reranking for relevance-diversity balance\")\n",
    "    print(\"- âœ“ Token budget management with smart truncation\")\n",
    "    print(\"- âœ“ Adaptive score threshold filtering\")\n",
    "    print(\"- âœ“ End-to-end optimization pipeline\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Some issues detected - check implementation\")\n",
    "\n",
    "# Save configuration for reuse\n",
    "config = {\n",
    "    \"similarity_threshold\": optimizer.similarity_threshold,\n",
    "    \"mmr_lambda\": optimizer.mmr_lambda,\n",
    "    \"max_tokens\": optimizer.max_tokens,\n",
    "    \"reserve_tokens\": optimizer.reserve_tokens,\n",
    "    \"min_score\": optimizer.min_score,\n",
    "    \"use_adaptive_threshold\": optimizer.use_adaptive_threshold\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ“ Current optimizer configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883154ad",
   "metadata": {},
   "source": [
    "## ğŸ¯ What We Built\n",
    "\n",
    "1. **å»é‡æ¼”ç®—æ³•**: ä½¿ç”¨ `rapidfuzz` æª¢æ¸¬æ–‡æœ¬ç›¸ä¼¼åº¦ï¼Œç§»é™¤å†—é¤˜æ®µè½\n",
    "2. **MMR é‡æ’**: å¹³è¡¡ç›¸é—œæ€§èˆ‡å¤šæ¨£æ€§ï¼Œé¿å…çµæœéæ–¼ç›¸ä¼¼\n",
    "3. **Token Budget ç®¡ç†**: å‹•æ…‹æˆªæ–·ï¼Œç¢ºä¿ä¸è¶…é context window é™åˆ¶\n",
    "4. **è‡ªé©æ‡‰åˆ†æ•¸é–¾å€¼**: æ ¹æ“šåˆ†æ•¸åˆ†å¸ƒè‡ªå‹•æ±ºå®šéæ¿¾é–€æª»\n",
    "5. **å®Œæ•´å„ªåŒ–æµæ°´ç·š**: æ•´åˆæ‰€æœ‰ç­–ç•¥çš„ `ContextOptimizer` é¡åˆ¥\n",
    "\n",
    "## âš ï¸ Pitfalls\n",
    "\n",
    "- **éåº¦å»é‡**: ç›¸ä¼¼åº¦é–¾å€¼è¨­å¤ªä½æœƒç§»é™¤ç›¸é—œä½†ä¸å®Œå…¨é‡è¤‡çš„å…§å®¹\n",
    "- **MMR Î» åƒæ•¸**: éœ€è¦æ ¹æ“šå…·é«”æ‡‰ç”¨èª¿æ•´ç›¸é—œæ€§èˆ‡å¤šæ¨£æ€§çš„å¹³è¡¡\n",
    "- **Token è¨ˆç®—**: ä¸åŒ tokenizer çµæœå·®ç•°å¤§ï¼Œéœ€è¦ç•™è¶³å¤  buffer\n",
    "- **åˆ†æ•¸æ¨™æº–åŒ–**: ä¸åŒæª¢ç´¢æ–¹æ³•çš„åˆ†æ•¸ç¯„åœå¯èƒ½ä¸åŒï¼Œéœ€è¦çµ±ä¸€è™•ç†\n",
    "\n",
    "## ğŸ”„ Next Steps\n",
    "\n",
    "1. **å¯¦é©—ä¸åŒåƒæ•¸çµ„åˆ**: é‡å°ä½ çš„è³‡æ–™é›†èª¿å„ª threshold, Î», token budget\n",
    "2. **æ•´åˆåˆ°æª¢ç´¢æµæ°´ç·š**: ä¿®æ”¹ `nb14` çš„æª¢ç´¢å‡½æ•¸åŠ å…¥å„ªåŒ–æ­¥é©Ÿ\n",
    "3. **æ·»åŠ è©•ä¼°æŒ‡æ¨™**: æ¸¬é‡å„ªåŒ–å‰å¾Œçš„ Recall@k å’Œ relevance è®ŠåŒ–\n",
    "4. **æ”¯æ´æ›´å¤šç­–ç•¥**: å¦‚ cluster-based å»é‡ã€semantic chunking ç­‰"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
