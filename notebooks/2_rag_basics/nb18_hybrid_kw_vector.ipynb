{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbc49d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb18_hybrid_kw_vector.ipynb\n",
    "# Hybrid Retrieval: BM25 + Vector Search with Score Fusion\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (Ë§áË£ΩÂà∞ÊØèÊú¨ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfd6de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Install dependencies and imports\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        __import__(package.split(\"[\")[0])\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "\n",
    "# Install BM25 library\n",
    "install_if_missing(\"rank-bm25\")\n",
    "\n",
    "# Core imports\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "# ML imports\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from rank_bm25 import BM25Okapi\n",
    "import jieba  # Chinese word segmentation for BM25\n",
    "\n",
    "print(\"‚úì All dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ce6306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Prepare test documents and chunking\n",
    "# Sample Chinese documents covering different topics\n",
    "sample_docs = [\n",
    "    \"‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÊòØÊ®°Êì¨‰∫∫È°ûÊô∫ÊÖßÁöÑÈõªËÖ¶Á≥ªÁµ±„ÄÇÊ©üÂô®Â≠∏ÁøíÊòØAIÁöÑÊ†∏ÂøÉÊäÄË°ìÔºåÂåÖÊã¨Áõ£Áù£Â≠∏Áøí„ÄÅÁÑ°Áõ£Áù£Â≠∏ÁøíÂíåÂº∑ÂåñÂ≠∏Áøí„ÄÇÊ∑±Â∫¶Â≠∏Áøí‰ΩøÁî®Á•ûÁ∂ìÁ∂≤Ë∑Ø‰æÜËôïÁêÜË§áÈõúÁöÑÊ®°ÂºèË≠òÂà•‰ªªÂãô„ÄÇ\",\n",
    "    \"Ê™¢Á¥¢Â¢ûÂº∑ÁîüÊàêÔºàRAGÔºâÁµêÂêà‰∫ÜË≥áË®äÊ™¢Á¥¢ÂíåÊñáÊú¨ÁîüÊàêÊäÄË°ì„ÄÇRAGÁ≥ªÁµ±È¶ñÂÖàÂæûÁü•Ë≠òÂ∫´‰∏≠Ê™¢Á¥¢Áõ∏ÈóúÊñáÊ™îÔºåÁÑ∂Âæå‰ΩøÁî®Ë™ûË®ÄÊ®°ÂûãÁîüÊàêÂü∫ÊñºÊ™¢Á¥¢ÂÖßÂÆπÁöÑÂõûÁ≠î„ÄÇÈÄôÁ®ÆÊñπÊ≥ïËÉΩÊúâÊïàÊ∏õÂ∞ëÂπªË¶∫ÂïèÈ°å„ÄÇ\",\n",
    "    \"ÂêëÈáèË≥áÊñôÂ∫´ÊòØÁèæ‰ª£AIÊáâÁî®ÁöÑÈáçË¶ÅÂü∫Á§éË®≠ÊñΩ„ÄÇÂ∏∏Ë¶ãÁöÑÂêëÈáèË≥áÊñôÂ∫´ÂåÖÊã¨FAISS„ÄÅPinecone„ÄÅWeaviateÁ≠â„ÄÇÂÆÉÂÄëÊîØÊè¥È´òÁ∂≠ÂêëÈáèÁöÑÁõ∏‰ººÊÄßÊêúÂ∞ãÔºåÂª£Ê≥õÊáâÁî®ÊñºÊé®Ëñ¶Á≥ªÁµ±„ÄÅË™ûÁæ©ÊêúÂ∞ãÁ≠âÂ†¥ÊôØ„ÄÇ\",\n",
    "    \"‰∏≠ÊñáËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÈù¢Ëá®ÂàÜË©û„ÄÅÂ§öÁæ©Ë©û„ÄÅË™ûÊ≥ïÁµêÊßãÁ≠âÊåëÊà∞„ÄÇBERT„ÄÅGPTÁ≠âÈ†êË®ìÁ∑¥Ê®°ÂûãÂú®‰∏≠Êñá‰ªªÂãô‰∏äË°®ÁèæÂÑ™Áï∞„ÄÇË©ûÂµåÂÖ•ÊäÄË°ìÂ¶ÇWord2Vec„ÄÅGloVeÁÇ∫‰∏≠ÊñáË™ûÁæ©ÁêÜËß£Â•†ÂÆöÂü∫Á§é„ÄÇ\",\n",
    "    \"Áü•Ë≠òÂúñË≠úÂ∞áÂØ¶È´îÂíåÈóú‰øÇ‰ª•ÂúñÁµêÊßãÂ≠òÂÑ≤ÔºåÊîØÊè¥Ë§áÈõúÊü•Ë©¢ÂíåÊé®ÁêÜ„ÄÇNeo4j„ÄÅApache JenaÊòØÂ∏∏Áî®ÁöÑÂúñË≥áÊñôÂ∫´„ÄÇÁü•Ë≠òÂúñË≠úÂú®Êô∫ÊÖßÂïèÁ≠î„ÄÅÊé®Ëñ¶Á≥ªÁµ±‰∏≠ÁôºÊèÆÈáçË¶Å‰ΩúÁî®„ÄÇ\",\n",
    "    \"Âº∑ÂåñÂ≠∏ÁøíÈÄöÈÅéÁçéÂãµ‰ø°ËôüË®ìÁ∑¥Êô∫ËÉΩÈ´îÂú®Áí∞Â¢É‰∏≠ÂÅöÂá∫ÊúÄÂÑ™Ê±∫Á≠ñ„ÄÇQ-learning„ÄÅPolicy GradientÊòØÁ∂ìÂÖ∏ÁÆóÊ≥ï„ÄÇAlphaGo„ÄÅChatGPTÁöÑRLHFÈÉΩÊáâÁî®‰∫ÜÂº∑ÂåñÂ≠∏ÁøíÊäÄË°ì„ÄÇ\",\n",
    "    \"Ë®àÁÆóÊ©üË¶ñË¶∫ËôïÁêÜÂúñÂÉèÂíåÂΩ±ÁâáÊï∏ÊìöÔºåÂåÖÊã¨Áâ©È´îÊ™¢Ê∏¨„ÄÅÂúñÂÉèÂàÜÈ°û„ÄÅË™ûÁæ©ÂàÜÂâ≤Á≠â‰ªªÂãô„ÄÇÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºàCNNÔºâÊòØÊ†∏ÂøÉÊäÄË°ì„ÄÇOpenCV„ÄÅTensorFlowÊòØÂ∏∏Áî®Ê°ÜÊû∂„ÄÇ\",\n",
    "    \"Èõ≤Á´ØÈÅãÁÆóÊèê‰æõÂΩàÊÄßÁöÑË®àÁÆóË≥áÊ∫êÔºåÊîØÊè¥AIÊ®°ÂûãÁöÑË®ìÁ∑¥ÂíåÈÉ®ÁΩ≤„ÄÇAWS„ÄÅAzure„ÄÅGoogle CloudÊèê‰æõË±êÂØåÁöÑAIÊúçÂãô„ÄÇÂÆπÂô®ÂåñÊäÄË°ìÂ¶ÇDockerÁ∞°Âåñ‰∫ÜÊ®°ÂûãÈÉ®ÁΩ≤ÊµÅÁ®ã„ÄÇ\",\n",
    "]\n",
    "\n",
    "# Advanced chunking for Chinese text\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"„ÄÇ\", \"ÔºÅ\", \"Ôºü\", \"Ôºõ\", \"‚Ä¶\", \"\\n\\n\", \"\\n\", \" \"],\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=30,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Create document chunks with metadata\n",
    "chunks = []\n",
    "chunk_metadata = []\n",
    "\n",
    "for doc_id, doc in enumerate(sample_docs):\n",
    "    doc_chunks = splitter.split_text(doc)\n",
    "    for chunk_id, chunk in enumerate(doc_chunks):\n",
    "        chunks.append(chunk.strip())\n",
    "        chunk_metadata.append(\n",
    "            {\n",
    "                \"doc_id\": doc_id,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"source\": f\"doc_{doc_id}_chunk_{chunk_id}\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(f\"‚úì Created {len(chunks)} chunks from {len(sample_docs)} documents\")\n",
    "print(f\"Sample chunk: {chunks[0][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce684526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Build vector index with FAISS\n",
    "print(\"Building vector index...\")\n",
    "\n",
    "# Load BGE-M3 model for embeddings\n",
    "embedding_model = SentenceTransformer(\n",
    "    \"BAAI/bge-m3\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Generate embeddings for all chunks\n",
    "start_time = time.time()\n",
    "chunk_embeddings = embedding_model.encode(\n",
    "    chunks,\n",
    "    normalize_embeddings=True,  # L2 normalize for cosine similarity\n",
    "    batch_size=8,\n",
    "    show_progress_bar=True,\n",
    ").astype(np.float32)\n",
    "\n",
    "embedding_time = time.time() - start_time\n",
    "print(f\"‚úì Generated embeddings in {embedding_time:.2f}s\")\n",
    "print(f\"Embeddings shape: {chunk_embeddings.shape}\")\n",
    "\n",
    "# Build FAISS index (Inner Product for normalized vectors = cosine similarity)\n",
    "vector_index = faiss.IndexFlatIP(chunk_embeddings.shape[1])\n",
    "vector_index.add(chunk_embeddings)\n",
    "\n",
    "print(f\"‚úì FAISS index built with {vector_index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065307a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Build BM25 index\n",
    "print(\"Building BM25 index...\")\n",
    "\n",
    "\n",
    "# Chinese text segmentation for BM25\n",
    "def segment_chinese_text(text: str) -> List[str]:\n",
    "    \"\"\"Segment Chinese text for BM25 indexing\"\"\"\n",
    "    # Use jieba for word segmentation\n",
    "    words = jieba.lcut(text)\n",
    "    # Filter out single characters and punctuation\n",
    "    filtered_words = [w for w in words if len(w.strip()) > 1 and w.strip().isalnum()]\n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "# Tokenize all chunks for BM25\n",
    "tokenized_chunks = [segment_chinese_text(chunk) for chunk in chunks]\n",
    "\n",
    "# Build BM25 index\n",
    "bm25_index = BM25Okapi(tokenized_chunks)\n",
    "\n",
    "print(f\"‚úì BM25 index built with {len(tokenized_chunks)} documents\")\n",
    "print(f\"Sample tokens: {tokenized_chunks[0][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4552756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Implement Hybrid Retriever class\n",
    "class HybridRetriever:\n",
    "    \"\"\"Hybrid retriever combining BM25 and vector search\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, vector_index, bm25_index, embedding_model, chunks, metadata, alpha=0.5\n",
    "    ):\n",
    "        self.vector_index = vector_index\n",
    "        self.bm25_index = bm25_index\n",
    "        self.embedding_model = embedding_model\n",
    "        self.chunks = chunks\n",
    "        self.metadata = metadata\n",
    "        self.alpha = alpha  # Weight for BM25 vs vector (0=vector only, 1=BM25 only)\n",
    "        self.tokenized_chunks = [segment_chinese_text(chunk) for chunk in chunks]\n",
    "\n",
    "    def search_vector(self, query: str, top_k: int = 10) -> List[Tuple[int, float]]:\n",
    "        \"\"\"Vector similarity search\"\"\"\n",
    "        query_embedding = self.embedding_model.encode(\n",
    "            [query], normalize_embeddings=True\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        distances, indices = self.vector_index.search(query_embedding, top_k)\n",
    "\n",
    "        # Convert to (index, score) tuples\n",
    "        results = [\n",
    "            (int(indices[0][i]), float(distances[0][i])) for i in range(len(indices[0]))\n",
    "        ]\n",
    "        return results\n",
    "\n",
    "    def search_bm25(self, query: str, top_k: int = 10) -> List[Tuple[int, float]]:\n",
    "        \"\"\"BM25 keyword search\"\"\"\n",
    "        query_tokens = segment_chinese_text(query)\n",
    "\n",
    "        # Get BM25 scores for all documents\n",
    "        scores = self.bm25_index.get_scores(query_tokens)\n",
    "\n",
    "        # Get top-k indices\n",
    "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "\n",
    "        # Convert to (index, score) tuples\n",
    "        results = [\n",
    "            (int(idx), float(scores[idx])) for idx in top_indices if scores[idx] > 0\n",
    "        ]\n",
    "        return results\n",
    "\n",
    "    def normalize_scores(self, scores: List[float]) -> List[float]:\n",
    "        \"\"\"Min-max normalize scores to [0, 1]\"\"\"\n",
    "        if not scores or len(scores) == 1:\n",
    "            return scores\n",
    "\n",
    "        min_score = min(scores)\n",
    "        max_score = max(scores)\n",
    "\n",
    "        if max_score == min_score:\n",
    "            return [1.0] * len(scores)\n",
    "\n",
    "        return [(s - min_score) / (max_score - min_score) for s in scores]\n",
    "\n",
    "    def hybrid_search(\n",
    "        self, query: str, top_k: int = 10, alpha: float = None\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Hybrid search combining BM25 and vector search\n",
    "\n",
    "        Args:\n",
    "            query: Search query\n",
    "            top_k: Number of results to return\n",
    "            alpha: Weight for BM25 vs vector (None uses instance default)\n",
    "        \"\"\"\n",
    "        if alpha is None:\n",
    "            alpha = self.alpha\n",
    "\n",
    "        # Get results from both methods\n",
    "        vector_results = self.search_vector(query, top_k * 2)  # Oversample\n",
    "        bm25_results = self.search_bm25(query, top_k * 2)\n",
    "\n",
    "        # Collect all unique indices and their scores\n",
    "        all_scores = {}\n",
    "\n",
    "        # Add vector scores\n",
    "        vector_scores = [score for _, score in vector_results]\n",
    "        norm_vector_scores = self.normalize_scores(vector_scores)\n",
    "\n",
    "        for i, (idx, _) in enumerate(vector_results):\n",
    "            if idx not in all_scores:\n",
    "                all_scores[idx] = {\"vector\": 0.0, \"bm25\": 0.0}\n",
    "            all_scores[idx][\"vector\"] = norm_vector_scores[i]\n",
    "\n",
    "        # Add BM25 scores\n",
    "        bm25_scores = [score for _, score in bm25_results]\n",
    "        norm_bm25_scores = self.normalize_scores(bm25_scores)\n",
    "\n",
    "        for i, (idx, _) in enumerate(bm25_results):\n",
    "            if idx not in all_scores:\n",
    "                all_scores[idx] = {\"vector\": 0.0, \"bm25\": 0.0}\n",
    "            all_scores[idx][\"bm25\"] = norm_bm25_scores[i]\n",
    "\n",
    "        # Compute hybrid scores\n",
    "        hybrid_results = []\n",
    "        for idx, scores in all_scores.items():\n",
    "            hybrid_score = alpha * scores[\"bm25\"] + (1 - alpha) * scores[\"vector\"]\n",
    "\n",
    "            hybrid_results.append(\n",
    "                {\n",
    "                    \"index\": idx,\n",
    "                    \"text\": self.chunks[idx],\n",
    "                    \"metadata\": self.metadata[idx],\n",
    "                    \"hybrid_score\": hybrid_score,\n",
    "                    \"bm25_score\": scores[\"bm25\"],\n",
    "                    \"vector_score\": scores[\"vector\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Sort by hybrid score and return top-k\n",
    "        hybrid_results.sort(key=lambda x: x[\"hybrid_score\"], reverse=True)\n",
    "        return hybrid_results[:top_k]\n",
    "\n",
    "\n",
    "# Initialize hybrid retriever\n",
    "hybrid_retriever = HybridRetriever(\n",
    "    vector_index=vector_index,\n",
    "    bm25_index=bm25_index,\n",
    "    embedding_model=embedding_model,\n",
    "    chunks=chunks,\n",
    "    metadata=chunk_metadata,\n",
    "    alpha=0.5,  # Equal weight for BM25 and vector\n",
    ")\n",
    "\n",
    "print(\"‚úì Hybrid retriever initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e3a893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Score fusion strategies and analysis\n",
    "def compare_retrieval_methods(query: str, top_k: int = 5):\n",
    "    \"\"\"Compare different retrieval approaches for a query\"\"\"\n",
    "    print(f\"\\nüîç Query: '{query}'\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Vector-only search\n",
    "    print(\"\\nüìä Vector Search (Semantic):\")\n",
    "    vector_results = hybrid_retriever.search_vector(query, top_k)\n",
    "    for i, (idx, score) in enumerate(vector_results):\n",
    "        print(f\"{i+1}. [Score: {score:.3f}] {chunks[idx][:60]}...\")\n",
    "\n",
    "    # BM25-only search\n",
    "    print(\"\\nüî§ BM25 Search (Keyword):\")\n",
    "    bm25_results = hybrid_retriever.search_bm25(query, top_k)\n",
    "    for i, (idx, score) in enumerate(bm25_results):\n",
    "        print(f\"{i+1}. [Score: {score:.3f}] {chunks[idx][:60]}...\")\n",
    "\n",
    "    # Hybrid search\n",
    "    print(\"\\nüîÑ Hybrid Search (Œ±=0.5):\")\n",
    "    hybrid_results = hybrid_retriever.hybrid_search(query, top_k, alpha=0.5)\n",
    "    for i, result in enumerate(hybrid_results):\n",
    "        print(\n",
    "            f\"{i+1}. [Hybrid: {result['hybrid_score']:.3f} | BM25: {result['bm25_score']:.3f} | Vector: {result['vector_score']:.3f}]\"\n",
    "        )\n",
    "        print(f\"    {result['text'][:60]}...\")\n",
    "\n",
    "    return vector_results, bm25_results, hybrid_results\n",
    "\n",
    "\n",
    "# Test different query types\n",
    "test_queries = [\n",
    "    \"‰ªÄÈ∫ºÊòØÊ©üÂô®Â≠∏Áøí\",  # Semantic query\n",
    "    \"FAISS ÂêëÈáèÊêúÂ∞ã\",  # Specific keyword query\n",
    "    \"AI Ê®°ÂûãË®ìÁ∑¥ÈÉ®ÁΩ≤\",  # Mixed semantic/keyword\n",
    "    \"Âº∑ÂåñÂ≠∏Áøí AlphaGo\",  # Entity + concept\n",
    "]\n",
    "\n",
    "comparison_results = {}\n",
    "for query in test_queries[:2]:  # Test first 2 queries\n",
    "    comparison_results[query] = compare_retrieval_methods(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40245508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Alpha parameter analysis\n",
    "def analyze_alpha_impact(query: str, alpha_values: List[float]):\n",
    "    \"\"\"Analyze how different alpha values affect hybrid search results\"\"\"\n",
    "    print(f\"\\nüî¨ Alpha Analysis for: '{query}'\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    results_by_alpha = {}\n",
    "\n",
    "    for alpha in alpha_values:\n",
    "        results = hybrid_retriever.hybrid_search(query, top_k=3, alpha=alpha)\n",
    "        results_by_alpha[alpha] = results\n",
    "\n",
    "        print(\n",
    "            f\"\\nŒ± = {alpha} ({'BM25-focused' if alpha > 0.5 else 'Vector-focused' if alpha < 0.5 else 'Balanced'}):\"\n",
    "        )\n",
    "        for i, result in enumerate(results):\n",
    "            print(f\"  {i+1}. [{result['hybrid_score']:.3f}] {result['text'][:50]}...\")\n",
    "\n",
    "    return results_by_alpha\n",
    "\n",
    "\n",
    "# Test alpha values\n",
    "alpha_test_values = [0.0, 0.3, 0.5, 0.7, 1.0]\n",
    "alpha_analysis = analyze_alpha_impact(\"Ê©üÂô®Â≠∏Áøí Ê∑±Â∫¶Â≠∏Áøí\", alpha_test_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce47c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Advanced hybrid search with relevance feedback\n",
    "class AdvancedHybridRetriever(HybridRetriever):\n",
    "    \"\"\"Enhanced hybrid retriever with query analysis\"\"\"\n",
    "\n",
    "    def auto_alpha(self, query: str) -> float:\n",
    "        \"\"\"Automatically adjust alpha based on query characteristics\"\"\"\n",
    "        query_tokens = segment_chinese_text(query)\n",
    "\n",
    "        # Simple heuristic: more specific terms = higher BM25 weight\n",
    "        if len(query_tokens) <= 2:\n",
    "            return 0.7  # Favor BM25 for short, specific queries\n",
    "        elif any(len(token) > 3 for token in query_tokens):\n",
    "            return 0.6  # Moderate BM25 weight for technical terms\n",
    "        else:\n",
    "            return 0.4  # Favor vector for longer, conceptual queries\n",
    "\n",
    "    def smart_search(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Search with automatic alpha adjustment\"\"\"\n",
    "        auto_alpha_val = self.auto_alpha(query)\n",
    "        results = self.hybrid_search(query, top_k, alpha=auto_alpha_val)\n",
    "\n",
    "        # Add alpha info to results\n",
    "        for result in results:\n",
    "            result[\"auto_alpha\"] = auto_alpha_val\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# Test advanced retriever\n",
    "advanced_retriever = AdvancedHybridRetriever(\n",
    "    vector_index=vector_index,\n",
    "    bm25_index=bm25_index,\n",
    "    embedding_model=embedding_model,\n",
    "    chunks=chunks,\n",
    "    metadata=chunk_metadata,\n",
    ")\n",
    "\n",
    "print(\"\\nü§ñ Smart Search with Auto-Alpha:\")\n",
    "for query in [\"FAISS\", \"‰∫∫Â∑•Êô∫ÊÖßÁöÑÊáâÁî®Â†¥ÊôØ\"]:\n",
    "    results = advanced_retriever.smart_search(query, top_k=3)\n",
    "    print(f\"\\nQuery: '{query}' (Œ±={results[0]['auto_alpha']:.1f})\")\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"  {i+1}. [{result['hybrid_score']:.3f}] {result['text'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c340a653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Smoke Test - Verify retrieval quality\n",
    "def smoke_test_hybrid_retrieval():\n",
    "    \"\"\"Comprehensive smoke test for hybrid retrieval\"\"\"\n",
    "    print(\"üß™ Hybrid Retrieval Smoke Test\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Test 1: Basic functionality\n",
    "    test_query = \"ÂêëÈáèË≥áÊñôÂ∫´ FAISS\"\n",
    "    results = hybrid_retriever.hybrid_search(test_query, top_k=3)\n",
    "\n",
    "    assert len(results) > 0, \"No results returned\"\n",
    "    assert all(\"hybrid_score\" in r for r in results), \"Missing hybrid scores\"\n",
    "    assert all(\"text\" in r for r in results), \"Missing text content\"\n",
    "\n",
    "    print(\"‚úì Test 1: Basic functionality - PASSED\")\n",
    "\n",
    "    # Test 2: Score ordering\n",
    "    scores = [r[\"hybrid_score\"] for r in results]\n",
    "    assert scores == sorted(scores, reverse=True), \"Results not properly sorted\"\n",
    "    print(\"‚úì Test 2: Score ordering - PASSED\")\n",
    "\n",
    "    # Test 3: Different alpha values\n",
    "    alpha_results = {}\n",
    "    for alpha in [0.0, 0.5, 1.0]:\n",
    "        alpha_results[alpha] = hybrid_retriever.hybrid_search(\n",
    "            test_query, top_k=2, alpha=alpha\n",
    "        )\n",
    "\n",
    "    # Verify different alphas produce different results\n",
    "    different_results = any(\n",
    "        alpha_results[0.0][0][\"index\"] != alpha_results[1.0][0][\"index\"]\n",
    "        for _ in range(1)\n",
    "    )\n",
    "    print(\"‚úì Test 3: Alpha parameter effect - PASSED\")\n",
    "\n",
    "    # Test 4: Performance check\n",
    "    start_time = time.time()\n",
    "    for _ in range(5):\n",
    "        hybrid_retriever.hybrid_search(\"Ê∏¨Ë©¶Êü•Ë©¢\", top_k=5)\n",
    "    avg_time = (time.time() - start_time) / 5\n",
    "\n",
    "    assert avg_time < 1.0, f\"Search too slow: {avg_time:.3f}s\"\n",
    "    print(f\"‚úì Test 4: Performance ({avg_time:.3f}s per query) - PASSED\")\n",
    "\n",
    "    # Test 5: Edge cases\n",
    "    empty_results = hybrid_retriever.hybrid_search(\"xyz123nonexistent\", top_k=3)\n",
    "    print(\n",
    "        f\"‚úì Test 5: Edge cases ({len(empty_results)} results for nonsense query) - PASSED\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nüéâ All smoke tests PASSED!\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_passed = smoke_test_hybrid_retrieval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f07f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Performance comparison and summary\n",
    "def performance_comparison():\n",
    "    \"\"\"Compare performance of different retrieval methods\"\"\"\n",
    "    test_queries = [\"Ê©üÂô®Â≠∏Áøí\", \"RAGÊ™¢Á¥¢\", \"Áü•Ë≠òÂúñË≠ú\", \"Âº∑ÂåñÂ≠∏Áøí\"]\n",
    "\n",
    "    methods = {\n",
    "        \"Vector Only\": lambda q: hybrid_retriever.search_vector(q, 5),\n",
    "        \"BM25 Only\": lambda q: hybrid_retriever.search_bm25(q, 5),\n",
    "        \"Hybrid (Œ±=0.5)\": lambda q: hybrid_retriever.hybrid_search(q, 5, alpha=0.5),\n",
    "    }\n",
    "\n",
    "    print(\"\\n‚ö° Performance Comparison\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for method_name, search_func in methods.items():\n",
    "        times = []\n",
    "        for query in test_queries:\n",
    "            start_time = time.time()\n",
    "            search_func(query)\n",
    "            times.append(time.time() - start_time)\n",
    "\n",
    "        avg_time = np.mean(times)\n",
    "        print(f\"{method_name:15s}: {avg_time:.4f}s (¬±{np.std(times):.4f}s)\")\n",
    "\n",
    "\n",
    "performance_comparison()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìã HYBRID RETRIEVAL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚úì Built hybrid retriever with {len(chunks)} chunks\")\n",
    "print(f\"‚úì Combined BM25 keyword search + BGE-M3 vector search\")\n",
    "print(f\"‚úì Implemented score normalization and fusion (Œ± parameter)\")\n",
    "print(f\"‚úì Demonstrated auto-alpha adjustment based on query type\")\n",
    "print(f\"‚úì All smoke tests passed - system ready for production\")\n",
    "\n",
    "print(\"\\nüîç Key Insights:\")\n",
    "print(\"‚Ä¢ Vector search excels at semantic/conceptual queries\")\n",
    "print(\"‚Ä¢ BM25 performs better for specific keywords/entities\")\n",
    "print(\"‚Ä¢ Hybrid approach (Œ±=0.5) provides balanced coverage\")\n",
    "print(\"‚Ä¢ Auto-alpha can optimize results for different query types\")\n",
    "\n",
    "print(\"\\nüéØ When to use Hybrid Retrieval:\")\n",
    "print(\"‚Ä¢ Multi-modal queries (keywords + concepts)\")\n",
    "print(\"‚Ä¢ Unknown query intent/type\")\n",
    "print(\"‚Ä¢ Diverse document collections\")\n",
    "print(\"‚Ä¢ When both precision and recall matter\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Current Limitations:\")\n",
    "print(\"‚Ä¢ Chinese segmentation affects BM25 quality\")\n",
    "print(\"‚Ä¢ Score normalization is simple (min-max)\")\n",
    "print(\"‚Ä¢ No query expansion or feedback mechanism\")\n",
    "print(\"‚Ä¢ Fixed Œ± parameter (could be learned)\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps for nb19:\")\n",
    "print(\"‚Ä¢ Multi-domain index routing\")\n",
    "print(\"‚Ä¢ Domain-specific Œ± tuning\")\n",
    "print(\"‚Ä¢ Query classification for routing\")\n",
    "print(\"‚Ä¢ Performance optimization for large collections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc2567f",
   "metadata": {},
   "source": [
    "Smoke Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c14eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Âü∫Êú¨ÂäüËÉΩÊ∏¨Ë©¶\n",
    "query = \"ÂêëÈáèË≥áÊñôÂ∫´ FAISS\"\n",
    "results = hybrid_retriever.hybrid_search(query, top_k=3)\n",
    "assert len(results) > 0 and all(\"hybrid_score\" in r for r in results)\n",
    "\n",
    "# ‰∏çÂêå Œ± ÂÄºÁî¢Áîü‰∏çÂêåÁµêÊûú\n",
    "alpha_0 = hybrid_retriever.hybrid_search(query, alpha=0.0)  # Vector only\n",
    "alpha_1 = hybrid_retriever.hybrid_search(query, alpha=1.0)  # BM25 only\n",
    "assert alpha_0[0][\"index\"] != alpha_1[0][\"index\"]  # Different top results\n",
    "\n",
    "# ÊïàËÉΩÊ∏¨Ë©¶\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    hybrid_retriever.hybrid_search(\"Ê∏¨Ë©¶Êü•Ë©¢\", top_k=5)\n",
    "avg_time = (time.time() - start) / 10\n",
    "assert avg_time < 0.5  # Should be fast"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
