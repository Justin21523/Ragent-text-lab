{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0235efa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb17_incremental_update.ipynb\n",
    "# Goals: Incremental document embedding and index updates\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (è¤‡è£½åˆ°æ¯æœ¬ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cdc2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import Dependencies and Reuse RAG Modules\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Create directories\n",
    "for dir_name in [\"indices\", \"outs\", \"data\"]:\n",
    "    Path(dir_name).mkdir(exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa4a171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Build Initial Index (Simulate Existing Knowledge Base)\n",
    "def create_initial_index():\n",
    "    \"\"\"Create a baseline index with some sample documents\"\"\"\n",
    "\n",
    "    # Sample Chinese documents (simulate existing knowledge base)\n",
    "    initial_docs = [\n",
    "        \"äººå·¥æ™ºæ…§æ˜¯é›»è…¦ç§‘å­¸çš„ä¸€å€‹åˆ†æ”¯ï¼Œè‡´åŠ›æ–¼å‰µå»ºèƒ½å¤ åŸ·è¡Œé€šå¸¸éœ€è¦äººé¡æ™ºæ…§çš„ä»»å‹™çš„ç³»çµ±ã€‚\",\n",
    "        \"æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§çš„ä¸€å€‹å­é ˜åŸŸï¼Œå°ˆæ³¨æ–¼é–‹ç™¼èƒ½å¤ å¾è³‡æ–™ä¸­å­¸ç¿’çš„æ¼”ç®—æ³•ã€‚\",\n",
    "        \"æ·±åº¦å­¸ç¿’ä½¿ç”¨äººå·¥ç¥ç¶“ç¶²è·¯ä¾†æ¨¡æ“¬äººè…¦çš„é‹ä½œæ–¹å¼ï¼Œåœ¨å½±åƒè­˜åˆ¥å’Œè‡ªç„¶èªè¨€è™•ç†æ–¹é¢è¡¨ç¾å‡ºè‰²ã€‚\",\n",
    "        \"è‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºæ…§çš„ä¸€å€‹åˆ†æ”¯ï¼Œå°ˆé–€è™•ç†é›»è…¦èˆ‡äººé¡èªè¨€ä¹‹é–“çš„äº’å‹•ã€‚\",\n",
    "        \"RAGï¼ˆæª¢ç´¢å¢å¼·ç”Ÿæˆï¼‰çµåˆäº†è³‡è¨Šæª¢ç´¢å’Œèªè¨€ç”Ÿæˆï¼Œæä¾›æ›´æº–ç¢ºçš„å•ç­”ç³»çµ±ã€‚\",\n",
    "    ]\n",
    "\n",
    "    # Text splitter for Chinese\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"ã€‚\", \"ï¼\", \"ï¼Ÿ\", \"ï¼›\", \"â€¦\", \"\\n\\n\", \"\\n\", \" \"],\n",
    "        chunk_size=200,\n",
    "        chunk_overlap=20,\n",
    "    )\n",
    "\n",
    "    # Create documents with metadata\n",
    "    documents = []\n",
    "    for i, text in enumerate(initial_docs):\n",
    "        doc = Document(\n",
    "            page_content=text,\n",
    "            metadata={\n",
    "                \"source_id\": f\"initial_doc_{i:03d}\",\n",
    "                \"domain\": \"ai_basics\",\n",
    "                \"created_at\": \"2024-01-01T00:00:00\",\n",
    "                \"version\": 1,\n",
    "            },\n",
    "        )\n",
    "        documents.append(doc)\n",
    "\n",
    "    # Initialize embedding model (low VRAM)\n",
    "    embedding_model = SentenceTransformer(\"BAAI/bge-small-zh-v1.5\")\n",
    "\n",
    "    # Split documents\n",
    "    all_chunks = []\n",
    "    for doc in documents:\n",
    "        chunks = splitter.split_documents([doc])\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "    # Generate embeddings\n",
    "    texts = [chunk.page_content for chunk in all_chunks]\n",
    "    vectors = embedding_model.encode(\n",
    "        texts, normalize_embeddings=True, batch_size=16\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    # Create FAISS index\n",
    "    dimension = vectors.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    index.add(vectors)\n",
    "\n",
    "    # Save chunks metadata\n",
    "    chunks_data = []\n",
    "    for i, chunk in enumerate(all_chunks):\n",
    "        chunks_data.append(\n",
    "            {\"id\": i, \"text\": chunk.page_content, \"metadata\": chunk.metadata}\n",
    "        )\n",
    "\n",
    "    # Save to files\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    faiss.write_index(index, f\"indices/knowledge_base_{timestamp}.faiss\")\n",
    "\n",
    "    with open(f\"indices/chunks_{timestamp}.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for chunk_data in chunks_data:\n",
    "            f.write(json.dumps(chunk_data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # Create current symlinks\n",
    "    if Path(\"indices/current.faiss\").exists():\n",
    "        Path(\"indices/current.faiss\").unlink()\n",
    "    if Path(\"indices/current_chunks.jsonl\").exists():\n",
    "        Path(\"indices/current_chunks.jsonl\").unlink()\n",
    "\n",
    "    Path(\"indices/current.faiss\").symlink_to(f\"knowledge_base_{timestamp}.faiss\")\n",
    "    Path(\"indices/current_chunks.jsonl\").symlink_to(f\"chunks_{timestamp}.jsonl\")\n",
    "\n",
    "    print(f\"âœ“ Initial index created with {len(chunks_data)} chunks\")\n",
    "    print(f\"  Index dimension: {dimension}\")\n",
    "    print(f\"  Index total: {index.ntotal}\")\n",
    "    print(f\"  Files: knowledge_base_{timestamp}.faiss, chunks_{timestamp}.jsonl\")\n",
    "\n",
    "    return embedding_model, index, chunks_data, timestamp\n",
    "\n",
    "\n",
    "# Create initial index\n",
    "embedding_model, current_index, current_chunks, base_timestamp = create_initial_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf1d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Incremental Update Functions\n",
    "class IncrementalRAGUpdater:\n",
    "    \"\"\"Handles incremental updates to RAG index\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model: SentenceTransformer):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"ã€‚\", \"ï¼\", \"ï¼Ÿ\", \"ï¼›\", \"â€¦\", \"\\n\\n\", \"\\n\", \" \"],\n",
    "            chunk_size=200,\n",
    "            chunk_overlap=20,\n",
    "        )\n",
    "\n",
    "    def add_documents(\n",
    "        self, new_docs: List[Dict[str, Any]], backup: bool = True\n",
    "    ) -> Tuple[faiss.Index, List[Dict], str]:\n",
    "        \"\"\"\n",
    "        Add new documents to existing index\n",
    "\n",
    "        Args:\n",
    "            new_docs: List of {\"text\": str, \"metadata\": dict}\n",
    "            backup: Whether to backup current index before update\n",
    "\n",
    "        Returns:\n",
    "            Updated index, updated chunks list, new timestamp\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ“¥ Processing {len(new_docs)} new documents...\")\n",
    "\n",
    "        # Load current index and chunks\n",
    "        current_index = faiss.read_index(\"indices/current.faiss\")\n",
    "        current_chunks = []\n",
    "        with open(\"indices/current_chunks.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                current_chunks.append(json.loads(line.strip()))\n",
    "\n",
    "        print(\n",
    "            f\"ğŸ“Š Current index stats: {current_index.ntotal} vectors, {len(current_chunks)} chunks\"\n",
    "        )\n",
    "\n",
    "        # Backup if requested\n",
    "        if backup:\n",
    "            backup_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_backup\"\n",
    "            shutil.copy(\n",
    "                \"indices/current.faiss\",\n",
    "                f\"indices/knowledge_base_{backup_timestamp}.faiss\",\n",
    "            )\n",
    "            shutil.copy(\n",
    "                \"indices/current_chunks.jsonl\",\n",
    "                f\"indices/chunks_{backup_timestamp}.jsonl\",\n",
    "            )\n",
    "            print(f\"ğŸ’¾ Backup created: {backup_timestamp}\")\n",
    "\n",
    "        # Process new documents\n",
    "        new_chunks = []\n",
    "        for doc in new_docs:\n",
    "            # Create document object\n",
    "            document = Document(\n",
    "                page_content=doc[\"text\"],\n",
    "                metadata={\n",
    "                    **doc.get(\"metadata\", {}),\n",
    "                    \"added_at\": datetime.now().isoformat(),\n",
    "                    \"version\": doc.get(\"metadata\", {}).get(\"version\", 1),\n",
    "                },\n",
    "            )\n",
    "\n",
    "            # Split into chunks\n",
    "            chunks = self.splitter.split_documents([document])\n",
    "            new_chunks.extend(chunks)\n",
    "\n",
    "        if not new_chunks:\n",
    "            print(\"âš ï¸ No new chunks generated from documents\")\n",
    "            return current_index, current_chunks, \"\"\n",
    "\n",
    "        # Generate embeddings for new chunks\n",
    "        new_texts = [chunk.page_content for chunk in new_chunks]\n",
    "        print(f\"ğŸ”¢ Generating embeddings for {len(new_texts)} new chunks...\")\n",
    "\n",
    "        new_vectors = self.embedding_model.encode(\n",
    "            new_texts, normalize_embeddings=True, batch_size=16, show_progress_bar=True\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        # Add to index\n",
    "        start_id = len(current_chunks)\n",
    "        current_index.add(new_vectors)\n",
    "\n",
    "        # Update chunks metadata\n",
    "        for i, chunk in enumerate(new_chunks):\n",
    "            chunk_data = {\n",
    "                \"id\": start_id + i,\n",
    "                \"text\": chunk.page_content,\n",
    "                \"metadata\": chunk.metadata,\n",
    "            }\n",
    "            current_chunks.append(chunk_data)\n",
    "\n",
    "        # Save updated index\n",
    "        new_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        new_index_path = f\"indices/knowledge_base_{new_timestamp}.faiss\"\n",
    "        new_chunks_path = f\"indices/chunks_{new_timestamp}.jsonl\"\n",
    "\n",
    "        faiss.write_index(current_index, new_index_path)\n",
    "\n",
    "        with open(new_chunks_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for chunk_data in current_chunks:\n",
    "                f.write(json.dumps(chunk_data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        # Update current symlinks\n",
    "        Path(\"indices/current.faiss\").unlink()\n",
    "        Path(\"indices/current_chunks.jsonl\").unlink()\n",
    "        Path(\"indices/current.faiss\").symlink_to(\n",
    "            f\"knowledge_base_{new_timestamp}.faiss\"\n",
    "        )\n",
    "        Path(\"indices/current_chunks.jsonl\").symlink_to(f\"chunks_{new_timestamp}.jsonl\")\n",
    "\n",
    "        print(f\"âœ… Index updated successfully!\")\n",
    "        print(\n",
    "            f\"  New total: {current_index.ntotal} vectors, {len(current_chunks)} chunks\"\n",
    "        )\n",
    "        print(f\"  Added: {len(new_chunks)} chunks from {len(new_docs)} documents\")\n",
    "        print(f\"  Files: {new_index_path}, {new_chunks_path}\")\n",
    "\n",
    "        return current_index, current_chunks, new_timestamp\n",
    "\n",
    "    def search(self, query: str, k: int = 5) -> List[Tuple[str, Dict, float]]:\n",
    "        \"\"\"Search updated index\"\"\"\n",
    "        # Load current index and chunks\n",
    "        index = faiss.read_index(\"indices/current.faiss\")\n",
    "        chunks = []\n",
    "        with open(\"indices/current_chunks.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                chunks.append(json.loads(line.strip()))\n",
    "\n",
    "        # Encode query\n",
    "        query_vector = self.embedding_model.encode(\n",
    "            [query], normalize_embeddings=True\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        # Search\n",
    "        scores, indices = index.search(query_vector, k)\n",
    "\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx < len(chunks):\n",
    "                chunk = chunks[idx]\n",
    "                results.append((chunk[\"text\"], chunk[\"metadata\"], float(score)))\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# Initialize updater\n",
    "updater = IncrementalRAGUpdater(embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76823033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Version Management and Backup Strategy\n",
    "def list_index_versions():\n",
    "    \"\"\"List all available index versions\"\"\"\n",
    "    indices_dir = Path(\"indices\")\n",
    "    versions = []\n",
    "\n",
    "    for file_path in indices_dir.glob(\"knowledge_base_*.faiss\"):\n",
    "        timestamp = file_path.stem.replace(\"knowledge_base_\", \"\")\n",
    "        size = file_path.stat().st_size\n",
    "\n",
    "        # Count chunks if corresponding file exists\n",
    "        chunks_file = indices_dir / f\"chunks_{timestamp}.jsonl\"\n",
    "        chunk_count = 0\n",
    "        if chunks_file.exists():\n",
    "            with open(chunks_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                chunk_count = sum(1 for _ in f)\n",
    "\n",
    "        versions.append(\n",
    "            {\n",
    "                \"timestamp\": timestamp,\n",
    "                \"size_mb\": round(size / 1024 / 1024, 2),\n",
    "                \"chunk_count\": chunk_count,\n",
    "                \"is_current\": file_path.name\n",
    "                == Path(\"indices/current.faiss\").resolve().name,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Sort by timestamp\n",
    "    versions.sort(key=lambda x: x[\"timestamp\"], reverse=True)\n",
    "    return versions\n",
    "\n",
    "\n",
    "def cleanup_old_versions(keep_latest: int = 5):\n",
    "    \"\"\"Clean up old index versions, keeping only the latest N\"\"\"\n",
    "    versions = list_index_versions()\n",
    "\n",
    "    if len(versions) <= keep_latest:\n",
    "        print(f\"ğŸ“ Only {len(versions)} versions found, no cleanup needed\")\n",
    "        return\n",
    "\n",
    "    # Keep current + latest N-1\n",
    "    to_delete = versions[keep_latest:]\n",
    "\n",
    "    print(f\"ğŸ—‘ï¸ Cleaning up {len(to_delete)} old versions...\")\n",
    "    for version in to_delete:\n",
    "        timestamp = version[\"timestamp\"]\n",
    "\n",
    "        # Skip if it's current (safety check)\n",
    "        if version[\"is_current\"]:\n",
    "            continue\n",
    "\n",
    "        # Delete files\n",
    "        faiss_file = Path(f\"indices/knowledge_base_{timestamp}.faiss\")\n",
    "        chunks_file = Path(f\"indices/chunks_{timestamp}.jsonl\")\n",
    "\n",
    "        if faiss_file.exists():\n",
    "            faiss_file.unlink()\n",
    "            print(f\"  Deleted: {faiss_file.name}\")\n",
    "        if chunks_file.exists():\n",
    "            chunks_file.unlink()\n",
    "            print(f\"  Deleted: {chunks_file.name}\")\n",
    "\n",
    "\n",
    "def rollback_to_version(timestamp: str):\n",
    "    \"\"\"Rollback to a specific version\"\"\"\n",
    "    target_faiss = Path(f\"indices/knowledge_base_{timestamp}.faiss\")\n",
    "    target_chunks = Path(f\"indices/chunks_{timestamp}.jsonl\")\n",
    "\n",
    "    if not target_faiss.exists() or not target_chunks.exists():\n",
    "        print(f\"âŒ Version {timestamp} not found\")\n",
    "        return False\n",
    "\n",
    "    # Update symlinks\n",
    "    Path(\"indices/current.faiss\").unlink()\n",
    "    Path(\"indices/current_chunks.jsonl\").unlink()\n",
    "    Path(\"indices/current.faiss\").symlink_to(f\"knowledge_base_{timestamp}.faiss\")\n",
    "    Path(\"indices/current_chunks.jsonl\").symlink_to(f\"chunks_{timestamp}.jsonl\")\n",
    "\n",
    "    print(f\"âª Rolled back to version {timestamp}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# Display current versions\n",
    "print(\"ğŸ“‹ Current index versions:\")\n",
    "versions = list_index_versions()\n",
    "for v in versions:\n",
    "    current_mark = \" (CURRENT)\" if v[\"is_current\"] else \"\"\n",
    "    print(\n",
    "        f\"  {v['timestamp']}: {v['chunk_count']} chunks, {v['size_mb']} MB{current_mark}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abccbe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Consistency Check and Validation\n",
    "def validate_index_consistency():\n",
    "    \"\"\"Validate that index and chunks are consistent\"\"\"\n",
    "    print(\"ğŸ” Validating index consistency...\")\n",
    "\n",
    "    # Load index and chunks\n",
    "    try:\n",
    "        index = faiss.read_index(\"indices/current.faiss\")\n",
    "        chunks = []\n",
    "        with open(\"indices/current_chunks.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                chunks.append(json.loads(line.strip()))\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading files: {e}\")\n",
    "        return False\n",
    "\n",
    "    # Check counts match\n",
    "    if index.ntotal != len(chunks):\n",
    "        print(\n",
    "            f\"âŒ Count mismatch: index has {index.ntotal} vectors, chunks file has {len(chunks)} entries\"\n",
    "        )\n",
    "        return False\n",
    "\n",
    "    # Check chunk IDs are sequential\n",
    "    expected_ids = set(range(len(chunks)))\n",
    "    actual_ids = set(chunk[\"id\"] for chunk in chunks)\n",
    "    if expected_ids != actual_ids:\n",
    "        missing = expected_ids - actual_ids\n",
    "        extra = actual_ids - expected_ids\n",
    "        if missing:\n",
    "            print(f\"âŒ Missing chunk IDs: {sorted(missing)[:10]}...\")\n",
    "        if extra:\n",
    "            print(f\"âŒ Extra chunk IDs: {sorted(extra)[:10]}...\")\n",
    "        return False\n",
    "\n",
    "    # Sample validation: re-embed a few chunks and check similarity\n",
    "    sample_size = min(5, len(chunks))\n",
    "    sample_chunks = chunks[:sample_size]\n",
    "    sample_texts = [chunk[\"text\"] for chunk in sample_chunks]\n",
    "\n",
    "    # Re-generate embeddings\n",
    "    sample_vectors = embedding_model.encode(\n",
    "        sample_texts, normalize_embeddings=True\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    # Get original vectors from index\n",
    "    original_vectors = index.reconstruct_batch(list(range(sample_size)))\n",
    "\n",
    "    # Check similarity (should be very close to 1.0)\n",
    "    similarities = []\n",
    "    for i in range(sample_size):\n",
    "        sim = np.dot(sample_vectors[i], original_vectors[i])\n",
    "        similarities.append(sim)\n",
    "\n",
    "    avg_similarity = np.mean(similarities)\n",
    "    if avg_similarity < 0.95:\n",
    "        print(f\"âŒ Vector similarity too low: {avg_similarity:.4f}\")\n",
    "        return False\n",
    "\n",
    "    print(f\"âœ… Index validation passed!\")\n",
    "    print(f\"  Vectors: {index.ntotal}\")\n",
    "    print(f\"  Chunks: {len(chunks)}\")\n",
    "    print(f\"  Vector similarity: {avg_similarity:.4f}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# Run validation\n",
    "is_consistent = validate_index_consistency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f585b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Performance Comparison (Incremental vs Full Rebuild)\n",
    "def benchmark_update_methods(new_docs: List[Dict[str, Any]]):\n",
    "    \"\"\"Compare incremental update vs full rebuild performance\"\"\"\n",
    "    print(\"âš¡ Benchmarking update methods...\")\n",
    "\n",
    "    # Save current state\n",
    "    current_chunks_backup = []\n",
    "    with open(\"indices/current_chunks.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            current_chunks_backup.append(json.loads(line.strip()))\n",
    "\n",
    "    original_count = len(current_chunks_backup)\n",
    "\n",
    "    # Method 1: Incremental Update\n",
    "    print(\"\\nğŸ”„ Testing incremental update...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    _, updated_chunks, _ = updater.add_documents(new_docs, backup=False)\n",
    "\n",
    "    incremental_time = time.time() - start_time\n",
    "    incremental_count = len(updated_chunks)\n",
    "\n",
    "    print(f\"  Time: {incremental_time:.2f}s\")\n",
    "    print(f\"  Final count: {incremental_count}\")\n",
    "\n",
    "    # Method 2: Full Rebuild (simulation)\n",
    "    print(\"\\nğŸ”„ Simulating full rebuild...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Combine all documents\n",
    "    all_texts = [chunk[\"text\"] for chunk in current_chunks_backup]\n",
    "    all_texts.extend([doc[\"text\"] for doc in new_docs])\n",
    "\n",
    "    # Re-embed everything (simulation - don't actually rebuild)\n",
    "    rebuild_vectors = embedding_model.encode(\n",
    "        all_texts, normalize_embeddings=True, batch_size=16, show_progress_bar=False\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    rebuild_time = time.time() - start_time\n",
    "\n",
    "    print(f\"  Time: {rebuild_time:.2f}s\")\n",
    "    print(f\"  Final count: {len(all_texts)}\")\n",
    "\n",
    "    # Results\n",
    "    print(f\"\\nğŸ“Š Performance Comparison:\")\n",
    "    print(f\"  Incremental: {incremental_time:.2f}s\")\n",
    "    print(f\"  Full rebuild: {rebuild_time:.2f}s\")\n",
    "    print(f\"  Speedup: {rebuild_time/incremental_time:.1f}x faster\")\n",
    "    print(f\"  Added documents: {len(new_docs)}\")\n",
    "    print(f\"  Original chunks: {original_count}\")\n",
    "    print(f\"  New total: {incremental_count}\")\n",
    "\n",
    "    return {\n",
    "        \"incremental_time\": incremental_time,\n",
    "        \"rebuild_time\": rebuild_time,\n",
    "        \"speedup\": rebuild_time / incremental_time,\n",
    "        \"docs_added\": len(new_docs),\n",
    "        \"original_count\": original_count,\n",
    "        \"final_count\": incremental_count,\n",
    "    }\n",
    "\n",
    "\n",
    "# Prepare test documents\n",
    "test_new_docs = [\n",
    "    {\n",
    "        \"text\": \"å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚GPTã€BERTç­‰ï¼Œåœ¨è‡ªç„¶èªè¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢å±•ç¾äº†é©šäººçš„èƒ½åŠ›ã€‚é€™äº›æ¨¡å‹é€šéåœ¨å¤§è¦æ¨¡æ–‡æœ¬è³‡æ–™ä¸Šé€²è¡Œé è¨“ç·´ï¼Œå­¸ç¿’åˆ°äº†è±å¯Œçš„èªè¨€çŸ¥è­˜ã€‚\",\n",
    "        \"metadata\": {\n",
    "            \"source_id\": \"new_doc_001\",\n",
    "            \"domain\": \"llm_research\",\n",
    "            \"author\": \"AIç ”ç©¶å“¡\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"å‘é‡è³‡æ–™åº«æ˜¯ç¾ä»£AIæ‡‰ç”¨çš„é‡è¦åŸºç¤è¨­æ–½ï¼Œå®ƒèƒ½å¤ é«˜æ•ˆåœ°å„²å­˜å’Œæª¢ç´¢é«˜ç¶­å‘é‡è³‡æ–™ã€‚FAISSã€Pineconeã€Weaviateç­‰æ˜¯å¸¸ç”¨çš„å‘é‡è³‡æ–™åº«è§£æ±ºæ–¹æ¡ˆã€‚\",\n",
    "        \"metadata\": {\n",
    "            \"source_id\": \"new_doc_002\",\n",
    "            \"domain\": \"vector_db\",\n",
    "            \"author\": \"è³‡æ–™å·¥ç¨‹å¸«\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"æç¤ºå·¥ç¨‹ï¼ˆPrompt Engineeringï¼‰æ˜¯å„ªåŒ–AIæ¨¡å‹è¼¸å‡ºçš„é—œéµæŠ€è¡“ã€‚é€éç²¾å¿ƒè¨­è¨ˆçš„æç¤ºè©ï¼Œæˆ‘å€‘å¯ä»¥å¼•å°æ¨¡å‹ç”¢ç”Ÿæ›´æº–ç¢ºã€æ›´æœ‰ç”¨çš„å›æ‡‰ã€‚\",\n",
    "        \"metadata\": {\n",
    "            \"source_id\": \"new_doc_003\",\n",
    "            \"domain\": \"prompt_engineering\",\n",
    "            \"author\": \"AIæ‡‰ç”¨å°ˆå®¶\",\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_results = benchmark_update_methods(test_new_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557fe371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Smoke Test - Verify New Content is Retrievable\n",
    "print(\"ğŸ§ª Smoke Test: Verifying new content is retrievable...\")\n",
    "\n",
    "# Test queries that should hit new content\n",
    "test_queries = [\n",
    "    \"ä»€éº¼æ˜¯å¤§å‹èªè¨€æ¨¡å‹ï¼Ÿ\",\n",
    "    \"å‘é‡è³‡æ–™åº«æœ‰å“ªäº›ï¼Ÿ\",\n",
    "    \"æç¤ºå·¥ç¨‹çš„ä½œç”¨\",\n",
    "    \"FAISSå‘é‡æª¢ç´¢\",\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ” Testing retrieval of new content:\")\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    results = updater.search(query, k=3)\n",
    "\n",
    "    for i, (text, metadata, score) in enumerate(results):\n",
    "        source_id = metadata.get(\"source_id\", \"unknown\")\n",
    "        is_new = source_id.startswith(\"new_doc_\")\n",
    "        new_flag = \" ğŸ†•\" if is_new else \"\"\n",
    "        print(f\"  {i+1}. [{source_id}]{new_flag} (score: {score:.3f})\")\n",
    "        print(f\"     {text[:100]}...\")\n",
    "\n",
    "# Check if new documents are being retrieved\n",
    "new_retrievals = 0\n",
    "total_retrievals = 0\n",
    "\n",
    "for query in test_queries:\n",
    "    results = updater.search(query, k=3)\n",
    "    for text, metadata, score in results:\n",
    "        total_retrievals += 1\n",
    "        if metadata.get(\"source_id\", \"\").startswith(\"new_doc_\"):\n",
    "            new_retrievals += 1\n",
    "\n",
    "new_content_ratio = new_retrievals / total_retrievals if total_retrievals > 0 else 0\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Smoke Test Results:\")\n",
    "print(f\"  Total retrievals: {total_retrievals}\")\n",
    "print(f\"  New content hits: {new_retrievals}\")\n",
    "print(f\"  New content ratio: {new_content_ratio:.1%}\")\n",
    "\n",
    "if new_content_ratio > 0:\n",
    "    print(\"âœ… PASS: New content is being retrieved successfully\")\n",
    "else:\n",
    "    print(\"âŒ FAIL: New content not being retrieved - check embedding/indexing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ddf42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Production Recommendations and Pitfalls\n",
    "print(\n",
    "    \"\"\"\n",
    "ğŸ­ Production Environment Recommendations:\n",
    "\n",
    "âœ… Best Practices:\n",
    "1. **Backup Strategy**: Always backup before updates (set backup=True)\n",
    "2. **Version Management**: Keep 3-5 recent versions for rollback\n",
    "3. **Batch Updates**: Group multiple documents for efficiency\n",
    "4. **Consistency Checks**: Validate after each update\n",
    "5. **Monitoring**: Track index size, update times, retrieval quality\n",
    "\n",
    "âš ï¸ Common Pitfalls:\n",
    "1. **ID Conflicts**: Ensure chunk IDs remain sequential and unique\n",
    "2. **Memory Issues**: Large batch updates may cause OOM\n",
    "3. **Index Corruption**: Always validate after updates\n",
    "4. **Symlink Issues**: Check file permissions in production\n",
    "5. **Vector Drift**: Recompute embeddings if model changes\n",
    "\n",
    "ğŸ”§ Scaling Considerations:\n",
    "1. **Large Indices**: Consider IVF/HNSW for >100K vectors\n",
    "2. **Distributed Updates**: Use sharded indices for very large datasets\n",
    "3. **Async Processing**: Queue updates for high-frequency scenarios\n",
    "4. **Monitoring**: Set up alerts for failed updates\n",
    "\n",
    "ğŸ“Š Key Metrics to Track:\n",
    "- Update latency vs batch size\n",
    "- Index size growth rate\n",
    "- Retrieval quality degradation\n",
    "- Memory usage during updates\n",
    "- Backup storage requirements\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Save benchmark results\n",
    "benchmark_summary = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"test_config\": {\n",
    "        \"embedding_model\": \"BAAI/bge-small-zh-v1.5\",\n",
    "        \"chunk_size\": 200,\n",
    "        \"chunk_overlap\": 20,\n",
    "        \"new_docs_count\": len(test_new_docs),\n",
    "    },\n",
    "    \"results\": benchmark_results,\n",
    "    \"smoke_test\": {\n",
    "        \"total_retrievals\": total_retrievals,\n",
    "        \"new_content_hits\": new_retrievals,\n",
    "        \"success_ratio\": new_content_ratio,\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(\"outs/nb17_incremental_update_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dumps(benchmark_summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nğŸ’¾ Results saved to: outs/nb17_incremental_update_results.json\")\n",
    "print(f\"ğŸ“ Index versions available: {len(list_index_versions())}\")\n",
    "print(\n",
    "    f\"ğŸ¯ Final index size: {faiss.read_index('indices/current.faiss').ntotal} vectors\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e52f87",
   "metadata": {},
   "source": [
    "Smoke Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e144bb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification that incremental update works\n",
    "test_doc = {\n",
    "    \"text\": \"æ¸¬è©¦æ–‡æª”ï¼šRAGå¢é‡æ›´æ–°åŠŸèƒ½é©—è­‰\",\n",
    "    \"metadata\": {\"source_id\": \"test_001\"},\n",
    "}\n",
    "updater.add_documents([test_doc])\n",
    "\n",
    "# Search for test content\n",
    "results = updater.search(\"RAGå¢é‡æ›´æ–°\", k=3)\n",
    "assert any(\n",
    "    \"test_001\" in result[1].get(\"source_id\", \"\") for result in results\n",
    "), \"New content not retrievable\"\n",
    "print(\"âœ… Smoke test passed: New content successfully indexed and retrievable\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
