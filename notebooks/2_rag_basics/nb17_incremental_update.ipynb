{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0235efa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb17_incremental_update.ipynb\n",
    "# Goals: Incremental document embedding and index updates\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (複製到每本 notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cdc2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import Dependencies and Reuse RAG Modules\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Create directories\n",
    "for dir_name in [\"indices\", \"outs\", \"data\"]:\n",
    "    Path(dir_name).mkdir(exist_ok=True)\n",
    "\n",
    "print(\"✓ Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa4a171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Build Initial Index (Simulate Existing Knowledge Base)\n",
    "def create_initial_index():\n",
    "    \"\"\"Create a baseline index with some sample documents\"\"\"\n",
    "\n",
    "    # Sample Chinese documents (simulate existing knowledge base)\n",
    "    initial_docs = [\n",
    "        \"人工智慧是電腦科學的一個分支，致力於創建能夠執行通常需要人類智慧的任務的系統。\",\n",
    "        \"機器學習是人工智慧的一個子領域，專注於開發能夠從資料中學習的演算法。\",\n",
    "        \"深度學習使用人工神經網路來模擬人腦的運作方式，在影像識別和自然語言處理方面表現出色。\",\n",
    "        \"自然語言處理（NLP）是人工智慧的一個分支，專門處理電腦與人類語言之間的互動。\",\n",
    "        \"RAG（檢索增強生成）結合了資訊檢索和語言生成，提供更準確的問答系統。\",\n",
    "    ]\n",
    "\n",
    "    # Text splitter for Chinese\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"。\", \"！\", \"？\", \"；\", \"…\", \"\\n\\n\", \"\\n\", \" \"],\n",
    "        chunk_size=200,\n",
    "        chunk_overlap=20,\n",
    "    )\n",
    "\n",
    "    # Create documents with metadata\n",
    "    documents = []\n",
    "    for i, text in enumerate(initial_docs):\n",
    "        doc = Document(\n",
    "            page_content=text,\n",
    "            metadata={\n",
    "                \"source_id\": f\"initial_doc_{i:03d}\",\n",
    "                \"domain\": \"ai_basics\",\n",
    "                \"created_at\": \"2024-01-01T00:00:00\",\n",
    "                \"version\": 1,\n",
    "            },\n",
    "        )\n",
    "        documents.append(doc)\n",
    "\n",
    "    # Initialize embedding model (low VRAM)\n",
    "    embedding_model = SentenceTransformer(\"BAAI/bge-small-zh-v1.5\")\n",
    "\n",
    "    # Split documents\n",
    "    all_chunks = []\n",
    "    for doc in documents:\n",
    "        chunks = splitter.split_documents([doc])\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "    # Generate embeddings\n",
    "    texts = [chunk.page_content for chunk in all_chunks]\n",
    "    vectors = embedding_model.encode(\n",
    "        texts, normalize_embeddings=True, batch_size=16\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    # Create FAISS index\n",
    "    dimension = vectors.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    index.add(vectors)\n",
    "\n",
    "    # Save chunks metadata\n",
    "    chunks_data = []\n",
    "    for i, chunk in enumerate(all_chunks):\n",
    "        chunks_data.append(\n",
    "            {\"id\": i, \"text\": chunk.page_content, \"metadata\": chunk.metadata}\n",
    "        )\n",
    "\n",
    "    # Save to files\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    faiss.write_index(index, f\"indices/knowledge_base_{timestamp}.faiss\")\n",
    "\n",
    "    with open(f\"indices/chunks_{timestamp}.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for chunk_data in chunks_data:\n",
    "            f.write(json.dumps(chunk_data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # Create current symlinks\n",
    "    if Path(\"indices/current.faiss\").exists():\n",
    "        Path(\"indices/current.faiss\").unlink()\n",
    "    if Path(\"indices/current_chunks.jsonl\").exists():\n",
    "        Path(\"indices/current_chunks.jsonl\").unlink()\n",
    "\n",
    "    Path(\"indices/current.faiss\").symlink_to(f\"knowledge_base_{timestamp}.faiss\")\n",
    "    Path(\"indices/current_chunks.jsonl\").symlink_to(f\"chunks_{timestamp}.jsonl\")\n",
    "\n",
    "    print(f\"✓ Initial index created with {len(chunks_data)} chunks\")\n",
    "    print(f\"  Index dimension: {dimension}\")\n",
    "    print(f\"  Index total: {index.ntotal}\")\n",
    "    print(f\"  Files: knowledge_base_{timestamp}.faiss, chunks_{timestamp}.jsonl\")\n",
    "\n",
    "    return embedding_model, index, chunks_data, timestamp\n",
    "\n",
    "\n",
    "# Create initial index\n",
    "embedding_model, current_index, current_chunks, base_timestamp = create_initial_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf1d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Incremental Update Functions\n",
    "class IncrementalRAGUpdater:\n",
    "    \"\"\"Handles incremental updates to RAG index\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model: SentenceTransformer):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"。\", \"！\", \"？\", \"；\", \"…\", \"\\n\\n\", \"\\n\", \" \"],\n",
    "            chunk_size=200,\n",
    "            chunk_overlap=20,\n",
    "        )\n",
    "\n",
    "    def add_documents(\n",
    "        self, new_docs: List[Dict[str, Any]], backup: bool = True\n",
    "    ) -> Tuple[faiss.Index, List[Dict], str]:\n",
    "        \"\"\"\n",
    "        Add new documents to existing index\n",
    "\n",
    "        Args:\n",
    "            new_docs: List of {\"text\": str, \"metadata\": dict}\n",
    "            backup: Whether to backup current index before update\n",
    "\n",
    "        Returns:\n",
    "            Updated index, updated chunks list, new timestamp\n",
    "        \"\"\"\n",
    "        print(f\"📥 Processing {len(new_docs)} new documents...\")\n",
    "\n",
    "        # Load current index and chunks\n",
    "        current_index = faiss.read_index(\"indices/current.faiss\")\n",
    "        current_chunks = []\n",
    "        with open(\"indices/current_chunks.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                current_chunks.append(json.loads(line.strip()))\n",
    "\n",
    "        print(\n",
    "            f\"📊 Current index stats: {current_index.ntotal} vectors, {len(current_chunks)} chunks\"\n",
    "        )\n",
    "\n",
    "        # Backup if requested\n",
    "        if backup:\n",
    "            backup_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_backup\"\n",
    "            shutil.copy(\n",
    "                \"indices/current.faiss\",\n",
    "                f\"indices/knowledge_base_{backup_timestamp}.faiss\",\n",
    "            )\n",
    "            shutil.copy(\n",
    "                \"indices/current_chunks.jsonl\",\n",
    "                f\"indices/chunks_{backup_timestamp}.jsonl\",\n",
    "            )\n",
    "            print(f\"💾 Backup created: {backup_timestamp}\")\n",
    "\n",
    "        # Process new documents\n",
    "        new_chunks = []\n",
    "        for doc in new_docs:\n",
    "            # Create document object\n",
    "            document = Document(\n",
    "                page_content=doc[\"text\"],\n",
    "                metadata={\n",
    "                    **doc.get(\"metadata\", {}),\n",
    "                    \"added_at\": datetime.now().isoformat(),\n",
    "                    \"version\": doc.get(\"metadata\", {}).get(\"version\", 1),\n",
    "                },\n",
    "            )\n",
    "\n",
    "            # Split into chunks\n",
    "            chunks = self.splitter.split_documents([document])\n",
    "            new_chunks.extend(chunks)\n",
    "\n",
    "        if not new_chunks:\n",
    "            print(\"⚠️ No new chunks generated from documents\")\n",
    "            return current_index, current_chunks, \"\"\n",
    "\n",
    "        # Generate embeddings for new chunks\n",
    "        new_texts = [chunk.page_content for chunk in new_chunks]\n",
    "        print(f\"🔢 Generating embeddings for {len(new_texts)} new chunks...\")\n",
    "\n",
    "        new_vectors = self.embedding_model.encode(\n",
    "            new_texts, normalize_embeddings=True, batch_size=16, show_progress_bar=True\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        # Add to index\n",
    "        start_id = len(current_chunks)\n",
    "        current_index.add(new_vectors)\n",
    "\n",
    "        # Update chunks metadata\n",
    "        for i, chunk in enumerate(new_chunks):\n",
    "            chunk_data = {\n",
    "                \"id\": start_id + i,\n",
    "                \"text\": chunk.page_content,\n",
    "                \"metadata\": chunk.metadata,\n",
    "            }\n",
    "            current_chunks.append(chunk_data)\n",
    "\n",
    "        # Save updated index\n",
    "        new_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        new_index_path = f\"indices/knowledge_base_{new_timestamp}.faiss\"\n",
    "        new_chunks_path = f\"indices/chunks_{new_timestamp}.jsonl\"\n",
    "\n",
    "        faiss.write_index(current_index, new_index_path)\n",
    "\n",
    "        with open(new_chunks_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for chunk_data in current_chunks:\n",
    "                f.write(json.dumps(chunk_data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        # Update current symlinks\n",
    "        Path(\"indices/current.faiss\").unlink()\n",
    "        Path(\"indices/current_chunks.jsonl\").unlink()\n",
    "        Path(\"indices/current.faiss\").symlink_to(\n",
    "            f\"knowledge_base_{new_timestamp}.faiss\"\n",
    "        )\n",
    "        Path(\"indices/current_chunks.jsonl\").symlink_to(f\"chunks_{new_timestamp}.jsonl\")\n",
    "\n",
    "        print(f\"✅ Index updated successfully!\")\n",
    "        print(\n",
    "            f\"  New total: {current_index.ntotal} vectors, {len(current_chunks)} chunks\"\n",
    "        )\n",
    "        print(f\"  Added: {len(new_chunks)} chunks from {len(new_docs)} documents\")\n",
    "        print(f\"  Files: {new_index_path}, {new_chunks_path}\")\n",
    "\n",
    "        return current_index, current_chunks, new_timestamp\n",
    "\n",
    "    def search(self, query: str, k: int = 5) -> List[Tuple[str, Dict, float]]:\n",
    "        \"\"\"Search updated index\"\"\"\n",
    "        # Load current index and chunks\n",
    "        index = faiss.read_index(\"indices/current.faiss\")\n",
    "        chunks = []\n",
    "        with open(\"indices/current_chunks.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                chunks.append(json.loads(line.strip()))\n",
    "\n",
    "        # Encode query\n",
    "        query_vector = self.embedding_model.encode(\n",
    "            [query], normalize_embeddings=True\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        # Search\n",
    "        scores, indices = index.search(query_vector, k)\n",
    "\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx < len(chunks):\n",
    "                chunk = chunks[idx]\n",
    "                results.append((chunk[\"text\"], chunk[\"metadata\"], float(score)))\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# Initialize updater\n",
    "updater = IncrementalRAGUpdater(embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76823033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Version Management and Backup Strategy\n",
    "def list_index_versions():\n",
    "    \"\"\"List all available index versions\"\"\"\n",
    "    indices_dir = Path(\"indices\")\n",
    "    versions = []\n",
    "\n",
    "    for file_path in indices_dir.glob(\"knowledge_base_*.faiss\"):\n",
    "        timestamp = file_path.stem.replace(\"knowledge_base_\", \"\")\n",
    "        size = file_path.stat().st_size\n",
    "\n",
    "        # Count chunks if corresponding file exists\n",
    "        chunks_file = indices_dir / f\"chunks_{timestamp}.jsonl\"\n",
    "        chunk_count = 0\n",
    "        if chunks_file.exists():\n",
    "            with open(chunks_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                chunk_count = sum(1 for _ in f)\n",
    "\n",
    "        versions.append(\n",
    "            {\n",
    "                \"timestamp\": timestamp,\n",
    "                \"size_mb\": round(size / 1024 / 1024, 2),\n",
    "                \"chunk_count\": chunk_count,\n",
    "                \"is_current\": file_path.name\n",
    "                == Path(\"indices/current.faiss\").resolve().name,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Sort by timestamp\n",
    "    versions.sort(key=lambda x: x[\"timestamp\"], reverse=True)\n",
    "    return versions\n",
    "\n",
    "\n",
    "def cleanup_old_versions(keep_latest: int = 5):\n",
    "    \"\"\"Clean up old index versions, keeping only the latest N\"\"\"\n",
    "    versions = list_index_versions()\n",
    "\n",
    "    if len(versions) <= keep_latest:\n",
    "        print(f\"📁 Only {len(versions)} versions found, no cleanup needed\")\n",
    "        return\n",
    "\n",
    "    # Keep current + latest N-1\n",
    "    to_delete = versions[keep_latest:]\n",
    "\n",
    "    print(f\"🗑️ Cleaning up {len(to_delete)} old versions...\")\n",
    "    for version in to_delete:\n",
    "        timestamp = version[\"timestamp\"]\n",
    "\n",
    "        # Skip if it's current (safety check)\n",
    "        if version[\"is_current\"]:\n",
    "            continue\n",
    "\n",
    "        # Delete files\n",
    "        faiss_file = Path(f\"indices/knowledge_base_{timestamp}.faiss\")\n",
    "        chunks_file = Path(f\"indices/chunks_{timestamp}.jsonl\")\n",
    "\n",
    "        if faiss_file.exists():\n",
    "            faiss_file.unlink()\n",
    "            print(f\"  Deleted: {faiss_file.name}\")\n",
    "        if chunks_file.exists():\n",
    "            chunks_file.unlink()\n",
    "            print(f\"  Deleted: {chunks_file.name}\")\n",
    "\n",
    "\n",
    "def rollback_to_version(timestamp: str):\n",
    "    \"\"\"Rollback to a specific version\"\"\"\n",
    "    target_faiss = Path(f\"indices/knowledge_base_{timestamp}.faiss\")\n",
    "    target_chunks = Path(f\"indices/chunks_{timestamp}.jsonl\")\n",
    "\n",
    "    if not target_faiss.exists() or not target_chunks.exists():\n",
    "        print(f\"❌ Version {timestamp} not found\")\n",
    "        return False\n",
    "\n",
    "    # Update symlinks\n",
    "    Path(\"indices/current.faiss\").unlink()\n",
    "    Path(\"indices/current_chunks.jsonl\").unlink()\n",
    "    Path(\"indices/current.faiss\").symlink_to(f\"knowledge_base_{timestamp}.faiss\")\n",
    "    Path(\"indices/current_chunks.jsonl\").symlink_to(f\"chunks_{timestamp}.jsonl\")\n",
    "\n",
    "    print(f\"⏪ Rolled back to version {timestamp}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# Display current versions\n",
    "print(\"📋 Current index versions:\")\n",
    "versions = list_index_versions()\n",
    "for v in versions:\n",
    "    current_mark = \" (CURRENT)\" if v[\"is_current\"] else \"\"\n",
    "    print(\n",
    "        f\"  {v['timestamp']}: {v['chunk_count']} chunks, {v['size_mb']} MB{current_mark}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abccbe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Consistency Check and Validation\n",
    "def validate_index_consistency():\n",
    "    \"\"\"Validate that index and chunks are consistent\"\"\"\n",
    "    print(\"🔍 Validating index consistency...\")\n",
    "\n",
    "    # Load index and chunks\n",
    "    try:\n",
    "        index = faiss.read_index(\"indices/current.faiss\")\n",
    "        chunks = []\n",
    "        with open(\"indices/current_chunks.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                chunks.append(json.loads(line.strip()))\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading files: {e}\")\n",
    "        return False\n",
    "\n",
    "    # Check counts match\n",
    "    if index.ntotal != len(chunks):\n",
    "        print(\n",
    "            f\"❌ Count mismatch: index has {index.ntotal} vectors, chunks file has {len(chunks)} entries\"\n",
    "        )\n",
    "        return False\n",
    "\n",
    "    # Check chunk IDs are sequential\n",
    "    expected_ids = set(range(len(chunks)))\n",
    "    actual_ids = set(chunk[\"id\"] for chunk in chunks)\n",
    "    if expected_ids != actual_ids:\n",
    "        missing = expected_ids - actual_ids\n",
    "        extra = actual_ids - expected_ids\n",
    "        if missing:\n",
    "            print(f\"❌ Missing chunk IDs: {sorted(missing)[:10]}...\")\n",
    "        if extra:\n",
    "            print(f\"❌ Extra chunk IDs: {sorted(extra)[:10]}...\")\n",
    "        return False\n",
    "\n",
    "    # Sample validation: re-embed a few chunks and check similarity\n",
    "    sample_size = min(5, len(chunks))\n",
    "    sample_chunks = chunks[:sample_size]\n",
    "    sample_texts = [chunk[\"text\"] for chunk in sample_chunks]\n",
    "\n",
    "    # Re-generate embeddings\n",
    "    sample_vectors = embedding_model.encode(\n",
    "        sample_texts, normalize_embeddings=True\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    # Get original vectors from index\n",
    "    original_vectors = index.reconstruct_batch(list(range(sample_size)))\n",
    "\n",
    "    # Check similarity (should be very close to 1.0)\n",
    "    similarities = []\n",
    "    for i in range(sample_size):\n",
    "        sim = np.dot(sample_vectors[i], original_vectors[i])\n",
    "        similarities.append(sim)\n",
    "\n",
    "    avg_similarity = np.mean(similarities)\n",
    "    if avg_similarity < 0.95:\n",
    "        print(f\"❌ Vector similarity too low: {avg_similarity:.4f}\")\n",
    "        return False\n",
    "\n",
    "    print(f\"✅ Index validation passed!\")\n",
    "    print(f\"  Vectors: {index.ntotal}\")\n",
    "    print(f\"  Chunks: {len(chunks)}\")\n",
    "    print(f\"  Vector similarity: {avg_similarity:.4f}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# Run validation\n",
    "is_consistent = validate_index_consistency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f585b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Performance Comparison (Incremental vs Full Rebuild)\n",
    "def benchmark_update_methods(new_docs: List[Dict[str, Any]]):\n",
    "    \"\"\"Compare incremental update vs full rebuild performance\"\"\"\n",
    "    print(\"⚡ Benchmarking update methods...\")\n",
    "\n",
    "    # Save current state\n",
    "    current_chunks_backup = []\n",
    "    with open(\"indices/current_chunks.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            current_chunks_backup.append(json.loads(line.strip()))\n",
    "\n",
    "    original_count = len(current_chunks_backup)\n",
    "\n",
    "    # Method 1: Incremental Update\n",
    "    print(\"\\n🔄 Testing incremental update...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    _, updated_chunks, _ = updater.add_documents(new_docs, backup=False)\n",
    "\n",
    "    incremental_time = time.time() - start_time\n",
    "    incremental_count = len(updated_chunks)\n",
    "\n",
    "    print(f\"  Time: {incremental_time:.2f}s\")\n",
    "    print(f\"  Final count: {incremental_count}\")\n",
    "\n",
    "    # Method 2: Full Rebuild (simulation)\n",
    "    print(\"\\n🔄 Simulating full rebuild...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Combine all documents\n",
    "    all_texts = [chunk[\"text\"] for chunk in current_chunks_backup]\n",
    "    all_texts.extend([doc[\"text\"] for doc in new_docs])\n",
    "\n",
    "    # Re-embed everything (simulation - don't actually rebuild)\n",
    "    rebuild_vectors = embedding_model.encode(\n",
    "        all_texts, normalize_embeddings=True, batch_size=16, show_progress_bar=False\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    rebuild_time = time.time() - start_time\n",
    "\n",
    "    print(f\"  Time: {rebuild_time:.2f}s\")\n",
    "    print(f\"  Final count: {len(all_texts)}\")\n",
    "\n",
    "    # Results\n",
    "    print(f\"\\n📊 Performance Comparison:\")\n",
    "    print(f\"  Incremental: {incremental_time:.2f}s\")\n",
    "    print(f\"  Full rebuild: {rebuild_time:.2f}s\")\n",
    "    print(f\"  Speedup: {rebuild_time/incremental_time:.1f}x faster\")\n",
    "    print(f\"  Added documents: {len(new_docs)}\")\n",
    "    print(f\"  Original chunks: {original_count}\")\n",
    "    print(f\"  New total: {incremental_count}\")\n",
    "\n",
    "    return {\n",
    "        \"incremental_time\": incremental_time,\n",
    "        \"rebuild_time\": rebuild_time,\n",
    "        \"speedup\": rebuild_time / incremental_time,\n",
    "        \"docs_added\": len(new_docs),\n",
    "        \"original_count\": original_count,\n",
    "        \"final_count\": incremental_count,\n",
    "    }\n",
    "\n",
    "\n",
    "# Prepare test documents\n",
    "test_new_docs = [\n",
    "    {\n",
    "        \"text\": \"大型語言模型（LLM）如GPT、BERT等，在自然語言理解和生成方面展現了驚人的能力。這些模型通過在大規模文本資料上進行預訓練，學習到了豐富的語言知識。\",\n",
    "        \"metadata\": {\n",
    "            \"source_id\": \"new_doc_001\",\n",
    "            \"domain\": \"llm_research\",\n",
    "            \"author\": \"AI研究員\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"向量資料庫是現代AI應用的重要基礎設施，它能夠高效地儲存和檢索高維向量資料。FAISS、Pinecone、Weaviate等是常用的向量資料庫解決方案。\",\n",
    "        \"metadata\": {\n",
    "            \"source_id\": \"new_doc_002\",\n",
    "            \"domain\": \"vector_db\",\n",
    "            \"author\": \"資料工程師\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"提示工程（Prompt Engineering）是優化AI模型輸出的關鍵技術。透過精心設計的提示詞，我們可以引導模型產生更準確、更有用的回應。\",\n",
    "        \"metadata\": {\n",
    "            \"source_id\": \"new_doc_003\",\n",
    "            \"domain\": \"prompt_engineering\",\n",
    "            \"author\": \"AI應用專家\",\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_results = benchmark_update_methods(test_new_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557fe371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Smoke Test - Verify New Content is Retrievable\n",
    "print(\"🧪 Smoke Test: Verifying new content is retrievable...\")\n",
    "\n",
    "# Test queries that should hit new content\n",
    "test_queries = [\n",
    "    \"什麼是大型語言模型？\",\n",
    "    \"向量資料庫有哪些？\",\n",
    "    \"提示工程的作用\",\n",
    "    \"FAISS向量檢索\",\n",
    "]\n",
    "\n",
    "print(\"\\n🔍 Testing retrieval of new content:\")\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    results = updater.search(query, k=3)\n",
    "\n",
    "    for i, (text, metadata, score) in enumerate(results):\n",
    "        source_id = metadata.get(\"source_id\", \"unknown\")\n",
    "        is_new = source_id.startswith(\"new_doc_\")\n",
    "        new_flag = \" 🆕\" if is_new else \"\"\n",
    "        print(f\"  {i+1}. [{source_id}]{new_flag} (score: {score:.3f})\")\n",
    "        print(f\"     {text[:100]}...\")\n",
    "\n",
    "# Check if new documents are being retrieved\n",
    "new_retrievals = 0\n",
    "total_retrievals = 0\n",
    "\n",
    "for query in test_queries:\n",
    "    results = updater.search(query, k=3)\n",
    "    for text, metadata, score in results:\n",
    "        total_retrievals += 1\n",
    "        if metadata.get(\"source_id\", \"\").startswith(\"new_doc_\"):\n",
    "            new_retrievals += 1\n",
    "\n",
    "new_content_ratio = new_retrievals / total_retrievals if total_retrievals > 0 else 0\n",
    "\n",
    "print(f\"\\n📈 Smoke Test Results:\")\n",
    "print(f\"  Total retrievals: {total_retrievals}\")\n",
    "print(f\"  New content hits: {new_retrievals}\")\n",
    "print(f\"  New content ratio: {new_content_ratio:.1%}\")\n",
    "\n",
    "if new_content_ratio > 0:\n",
    "    print(\"✅ PASS: New content is being retrieved successfully\")\n",
    "else:\n",
    "    print(\"❌ FAIL: New content not being retrieved - check embedding/indexing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ddf42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Production Recommendations and Pitfalls\n",
    "print(\n",
    "    \"\"\"\n",
    "🏭 Production Environment Recommendations:\n",
    "\n",
    "✅ Best Practices:\n",
    "1. **Backup Strategy**: Always backup before updates (set backup=True)\n",
    "2. **Version Management**: Keep 3-5 recent versions for rollback\n",
    "3. **Batch Updates**: Group multiple documents for efficiency\n",
    "4. **Consistency Checks**: Validate after each update\n",
    "5. **Monitoring**: Track index size, update times, retrieval quality\n",
    "\n",
    "⚠️ Common Pitfalls:\n",
    "1. **ID Conflicts**: Ensure chunk IDs remain sequential and unique\n",
    "2. **Memory Issues**: Large batch updates may cause OOM\n",
    "3. **Index Corruption**: Always validate after updates\n",
    "4. **Symlink Issues**: Check file permissions in production\n",
    "5. **Vector Drift**: Recompute embeddings if model changes\n",
    "\n",
    "🔧 Scaling Considerations:\n",
    "1. **Large Indices**: Consider IVF/HNSW for >100K vectors\n",
    "2. **Distributed Updates**: Use sharded indices for very large datasets\n",
    "3. **Async Processing**: Queue updates for high-frequency scenarios\n",
    "4. **Monitoring**: Set up alerts for failed updates\n",
    "\n",
    "📊 Key Metrics to Track:\n",
    "- Update latency vs batch size\n",
    "- Index size growth rate\n",
    "- Retrieval quality degradation\n",
    "- Memory usage during updates\n",
    "- Backup storage requirements\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Save benchmark results\n",
    "benchmark_summary = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"test_config\": {\n",
    "        \"embedding_model\": \"BAAI/bge-small-zh-v1.5\",\n",
    "        \"chunk_size\": 200,\n",
    "        \"chunk_overlap\": 20,\n",
    "        \"new_docs_count\": len(test_new_docs),\n",
    "    },\n",
    "    \"results\": benchmark_results,\n",
    "    \"smoke_test\": {\n",
    "        \"total_retrievals\": total_retrievals,\n",
    "        \"new_content_hits\": new_retrievals,\n",
    "        \"success_ratio\": new_content_ratio,\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(\"outs/nb17_incremental_update_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dumps(benchmark_summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n💾 Results saved to: outs/nb17_incremental_update_results.json\")\n",
    "print(f\"📁 Index versions available: {len(list_index_versions())}\")\n",
    "print(\n",
    "    f\"🎯 Final index size: {faiss.read_index('indices/current.faiss').ntotal} vectors\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e52f87",
   "metadata": {},
   "source": [
    "Smoke Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e144bb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification that incremental update works\n",
    "test_doc = {\n",
    "    \"text\": \"測試文檔：RAG增量更新功能驗證\",\n",
    "    \"metadata\": {\"source_id\": \"test_001\"},\n",
    "}\n",
    "updater.add_documents([test_doc])\n",
    "\n",
    "# Search for test content\n",
    "results = updater.search(\"RAG增量更新\", k=3)\n",
    "assert any(\n",
    "    \"test_001\" in result[1].get(\"source_id\", \"\") for result in results\n",
    "), \"New content not retrievable\"\n",
    "print(\"✅ Smoke test passed: New content successfully indexed and retrievable\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
