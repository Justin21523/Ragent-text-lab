{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e85b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb12_embeddings_bge_m3.ipynb\n",
    "# Stage 2 - 嵌入模型 BGE-M3 實作\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (複製到每本 notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcb42e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 2: BGE-M3 模型載入與設定\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import List, Union, Optional\n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "\n",
    "class BGEEmbedder:\n",
    "    \"\"\"BGE-M3 嵌入模型封裝，支援中英文與批次處理\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"BAAI/bge-m3\",\n",
    "        device: str = \"auto\",\n",
    "        normalize: bool = True,\n",
    "        max_seq_length: int = 512,\n",
    "        cache_dir: Optional[str] = None,\n",
    "    ):\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.normalize = normalize\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.cache_dir = cache_dir or f\"{AI_CACHE_ROOT}/embeddings\"\n",
    "        pathlib.Path(self.cache_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print(f\"[BGE] Loading model: {model_name}\")\n",
    "        self.model = SentenceTransformer(\n",
    "            model_name,\n",
    "            device=device if device != \"auto\" else None,\n",
    "            cache_folder=f\"{AI_CACHE_ROOT}/hf/sentence-transformers\",\n",
    "        )\n",
    "\n",
    "        # Set max sequence length for efficiency\n",
    "        if hasattr(self.model, \"max_seq_length\"):\n",
    "            self.model.max_seq_length = max_seq_length\n",
    "\n",
    "        print(f\"[BGE] Model loaded on device: {self.model.device}\")\n",
    "        print(f\"[BGE] Max sequence length: {max_seq_length}\")\n",
    "        print(f\"[BGE] Normalize embeddings: {normalize}\")\n",
    "\n",
    "\n",
    "# Initialize BGE-M3 embedder\n",
    "embedder = BGEEmbedder(model_name=\"BAAI/bge-m3\", normalize=True, max_seq_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05db71d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 3: 文本嵌入基礎功能（單一文本）\n",
    "def encode_single_text(text: str, show_info: bool = True) -> np.ndarray:\n",
    "    \"\"\"編碼單一文本為向量\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Encode text to embedding\n",
    "    embedding = embedder.model.encode(\n",
    "        text, normalize_embeddings=embedder.normalize, convert_to_numpy=True\n",
    "    )\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    if show_info:\n",
    "        print(f\"[Encode] Text length: {len(text)} chars\")\n",
    "        print(f\"[Encode] Embedding shape: {embedding.shape}\")\n",
    "        print(f\"[Encode] Embedding dtype: {embedding.dtype}\")\n",
    "        print(f\"[Encode] Time elapsed: {elapsed:.3f}s\")\n",
    "\n",
    "        if embedder.normalize:\n",
    "            norm = np.linalg.norm(embedding)\n",
    "            print(f\"[Encode] L2 norm (should be ~1.0): {norm:.6f}\")\n",
    "\n",
    "    return embedding\n",
    "\n",
    "\n",
    "# Test with Chinese text\n",
    "zh_text = \"檢索增強生成（RAG）是一種結合了檢索與生成的自然語言處理技術。\"\n",
    "zh_embedding = encode_single_text(zh_text)\n",
    "\n",
    "# Test with English text\n",
    "en_text = \"Retrieval-Augmented Generation (RAG) combines retrieval and generation for NLP tasks.\"\n",
    "en_embedding = encode_single_text(en_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ecac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 4: 批次嵌入與正規化\n",
    "def encode_batch_texts(\n",
    "    texts: List[str], batch_size: int = 32, show_progress: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"批次編碼文本列表為向量矩陣\"\"\"\n",
    "\n",
    "    if not texts:\n",
    "        return np.array([])\n",
    "\n",
    "    total_texts = len(texts)\n",
    "    print(f\"[Batch] Encoding {total_texts} texts with batch_size={batch_size}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Use sentence-transformers batch encoding\n",
    "    embeddings = embedder.model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        normalize_embeddings=embedder.normalize,\n",
    "        convert_to_numpy=True,\n",
    "        show_progress_bar=show_progress,\n",
    "    )\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    print(f\"[Batch] Output shape: {embeddings.shape}\")\n",
    "    print(f\"[Batch] Output dtype: {embeddings.dtype}\")\n",
    "    print(f\"[Batch] Total time: {elapsed:.3f}s\")\n",
    "    print(f\"[Batch] Average time per text: {elapsed/total_texts:.4f}s\")\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Test batch encoding with mixed Chinese/English\n",
    "test_texts = [\n",
    "    \"人工智慧的發展正在改變世界。\",\n",
    "    \"Artificial intelligence is transforming the world.\",\n",
    "    \"機器學習是人工智慧的重要分支。\",\n",
    "    \"Machine learning is a crucial branch of AI.\",\n",
    "    \"深度學習使用神經網路來學習複雜的模式。\",\n",
    "    \"Deep learning uses neural networks to learn complex patterns.\",\n",
    "    \"自然語言處理讓電腦理解人類語言。\",\n",
    "    \"Natural language processing helps computers understand human language.\",\n",
    "]\n",
    "\n",
    "batch_embeddings = encode_batch_texts(test_texts, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7157793",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 5: 嵌入快取系統\n",
    "class EmbeddingCache:\n",
    "    \"\"\"嵌入向量快取系統，避免重複計算\"\"\"\n",
    "\n",
    "    def __init__(self, cache_dir: str):\n",
    "        self.cache_dir = pathlib.Path(cache_dir)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.cache_file = self.cache_dir / \"embedding_cache.jsonl\"\n",
    "        self.memory_cache = {}\n",
    "\n",
    "    def _get_text_hash(self, text: str) -> str:\n",
    "        \"\"\"生成文本的MD5雜湊值作為快取鍵\"\"\"\n",
    "        return hashlib.md5(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    def get(self, text: str) -> Optional[np.ndarray]:\n",
    "        \"\"\"從快取中獲取嵌入向量\"\"\"\n",
    "        text_hash = self._get_text_hash(text)\n",
    "\n",
    "        # Check memory cache first\n",
    "        if text_hash in self.memory_cache:\n",
    "            return self.memory_cache[text_hash]\n",
    "\n",
    "        # Check disk cache\n",
    "        if self.cache_file.exists():\n",
    "            try:\n",
    "                with open(self.cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        data = json.loads(line.strip())\n",
    "                        if data[\"hash\"] == text_hash:\n",
    "                            embedding = np.array(data[\"embedding\"], dtype=np.float32)\n",
    "                            self.memory_cache[text_hash] = embedding\n",
    "                            return embedding\n",
    "            except Exception as e:\n",
    "                print(f\"[Cache] Error reading cache: {e}\")\n",
    "\n",
    "        return None\n",
    "\n",
    "    def set(self, text: str, embedding: np.ndarray):\n",
    "        \"\"\"將嵌入向量存入快取\"\"\"\n",
    "        text_hash = self._get_text_hash(text)\n",
    "\n",
    "        # Store in memory cache\n",
    "        self.memory_cache[text_hash] = embedding\n",
    "\n",
    "        # Append to disk cache\n",
    "        try:\n",
    "            cache_entry = {\n",
    "                \"hash\": text_hash,\n",
    "                \"text_preview\": text[:100],  # First 100 chars for debugging\n",
    "                \"embedding\": embedding.tolist(),\n",
    "                \"timestamp\": time.time(),\n",
    "            }\n",
    "\n",
    "            with open(self.cache_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(cache_entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Cache] Error writing cache: {e}\")\n",
    "\n",
    "    def encode_with_cache(self, text: str) -> np.ndarray:\n",
    "        \"\"\"帶快取的文本編碼\"\"\"\n",
    "        cached_embedding = self.get(text)\n",
    "        if cached_embedding is not None:\n",
    "            print(f\"[Cache] Hit for text: {text[:50]}...\")\n",
    "            return cached_embedding\n",
    "\n",
    "        print(f\"[Cache] Miss, encoding: {text[:50]}...\")\n",
    "        embedding = encode_single_text(text, show_info=False)\n",
    "        self.set(text, embedding)\n",
    "        return embedding\n",
    "\n",
    "\n",
    "# Initialize embedding cache\n",
    "cache = EmbeddingCache(f\"{AI_CACHE_ROOT}/embeddings\")\n",
    "\n",
    "# Test caching with repeated texts\n",
    "test_text = \"這是一個測試文本，用來驗證快取功能是否正常運作。\"\n",
    "\n",
    "print(\"First encoding (should miss cache):\")\n",
    "emb1 = cache.encode_with_cache(test_text)\n",
    "\n",
    "print(\"\\nSecond encoding (should hit cache):\")\n",
    "emb2 = cache.encode_with_cache(test_text)\n",
    "\n",
    "print(f\"\\nEmbeddings are identical: {np.allclose(emb1, emb2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f138b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 6: 向量相似度計算與檢索\n",
    "def compute_similarity(\n",
    "    query_embedding: np.ndarray, doc_embeddings: np.ndarray, metric: str = \"cosine\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"計算查詢向量與文檔向量之間的相似度\"\"\"\n",
    "\n",
    "    if metric == \"cosine\":\n",
    "        # For normalized embeddings, cosine similarity = dot product\n",
    "        if len(query_embedding.shape) == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "\n",
    "        similarities = np.dot(doc_embeddings, query_embedding.T).flatten()\n",
    "\n",
    "    elif metric == \"euclidean\":\n",
    "        # Euclidean distance (lower is more similar)\n",
    "        distances = np.linalg.norm(doc_embeddings - query_embedding, axis=1)\n",
    "        similarities = 1 / (1 + distances)  # Convert to similarity score\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported metric: {metric}\")\n",
    "\n",
    "    return similarities\n",
    "\n",
    "\n",
    "def find_most_similar(\n",
    "    query: str, documents: List[str], top_k: int = 3, metric: str = \"cosine\"\n",
    ") -> List[tuple]:\n",
    "    \"\"\"找到與查詢最相似的文檔\"\"\"\n",
    "\n",
    "    print(f\"[Search] Query: {query}\")\n",
    "    print(f\"[Search] Searching in {len(documents)} documents\")\n",
    "\n",
    "    # Encode query and documents\n",
    "    query_emb = cache.encode_with_cache(query)\n",
    "    doc_embs = []\n",
    "\n",
    "    for doc in documents:\n",
    "        doc_emb = cache.encode_with_cache(doc)\n",
    "        doc_embs.append(doc_emb)\n",
    "\n",
    "    doc_embs = np.array(doc_embs)\n",
    "\n",
    "    # Compute similarities\n",
    "    similarities = compute_similarity(query_emb, doc_embs, metric)\n",
    "\n",
    "    # Get top-k results\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "    results = []\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        results.append(\n",
    "            (\n",
    "                idx,  # Document index\n",
    "                documents[idx],  # Document text\n",
    "                float(similarities[idx]),  # Similarity score\n",
    "            )\n",
    "        )\n",
    "        print(\n",
    "            f\"[Result {i+1}] Score: {similarities[idx]:.4f} | Doc: {documents[idx][:100]}...\"\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Test similarity search\n",
    "query_text = \"人工智慧的應用\"\n",
    "search_results = find_most_similar(query_text, test_texts, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a80a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 7: 中英文混合文本測試\n",
    "def test_multilingual_embeddings():\n",
    "    \"\"\"測試中英文混合文本的嵌入效果\"\"\"\n",
    "\n",
    "    multilingual_texts = [\n",
    "        \"AI人工智慧 artificial intelligence\",\n",
    "        \"機器學習 machine learning algorithms\",\n",
    "        \"Natural language processing 自然語言處理\",\n",
    "        \"Deep neural networks 深度神經網路\",\n",
    "        \"Computer vision 電腦視覺技術\",\n",
    "        \"語音識別 speech recognition systems\",\n",
    "    ]\n",
    "\n",
    "    print(\"[Multilingual] Testing mixed language embeddings...\")\n",
    "\n",
    "    # Encode all texts\n",
    "    embeddings = encode_batch_texts(\n",
    "        multilingual_texts, batch_size=6, show_progress=False\n",
    "    )\n",
    "\n",
    "    # Test cross-language similarity\n",
    "    zh_query = \"人工智慧技術\"\n",
    "    en_query = \"artificial intelligence technology\"\n",
    "\n",
    "    zh_query_emb = cache.encode_with_cache(zh_query)\n",
    "    en_query_emb = cache.encode_with_cache(en_query)\n",
    "\n",
    "    # Cross-language similarity (should be high)\n",
    "    cross_lang_sim = compute_similarity(zh_query_emb, en_query_emb.reshape(1, -1))[0]\n",
    "    print(f\"[Multilingual] ZH-EN cross-language similarity: {cross_lang_sim:.4f}\")\n",
    "\n",
    "    # Find similar documents for both queries\n",
    "    print(f\"\\n[Multilingual] Similar docs for '{zh_query}':\")\n",
    "    zh_results = find_most_similar(zh_query, multilingual_texts, top_k=2)\n",
    "\n",
    "    print(f\"\\n[Multilingual] Similar docs for '{en_query}':\")\n",
    "    en_results = find_most_similar(en_query, multilingual_texts, top_k=2)\n",
    "\n",
    "    return cross_lang_sim, zh_results, en_results\n",
    "\n",
    "\n",
    "multilingual_test_results = test_multilingual_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8d59cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 8: Smoke Test - 完整嵌入流水線\n",
    "def smoke_test_embeddings():\n",
    "    \"\"\"煙霧測試：驗證完整嵌入流水線\"\"\"\n",
    "\n",
    "    print(\"=== BGE-M3 Embedding Pipeline Smoke Test ===\")\n",
    "\n",
    "    # Test data\n",
    "    sample_docs = [\n",
    "        \"檢索增強生成是結合檢索與生成的技術\",\n",
    "        \"RAG combines retrieval and generation\",\n",
    "        \"向量資料庫用於儲存文檔嵌入\",\n",
    "        \"Vector databases store document embeddings\",\n",
    "        \"語義搜索基於向量相似度匹配\",\n",
    "    ]\n",
    "\n",
    "    # 1. Batch encoding test\n",
    "    print(\"\\n1. Testing batch encoding...\")\n",
    "    batch_embs = encode_batch_texts(sample_docs, batch_size=3, show_progress=False)\n",
    "    assert batch_embs.shape[0] == len(sample_docs), \"Batch size mismatch\"\n",
    "    assert batch_embs.shape[1] > 0, \"Empty embeddings\"\n",
    "    print(\"✓ Batch encoding passed\")\n",
    "\n",
    "    # 2. Cache functionality test\n",
    "    print(\"\\n2. Testing cache functionality...\")\n",
    "    test_doc = sample_docs[0]\n",
    "    emb1 = cache.encode_with_cache(test_doc)\n",
    "    emb2 = cache.encode_with_cache(test_doc)  # Should hit cache\n",
    "    assert np.allclose(emb1, emb2), \"Cache inconsistency\"\n",
    "    print(\"✓ Cache functionality passed\")\n",
    "\n",
    "    # 3. Similarity search test\n",
    "    print(\"\\n3. Testing similarity search...\")\n",
    "    query = \"向量檢索技術\"\n",
    "    results = find_most_similar(query, sample_docs, top_k=2)\n",
    "    assert len(results) == 2, \"Incorrect number of results\"\n",
    "    assert all(score > 0 for _, _, score in results), \"Invalid similarity scores\"\n",
    "    print(\"✓ Similarity search passed\")\n",
    "\n",
    "    # 4. Normalization test\n",
    "    print(\"\\n4. Testing embedding normalization...\")\n",
    "    test_emb = encode_single_text(\"測試正規化\", show_info=False)\n",
    "    norm = np.linalg.norm(test_emb)\n",
    "    assert abs(norm - 1.0) < 1e-5, f\"Embedding not normalized: {norm}\"\n",
    "    print(\"✓ Normalization passed\")\n",
    "\n",
    "    print(\"\\n=== All tests passed! ===\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_result = smoke_test_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6761c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 9: 效能測試與記憶體監控\n",
    "def performance_benchmark():\n",
    "    \"\"\"效能基準測試\"\"\"\n",
    "\n",
    "    print(\"=== Performance Benchmark ===\")\n",
    "\n",
    "    # Generate test data\n",
    "    test_sizes = [10, 50, 100]\n",
    "    base_text = \"這是一個用於測試嵌入模型效能的範例文本。\"\n",
    "\n",
    "    for size in test_sizes:\n",
    "        print(f\"\\n--- Testing {size} documents ---\")\n",
    "\n",
    "        # Generate test documents\n",
    "        docs = [f\"{base_text} 文檔編號 {i}\" for i in range(size)]\n",
    "\n",
    "        # Measure encoding time\n",
    "        start_time = time.time()\n",
    "        embeddings = encode_batch_texts(docs, batch_size=16, show_progress=False)\n",
    "        encoding_time = time.time() - start_time\n",
    "\n",
    "        # Measure similarity search time\n",
    "        query = \"測試查詢文本\"\n",
    "        search_start = time.time()\n",
    "        results = find_most_similar(query, docs, top_k=5)\n",
    "        search_time = time.time() - search_start\n",
    "\n",
    "        # Memory usage (approximate)\n",
    "        emb_memory_mb = embeddings.nbytes / (1024 * 1024)\n",
    "\n",
    "        print(\n",
    "            f\"Encoding time: {encoding_time:.3f}s ({encoding_time/size:.4f}s per doc)\"\n",
    "        )\n",
    "        print(f\"Search time: {search_time:.3f}s\")\n",
    "        print(f\"Memory usage: {emb_memory_mb:.2f} MB\")\n",
    "        print(f\"Throughput: {size/encoding_time:.1f} docs/sec\")\n",
    "\n",
    "    # GPU memory check if available\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / (1024**3)\n",
    "        cached = torch.cuda.memory_reserved() / (1024**3)\n",
    "        print(f\"\\nGPU Memory - Allocated: {allocated:.2f}GB, Cached: {cached:.2f}GB\")\n",
    "\n",
    "\n",
    "performance_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4198f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 10: Key Parameters & When to Use\n",
    "print(\n",
    "    \"\"\"\n",
    "=== Key Parameters for BGE-M3 Embeddings ===\n",
    "\n",
    "Model Configuration:\n",
    "- model_name: \"BAAI/bge-m3\" (multilingual, recommended)\n",
    "- normalize: True (enables cosine similarity via dot product)\n",
    "- max_seq_length: 512 (balance between quality and speed)\n",
    "- batch_size: 16-32 (adjust based on GPU memory)\n",
    "\n",
    "Low-VRAM Options:\n",
    "- Use CPU: device=\"cpu\"\n",
    "- Smaller batch: batch_size=8\n",
    "- Alternative model: \"BAAI/bge-small-zh-v1.5\" (lighter)\n",
    "\n",
    "Cache Settings:\n",
    "- Enable caching for repeated texts\n",
    "- Cache directory: AI_CACHE_ROOT/embeddings\n",
    "- Memory + disk dual-layer cache\n",
    "\n",
    "Performance Tips:\n",
    "- Batch encoding is much faster than individual encoding\n",
    "- Normalized embeddings allow fast cosine similarity via dot product\n",
    "- Cache frequently accessed embeddings\n",
    "- Monitor GPU memory usage\n",
    "\n",
    "=== When to Use BGE-M3 ===\n",
    "\n",
    "✓ Good for:\n",
    "- Multilingual text (Chinese + English)\n",
    "- Semantic search and similarity matching\n",
    "- RAG retrieval systems\n",
    "- Document clustering and classification\n",
    "- Cross-language information retrieval\n",
    "\n",
    "✗ Avoid for:\n",
    "- Very long documents (>512 tokens) without chunking\n",
    "- Real-time applications requiring <50ms latency\n",
    "- Environments with <4GB VRAM without CPU fallback\n",
    "\n",
    "=== Next Steps ===\n",
    "- Use these embeddings in nb13 for FAISS index building\n",
    "- Integrate with chunked documents from nb11\n",
    "- Add reranking in nb15 for better retrieval quality\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(f\"\\n=== Notebook nb12 Completed Successfully ===\")\n",
    "print(f\"BGE-M3 embedder ready for Stage 2 RAG pipeline!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
