{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a257e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb11_zh_chunking_strategies.ipynb\n",
    "# 中文分段策略：階層式切分與參數優化\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (複製到每本 notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10d8acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 2: Dependencies & Sample Data Preparation\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "import tiktoken\n",
    "\n",
    "# Create sample Chinese documents for testing\n",
    "SAMPLE_ZH_DOCS = {\n",
    "    \"tech_article\": \"\"\"# 大型語言模型的檢索增強生成技術\n",
    "\n",
    "## 1. 概述\n",
    "\n",
    "檢索增強生成（Retrieval-Augmented Generation, RAG）是一種結合外部知識檢索與語言模型生成的技術。它能夠有效解決大型語言模型在處理特定領域知識時的局限性。\n",
    "\n",
    "RAG 系統主要包含三個核心組件：文檔編碼器、檢索器和生成器。當用戶提出問題時，系統首先從知識庫中檢索相關文檔，然後將這些文檔與原始問題一起輸入生成器，產生最終答案。\n",
    "\n",
    "## 2. 技術架構\n",
    "\n",
    "### 2.1 文檔處理流程\n",
    "\n",
    "文檔處理包括以下步驟：文本清理、分段處理、向量化編碼。其中，分段處理是關鍵環節，需要在保持語義完整性的同時，確保片段長度適合檢索模型。\n",
    "\n",
    "對於中文文本，分段策略需要考慮以下因素：\n",
    "- 句號、問號、感嘆號等強停頓標點\n",
    "- 分號、冒號等中等停頓標點\n",
    "- 段落結構和章節標題\n",
    "- 語義單元的完整性\n",
    "\n",
    "### 2.2 檢索機制\n",
    "\n",
    "傳統的關鍵詞檢索方法在處理語義相似但詞彙不同的查詢時效果有限。向量檢索通過將文本映射到高維語義空間，能夠捕捉更深層的語義關係。\n",
    "\n",
    "混合檢索結合了關鍵詞檢索的精確性和向量檢索的語義理解能力，通常能獲得更好的檢索效果。\n",
    "\n",
    "## 3. 實際應用\n",
    "\n",
    "RAG 技術已經在多個領域得到應用，包括智能客服、知識問答、文檔分析等。在實際部署中，需要根據具體業務場景調整檢索策略和生成參數。\n",
    "\n",
    "性能優化方面，可以考慮使用快取機制、批量處理、模型量化等技術來提升系統響應速度和降低資源消耗。\"\"\",\n",
    "    \"education_content\": \"\"\"# 第一章 學習方法論\n",
    "\n",
    "## 1.1 主動學習策略\n",
    "\n",
    "主動學習是一種以學習者為中心的教學方法。它強調學習者在學習過程中的主動參與和思考。\n",
    "\n",
    "有效的主動學習包括以下要素：\n",
    "1. 明確的學習目標\n",
    "2. 積極的參與態度\n",
    "3. 反思與總結\n",
    "\n",
    "### 提問技巧\n",
    "\n",
    "提問是主動學習的重要工具。好的問題能夠：\n",
    "- 引發深度思考\n",
    "- 促進知識連結\n",
    "- 檢驗理解程度\n",
    "\n",
    "## 1.2 記憶技巧\n",
    "\n",
    "### 間隔重複法\n",
    "\n",
    "間隔重複是基於遺忘曲線理論的記憶方法。通過在特定時間間隔內重複學習，可以有效提高長期記憶效果。\n",
    "\n",
    "具體實施方法：\n",
    "第一次複習：1天後\n",
    "第二次複習：3天後\n",
    "第三次複習：1週後\n",
    "第四次複習：2週後\n",
    "第五次複習：1個月後\n",
    "\n",
    "### 聯想記憶法\n",
    "\n",
    "聯想記憶通過建立新舊知識間的連結來幫助記憶。常用技巧包括：\n",
    "- 圖像聯想\n",
    "- 故事串聯\n",
    "- 邏輯歸納\n",
    "\n",
    "## 1.3 批判性思維\n",
    "\n",
    "批判性思維是現代教育的核心能力之一。它包括：\n",
    "\n",
    "分析能力：能夠拆解複雜問題，識別關鍵要素和內在邏輯。\n",
    "評估能力：對信息來源的可靠性、論證的有效性進行判斷。\n",
    "推理能力：基於已知信息得出合理結論。\n",
    "反思能力：檢視自己的思維過程和結論。\n",
    "\n",
    "培養批判性思維需要持續的練習和指導。\"\"\",\n",
    "}\n",
    "\n",
    "# Save sample documents\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "for doc_name, content in SAMPLE_ZH_DOCS.items():\n",
    "    with open(f\"data/{doc_name}_zh.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"✅ Sample Chinese documents prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273a21b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 3: Chinese-Friendly Splitter Implementation\n",
    "@dataclass\n",
    "class ChunkingConfig:\n",
    "    \"\"\"Configuration for Chinese text chunking\"\"\"\n",
    "\n",
    "    chunk_size: int = 800  # Optimal for Chinese (vs 1000 for English)\n",
    "    chunk_overlap: int = 80  # 10% overlap\n",
    "    separators: List[str] = None\n",
    "    add_start_index: bool = True\n",
    "    strip_whitespace: bool = True\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.separators is None:\n",
    "            # Hierarchical separators for Chinese\n",
    "            self.separators = [\n",
    "                \"\\n### \",  # H3 headers\n",
    "                \"\\n## \",  # H2 headers\n",
    "                \"\\n# \",  # H1 headers\n",
    "                \"\\n第.*?章\",  # Chapter markers (第一章, 第二章...)\n",
    "                \"\\n第.*?節\",  # Section markers\n",
    "                \"。\",  # Period (strongest sentence boundary)\n",
    "                \"！\",  # Exclamation\n",
    "                \"？\",  # Question mark\n",
    "                \"；\",  # Semicolon\n",
    "                \"：\",  # Colon\n",
    "                \"…\",  # Ellipsis\n",
    "                \"\\n\\n\",  # Double newline (paragraph)\n",
    "                \"\\n\",  # Single newline\n",
    "                \" \",  # Space (weakest)\n",
    "            ]\n",
    "\n",
    "\n",
    "class ChineseTextSplitter(RecursiveCharacterTextSplitter):\n",
    "    \"\"\"Enhanced splitter for Chinese text with semantic awareness\"\"\"\n",
    "\n",
    "    def __init__(self, config: ChunkingConfig):\n",
    "        super().__init__(\n",
    "            separators=config.separators,\n",
    "            chunk_size=config.chunk_size,\n",
    "            chunk_overlap=config.chunk_overlap,\n",
    "            add_start_index=config.add_start_index,\n",
    "            strip_whitespace=config.strip_whitespace,\n",
    "            length_function=self._chinese_length_function,\n",
    "        )\n",
    "        self.config = config\n",
    "\n",
    "    def _chinese_length_function(self, text: str) -> int:\n",
    "        \"\"\"Custom length function considering Chinese character density\"\"\"\n",
    "        # Chinese characters are typically \"denser\" in meaning\n",
    "        # Adjust token estimation accordingly\n",
    "        try:\n",
    "            encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "            return len(encoding.encode(text))\n",
    "        except:\n",
    "            # Fallback: rough estimation (Chinese char ≈ 1.5 tokens)\n",
    "            chinese_chars = len(re.findall(r\"[\\u4e00-\\u9fff]\", text))\n",
    "            other_chars = len(text) - chinese_chars\n",
    "            return int(chinese_chars * 1.5 + other_chars * 0.8)\n",
    "\n",
    "    def create_documents_with_metadata(\n",
    "        self, texts: List[str], metadatas: List[Dict] = None\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Create documents with enhanced metadata for Chinese content\"\"\"\n",
    "        if metadatas is None:\n",
    "            metadatas = [{}] * len(texts)\n",
    "\n",
    "        documents = []\n",
    "        for i, text in enumerate(texts):\n",
    "            # Create chunks\n",
    "            chunks = self.split_text(text)\n",
    "\n",
    "            for j, chunk in enumerate(chunks):\n",
    "                # Enhanced metadata for Chinese content\n",
    "                chunk_meta = {\n",
    "                    **metadatas[i],\n",
    "                    \"chunk_id\": f\"{i}_{j}\",\n",
    "                    \"chunk_index\": j,\n",
    "                    \"total_chunks\": len(chunks),\n",
    "                    \"char_count\": len(chunk),\n",
    "                    \"est_tokens\": self._chinese_length_function(chunk),\n",
    "                    \"has_title\": bool(re.search(r\"^#+\\s+|^第.*?[章節]\", chunk.strip())),\n",
    "                    \"punctuation_density\": len(re.findall(r\"[。！？；：]\", chunk))\n",
    "                    / max(1, len(chunk)),\n",
    "                }\n",
    "\n",
    "                documents.append(Document(page_content=chunk, metadata=chunk_meta))\n",
    "\n",
    "        return documents\n",
    "\n",
    "\n",
    "# Initialize different chunking strategies for comparison\n",
    "configs = {\n",
    "    \"conservative\": ChunkingConfig(chunk_size=600, chunk_overlap=60),\n",
    "    \"balanced\": ChunkingConfig(chunk_size=800, chunk_overlap=80),\n",
    "    \"aggressive\": ChunkingConfig(chunk_size=1200, chunk_overlap=120),\n",
    "}\n",
    "\n",
    "splitters = {name: ChineseTextSplitter(config) for name, config in configs.items()}\n",
    "\n",
    "print(\"✅ Chinese text splitters initialized\")\n",
    "print(f\"Available strategies: {list(splitters.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6bef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 4: Chunking Strategy Comparison\n",
    "def analyze_chunking_results(documents: List[Document], strategy_name: str) -> Dict:\n",
    "    \"\"\"Analyze chunking quality metrics\"\"\"\n",
    "    if not documents:\n",
    "        return {}\n",
    "\n",
    "    chunk_lengths = [doc.metadata.get(\"char_count\", 0) for doc in documents]\n",
    "    token_counts = [doc.metadata.get(\"est_tokens\", 0) for doc in documents]\n",
    "    punct_densities = [doc.metadata.get(\"punctuation_density\", 0) for doc in documents]\n",
    "\n",
    "    return {\n",
    "        \"strategy\": strategy_name,\n",
    "        \"total_chunks\": len(documents),\n",
    "        \"avg_char_length\": sum(chunk_lengths) / len(chunk_lengths),\n",
    "        \"avg_token_count\": sum(token_counts) / len(token_counts),\n",
    "        \"avg_punct_density\": sum(punct_densities) / len(punct_densities),\n",
    "        \"min_length\": min(chunk_lengths),\n",
    "        \"max_length\": max(chunk_lengths),\n",
    "        \"chunks_with_titles\": sum(\n",
    "            1 for doc in documents if doc.metadata.get(\"has_title\", False)\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "# Test all strategies on sample documents\n",
    "results = {}\n",
    "\n",
    "for doc_name, content in SAMPLE_ZH_DOCS.items():\n",
    "    print(f\"\\n📄 Processing: {doc_name}\")\n",
    "    print(f\"Original length: {len(content)} characters\")\n",
    "\n",
    "    results[doc_name] = {}\n",
    "\n",
    "    for strategy_name, splitter in splitters.items():\n",
    "        # Create metadata for this document\n",
    "        metadata = {\n",
    "            \"source\": doc_name,\n",
    "            \"language\": \"zh\",\n",
    "            \"content_type\": \"educational\" if \"education\" in doc_name else \"technical\",\n",
    "        }\n",
    "\n",
    "        # Split the document\n",
    "        documents = splitter.create_documents_with_metadata([content], [metadata])\n",
    "\n",
    "        # Analyze results\n",
    "        analysis = analyze_chunking_results(documents, strategy_name)\n",
    "        results[doc_name][strategy_name] = {\n",
    "            \"analysis\": analysis,\n",
    "            \"documents\": documents,\n",
    "        }\n",
    "\n",
    "        print(\n",
    "            f\"  {strategy_name:>12}: {analysis['total_chunks']:2d} chunks, \"\n",
    "            f\"avg {analysis['avg_char_length']:.0f} chars, \"\n",
    "            f\"avg {analysis['avg_token_count']:.0f} tokens\"\n",
    "        )\n",
    "\n",
    "print(\"\\n✅ Chunking analysis completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f6289",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 5: Chunk Quality Assessment\n",
    "def assess_chunk_quality(chunk: str) -> Dict[str, float]:\n",
    "    \"\"\"Assess semantic completeness and quality of a chunk\"\"\"\n",
    "\n",
    "    # Check for complete sentences\n",
    "    complete_sentences = len(re.findall(r\"[。！？](?:\\s|$)\", chunk))\n",
    "    total_possible_sentences = len(re.findall(r\"[。！？]\", chunk))\n",
    "    sentence_completeness = complete_sentences / max(1, total_possible_sentences)\n",
    "\n",
    "    # Check for structural elements\n",
    "    has_heading = bool(re.search(r\"^#+\\s+|^第.*?[章節]\", chunk.strip()))\n",
    "    has_list_structure = bool(re.search(r\"^\\d+\\.|^-\\s+|^•\\s+\", chunk, re.MULTILINE))\n",
    "\n",
    "    # Coherence indicators (simplified)\n",
    "    transition_words = [\n",
    "        \"因此\",\n",
    "        \"所以\",\n",
    "        \"但是\",\n",
    "        \"然而\",\n",
    "        \"此外\",\n",
    "        \"另外\",\n",
    "        \"首先\",\n",
    "        \"其次\",\n",
    "        \"最後\",\n",
    "    ]\n",
    "    transition_score = (\n",
    "        sum(1 for word in transition_words if word in chunk) / len(chunk) * 1000\n",
    "    )\n",
    "\n",
    "    # Content density (Chinese characters vs total)\n",
    "    chinese_chars = len(re.findall(r\"[\\u4e00-\\u9fff]\", chunk))\n",
    "    content_density = chinese_chars / max(1, len(chunk))\n",
    "\n",
    "    return {\n",
    "        \"sentence_completeness\": sentence_completeness,\n",
    "        \"has_heading\": has_heading,\n",
    "        \"has_structure\": has_list_structure,\n",
    "        \"transition_density\": min(1.0, transition_score),\n",
    "        \"content_density\": content_density,\n",
    "        \"overall_quality\": (\n",
    "            sentence_completeness + content_density + transition_score / 2\n",
    "        )\n",
    "        / 2.5,\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate chunk quality for each strategy\n",
    "print(\"🎯 Chunk Quality Assessment\\n\")\n",
    "\n",
    "for doc_name in SAMPLE_ZH_DOCS.keys():\n",
    "    print(f\"📄 Document: {doc_name}\")\n",
    "\n",
    "    for strategy_name in splitters.keys():\n",
    "        documents = results[doc_name][strategy_name][\"documents\"]\n",
    "\n",
    "        # Calculate average quality metrics\n",
    "        quality_scores = [assess_chunk_quality(doc.page_content) for doc in documents]\n",
    "\n",
    "        avg_quality = {\n",
    "            metric: sum(score[metric] for score in quality_scores) / len(quality_scores)\n",
    "            for metric in quality_scores[0].keys()\n",
    "        }\n",
    "\n",
    "        print(\n",
    "            f\"  {strategy_name:>12}: \"\n",
    "            f\"Quality={avg_quality['overall_quality']:.2f}, \"\n",
    "            f\"Completeness={avg_quality['sentence_completeness']:.2f}, \"\n",
    "            f\"Density={avg_quality['content_density']:.2f}\"\n",
    "        )\n",
    "\n",
    "        # Store quality metrics\n",
    "        results[doc_name][strategy_name][\"quality\"] = avg_quality\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0903c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 6: Optimal Strategy Selection & Recommendations\n",
    "def recommend_strategy(\n",
    "    document_type: str, content_characteristics: Dict\n",
    ") -> Tuple[str, str]:\n",
    "    \"\"\"Recommend optimal chunking strategy based on content analysis\"\"\"\n",
    "\n",
    "    if content_characteristics.get(\"has_many_headings\", False):\n",
    "        return \"conservative\", \"文檔結構豐富，使用保守策略保持標題完整性\"\n",
    "\n",
    "    if content_characteristics.get(\"high_punctuation_density\", False):\n",
    "        return \"balanced\", \"標點密度高，平衡策略可保持語義完整\"\n",
    "\n",
    "    if document_type == \"technical\":\n",
    "        return \"aggressive\", \"技術文檔通常邏輯性強，可使用較大塊\"\n",
    "\n",
    "    return \"balanced\", \"一般文檔推薦使用平衡策略\"\n",
    "\n",
    "\n",
    "# Generate recommendations and best practices\n",
    "print(\"💡 Chunking Strategy Recommendations\\n\")\n",
    "\n",
    "best_strategies = {}\n",
    "for doc_name in SAMPLE_ZH_DOCS.keys():\n",
    "    print(f\"📄 {doc_name}:\")\n",
    "\n",
    "    # Find strategy with best overall quality\n",
    "    best_strategy = max(\n",
    "        results[doc_name].keys(),\n",
    "        key=lambda s: results[doc_name][s][\"quality\"][\"overall_quality\"],\n",
    "    )\n",
    "\n",
    "    best_quality = results[doc_name][best_strategy][\"quality\"][\"overall_quality\"]\n",
    "    best_strategies[doc_name] = best_strategy\n",
    "\n",
    "    print(f\"  🏆 Best strategy: {best_strategy} (quality: {best_quality:.3f})\")\n",
    "\n",
    "    # Show sample chunk\n",
    "    sample_chunk = results[doc_name][best_strategy][\"documents\"][0].page_content[:200]\n",
    "    print(f\"  📝 Sample chunk: {sample_chunk}...\")\n",
    "    print()\n",
    "\n",
    "# Save chunked documents for next notebook\n",
    "print(\"💾 Saving chunked documents for RAG indexing...\")\n",
    "\n",
    "os.makedirs(\"data/chunks\", exist_ok=True)\n",
    "\n",
    "for doc_name in SAMPLE_ZH_DOCS.keys():\n",
    "    best_strategy = best_strategies[doc_name]\n",
    "    documents = results[doc_name][best_strategy][\"documents\"]\n",
    "\n",
    "    # Save as JSONL for easy loading\n",
    "    output_file = f\"data/chunks/{doc_name}_chunked.jsonl\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for doc in documents:\n",
    "            chunk_data = {\"text\": doc.page_content, \"metadata\": doc.metadata}\n",
    "            f.write(json.dumps(chunk_data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"  ✅ Saved {len(documents)} chunks to {output_file}\")\n",
    "\n",
    "print(\"\\n🎉 Chunking strategies analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e112bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 7: Smoke Test - Quick Validation\n",
    "# Smoke test: Verify chunking works correctly\n",
    "print(\"🧪 Smoke Test: Chinese Chunking\")\n",
    "\n",
    "test_text = \"\"\"# 測試文檔\n",
    "\n",
    "這是一個測試段落。它包含多個句子；用來驗證分段效果！\n",
    "\n",
    "## 小節標題\n",
    "\n",
    "另一個段落的內容。包含：列表項目、技術詞彙。最後一句話。\"\"\"\n",
    "\n",
    "# Test default balanced strategy\n",
    "splitter = splitters[\"balanced\"]\n",
    "chunks = splitter.split_text(test_text)\n",
    "\n",
    "print(f\"✅ Input: {len(test_text)} chars\")\n",
    "print(f\"✅ Output: {len(chunks)} chunks\")\n",
    "print(f\"✅ Avg chunk size: {sum(len(c) for c in chunks) / len(chunks):.0f} chars\")\n",
    "\n",
    "# Verify chunks are semantically meaningful\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"✅ Chunk {i+1}: {chunk.strip()[:50]}...\")\n",
    "\n",
    "assert len(chunks) > 0, \"Should produce at least one chunk\"\n",
    "assert all(len(chunk.strip()) > 0 for chunk in chunks), \"All chunks should have content\"\n",
    "\n",
    "print(\"\\n🎯 Smoke test passed! Chunking strategy working correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb17d095",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 8: When to Use This & Next Steps\n",
    "print(\n",
    "    \"\"\"\n",
    "🎯 When to Use This Notebook:\n",
    "\n",
    "1. **文檔預處理階段** - 在建立 RAG 系統之前，優化文本分段策略\n",
    "2. **多語言處理** - 處理中文文檔時，需要考慮語言特性\n",
    "3. **檢索品質優化** - 當檢索結果不理想時，重新評估分段策略\n",
    "4. **系統性能調優** - 平衡檢索精度與計算效率\n",
    "\n",
    "🔧 Key Parameters to Remember:\n",
    "- chunk_size: 600-1200 (中文建議 800)\n",
    "- chunk_overlap: 10-15% of chunk_size\n",
    "- separators: 階層式標點符號優先級\n",
    "- length_function: 考慮中文字符密度\n",
    "\n",
    "⚠️ Common Pitfalls:\n",
    "- 忽略中文標點符號的語義邊界\n",
    "- chunk_size 設置過大導致語義混雜\n",
    "- overlap 太小失去上下文，太大浪費資源\n",
    "- 未考慮文檔結構（標題、列表）\n",
    "\n",
    "🚀 Next Steps:\n",
    "- nb12: 使用 bge-m3 對分段文本進行向量化\n",
    "- nb13: 建立 FAISS 索引進行高效檢索\n",
    "- nb14: 實現查詢與引用功能\n",
    "\n",
    "📁 Generated Files:\n",
    "- data/chunks/*.jsonl - 分段結果，供後續 notebook 使用\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df7e52d",
   "metadata": {},
   "source": [
    "# nb11_zh_chunking_strategies.ipynb 實作計畫\n",
    "\n",
    "## 目標（Goals）\n",
    "\n",
    "1. **中文分段策略**：實作適合中文文本的分段方法，處理無空格分詞特性\n",
    "2. **多層級分段**：章節標題 → 段落 → 標點符號的階層式切分\n",
    "3. **參數優化**：針對中文調整 chunk_size/overlap，考慮標點密度與語義完整性\n",
    "4. **分段品質評估**：比較不同策略的語義保持度與檢索效果\n",
    "5. **實際應用**：為後續 RAG 流程提供高品質的文本片段\n",
    "\n",
    "## Notebook 大綱（Cells & Purpose）## 核心代碼要點（Core Code Blocks）\n",
    "\n",
    "### 1. 中文特化分段器\n",
    "```python\n",
    "class ChineseTextSplitter(RecursiveCharacterTextSplitter):\n",
    "    def __init__(self, config: ChunkingConfig):\n",
    "        # 階層式分隔符：標題 → 章節 → 強標點 → 弱標點\n",
    "        separators = [\"\\n### \", \"\\n## \", \"\\n# \", \"。\", \"！\", \"？\", \"；\", \"：\"]\n",
    "        super().__init__(separators=separators, chunk_size=800, chunk_overlap=80)\n",
    "```\n",
    "\n",
    "### 2. 中文長度計算函數\n",
    "```python\n",
    "def _chinese_length_function(self, text: str) -> int:\n",
    "    # 中文字符密度較高，需要調整 token 估算\n",
    "    chinese_chars = len(re.findall(r'[\\u4e00-\\u9fff]', text))\n",
    "    return int(chinese_chars * 1.5 + (len(text) - chinese_chars) * 0.8)\n",
    "```\n",
    "\n",
    "### 3. 品質評估指標\n",
    "```python\n",
    "def assess_chunk_quality(chunk: str) -> Dict[str, float]:\n",
    "    # 句子完整性、結構性、連貫性、內容密度\n",
    "    complete_sentences = len(re.findall(r'[。！？](?:\\s|$)', chunk))\n",
    "    content_density = len(re.findall(r'[\\u4e00-\\u9fff]', chunk)) / len(chunk)\n",
    "    return {\"overall_quality\": (completeness + density) / 2}\n",
    "```\n",
    "\n",
    "## Smoke Test（煙霧測試）\n",
    "\n",
    "```python\n",
    "# 快速驗證分段功能\n",
    "test_text = \"\"\"# 測試\\n這是第一段。包含多句話！\\n## 小節\\n另一段內容；結束。\"\"\"\n",
    "chunks = splitter.split_text(test_text)\n",
    "assert len(chunks) > 0 and all(len(c.strip()) > 0 for c in chunks)\n",
    "print(f\"✅ 生成 {len(chunks)} 個語義完整的片段\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Stage 2 進度總結\n",
    "\n",
    "### Completed（已完成）\n",
    "- ✅ **nb10**: 文檔載入與清理（HTML/PDF → 純文字）\n",
    "- ✅ **nb11**: 中文分段策略（階層式切分 + 品質評估）\n",
    "\n",
    "### Core Concepts（核心概念）\n",
    "- **階層式分段**：標題 → 段落 → 標點符號的優先級\n",
    "- **中文特化處理**：考慮字符密度與標點語義邊界\n",
    "- **品質評估**：句子完整性、結構性、內容密度指標\n",
    "- **參數調優**：chunk_size/overlap 針對中文優化\n",
    "\n",
    "### Pitfalls（常見陷阱）\n",
    "- ⚠️ chunk_size 設置需考慮中文 token 密度（建議 600-1200）\n",
    "- ⚠️ 分隔符順序影響語義邊界保持\n",
    "- ⚠️ overlap 太小失去上下文，太大浪費計算資源\n",
    "- ⚠️ 忽略文檔結構（標題、列表）的完整性\n",
    "\n",
    "### Next Actions（下一步）\n",
    "1. **nb12**: bge-m3 嵌入模型對分段文本向量化\n",
    "2. **nb13**: FAISS 索引建立與存儲\n",
    "3. **nb14**: 查詢檢索與引用格式化\n",
    "4. **nb15**: bge-reranker 重排器提升檢索精度\n",
    "\n",
    "分段是 RAG 系統的基礎，高品質的分段直接影響後續檢索與生成效果。nb11 為我們建立了中文友好的分段基線，接下來進入向量化階段！"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
