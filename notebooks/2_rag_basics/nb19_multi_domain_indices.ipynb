{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c54264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb19_multi_domain_indices.ipynb\n",
    "# Multi-Domain RAG Indices with Smart Routing\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (Ë§áË£ΩÂà∞ÊØèÊú¨ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9ed46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Dependencies and Domain Sample Data\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Sample documents for different domains\n",
    "DOMAIN_SAMPLES = {\n",
    "    \"tech\": [\n",
    "        \"Large Language Models (LLMs) like GPT and BERT use transformer architecture for natural language processing.\",\n",
    "        \"RAG (Retrieval-Augmented Generation) combines vector databases with language models for grounded responses.\",\n",
    "        \"FAISS (Facebook AI Similarity Search) provides efficient similarity search and clustering of dense vectors.\",\n",
    "        \"Gradient descent optimization algorithms like Adam and SGD are fundamental to training neural networks.\",\n",
    "        \"Docker containers enable consistent deployment environments across development and production systems.\",\n",
    "    ],\n",
    "    \"edu\": [\n",
    "        \"Constructivist learning theory emphasizes that students build knowledge through active engagement with materials.\",\n",
    "        \"Bloom's taxonomy categorizes learning objectives into six levels from remembering to creating.\",\n",
    "        \"Differentiated instruction adapts teaching methods to accommodate diverse learning styles and abilities.\",\n",
    "        \"Assessment rubrics provide clear criteria and standards for evaluating student performance.\",\n",
    "        \"Project-based learning engages students in real-world problems to develop critical thinking skills.\",\n",
    "    ],\n",
    "    \"legal\": [\n",
    "        \"Contract law governs the formation, performance, and enforcement of agreements between parties.\",\n",
    "        \"Due process ensures fair treatment through the normal judicial system, especially as a citizen's entitlement.\",\n",
    "        \"Intellectual property rights protect creators' ownership of their innovations, writings, and artistic works.\",\n",
    "        \"Tort law addresses civil wrongs that cause harm or loss, resulting in legal liability for the actor.\",\n",
    "        \"Constitutional law establishes the framework of government and fundamental rights of citizens.\",\n",
    "    ],\n",
    "    \"general\": [\n",
    "        \"Climate change refers to long-term shifts in global temperatures and weather patterns.\",\n",
    "        \"The scientific method involves observation, hypothesis formation, experimentation, and conclusion.\",\n",
    "        \"Renewable energy sources like solar and wind power help reduce carbon emissions.\",\n",
    "        \"Biodiversity conservation protects ecosystems and maintains ecological balance.\",\n",
    "        \"Sustainable development meets present needs without compromising future generations' ability.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(\"‚úì Domain sample data prepared\")\n",
    "for domain, docs in DOMAIN_SAMPLES.items():\n",
    "    print(f\"  {domain}: {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9b84ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Domain Index Builder\n",
    "class DomainIndexBuilder:\n",
    "    def __init__(self, model_name=\"BAAI/bge-m3\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"„ÄÇ\", \"ÔºÅ\", \"Ôºü\", \"Ôºõ\", \"‚Ä¶\", \"\\n\\n\", \"\\n\", \" \"],\n",
    "            chunk_size=400,  # Smaller chunks for domain-specific content\n",
    "            chunk_overlap=40,\n",
    "        )\n",
    "\n",
    "    def build_domain_index(\n",
    "        self, domain: str, documents: List[str], index_dir: str = \"indices\"\n",
    "    ) -> Tuple[faiss.Index, List[Dict]]:\n",
    "        \"\"\"Build FAISS index for specific domain\"\"\"\n",
    "        # Create chunks with domain metadata\n",
    "        chunks_data = []\n",
    "        all_texts = []\n",
    "\n",
    "        for doc_id, doc in enumerate(documents):\n",
    "            chunks = self.splitter.create_documents([doc])\n",
    "            for chunk_id, chunk in enumerate(chunks):\n",
    "                chunk_meta = {\n",
    "                    \"id\": f\"{domain}_{doc_id}_{chunk_id}\",\n",
    "                    \"text\": chunk.page_content,\n",
    "                    \"domain\": domain,\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                }\n",
    "                chunks_data.append(chunk_meta)\n",
    "                all_texts.append(chunk.page_content)\n",
    "\n",
    "        # Generate embeddings\n",
    "        print(f\"Embedding {len(all_texts)} chunks for domain '{domain}'...\")\n",
    "        embeddings = self.model.encode(\n",
    "            all_texts, normalize_embeddings=True, batch_size=16, show_progress_bar=True\n",
    "        ).astype(\"float32\")\n",
    "\n",
    "        # Build FAISS index\n",
    "        index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "        index.add(embeddings)\n",
    "\n",
    "        # Save index and metadata\n",
    "        Path(index_dir).mkdir(exist_ok=True)\n",
    "        faiss.write_index(index, f\"{index_dir}/{domain}.faiss\")\n",
    "\n",
    "        with open(f\"{index_dir}/{domain}_chunks.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "            for chunk in chunks_data:\n",
    "                f.write(json.dumps(chunk, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        print(f\"‚úì Built index for '{domain}': {index.ntotal} vectors\")\n",
    "        return index, chunks_data\n",
    "\n",
    "\n",
    "# Build indices for all domains\n",
    "builder = DomainIndexBuilder()\n",
    "domain_indices = {}\n",
    "domain_chunks = {}\n",
    "\n",
    "for domain, docs in DOMAIN_SAMPLES.items():\n",
    "    index, chunks = builder.build_domain_index(domain, docs)\n",
    "    domain_indices[domain] = index\n",
    "    domain_chunks[domain] = chunks\n",
    "\n",
    "print(f\"\\n‚úì Built {len(domain_indices)} domain indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6b1236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Smart Query Router\n",
    "class QueryRouter:\n",
    "    def __init__(self, model_name=\"BAAI/bge-m3\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "        # Domain descriptions for semantic routing\n",
    "        self.domain_descriptions = {\n",
    "            \"tech\": \"artificial intelligence, machine learning, programming, software engineering, computer science, technology, algorithms, data science\",\n",
    "            \"edu\": \"education, teaching, learning, pedagogy, instruction, assessment, curriculum, students, classroom management\",\n",
    "            \"legal\": \"law, legal, court, contract, rights, constitution, tort, legislation, justice, attorney, litigation\",\n",
    "            \"general\": \"science, environment, climate, nature, research, sustainability, general knowledge, news, current events\",\n",
    "        }\n",
    "\n",
    "        # Pre-compute domain description embeddings\n",
    "        self.domain_embeddings = {}\n",
    "        for domain, desc in self.domain_descriptions.items():\n",
    "            emb = self.model.encode([desc], normalize_embeddings=True)[0]\n",
    "            self.domain_embeddings[domain] = emb.astype(\"float32\")\n",
    "\n",
    "        # Rule-based keywords\n",
    "        self.domain_keywords = {\n",
    "            \"tech\": [\n",
    "                \"LLM\",\n",
    "                \"AI\",\n",
    "                \"Ê©üÂô®Â≠∏Áøí\",\n",
    "                \"Á®ãÂºè\",\n",
    "                \"ÊºîÁÆóÊ≥ï\",\n",
    "                \"Á®ãÂºèÁ¢º\",\n",
    "                \"Á•ûÁ∂ìÁ∂≤Ë∑Ø\",\n",
    "                \"transformer\",\n",
    "                \"API\",\n",
    "                \"Ë≥áÊñôÂ∫´\",\n",
    "            ],\n",
    "            \"edu\": [\n",
    "                \"ÊïôÂ≠∏\",\n",
    "                \"Â≠∏Áøí\",\n",
    "                \"ÊïôËÇ≤\",\n",
    "                \"Â≠∏Áîü\",\n",
    "                \"Ë™≤Á®ã\",\n",
    "                \"Ë©ïÈáè\",\n",
    "                \"ÊïôÊùê\",\n",
    "                \"pedagogy\",\n",
    "                \"assessment\",\n",
    "                \"curriculum\",\n",
    "            ],\n",
    "            \"legal\": [\n",
    "                \"Ê≥ïÂæã\",\n",
    "                \"ÂêàÁ¥Ñ\",\n",
    "                \"ÊÜ≤Ê≥ï\",\n",
    "                \"Ê¨äÂà©\",\n",
    "                \"Ê≥ïÈô¢\",\n",
    "                \"Ë®¥Ë®ü\",\n",
    "                \"contract\",\n",
    "                \"tort\",\n",
    "                \"legislation\",\n",
    "                \"constitutional\",\n",
    "            ],\n",
    "            \"general\": [\n",
    "                \"Ê∞£ÂÄô\",\n",
    "                \"Áí∞Â¢É\",\n",
    "                \"ÁßëÂ≠∏\",\n",
    "                \"Á†îÁ©∂\",\n",
    "                \"Ê∞∏Á∫å\",\n",
    "                \"climate\",\n",
    "                \"environment\",\n",
    "                \"research\",\n",
    "                \"sustainable\",\n",
    "            ],\n",
    "        }\n",
    "\n",
    "    def route_semantic(self, query: str, top_k: int = 2) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Semantic routing based on query-domain similarity\"\"\"\n",
    "        query_emb = self.model.encode([query], normalize_embeddings=True)[0]\n",
    "\n",
    "        similarities = []\n",
    "        for domain, domain_emb in self.domain_embeddings.items():\n",
    "            sim = np.dot(query_emb, domain_emb)\n",
    "            similarities.append((domain, float(sim)))\n",
    "\n",
    "        # Sort by similarity and return top_k\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "\n",
    "    def route_rule_based(self, query: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Rule-based routing using keyword matching\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        scores = {domain: 0.0 for domain in self.domain_keywords}\n",
    "\n",
    "        for domain, keywords in self.domain_keywords.items():\n",
    "            for keyword in keywords:\n",
    "                if keyword.lower() in query_lower:\n",
    "                    scores[domain] += 1.0\n",
    "\n",
    "        # Normalize scores\n",
    "        max_score = max(scores.values()) if max(scores.values()) > 0 else 1.0\n",
    "        normalized = [(domain, score / max_score) for domain, score in scores.items()]\n",
    "\n",
    "        # Filter and sort\n",
    "        filtered = [(d, s) for d, s in normalized if s > 0]\n",
    "        filtered.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return filtered if filtered else [(\"general\", 1.0)]\n",
    "\n",
    "    def route_hybrid(\n",
    "        self, query: str, semantic_weight: float = 0.7\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Hybrid routing combining semantic and rule-based approaches\"\"\"\n",
    "        semantic_routes = self.route_semantic(query, top_k=4)\n",
    "        rule_routes = self.route_rule_based(query)\n",
    "\n",
    "        # Combine scores\n",
    "        combined_scores = {}\n",
    "\n",
    "        # Add semantic scores\n",
    "        for domain, score in semantic_routes:\n",
    "            combined_scores[domain] = semantic_weight * score\n",
    "\n",
    "        # Add rule-based scores\n",
    "        rule_dict = dict(rule_routes)\n",
    "        for domain in combined_scores:\n",
    "            if domain in rule_dict:\n",
    "                combined_scores[domain] += (1 - semantic_weight) * rule_dict[domain]\n",
    "\n",
    "        # Add purely rule-based domains\n",
    "        for domain, score in rule_routes:\n",
    "            if domain not in combined_scores:\n",
    "                combined_scores[domain] = (1 - semantic_weight) * score\n",
    "\n",
    "        # Sort and return\n",
    "        result = [(domain, score) for domain, score in combined_scores.items()]\n",
    "        result.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "# Initialize router\n",
    "router = QueryRouter()\n",
    "\n",
    "# Test routing examples\n",
    "test_queries = [\n",
    "    \"What is transformer architecture in machine learning?\",\n",
    "    \"How to implement project-based learning in classroom?\",\n",
    "    \"What are the key principles of contract law?\",\n",
    "    \"How does climate change affect biodiversity?\",\n",
    "]\n",
    "\n",
    "print(\"Query Routing Test Results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "\n",
    "    semantic = router.route_semantic(query, top_k=2)\n",
    "    rule_based = router.route_rule_based(query)\n",
    "    hybrid = router.route_hybrid(query)\n",
    "\n",
    "    print(f\"  Semantic: {semantic}\")\n",
    "    print(f\"  Rule-based: {rule_based[:2]}\")\n",
    "    print(f\"  Hybrid: {hybrid[:2]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97af1952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Multi-Domain Index Manager\n",
    "class MultiDomainIndexManager:\n",
    "    def __init__(self, model_name=\"BAAI/bge-m3\", index_dir=\"indices\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.router = QueryRouter(model_name)\n",
    "        self.index_dir = Path(index_dir)\n",
    "\n",
    "        # Load indices and chunks\n",
    "        self.indices = {}\n",
    "        self.chunks = {}\n",
    "        self.load_all_indices()\n",
    "\n",
    "        # Routing configuration\n",
    "        self.config = {\n",
    "            \"routing_strategy\": \"hybrid\",  # semantic, rule, hybrid\n",
    "            \"semantic_weight\": 0.7,\n",
    "            \"max_domains\": 2,\n",
    "            \"fallback_domain\": \"general\",\n",
    "            \"similarity_threshold\": 0.1,\n",
    "        }\n",
    "\n",
    "    def load_all_indices(self):\n",
    "        \"\"\"Load all available domain indices\"\"\"\n",
    "        for domain in [\"tech\", \"edu\", \"legal\", \"general\"]:\n",
    "            index_path = self.index_dir / f\"{domain}.faiss\"\n",
    "            chunks_path = self.index_dir / f\"{domain}_chunks.jsonl\"\n",
    "\n",
    "            if index_path.exists() and chunks_path.exists():\n",
    "                # Load FAISS index\n",
    "                index = faiss.read_index(str(index_path))\n",
    "                self.indices[domain] = index\n",
    "\n",
    "                # Load chunks metadata\n",
    "                chunks = []\n",
    "                with open(chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        chunks.append(json.loads(line.strip()))\n",
    "                self.chunks[domain] = chunks\n",
    "\n",
    "                print(f\"‚úì Loaded {domain} index: {index.ntotal} vectors\")\n",
    "\n",
    "    def route_query(self, query: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Route query to appropriate domains\"\"\"\n",
    "        strategy = self.config[\"routing_strategy\"]\n",
    "\n",
    "        if strategy == \"semantic\":\n",
    "            routes = self.router.route_semantic(query, self.config[\"max_domains\"])\n",
    "        elif strategy == \"rule\":\n",
    "            routes = self.router.route_rule_based(query)[: self.config[\"max_domains\"]]\n",
    "        else:  # hybrid\n",
    "            routes = self.router.route_hybrid(query, self.config[\"semantic_weight\"])[\n",
    "                : self.config[\"max_domains\"]\n",
    "            ]\n",
    "\n",
    "        # Filter by threshold\n",
    "        filtered = [\n",
    "            (d, s) for d, s in routes if s >= self.config[\"similarity_threshold\"]\n",
    "        ]\n",
    "\n",
    "        # Fallback if no routes meet threshold\n",
    "        if not filtered:\n",
    "            filtered = [(self.config[\"fallback_domain\"], 1.0)]\n",
    "\n",
    "        return filtered\n",
    "\n",
    "    def search_multi_domain(\n",
    "        self, query: str, k: int = 5\n",
    "    ) -> List[Tuple[str, Dict, float]]:\n",
    "        \"\"\"Search across multiple domains with routing\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Route query to domains\n",
    "        domain_routes = self.route_query(query)\n",
    "        print(f\"Routed to domains: {domain_routes}\")\n",
    "\n",
    "        # Generate query embedding\n",
    "        query_emb = self.model.encode([query], normalize_embeddings=True).astype(\n",
    "            \"float32\"\n",
    "        )\n",
    "\n",
    "        # Search each routed domain\n",
    "        all_results = []\n",
    "\n",
    "        for domain, weight in domain_routes:\n",
    "            if domain not in self.indices:\n",
    "                continue\n",
    "\n",
    "            # Search this domain\n",
    "            index = self.indices[domain]\n",
    "            chunks = self.chunks[domain]\n",
    "\n",
    "            # Get more candidates to account for domain weighting\n",
    "            search_k = min(k * 2, index.ntotal)\n",
    "            D, I = index.search(query_emb, search_k)\n",
    "\n",
    "            # Collect results with domain weighting\n",
    "            for i, (idx, score) in enumerate(zip(I[0], D[0])):\n",
    "                if idx >= 0:  # Valid index\n",
    "                    chunk = chunks[idx]\n",
    "                    weighted_score = float(score) * weight\n",
    "                    all_results.append((chunk[\"text\"], chunk, weighted_score))\n",
    "\n",
    "        # Sort by weighted score and take top k\n",
    "        all_results.sort(key=lambda x: x[2], reverse=True)\n",
    "        final_results = all_results[:k]\n",
    "\n",
    "        search_time = (time.time() - start_time) * 1000\n",
    "        print(f\"Multi-domain search completed in {search_time:.1f}ms\")\n",
    "\n",
    "        return final_results\n",
    "\n",
    "    def search_single_domain(\n",
    "        self, query: str, domain: str, k: int = 5\n",
    "    ) -> List[Tuple[str, Dict, float]]:\n",
    "        \"\"\"Search within a specific domain\"\"\"\n",
    "        if domain not in self.indices:\n",
    "            raise ValueError(f\"Domain '{domain}' not available\")\n",
    "\n",
    "        query_emb = self.model.encode([query], normalize_embeddings=True).astype(\n",
    "            \"float32\"\n",
    "        )\n",
    "\n",
    "        index = self.indices[domain]\n",
    "        chunks = self.chunks[domain]\n",
    "\n",
    "        D, I = index.search(query_emb, k)\n",
    "\n",
    "        results = []\n",
    "        for idx, score in zip(I[0], D[0]):\n",
    "            if idx >= 0:\n",
    "                chunk = chunks[idx]\n",
    "                results.append((chunk[\"text\"], chunk, float(score)))\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# Initialize multi-domain manager\n",
    "manager = MultiDomainIndexManager()\n",
    "\n",
    "print(f\"‚úì Loaded {len(manager.indices)} domain indices\")\n",
    "print(\"Available domains:\", list(manager.indices.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1c4477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Multi-Domain Retrieval Comparison\n",
    "def test_retrieval_comparison():\n",
    "    \"\"\"Compare single vs multi-domain retrieval performance\"\"\"\n",
    "\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"query\": \"What is RAG in machine learning?\",\n",
    "            \"expected_domain\": \"tech\",\n",
    "            \"description\": \"Technical AI query\",\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"How to assess student learning outcomes?\",\n",
    "            \"expected_domain\": \"edu\",\n",
    "            \"description\": \"Educational assessment query\",\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"What are intellectual property rights?\",\n",
    "            \"expected_domain\": \"legal\",\n",
    "            \"description\": \"Legal concepts query\",\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"How does renewable energy help environment?\",\n",
    "            \"expected_domain\": \"general\",\n",
    "            \"description\": \"General science query\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    print(\"Retrieval Comparison Results:\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        query = test_case[\"query\"]\n",
    "        expected = test_case[\"expected_domain\"]\n",
    "        desc = test_case[\"description\"]\n",
    "\n",
    "        print(f\"\\n{i}. {desc}\")\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"Expected domain: {expected}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Multi-domain search\n",
    "        start = time.time()\n",
    "        multi_results = manager.search_multi_domain(query, k=3)\n",
    "        multi_time = (time.time() - start) * 1000\n",
    "\n",
    "        print(f\"Multi-domain results ({multi_time:.1f}ms):\")\n",
    "        for j, (text, meta, score) in enumerate(multi_results, 1):\n",
    "            domain = meta.get(\"domain\", \"unknown\")\n",
    "            print(f\"  {j}. [{domain}] (score: {score:.3f})\")\n",
    "            print(f\"     {text[:80]}...\")\n",
    "\n",
    "        # Single expected domain search\n",
    "        if expected in manager.indices:\n",
    "            start = time.time()\n",
    "            single_results = manager.search_single_domain(query, expected, k=3)\n",
    "            single_time = (time.time() - start) * 1000\n",
    "\n",
    "            print(f\"\\nSingle-domain '{expected}' results ({single_time:.1f}ms):\")\n",
    "            for j, (text, meta, score) in enumerate(single_results, 1):\n",
    "                print(f\"  {j}. (score: {score:.3f})\")\n",
    "                print(f\"     {text[:80]}...\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 40)\n",
    "\n",
    "\n",
    "# Run comparison test\n",
    "test_retrieval_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcad3eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Smoke Test - End-to-End Multi-Domain RAG\n",
    "def smoke_test_multidomain_rag():\n",
    "    \"\"\"Quick smoke test for multi-domain RAG system\"\"\"\n",
    "\n",
    "    print(\"üî• Multi-Domain RAG Smoke Test\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    test_queries = [\n",
    "        \"Explain transformer architecture\",\n",
    "        \"Best practices for classroom assessment\",\n",
    "        \"What is due process in law?\",\n",
    "        \"Impact of climate change\",\n",
    "    ]\n",
    "\n",
    "    success_count = 0\n",
    "\n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        try:\n",
    "            print(f\"\\n{i}. Testing: {query}\")\n",
    "\n",
    "            # Multi-domain retrieval\n",
    "            results = manager.search_multi_domain(query, k=2)\n",
    "\n",
    "            if results:\n",
    "                print(f\"   ‚úì Found {len(results)} results\")\n",
    "                best_result = results[0]\n",
    "                domain = best_result[1].get(\"domain\", \"unknown\")\n",
    "                score = best_result[2]\n",
    "                print(f\"   ‚úì Best match from '{domain}' domain (score: {score:.3f})\")\n",
    "                print(f\"   ‚úì Content: {best_result[0][:60]}...\")\n",
    "                success_count += 1\n",
    "            else:\n",
    "                print(f\"   ‚úó No results found\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚úó Error: {e}\")\n",
    "\n",
    "    print(f\"\\nüéØ Smoke Test Results: {success_count}/{len(test_queries)} passed\")\n",
    "\n",
    "    if success_count == len(test_queries):\n",
    "        print(\"‚úÖ All tests passed! Multi-domain RAG system is working correctly.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Some tests failed. Check the implementation.\")\n",
    "\n",
    "    return success_count == len(test_queries)\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_result = smoke_test_multidomain_rag()\n",
    "\n",
    "# Performance summary\n",
    "print(f\"\\nüìä Multi-Domain Index Summary:\")\n",
    "print(f\"Total domains: {len(manager.indices)}\")\n",
    "print(f\"Total vectors: {sum(idx.ntotal for idx in manager.indices.values())}\")\n",
    "print(f\"Router strategy: {manager.config['routing_strategy']}\")\n",
    "print(f\"Semantic weight: {manager.config['semantic_weight']}\")\n",
    "print(f\"Max domains per query: {manager.config['max_domains']}\")\n",
    "\n",
    "# What we built\n",
    "print(f\"\\nüîß What We Built:\")\n",
    "print(f\"‚úì Multi-domain vector indices (tech/edu/legal/general)\")\n",
    "print(f\"‚úì Smart query router (semantic + rule-based + hybrid)\")\n",
    "print(f\"‚úì Index manager with fallback strategies\")\n",
    "print(f\"‚úì Performance comparison tools\")\n",
    "print(f\"‚úì End-to-end multi-domain RAG pipeline\")\n",
    "\n",
    "print(f\"\\nüìù Key Parameters (Low-VRAM options):\")\n",
    "print(f\"‚úì Model: BAAI/bge-m3 (supports both Chinese/English)\")\n",
    "print(f\"‚úì Chunk size: 400 tokens (domain-optimized)\")\n",
    "print(f\"‚úì Batch size: 16 (memory efficient)\")\n",
    "print(f\"‚úì Normalize embeddings: True (for IP similarity)\")\n",
    "print(f\"‚úì Index type: FlatIP (simple, no training needed)\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Pitfalls to Avoid:\")\n",
    "print(f\"‚Ä¢ Domain routing accuracy depends on good descriptions\")\n",
    "print(f\"‚Ä¢ Need sufficient domain-specific training data\")\n",
    "print(f\"‚Ä¢ Balance semantic vs rule-based weights carefully\")\n",
    "print(f\"‚Ä¢ Monitor cross-domain contamination in results\")\n",
    "print(f\"‚Ä¢ Consider index maintenance for incremental updates\")\n",
    "\n",
    "print(f\"\\nüéØ When to Use Multi-Domain Indices:\")\n",
    "print(f\"‚Ä¢ Large knowledge bases with distinct topics\")\n",
    "print(f\"‚Ä¢ Need domain-specific relevance optimization\")\n",
    "print(f\"‚Ä¢ Want to control/audit which domains are searched\")\n",
    "print(f\"‚Ä¢ Have computational constraints (search subset)\")\n",
    "print(f\"‚Ä¢ Building specialized expert systems\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
