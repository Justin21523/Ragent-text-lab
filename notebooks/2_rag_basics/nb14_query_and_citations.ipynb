{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e62b65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb14_query_and_citations.ipynb\n",
    "# RAG 查詢與引用系統實作\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (複製到每本 notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af69524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 2: Dependencies & Setup ==========\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Core ML libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import faiss\n",
    "import tiktoken\n",
    "\n",
    "# Text processing\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2153501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 3: Load Previous Modules & Data ==========\n",
    "# Sample Chinese text data (simulating processed documents)\n",
    "SAMPLE_DOCS = [\n",
    "    {\n",
    "        \"text\": \"檢索增強生成（RAG）是一種結合檢索和生成的方法，它可以從大型文檔庫中找到相關信息，然後基於這些信息生成準確的回答。RAG 技術在問答系統中表現出色。\",\n",
    "        \"meta\": {\"source_id\": \"doc_001\", \"title\": \"RAG技術概述\", \"page\": 1},\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"向量嵌入是將文本轉換為數值向量的技術。bge-m3 是一個優秀的中文嵌入模型，支持多語言和多功能。它在語義相似度任務上表現優異。\",\n",
    "        \"meta\": {\"source_id\": \"doc_002\", \"title\": \"嵌入模型介紹\", \"page\": 1},\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"FAISS（Facebook AI Similarity Search）是一個高效的向量相似度搜索庫。它支持大規模向量檢索，並提供多種索引類型，如 IndexFlatIP 和 IndexIVF。\",\n",
    "        \"meta\": {\"source_id\": \"doc_003\", \"title\": \"FAISS索引技術\", \"page\": 2},\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"中文文本分段需要考慮標點符號和語義完整性。常用的分段策略包括按段落、按句號等標點符號，以及基於語義的智能分段。\",\n",
    "        \"meta\": {\"source_id\": \"doc_004\", \"title\": \"中文分段策略\", \"page\": 1},\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"提示詞工程是優化 LLM 輸出的重要技術。好的提示詞應該清晰、具體，並包含適當的上下文信息。在 RAG 系統中，提示詞需要整合檢索到的相關文檔。\",\n",
    "        \"meta\": {\"source_id\": \"doc_005\", \"title\": \"提示詞工程\", \"page\": 3},\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(SAMPLE_DOCS)} sample documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68044949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 4: Initialize Embedding Model & Build Index ==========\n",
    "# Initialize embedding model (low-VRAM friendly)\n",
    "print(\"Loading bge-m3 embedding model...\")\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-m3\", device=\"auto\")\n",
    "\n",
    "# Extract texts and encode\n",
    "texts = [doc[\"text\"] for doc in SAMPLE_DOCS]\n",
    "print(f\"Encoding {len(texts)} documents...\")\n",
    "\n",
    "# Encode with normalization for cosine similarity\n",
    "embeddings = embedding_model.encode(\n",
    "    texts,\n",
    "    normalize_embeddings=True,  # Important for cosine similarity\n",
    "    batch_size=8,\n",
    "    show_progress_bar=True,\n",
    ").astype(\"float32\")\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Build FAISS index (Inner Product for normalized vectors = cosine similarity)\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner Product index\n",
    "index.add(embeddings)\n",
    "\n",
    "print(f\"FAISS index built with {index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dec532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 5: RAG Citation System Core ==========\n",
    "@dataclass\n",
    "class RetrievalResult:\n",
    "    \"\"\"Single retrieval result with citation info\"\"\"\n",
    "\n",
    "    text: str\n",
    "    score: float\n",
    "    source_id: str\n",
    "    title: str\n",
    "    page: int\n",
    "    citation_id: int\n",
    "\n",
    "\n",
    "class RAGCitationSystem:\n",
    "    \"\"\"RAG system with built-in citation tracking\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, embedding_model, index, documents, tokenizer_name=\"gpt-3.5-turbo\"\n",
    "    ):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.index = index\n",
    "        self.documents = documents\n",
    "        self.tokenizer = tiktoken.encoding_for_model(tokenizer_name)\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5) -> List[RetrievalResult]:\n",
    "        \"\"\"Retrieve relevant documents with citation tracking\"\"\"\n",
    "        # Encode query\n",
    "        query_embedding = self.embedding_model.encode(\n",
    "            [query], normalize_embeddings=True\n",
    "        ).astype(\"float32\")\n",
    "\n",
    "        # Search in FAISS index\n",
    "        scores, indices = self.index.search(query_embedding, top_k)\n",
    "\n",
    "        # Build results with citation IDs\n",
    "        results = []\n",
    "        for i, (score, doc_idx) in enumerate(zip(scores[0], indices[0])):\n",
    "            if doc_idx < len(self.documents):  # Valid index\n",
    "                doc = self.documents[doc_idx]\n",
    "                result = RetrievalResult(\n",
    "                    text=doc[\"text\"],\n",
    "                    score=float(score),\n",
    "                    source_id=doc[\"meta\"][\"source_id\"],\n",
    "                    title=doc[\"meta\"][\"title\"],\n",
    "                    page=doc[\"meta\"][\"page\"],\n",
    "                    citation_id=i + 1,  # 1-based citation numbering\n",
    "                )\n",
    "                results.append(result)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def build_context(\n",
    "        self, results: List[RetrievalResult], max_tokens: int = 2000\n",
    "    ) -> Tuple[str, str]:\n",
    "        \"\"\"Build context string with token budget control\"\"\"\n",
    "        context_parts = []\n",
    "        citations = []\n",
    "        current_tokens = 0\n",
    "\n",
    "        for result in results:\n",
    "            # Format: [citation_id] text\n",
    "            chunk_text = f\"[{result.citation_id}] {result.text}\"\n",
    "            chunk_tokens = len(self.tokenizer.encode(chunk_text))\n",
    "\n",
    "            if current_tokens + chunk_tokens > max_tokens:\n",
    "                break\n",
    "\n",
    "            context_parts.append(chunk_text)\n",
    "            citations.append(\n",
    "                f\"[{result.citation_id}] {result.source_id} - {result.title} (p.{result.page})\"\n",
    "            )\n",
    "            current_tokens += chunk_tokens\n",
    "\n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        citation_list = \"\\n\".join(citations)\n",
    "\n",
    "        return context, citation_list\n",
    "\n",
    "    def format_prompt(self, query: str, context: str) -> str:\n",
    "        \"\"\"Format RAG prompt with citation requirements\"\"\"\n",
    "        prompt = f\"\"\"你是一個專業的問答助手。請根據提供的上下文信息回答問題。\n",
    "\n",
    "重要要求：\n",
    "1. 回答必須基於提供的上下文信息\n",
    "2. 在回答中使用引用標註，格式為 [1], [2] 等\n",
    "3. 如果上下文中沒有相關信息，請明確說明\n",
    "4. 回答要準確、簡潔、有條理\n",
    "\n",
    "上下文信息：\n",
    "{context}\n",
    "\n",
    "問題：{query}\n",
    "\n",
    "請提供詳細回答（包含適當的引用標註）：\"\"\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "\n",
    "# Initialize RAG system\n",
    "rag_system = RAGCitationSystem(\n",
    "    embedding_model=embedding_model, index=index, documents=SAMPLE_DOCS\n",
    ")\n",
    "\n",
    "print(\"RAG Citation System initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e5561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 6: LLM Integration for Complete RAG ==========\n",
    "# Initialize a small LLM for demonstration (adjust based on available VRAM)\n",
    "MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"  # Can switch to smaller models if needed\n",
    "\n",
    "print(f\"Loading LLM: {MODEL_ID}\")\n",
    "print(\"Note: This may take a few minutes on first run...\")\n",
    "\n",
    "try:\n",
    "    # Load with memory optimization\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        load_in_8bit=True,  # 8-bit quantization for VRAM efficiency\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    print(f\"Model loaded successfully on device: {model.device}\")\n",
    "    llm_available = True\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"LLM loading failed: {e}\")\n",
    "    print(\"Continuing with retrieval-only demo...\")\n",
    "    llm_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e69ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 7: Complete RAG Function ==========\n",
    "def complete_rag_query(query: str, top_k: int = 3, max_tokens: int = 256) -> Dict:\n",
    "    \"\"\"Complete RAG pipeline: retrieve → generate → format citations\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    retrieval_results = rag_system.retrieve(query, top_k)\n",
    "\n",
    "    if not retrieval_results:\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"answer\": \"抱歉，我在知識庫中找不到相關信息來回答您的問題。\",\n",
    "            \"citations\": [],\n",
    "            \"retrieval_time\": time.time() - start_time,\n",
    "            \"generation_time\": 0,\n",
    "        }\n",
    "\n",
    "    # Step 2: Build context with token budget\n",
    "    context, citation_list = rag_system.build_context(\n",
    "        retrieval_results, max_tokens=1500\n",
    "    )\n",
    "\n",
    "    # Step 3: Format prompt\n",
    "    prompt = rag_system.format_prompt(query, context)\n",
    "\n",
    "    generation_start = time.time()\n",
    "\n",
    "    # Step 4: Generate answer (if LLM available)\n",
    "    if llm_available:\n",
    "        try:\n",
    "            # Tokenize input\n",
    "            inputs = tokenizer(\n",
    "                prompt, return_tensors=\"pt\", truncation=True, max_length=2048\n",
    "            )\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "            # Generate with controlled parameters\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_tokens,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    repetition_penalty=1.1,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "\n",
    "            # Decode response\n",
    "            full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            answer = full_response[len(prompt) :].strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            answer = f\"生成過程中出現錯誤：{str(e)}\"\n",
    "    else:\n",
    "        # Fallback: return formatted context\n",
    "        answer = f\"基於檢索到的信息：\\n\\n{context}\\n\\n（注意：由於 LLM 未載入，這裡顯示的是原始檢索結果）\"\n",
    "\n",
    "    generation_time = time.time() - generation_start\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"answer\": answer,\n",
    "        \"citations\": citation_list.split(\"\\n\"),\n",
    "        \"retrieval_results\": len(retrieval_results),\n",
    "        \"retrieval_time\": generation_start - start_time,\n",
    "        \"generation_time\": generation_time,\n",
    "        \"total_time\": total_time,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Complete RAG function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff17f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 8: Smoke Test - Chinese Q&A with Citations ==========\n",
    "print(\"=== RAG 問答系統測試 ===\\n\")\n",
    "\n",
    "# Test questions\n",
    "test_queries = [\n",
    "    \"什麼是 RAG 技術？它有什麼優點？\",\n",
    "    \"bge-m3 模型有什麼特點？\",\n",
    "    \"FAISS 支援哪些索引類型？\",\n",
    "    \"中文文本應該如何分段？\",\n",
    "]\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"問題 {i}：{query}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    result = complete_rag_query(query, top_k=3, max_tokens=200)\n",
    "\n",
    "    print(f\"回答：\\n{result['answer']}\\n\")\n",
    "\n",
    "    print(\"引用來源：\")\n",
    "    for citation in result[\"citations\"]:\n",
    "        print(f\"  {citation}\")\n",
    "\n",
    "    print(f\"\\n效能指標：\")\n",
    "    print(f\"  檢索到 {result['retrieval_results']} 個相關文檔\")\n",
    "    print(f\"  檢索時間：{result['retrieval_time']:.3f}s\")\n",
    "    print(f\"  生成時間：{result['generation_time']:.3f}s\")\n",
    "    print(f\"  總時間：{result['total_time']:.3f}s\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e1571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 9: Citation Quality Analysis ==========\n",
    "def analyze_citation_quality(query: str, answer: str, citations: List[str]) -> Dict:\n",
    "    \"\"\"Simple citation quality analysis\"\"\"\n",
    "\n",
    "    # Count citation markers in answer\n",
    "    citation_markers = []\n",
    "    import re\n",
    "\n",
    "    markers = re.findall(r\"\\[(\\d+)\\]\", answer)\n",
    "    citation_markers = list(set(int(m) for m in markers))\n",
    "\n",
    "    # Check coverage\n",
    "    available_citations = len(citations)\n",
    "    used_citations = len(citation_markers)\n",
    "\n",
    "    return {\n",
    "        \"total_citations_available\": available_citations,\n",
    "        \"citations_used_in_answer\": used_citations,\n",
    "        \"citation_markers_found\": citation_markers,\n",
    "        \"citation_coverage\": used_citations / max(1, available_citations),\n",
    "        \"proper_format\": len([m for m in markers if m.isdigit()]) == len(markers),\n",
    "    }\n",
    "\n",
    "\n",
    "# Test citation quality\n",
    "test_query = \"RAG 技術的核心組件有哪些？\"\n",
    "result = complete_rag_query(test_query, top_k=4)\n",
    "\n",
    "print(\"=== 引用品質分析 ===\")\n",
    "print(f\"測試問題：{test_query}\\n\")\n",
    "\n",
    "quality = analyze_citation_quality(test_query, result[\"answer\"], result[\"citations\"])\n",
    "\n",
    "print(\"引用使用分析：\")\n",
    "for key, value in quality.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\n回答內容：\\n{result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355dd572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 10: Token Budget Control Demo ==========\n",
    "def test_token_budget_control():\n",
    "    \"\"\"Demonstrate context length management\"\"\"\n",
    "    print(\"=== Token 預算控制測試 ===\\n\")\n",
    "\n",
    "    long_query = \"請詳細說明 RAG、FAISS、嵌入模型和提示詞工程的相關技術\"\n",
    "\n",
    "    # Test different budget limits\n",
    "    budgets = [500, 1000, 2000]\n",
    "\n",
    "    for budget in budgets:\n",
    "        print(f\"Token 預算限制：{budget}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        # Retrieve with budget limit\n",
    "        results = rag_system.retrieve(long_query, top_k=5)\n",
    "        context, citations = rag_system.build_context(results, max_tokens=budget)\n",
    "\n",
    "        # Count actual tokens\n",
    "        actual_tokens = len(rag_system.tokenizer.encode(context))\n",
    "\n",
    "        print(f\"實際使用 tokens：{actual_tokens}\")\n",
    "        print(f\"包含文檔數量：{len(citations.split(chr(10)))}\")\n",
    "        print(f\"Context 預覽：{context[:100]}...\")\n",
    "        print()\n",
    "\n",
    "\n",
    "test_token_budget_control()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e92143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 11: Extensions & Next Steps ==========\n",
    "print(\"=== 本筆記本完成 ===\")\n",
    "print(\"\\nWhat we built:\")\n",
    "print(\"✓ 完整的 RAG 檢索管道（查詢→嵌入→FAISS檢索→排序）\")\n",
    "print(\"✓ 引用追蹤系統（[1], [2] 格式，含來源信息）\")\n",
    "print(\"✓ Token 預算控制（避免超出 context window）\")\n",
    "print(\"✓ 中文問答測試與引用品質分析\")\n",
    "print(\"✓ 效能監控（檢索/生成時間分離）\")\n",
    "\n",
    "print(\"\\nKey concepts:\")\n",
    "print(\"• Citation tracking: 每個檢索結果分配唯一編號\")\n",
    "print(\"• Context assembly: 根據 token 預算組合上下文\")\n",
    "print(\"• Prompt engineering: 明確要求模型使用引用\")\n",
    "print(\"• Quality analysis: 檢查引用使用率與格式正確性\")\n",
    "\n",
    "print(\"\\nPitfalls to avoid:\")\n",
    "print(\"• 引用編號錯位：去重/重排後要重新編號\")\n",
    "print(\"• Token 估算不準：不同 tokenizer 差異大\")\n",
    "print(\"• 過度檢索：top_k 太大導致 context 稀釋\")\n",
    "print(\"• 缺少回退：檢索失敗時要有適當回應\")\n",
    "\n",
    "print(\"\\nNext steps (nb15):\")\n",
    "print(\"• 加入重排器（bge-reranker）提升檢索精度\")\n",
    "print(\"• 實現 MMR（最大邊際相關性）去重\")\n",
    "print(\"• 測試 Hybrid 檢索（關鍵詞+向量）\")\n",
    "print(\"• 建立評估指標（Recall@k, NDCG）\")\n",
    "\n",
    "print(\"\\nRepro tips:\")\n",
    "print(\"• 調整 MODEL_ID 和 load_in_8bit 參數適應硬體\")\n",
    "print(\"• 增加更多樣本文檔測試不同領域\")\n",
    "print(\"• 修改 max_tokens 參數平衡速度與品質\")\n",
    "print(\"• 使用 temperature=0.3 獲得更穩定的引用格式\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371da0a8",
   "metadata": {},
   "source": [
    "# RAG 查詢與引用實作：nb14_query_and_citations.ipynb\n",
    "\n",
    "## Goals（目標）\n",
    "\n",
    "- 整合前期模組（chunking + embeddings + FAISS index）建立完整 RAG 檢索流程\n",
    "- 實現**引用標註**系統：查詢→檢索→組合 context→生成答案＋引用\n",
    "- 測試中文問答準確性與引用格式（`[1]`, `[2]` 格式）\n",
    "- 驗證 token budget 控制與 context window 管理\n",
    "- 準備重排器（reranker）的接入點\n",
    "\n",
    "## Notebook Outline（筆記本大綱）\n",
    "\n",
    "1. **Shared Cache Bootstrap**（標準第一格）\n",
    "2. **載入前期模組**：chunking, embeddings, FAISS index\n",
    "3. **RAG Pipeline 核心**：檢索函數 + context 組裝\n",
    "4. **Citation System**：引用編號與來源追蹤\n",
    "5. **Query Template**：提示詞模板（含引用要求）\n",
    "6. **Complete RAG Function**：端到端問答\n",
    "7. **Smoke Test**：中文問答＋引用驗證\n",
    "8. **Token Budget 控制**：context 長度管理\n",
    "9. **Pitfalls & Extensions**## Core Code Blocks（核心程式碼）\n",
    "\n",
    "\n",
    "## Key Parameters（關鍵參數）\n",
    "\n",
    "| 參數 | 預設值 | 低 VRAM 選項 | 說明 |\n",
    "|------|--------|--------------|------|\n",
    "| `MODEL_ID` | Qwen2.5-7B-Instruct | Qwen2.5-1.5B-Instruct | LLM 模型選擇 |\n",
    "| `load_in_8bit` | True | True | 8bit 量化節省 VRAM |\n",
    "| `top_k` | 3-5 | 3 | 檢索文檔數量 |\n",
    "| `max_tokens` | 256 | 128 | 生成長度控制 |\n",
    "| `context_budget` | 1500 | 1000 | Context token 預算 |\n",
    "\n",
    "\n",
    "## When to Use This（使用時機）\n",
    "\n",
    "- 需要**可驗證答案**的問答系統（學術、法律、醫療）\n",
    "- 要求**透明度**與**可追溯性**的 AI 應用\n",
    "- **多文檔**知識庫檢索與摘要\n",
    "- 在**有限 VRAM**環境中部署 RAG 系統\n",
    "- 準備整合**重排器**前的基礎管道測試\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f42751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RAG Citation System 核心類別\n",
    "@dataclass\n",
    "class RetrievalResult:\n",
    "    text: str\n",
    "    score: float\n",
    "    source_id: str\n",
    "    title: str\n",
    "    page: int\n",
    "    citation_id: int\n",
    "\n",
    "class RAGCitationSystem:\n",
    "    def retrieve(self, query: str, top_k: int = 5) -> List[RetrievalResult]:\n",
    "        # 查詢嵌入 + FAISS 檢索 + 引用編號分配\n",
    "\n",
    "    def build_context(self, results: List[RetrievalResult], max_tokens: int = 2000):\n",
    "        # Token 預算控制 + Context 組裝\n",
    "\n",
    "    def format_prompt(self, query: str, context: str) -> str:\n",
    "        # 包含引用要求的提示詞模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72904a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 完整 RAG 查詢流程\n",
    "def complete_rag_query(query: str, top_k: int = 3, max_tokens: int = 256):\n",
    "    # 檢索 → 組合 → 生成 → 引用格式化\n",
    "    retrieval_results = rag_system.retrieve(query, top_k)\n",
    "    context, citation_list = rag_system.build_context(retrieval_results)\n",
    "    prompt = rag_system.format_prompt(query, context)\n",
    "    # LLM 生成 + 時間統計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a63d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Smoke Test Cell（煙霧測試）\n",
    "\n",
    "# 測試中文問答 + 引用\n",
    "test_query = \"什麼是 RAG 技術？\"\n",
    "result = complete_rag_query(test_query, top_k=3)\n",
    "print(f\"答案：{result['answer']}\")\n",
    "print(f\"引用：{result['citations']}\")\n",
    "assert \"[1]\" in result['answer'] or \"檢索失敗\" in result['answer']\n",
    "print(\"✓ RAG 查詢與引用系統測試通過\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
