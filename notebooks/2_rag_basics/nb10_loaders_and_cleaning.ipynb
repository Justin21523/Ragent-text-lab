{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6aea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 - 中文 RAG 基礎\n",
    "# nb10_loaders_and_cleaning.ipynb\n",
    "# Goals: PDF/MD/HTML 讀取、trafilatura 抽取、繁簡轉換、文字正規化\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (複製到每本 notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f16501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 2: Import Dependencies\n",
    "# ============================================================================\n",
    "\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Core text processing\n",
    "try:\n",
    "    from trafilatura import extract, bare_extraction\n",
    "\n",
    "    print(\"✓ trafilatura available\")\n",
    "except ImportError:\n",
    "    print(\"⚠ Installing trafilatura...\")\n",
    "    os.system(\"pip install trafilatura>=1.6.0\")\n",
    "    from trafilatura import extract, bare_extraction\n",
    "\n",
    "try:\n",
    "    from opencc import OpenCC\n",
    "\n",
    "    print(\"✓ opencc available\")\n",
    "except ImportError:\n",
    "    print(\"⚠ Installing opencc...\")\n",
    "    os.system(\"pip install opencc>=1.1.0\")\n",
    "    from opencc import OpenCC\n",
    "\n",
    "try:\n",
    "    from rapidfuzz import fuzz\n",
    "\n",
    "    print(\"✓ rapidfuzz available\")\n",
    "except ImportError:\n",
    "    print(\"⚠ Installing rapidfuzz...\")\n",
    "    os.system(\"pip install rapidfuzz>=2.13.0\")\n",
    "    from rapidfuzz import fuzz\n",
    "\n",
    "# File handling\n",
    "try:\n",
    "    import PyPDF2\n",
    "\n",
    "    print(\"✓ PyPDF2 available\")\n",
    "except ImportError:\n",
    "    print(\"⚠ Installing PyPDF2...\")\n",
    "    os.system(\"pip install PyPDF2>=3.0.0\")\n",
    "    import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bb607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 3: Data Structures\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DocumentMeta:\n",
    "    \"\"\"Document metadata for source tracking\"\"\"\n",
    "\n",
    "    source_id: str\n",
    "    uri: Optional[str] = None\n",
    "    title: Optional[str] = None\n",
    "    page: Optional[int] = None\n",
    "    section: Optional[str] = None\n",
    "    lang: Optional[str] = None\n",
    "    content_type: str = \"text\"\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return {k: v for k, v in self.__dict__.items() if v is not None}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CleanDocument:\n",
    "    \"\"\"Cleaned document with metadata\"\"\"\n",
    "\n",
    "    text: str\n",
    "    meta: DocumentMeta\n",
    "    word_count: int = 0\n",
    "    char_count: int = 0\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.word_count = len(self.text.split())\n",
    "        self.char_count = len(self.text)\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            \"text\": self.text,\n",
    "            \"meta\": self.meta.to_dict(),\n",
    "            \"word_count\": self.word_count,\n",
    "            \"char_count\": self.char_count,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d497b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 4: Text Normalizer\n",
    "# ============================================================================\n",
    "\n",
    "class ChineseTextNormalizer:\n",
    "    \"\"\"Chinese text normalization utilities\"\"\"\n",
    "\n",
    "    def __init__(self, t2s: bool = False, s2t: bool = False):\n",
    "        self.converter = None\n",
    "        if t2s:\n",
    "            self.converter = OpenCC('t2s')  # Traditional to Simplified\n",
    "            print(\"Initialized T2S converter\")\n",
    "        elif s2t:\n",
    "            self.converter = OpenCC('s2t')  # Simplified to Traditional\n",
    "            print(\"Initialized S2T converter\")\n",
    "\n",
    "    def normalize_whitespace(self, text: str) -> str:\n",
    "        \"\"\"Normalize whitespace and line breaks\"\"\"\n",
    "        # Replace multiple spaces with single space\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        # Replace multiple newlines with double newline\n",
    "        text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "        # Clean up mixed whitespace\n",
    "        text = re.sub(r'[ \\t]+\\n', '\\n', text)\n",
    "        text = re.sub(r'\\n[ \\t]+', '\\n', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def normalize_punctuation(self, text: str) -> str:\n",
    "        \"\"\"Normalize Chinese punctuation marks\"\"\"\n",
    "        # Full/half-width punctuation mapping\n",
    "        punct_map = {\n",
    "            '，': '，', '。': '。', '！': '！', '？': '？',\n",
    "            '；': '；', '：': '：', '\"': '\"', '\"': '\"',\n",
    "            ''': ''', ''': ''', '（': '（', '）': '）',\n",
    "            '【': '【', '】': '】', '《': '《', '》': '》'\n",
    "        }\n",
    "\n",
    "        for half, full in punct_map.items():\n",
    "            text = text.replace(half, full)\n",
    "\n",
    "        # Remove duplicate punctuation\n",
    "        text = re.sub(r'([。！？；]){2,}', r'\\1', text)\n",
    "        text = re.sub(r'([，：]){2,}', r'\\1', text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def full_normalize(self, text: str) -> str:\n",
    "        \"\"\"Apply all normalization steps\"\"\"\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return \"\"\n",
    "\n",
    "        # Basic cleanup\n",
    "        text = self.normalize_whitespace(text)\n",
    "        text = self.normalize_punctuation(text)\n",
    "\n",
    "        # Traditional/Simplified conversion\n",
    "        if self.converter:\n",
    "            text = self.converter.convert(text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1379561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 5: Document Loaders\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class DocumentLoader:\n",
    "    \"\"\"Multi-format document loader with Chinese support\"\"\"\n",
    "\n",
    "    def __init__(self, normalizer: Optional[ChineseTextNormalizer] = None):\n",
    "        self.normalizer = normalizer or ChineseTextNormalizer()\n",
    "\n",
    "    def load_html(\n",
    "        self, content: str, uri: str = \"\", source_id: str = \"\"\n",
    "    ) -> Optional[CleanDocument]:\n",
    "        \"\"\"Extract text from HTML using trafilatura\"\"\"\n",
    "        try:\n",
    "            # Basic extraction\n",
    "            text = extract(content, include_links=False, include_images=False)\n",
    "            if not text:\n",
    "                # Fallback to bare extraction\n",
    "                result = bare_extraction(content)\n",
    "                text = result.get(\"text\", \"\") if result else \"\"\n",
    "\n",
    "            if not text or len(text.strip()) < 10:\n",
    "                return None\n",
    "\n",
    "            # Normalize text\n",
    "            clean_text = self.normalizer.full_normalize(text)\n",
    "\n",
    "            # Extract metadata\n",
    "            meta_result = bare_extraction(content, include_links=False)\n",
    "            title = meta_result.get(\"title\", \"\") if meta_result else \"\"\n",
    "\n",
    "            meta = DocumentMeta(\n",
    "                source_id=source_id\n",
    "                or f\"html_{hashlib.md5(content[:100].encode()).hexdigest()[:8]}\",\n",
    "                uri=uri,\n",
    "                title=title,\n",
    "                content_type=\"html\",\n",
    "            )\n",
    "\n",
    "            return CleanDocument(text=clean_text, meta=meta)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"HTML extraction failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    def load_pdf(self, file_path: str, source_id: str = \"\") -> List[CleanDocument]:\n",
    "        \"\"\"Extract text from PDF page by page\"\"\"\n",
    "        documents = []\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                reader = PyPDF2.PdfReader(f)\n",
    "\n",
    "                for page_num, page in enumerate(reader.pages):\n",
    "                    text = page.extract_text()\n",
    "                    if not text or len(text.strip()) < 10:\n",
    "                        continue\n",
    "\n",
    "                    clean_text = self.normalizer.full_normalize(text)\n",
    "\n",
    "                    meta = DocumentMeta(\n",
    "                        source_id=source_id or f\"pdf_{Path(file_path).stem}\",\n",
    "                        uri=f\"file://{file_path}\",\n",
    "                        page=page_num + 1,\n",
    "                        content_type=\"pdf\",\n",
    "                    )\n",
    "\n",
    "                    documents.append(CleanDocument(text=clean_text, meta=meta))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"PDF extraction failed: {e}\")\n",
    "\n",
    "        return documents\n",
    "\n",
    "    def load_markdown(\n",
    "        self, content: str, source_id: str = \"\", uri: str = \"\"\n",
    "    ) -> CleanDocument:\n",
    "        \"\"\"Load and clean Markdown content\"\"\"\n",
    "        # Basic markdown cleanup (remove common markdown syntax)\n",
    "        text = re.sub(r\"^#{1,6}\\s+\", \"\", content, flags=re.MULTILINE)  # Headers\n",
    "        text = re.sub(r\"\\*\\*(.*?)\\*\\*\", r\"\\1\", text)  # Bold\n",
    "        text = re.sub(r\"\\*(.*?)\\*\", r\"\\1\", text)  # Italic\n",
    "        text = re.sub(r\"`(.*?)`\", r\"\\1\", text)  # Inline code\n",
    "        text = re.sub(r\"```.*?```\", \"\", text, flags=re.DOTALL)  # Code blocks\n",
    "        text = re.sub(r\"\\[([^\\]]+)\\]\\([^)]+\\)\", r\"\\1\", text)  # Links\n",
    "\n",
    "        clean_text = self.normalizer.full_normalize(text)\n",
    "\n",
    "        meta = DocumentMeta(\n",
    "            source_id=source_id\n",
    "            or f\"md_{hashlib.md5(content[:100].encode()).hexdigest()[:8]}\",\n",
    "            uri=uri,\n",
    "            content_type=\"markdown\",\n",
    "        )\n",
    "\n",
    "        return CleanDocument(text=clean_text, meta=meta)\n",
    "\n",
    "    def load_text(\n",
    "        self, content: str, source_id: str = \"\", uri: str = \"\"\n",
    "    ) -> CleanDocument:\n",
    "        \"\"\"Load plain text content\"\"\"\n",
    "        clean_text = self.normalizer.full_normalize(content)\n",
    "\n",
    "        meta = DocumentMeta(\n",
    "            source_id=source_id\n",
    "            or f\"txt_{hashlib.md5(content[:100].encode()).hexdigest()[:8]}\",\n",
    "            uri=uri,\n",
    "            content_type=\"text\",\n",
    "        )\n",
    "\n",
    "        return CleanDocument(text=clean_text, meta=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c7df04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 6: Deduplication\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class DocumentDeduplicator:\n",
    "    \"\"\"Remove duplicate or highly similar documents\"\"\"\n",
    "\n",
    "    def __init__(self, similarity_threshold: float = 0.95):\n",
    "        self.threshold = similarity_threshold\n",
    "\n",
    "    def get_text_hash(self, text: str) -> str:\n",
    "        \"\"\"Generate hash for exact duplicate detection\"\"\"\n",
    "        # Normalize for hashing (remove all whitespace differences)\n",
    "        normalized = re.sub(r\"\\s+\", \" \", text.lower().strip())\n",
    "        return hashlib.md5(normalized.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    def calculate_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate text similarity using rapidfuzz\"\"\"\n",
    "        if len(text1) < 10 or len(text2) < 10:\n",
    "            return 0.0\n",
    "\n",
    "        # Use token_sort_ratio for better Chinese text comparison\n",
    "        return fuzz.token_sort_ratio(text1, text2) / 100.0\n",
    "\n",
    "    def deduplicate(\n",
    "        self, documents: List[CleanDocument]\n",
    "    ) -> Tuple[List[CleanDocument], int]:\n",
    "        \"\"\"Remove duplicates and highly similar documents\"\"\"\n",
    "        if not documents:\n",
    "            return [], 0\n",
    "\n",
    "        unique_docs = []\n",
    "        seen_hashes = set()\n",
    "        removed_count = 0\n",
    "\n",
    "        for doc in documents:\n",
    "            # Check exact duplicates first\n",
    "            text_hash = self.get_text_hash(doc.text)\n",
    "            if text_hash in seen_hashes:\n",
    "                removed_count += 1\n",
    "                continue\n",
    "\n",
    "            # Check similarity with existing docs\n",
    "            is_duplicate = False\n",
    "            for existing_doc in unique_docs:\n",
    "                similarity = self.calculate_similarity(doc.text, existing_doc.text)\n",
    "                if similarity >= self.threshold:\n",
    "                    print(f\"Similar doc found: {similarity:.3f} similarity\")\n",
    "                    is_duplicate = True\n",
    "                    removed_count += 1\n",
    "                    break\n",
    "\n",
    "            if not is_duplicate:\n",
    "                unique_docs.append(doc)\n",
    "                seen_hashes.add(text_hash)\n",
    "\n",
    "        return unique_docs, removed_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecc9c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 7: Document Pipeline\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class DocumentPipeline:\n",
    "    \"\"\"Complete document processing pipeline\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        convert_to_simplified: bool = False,\n",
    "        convert_to_traditional: bool = False,\n",
    "        dedup_threshold: float = 0.95,\n",
    "    ):\n",
    "\n",
    "        self.normalizer = ChineseTextNormalizer(\n",
    "            t2s=convert_to_simplified, s2t=convert_to_traditional\n",
    "        )\n",
    "        self.loader = DocumentLoader(self.normalizer)\n",
    "        self.deduplicator = DocumentDeduplicator(dedup_threshold)\n",
    "\n",
    "    def process_files(self, file_paths: List[str]) -> List[CleanDocument]:\n",
    "        \"\"\"Process multiple files and return clean documents\"\"\"\n",
    "        all_docs = []\n",
    "\n",
    "        for file_path in file_paths:\n",
    "            path_obj = Path(file_path)\n",
    "            if not path_obj.exists():\n",
    "                print(f\"File not found: {file_path}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing: {file_path}\")\n",
    "\n",
    "            try:\n",
    "                if path_obj.suffix.lower() == \".pdf\":\n",
    "                    docs = self.loader.load_pdf(file_path, source_id=path_obj.stem)\n",
    "                    all_docs.extend(docs)\n",
    "\n",
    "                elif path_obj.suffix.lower() in [\".html\", \".htm\"]:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        content = f.read()\n",
    "                    doc = self.loader.load_html(\n",
    "                        content, uri=f\"file://{file_path}\", source_id=path_obj.stem\n",
    "                    )\n",
    "                    if doc:\n",
    "                        all_docs.append(doc)\n",
    "\n",
    "                elif path_obj.suffix.lower() in [\".md\", \".markdown\"]:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        content = f.read()\n",
    "                    doc = self.loader.load_markdown(\n",
    "                        content, source_id=path_obj.stem, uri=f\"file://{file_path}\"\n",
    "                    )\n",
    "                    all_docs.append(doc)\n",
    "\n",
    "                elif path_obj.suffix.lower() == \".txt\":\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        content = f.read()\n",
    "                    doc = self.loader.load_text(\n",
    "                        content, source_id=path_obj.stem, uri=f\"file://{file_path}\"\n",
    "                    )\n",
    "                    all_docs.append(doc)\n",
    "\n",
    "                else:\n",
    "                    print(f\"Unsupported file type: {path_obj.suffix}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "        # Deduplicate\n",
    "        clean_docs, removed = self.deduplicator.deduplicate(all_docs)\n",
    "        print(f\"Processed {len(all_docs)} documents, removed {removed} duplicates\")\n",
    "\n",
    "        return clean_docs\n",
    "\n",
    "    def save_documents(self, documents: List[CleanDocument], output_path: str):\n",
    "        \"\"\"Save documents to JSONL format\"\"\"\n",
    "        output_dir = Path(output_path).parent\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for doc in documents:\n",
    "                f.write(json.dumps(doc.to_dict(), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        print(f\"Saved {len(documents)} documents to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274ce598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 8: Smoke Test\n",
    "# ============================================================================\n",
    "\n",
    "# Create test data directory\n",
    "test_data_dir = Path(\"data/test_samples\")\n",
    "test_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Sample HTML content (Chinese)\n",
    "sample_html = \"\"\"\n",
    "<html>\n",
    "<head><title>人工智慧簡介</title></head>\n",
    "<body>\n",
    "<h1>什麼是人工智慧？</h1>\n",
    "<p>人工智慧（Artificial Intelligence，簡稱AI）是電腦科學的一個分支，致力於創造能夠執行通常需要人類智慧的任務的機器。</p>\n",
    "<p>AI的應用領域包括：</p>\n",
    "<ul>\n",
    "<li>機器學習（Machine Learning）</li>\n",
    "<li>自然語言處理（Natural Language Processing）</li>\n",
    "<li>電腦視覺（Computer Vision）</li>\n",
    "</ul>\n",
    "<p>隨著深度學習技術的發展，AI在許多領域都取得了突破性的進展。</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Sample Markdown content (Traditional Chinese)\n",
    "sample_markdown = \"\"\"\n",
    "# RAG 系統架構\n",
    "\n",
    "## 什麼是 RAG？\n",
    "檢索增強生成（Retrieval-Augmented Generation，RAG）是一種結合資訊檢索與文本生成的AI技術。\n",
    "\n",
    "## 核心組件\n",
    "1. **文檔載入器** - 處理各種格式的文檔\n",
    "2. **向量化** - 將文本轉換為向量表示\n",
    "3. **檢索器** - 根據查詢找到相關文檔\n",
    "4. **生成器** - 基於檢索結果生成回答\n",
    "\n",
    "## 優勢\n",
    "- 提供有據可依的回答\n",
    "- 可以處理最新資訊\n",
    "- 減少模型幻覺問題\n",
    "\"\"\"\n",
    "\n",
    "# Save test files\n",
    "(test_data_dir / \"sample.html\").write_text(sample_html, encoding=\"utf-8\")\n",
    "(test_data_dir / \"sample.md\").write_text(sample_markdown, encoding=\"utf-8\")\n",
    "\n",
    "print(\"✓ Test data created\")\n",
    "\n",
    "# Initialize pipeline (convert Traditional to Simplified for demo)\n",
    "pipeline = DocumentPipeline(convert_to_simplified=True, dedup_threshold=0.95)\n",
    "\n",
    "# Process test files\n",
    "test_files = [str(test_data_dir / \"sample.html\"), str(test_data_dir / \"sample.md\")]\n",
    "\n",
    "documents = pipeline.process_files(test_files)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n📄 Processed {len(documents)} documents:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"\\nDoc {i+1}:\")\n",
    "    print(f\"  Source: {doc.meta.source_id}\")\n",
    "    print(f\"  Type: {doc.meta.content_type}\")\n",
    "    print(f\"  Chars: {doc.char_count}\")\n",
    "    print(\n",
    "        f\"  Preview: {doc.text[:100]}...\"\n",
    "        if len(doc.text) > 100\n",
    "        else f\"  Text: {doc.text}\"\n",
    "    )\n",
    "\n",
    "# Save to JSONL\n",
    "output_dir = Path(\"outs/rag\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "pipeline.save_documents(documents, \"outs/rag/clean_docs.jsonl\")\n",
    "\n",
    "print(\"\\n✅ Smoke test completed successfully!\")\n",
    "print(\"📁 Output saved to: outs/rag/clean_docs.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5644a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 9: Key Parameters & Configuration\n",
    "# ============================================================================\n",
    "\n",
    "print(\"🔧 Key Parameters & Configuration:\")\n",
    "print(\n",
    "    \"\"\"\n",
    "DocumentPipeline 主要參數：\n",
    "├── convert_to_simplified: bool = False    # T2S 繁簡轉換\n",
    "├── convert_to_traditional: bool = False   # S2T 繁簡轉換\n",
    "├── dedup_threshold: float = 0.95          # 去重相似度門檻\n",
    "\n",
    "ChineseTextNormalizer 功能：\n",
    "├── normalize_whitespace()                 # 空白字符正規化\n",
    "├── normalize_punctuation()               # 中文標點正規化\n",
    "└── full_normalize()                      # 完整正規化流程\n",
    "\n",
    "支援格式：\n",
    "├── .pdf    → 分頁處理，保留頁碼資訊\n",
    "├── .html   → trafilatura 抽取，去除標籤\n",
    "├── .md     → Markdown 語法清理\n",
    "└── .txt    → 純文字正規化\n",
    "\n",
    "DocumentMeta 追蹤欄位：\n",
    "├── source_id: 來源識別ID\n",
    "├── uri: 檔案或網址路徑\n",
    "├── page: 頁碼（PDF適用）\n",
    "├── content_type: 文檔類型\n",
    "└── title: 標題（如果有）\n",
    "\n",
    "Low-VRAM 建議：\n",
    "- 大檔案分批處理（batch_size < 100）\n",
    "- 啟用 incremental processing\n",
    "- 限制 dedup_threshold 降低記憶體使用\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0f88ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 10: When to Use This & Next Steps\n",
    "# ============================================================================\n",
    "\n",
    "print(\"🎯 When to Use This:\")\n",
    "print(\n",
    "    \"\"\"\n",
    "適用場景：\n",
    "✅ 中文文檔RAG系統的資料預處理階段\n",
    "✅ 多格式文檔統一清理與正規化\n",
    "✅ 需要繁簡轉換的跨地區文本處理\n",
    "✅ 建立乾淨、可追蹤來源的文檔語料庫\n",
    "\n",
    "不適用：\n",
    "❌ 需要保留格式資訊的場景（如表格結構）\n",
    "❌ 即時線上文檔處理（批次處理設計）\n",
    "❌ 非文字內容提取（圖片、影音）\n",
    "\n",
    "Next Steps (nb11-nb19)：\n",
    "→ nb11: 中文分段策略（chunk_size/overlap 調優）\n",
    "→ nb12: bge-m3 嵌入向量化\n",
    "→ nb13: FAISS 索引建構與儲存\n",
    "→ nb14: 查詢與引用系統\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"\\n🔍 Pitfalls & Tips:\")\n",
    "print(\n",
    "    \"\"\"\n",
    "常見陷阱：\n",
    "1. encoding 問題 → 統一使用 utf-8\n",
    "2. 大檔案 OOM → 分批載入，控制記憶體使用\n",
    "3. trafilatura 失效 → 提供 bare_extraction 備用方案\n",
    "4. 過度去重 → threshold 過低會誤刪有用內容\n",
    "\n",
    "Reproducibility Tips：\n",
    "- 固定 dedup_threshold 和轉換設定\n",
    "- 保存 pipeline 配置到 configs/rag.yaml\n",
    "- 記錄處理統計資訊（文檔數、去重數、字符數）\n",
    "\"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
