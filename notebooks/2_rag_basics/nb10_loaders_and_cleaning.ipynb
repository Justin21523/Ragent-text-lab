{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6aea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 - ä¸­æ–‡ RAG åŸºç¤\n",
    "# nb10_loaders_and_cleaning.ipynb\n",
    "# Goals: PDF/MD/HTML è®€å–ã€trafilatura æŠ½å–ã€ç¹ç°¡è½‰æ›ã€æ–‡å­—æ­£è¦åŒ–\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (è¤‡è£½åˆ°æ¯æœ¬ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f16501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 2: Import Dependencies\n",
    "# ============================================================================\n",
    "\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Core text processing\n",
    "try:\n",
    "    from trafilatura import extract, bare_extraction\n",
    "\n",
    "    print(\"âœ“ trafilatura available\")\n",
    "except ImportError:\n",
    "    print(\"âš  Installing trafilatura...\")\n",
    "    os.system(\"pip install trafilatura>=1.6.0\")\n",
    "    from trafilatura import extract, bare_extraction\n",
    "\n",
    "try:\n",
    "    from opencc import OpenCC\n",
    "\n",
    "    print(\"âœ“ opencc available\")\n",
    "except ImportError:\n",
    "    print(\"âš  Installing opencc...\")\n",
    "    os.system(\"pip install opencc>=1.1.0\")\n",
    "    from opencc import OpenCC\n",
    "\n",
    "try:\n",
    "    from rapidfuzz import fuzz\n",
    "\n",
    "    print(\"âœ“ rapidfuzz available\")\n",
    "except ImportError:\n",
    "    print(\"âš  Installing rapidfuzz...\")\n",
    "    os.system(\"pip install rapidfuzz>=2.13.0\")\n",
    "    from rapidfuzz import fuzz\n",
    "\n",
    "# File handling\n",
    "try:\n",
    "    import PyPDF2\n",
    "\n",
    "    print(\"âœ“ PyPDF2 available\")\n",
    "except ImportError:\n",
    "    print(\"âš  Installing PyPDF2...\")\n",
    "    os.system(\"pip install PyPDF2>=3.0.0\")\n",
    "    import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bb607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 3: Data Structures\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DocumentMeta:\n",
    "    \"\"\"Document metadata for source tracking\"\"\"\n",
    "\n",
    "    source_id: str\n",
    "    uri: Optional[str] = None\n",
    "    title: Optional[str] = None\n",
    "    page: Optional[int] = None\n",
    "    section: Optional[str] = None\n",
    "    lang: Optional[str] = None\n",
    "    content_type: str = \"text\"\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return {k: v for k, v in self.__dict__.items() if v is not None}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CleanDocument:\n",
    "    \"\"\"Cleaned document with metadata\"\"\"\n",
    "\n",
    "    text: str\n",
    "    meta: DocumentMeta\n",
    "    word_count: int = 0\n",
    "    char_count: int = 0\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.word_count = len(self.text.split())\n",
    "        self.char_count = len(self.text)\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            \"text\": self.text,\n",
    "            \"meta\": self.meta.to_dict(),\n",
    "            \"word_count\": self.word_count,\n",
    "            \"char_count\": self.char_count,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d497b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 4: Text Normalizer\n",
    "# ============================================================================\n",
    "\n",
    "class ChineseTextNormalizer:\n",
    "    \"\"\"Chinese text normalization utilities\"\"\"\n",
    "\n",
    "    def __init__(self, t2s: bool = False, s2t: bool = False):\n",
    "        self.converter = None\n",
    "        if t2s:\n",
    "            self.converter = OpenCC('t2s')  # Traditional to Simplified\n",
    "            print(\"Initialized T2S converter\")\n",
    "        elif s2t:\n",
    "            self.converter = OpenCC('s2t')  # Simplified to Traditional\n",
    "            print(\"Initialized S2T converter\")\n",
    "\n",
    "    def normalize_whitespace(self, text: str) -> str:\n",
    "        \"\"\"Normalize whitespace and line breaks\"\"\"\n",
    "        # Replace multiple spaces with single space\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        # Replace multiple newlines with double newline\n",
    "        text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "        # Clean up mixed whitespace\n",
    "        text = re.sub(r'[ \\t]+\\n', '\\n', text)\n",
    "        text = re.sub(r'\\n[ \\t]+', '\\n', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def normalize_punctuation(self, text: str) -> str:\n",
    "        \"\"\"Normalize Chinese punctuation marks\"\"\"\n",
    "        # Full/half-width punctuation mapping\n",
    "        punct_map = {\n",
    "            'ï¼Œ': 'ï¼Œ', 'ã€‚': 'ã€‚', 'ï¼': 'ï¼', 'ï¼Ÿ': 'ï¼Ÿ',\n",
    "            'ï¼›': 'ï¼›', 'ï¼š': 'ï¼š', '\"': '\"', '\"': '\"',\n",
    "            ''': ''', ''': ''', 'ï¼ˆ': 'ï¼ˆ', 'ï¼‰': 'ï¼‰',\n",
    "            'ã€': 'ã€', 'ã€‘': 'ã€‘', 'ã€Š': 'ã€Š', 'ã€‹': 'ã€‹'\n",
    "        }\n",
    "\n",
    "        for half, full in punct_map.items():\n",
    "            text = text.replace(half, full)\n",
    "\n",
    "        # Remove duplicate punctuation\n",
    "        text = re.sub(r'([ã€‚ï¼ï¼Ÿï¼›]){2,}', r'\\1', text)\n",
    "        text = re.sub(r'([ï¼Œï¼š]){2,}', r'\\1', text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def full_normalize(self, text: str) -> str:\n",
    "        \"\"\"Apply all normalization steps\"\"\"\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return \"\"\n",
    "\n",
    "        # Basic cleanup\n",
    "        text = self.normalize_whitespace(text)\n",
    "        text = self.normalize_punctuation(text)\n",
    "\n",
    "        # Traditional/Simplified conversion\n",
    "        if self.converter:\n",
    "            text = self.converter.convert(text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1379561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 5: Document Loaders\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class DocumentLoader:\n",
    "    \"\"\"Multi-format document loader with Chinese support\"\"\"\n",
    "\n",
    "    def __init__(self, normalizer: Optional[ChineseTextNormalizer] = None):\n",
    "        self.normalizer = normalizer or ChineseTextNormalizer()\n",
    "\n",
    "    def load_html(\n",
    "        self, content: str, uri: str = \"\", source_id: str = \"\"\n",
    "    ) -> Optional[CleanDocument]:\n",
    "        \"\"\"Extract text from HTML using trafilatura\"\"\"\n",
    "        try:\n",
    "            # Basic extraction\n",
    "            text = extract(content, include_links=False, include_images=False)\n",
    "            if not text:\n",
    "                # Fallback to bare extraction\n",
    "                result = bare_extraction(content)\n",
    "                text = result.get(\"text\", \"\") if result else \"\"\n",
    "\n",
    "            if not text or len(text.strip()) < 10:\n",
    "                return None\n",
    "\n",
    "            # Normalize text\n",
    "            clean_text = self.normalizer.full_normalize(text)\n",
    "\n",
    "            # Extract metadata\n",
    "            meta_result = bare_extraction(content, include_links=False)\n",
    "            title = meta_result.get(\"title\", \"\") if meta_result else \"\"\n",
    "\n",
    "            meta = DocumentMeta(\n",
    "                source_id=source_id\n",
    "                or f\"html_{hashlib.md5(content[:100].encode()).hexdigest()[:8]}\",\n",
    "                uri=uri,\n",
    "                title=title,\n",
    "                content_type=\"html\",\n",
    "            )\n",
    "\n",
    "            return CleanDocument(text=clean_text, meta=meta)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"HTML extraction failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    def load_pdf(self, file_path: str, source_id: str = \"\") -> List[CleanDocument]:\n",
    "        \"\"\"Extract text from PDF page by page\"\"\"\n",
    "        documents = []\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                reader = PyPDF2.PdfReader(f)\n",
    "\n",
    "                for page_num, page in enumerate(reader.pages):\n",
    "                    text = page.extract_text()\n",
    "                    if not text or len(text.strip()) < 10:\n",
    "                        continue\n",
    "\n",
    "                    clean_text = self.normalizer.full_normalize(text)\n",
    "\n",
    "                    meta = DocumentMeta(\n",
    "                        source_id=source_id or f\"pdf_{Path(file_path).stem}\",\n",
    "                        uri=f\"file://{file_path}\",\n",
    "                        page=page_num + 1,\n",
    "                        content_type=\"pdf\",\n",
    "                    )\n",
    "\n",
    "                    documents.append(CleanDocument(text=clean_text, meta=meta))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"PDF extraction failed: {e}\")\n",
    "\n",
    "        return documents\n",
    "\n",
    "    def load_markdown(\n",
    "        self, content: str, source_id: str = \"\", uri: str = \"\"\n",
    "    ) -> CleanDocument:\n",
    "        \"\"\"Load and clean Markdown content\"\"\"\n",
    "        # Basic markdown cleanup (remove common markdown syntax)\n",
    "        text = re.sub(r\"^#{1,6}\\s+\", \"\", content, flags=re.MULTILINE)  # Headers\n",
    "        text = re.sub(r\"\\*\\*(.*?)\\*\\*\", r\"\\1\", text)  # Bold\n",
    "        text = re.sub(r\"\\*(.*?)\\*\", r\"\\1\", text)  # Italic\n",
    "        text = re.sub(r\"`(.*?)`\", r\"\\1\", text)  # Inline code\n",
    "        text = re.sub(r\"```.*?```\", \"\", text, flags=re.DOTALL)  # Code blocks\n",
    "        text = re.sub(r\"\\[([^\\]]+)\\]\\([^)]+\\)\", r\"\\1\", text)  # Links\n",
    "\n",
    "        clean_text = self.normalizer.full_normalize(text)\n",
    "\n",
    "        meta = DocumentMeta(\n",
    "            source_id=source_id\n",
    "            or f\"md_{hashlib.md5(content[:100].encode()).hexdigest()[:8]}\",\n",
    "            uri=uri,\n",
    "            content_type=\"markdown\",\n",
    "        )\n",
    "\n",
    "        return CleanDocument(text=clean_text, meta=meta)\n",
    "\n",
    "    def load_text(\n",
    "        self, content: str, source_id: str = \"\", uri: str = \"\"\n",
    "    ) -> CleanDocument:\n",
    "        \"\"\"Load plain text content\"\"\"\n",
    "        clean_text = self.normalizer.full_normalize(content)\n",
    "\n",
    "        meta = DocumentMeta(\n",
    "            source_id=source_id\n",
    "            or f\"txt_{hashlib.md5(content[:100].encode()).hexdigest()[:8]}\",\n",
    "            uri=uri,\n",
    "            content_type=\"text\",\n",
    "        )\n",
    "\n",
    "        return CleanDocument(text=clean_text, meta=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c7df04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 6: Deduplication\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class DocumentDeduplicator:\n",
    "    \"\"\"Remove duplicate or highly similar documents\"\"\"\n",
    "\n",
    "    def __init__(self, similarity_threshold: float = 0.95):\n",
    "        self.threshold = similarity_threshold\n",
    "\n",
    "    def get_text_hash(self, text: str) -> str:\n",
    "        \"\"\"Generate hash for exact duplicate detection\"\"\"\n",
    "        # Normalize for hashing (remove all whitespace differences)\n",
    "        normalized = re.sub(r\"\\s+\", \" \", text.lower().strip())\n",
    "        return hashlib.md5(normalized.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    def calculate_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate text similarity using rapidfuzz\"\"\"\n",
    "        if len(text1) < 10 or len(text2) < 10:\n",
    "            return 0.0\n",
    "\n",
    "        # Use token_sort_ratio for better Chinese text comparison\n",
    "        return fuzz.token_sort_ratio(text1, text2) / 100.0\n",
    "\n",
    "    def deduplicate(\n",
    "        self, documents: List[CleanDocument]\n",
    "    ) -> Tuple[List[CleanDocument], int]:\n",
    "        \"\"\"Remove duplicates and highly similar documents\"\"\"\n",
    "        if not documents:\n",
    "            return [], 0\n",
    "\n",
    "        unique_docs = []\n",
    "        seen_hashes = set()\n",
    "        removed_count = 0\n",
    "\n",
    "        for doc in documents:\n",
    "            # Check exact duplicates first\n",
    "            text_hash = self.get_text_hash(doc.text)\n",
    "            if text_hash in seen_hashes:\n",
    "                removed_count += 1\n",
    "                continue\n",
    "\n",
    "            # Check similarity with existing docs\n",
    "            is_duplicate = False\n",
    "            for existing_doc in unique_docs:\n",
    "                similarity = self.calculate_similarity(doc.text, existing_doc.text)\n",
    "                if similarity >= self.threshold:\n",
    "                    print(f\"Similar doc found: {similarity:.3f} similarity\")\n",
    "                    is_duplicate = True\n",
    "                    removed_count += 1\n",
    "                    break\n",
    "\n",
    "            if not is_duplicate:\n",
    "                unique_docs.append(doc)\n",
    "                seen_hashes.add(text_hash)\n",
    "\n",
    "        return unique_docs, removed_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecc9c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 7: Document Pipeline\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class DocumentPipeline:\n",
    "    \"\"\"Complete document processing pipeline\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        convert_to_simplified: bool = False,\n",
    "        convert_to_traditional: bool = False,\n",
    "        dedup_threshold: float = 0.95,\n",
    "    ):\n",
    "\n",
    "        self.normalizer = ChineseTextNormalizer(\n",
    "            t2s=convert_to_simplified, s2t=convert_to_traditional\n",
    "        )\n",
    "        self.loader = DocumentLoader(self.normalizer)\n",
    "        self.deduplicator = DocumentDeduplicator(dedup_threshold)\n",
    "\n",
    "    def process_files(self, file_paths: List[str]) -> List[CleanDocument]:\n",
    "        \"\"\"Process multiple files and return clean documents\"\"\"\n",
    "        all_docs = []\n",
    "\n",
    "        for file_path in file_paths:\n",
    "            path_obj = Path(file_path)\n",
    "            if not path_obj.exists():\n",
    "                print(f\"File not found: {file_path}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing: {file_path}\")\n",
    "\n",
    "            try:\n",
    "                if path_obj.suffix.lower() == \".pdf\":\n",
    "                    docs = self.loader.load_pdf(file_path, source_id=path_obj.stem)\n",
    "                    all_docs.extend(docs)\n",
    "\n",
    "                elif path_obj.suffix.lower() in [\".html\", \".htm\"]:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        content = f.read()\n",
    "                    doc = self.loader.load_html(\n",
    "                        content, uri=f\"file://{file_path}\", source_id=path_obj.stem\n",
    "                    )\n",
    "                    if doc:\n",
    "                        all_docs.append(doc)\n",
    "\n",
    "                elif path_obj.suffix.lower() in [\".md\", \".markdown\"]:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        content = f.read()\n",
    "                    doc = self.loader.load_markdown(\n",
    "                        content, source_id=path_obj.stem, uri=f\"file://{file_path}\"\n",
    "                    )\n",
    "                    all_docs.append(doc)\n",
    "\n",
    "                elif path_obj.suffix.lower() == \".txt\":\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        content = f.read()\n",
    "                    doc = self.loader.load_text(\n",
    "                        content, source_id=path_obj.stem, uri=f\"file://{file_path}\"\n",
    "                    )\n",
    "                    all_docs.append(doc)\n",
    "\n",
    "                else:\n",
    "                    print(f\"Unsupported file type: {path_obj.suffix}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "        # Deduplicate\n",
    "        clean_docs, removed = self.deduplicator.deduplicate(all_docs)\n",
    "        print(f\"Processed {len(all_docs)} documents, removed {removed} duplicates\")\n",
    "\n",
    "        return clean_docs\n",
    "\n",
    "    def save_documents(self, documents: List[CleanDocument], output_path: str):\n",
    "        \"\"\"Save documents to JSONL format\"\"\"\n",
    "        output_dir = Path(output_path).parent\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for doc in documents:\n",
    "                f.write(json.dumps(doc.to_dict(), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        print(f\"Saved {len(documents)} documents to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274ce598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 8: Smoke Test\n",
    "# ============================================================================\n",
    "\n",
    "# Create test data directory\n",
    "test_data_dir = Path(\"data/test_samples\")\n",
    "test_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Sample HTML content (Chinese)\n",
    "sample_html = \"\"\"\n",
    "<html>\n",
    "<head><title>äººå·¥æ™ºæ…§ç°¡ä»‹</title></head>\n",
    "<body>\n",
    "<h1>ä»€éº¼æ˜¯äººå·¥æ™ºæ…§ï¼Ÿ</h1>\n",
    "<p>äººå·¥æ™ºæ…§ï¼ˆArtificial Intelligenceï¼Œç°¡ç¨±AIï¼‰æ˜¯é›»è…¦ç§‘å­¸çš„ä¸€å€‹åˆ†æ”¯ï¼Œè‡´åŠ›æ–¼å‰µé€ èƒ½å¤ åŸ·è¡Œé€šå¸¸éœ€è¦äººé¡æ™ºæ…§çš„ä»»å‹™çš„æ©Ÿå™¨ã€‚</p>\n",
    "<p>AIçš„æ‡‰ç”¨é ˜åŸŸåŒ…æ‹¬ï¼š</p>\n",
    "<ul>\n",
    "<li>æ©Ÿå™¨å­¸ç¿’ï¼ˆMachine Learningï¼‰</li>\n",
    "<li>è‡ªç„¶èªè¨€è™•ç†ï¼ˆNatural Language Processingï¼‰</li>\n",
    "<li>é›»è…¦è¦–è¦ºï¼ˆComputer Visionï¼‰</li>\n",
    "</ul>\n",
    "<p>éš¨è‘—æ·±åº¦å­¸ç¿’æŠ€è¡“çš„ç™¼å±•ï¼ŒAIåœ¨è¨±å¤šé ˜åŸŸéƒ½å–å¾—äº†çªç ´æ€§çš„é€²å±•ã€‚</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Sample Markdown content (Traditional Chinese)\n",
    "sample_markdown = \"\"\"\n",
    "# RAG ç³»çµ±æ¶æ§‹\n",
    "\n",
    "## ä»€éº¼æ˜¯ RAGï¼Ÿ\n",
    "æª¢ç´¢å¢å¼·ç”Ÿæˆï¼ˆRetrieval-Augmented Generationï¼ŒRAGï¼‰æ˜¯ä¸€ç¨®çµåˆè³‡è¨Šæª¢ç´¢èˆ‡æ–‡æœ¬ç”Ÿæˆçš„AIæŠ€è¡“ã€‚\n",
    "\n",
    "## æ ¸å¿ƒçµ„ä»¶\n",
    "1. **æ–‡æª”è¼‰å…¥å™¨** - è™•ç†å„ç¨®æ ¼å¼çš„æ–‡æª”\n",
    "2. **å‘é‡åŒ–** - å°‡æ–‡æœ¬è½‰æ›ç‚ºå‘é‡è¡¨ç¤º\n",
    "3. **æª¢ç´¢å™¨** - æ ¹æ“šæŸ¥è©¢æ‰¾åˆ°ç›¸é—œæ–‡æª”\n",
    "4. **ç”Ÿæˆå™¨** - åŸºæ–¼æª¢ç´¢çµæœç”Ÿæˆå›ç­”\n",
    "\n",
    "## å„ªå‹¢\n",
    "- æä¾›æœ‰æ“šå¯ä¾çš„å›ç­”\n",
    "- å¯ä»¥è™•ç†æœ€æ–°è³‡è¨Š\n",
    "- æ¸›å°‘æ¨¡å‹å¹»è¦ºå•é¡Œ\n",
    "\"\"\"\n",
    "\n",
    "# Save test files\n",
    "(test_data_dir / \"sample.html\").write_text(sample_html, encoding=\"utf-8\")\n",
    "(test_data_dir / \"sample.md\").write_text(sample_markdown, encoding=\"utf-8\")\n",
    "\n",
    "print(\"âœ“ Test data created\")\n",
    "\n",
    "# Initialize pipeline (convert Traditional to Simplified for demo)\n",
    "pipeline = DocumentPipeline(convert_to_simplified=True, dedup_threshold=0.95)\n",
    "\n",
    "# Process test files\n",
    "test_files = [str(test_data_dir / \"sample.html\"), str(test_data_dir / \"sample.md\")]\n",
    "\n",
    "documents = pipeline.process_files(test_files)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nğŸ“„ Processed {len(documents)} documents:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"\\nDoc {i+1}:\")\n",
    "    print(f\"  Source: {doc.meta.source_id}\")\n",
    "    print(f\"  Type: {doc.meta.content_type}\")\n",
    "    print(f\"  Chars: {doc.char_count}\")\n",
    "    print(\n",
    "        f\"  Preview: {doc.text[:100]}...\"\n",
    "        if len(doc.text) > 100\n",
    "        else f\"  Text: {doc.text}\"\n",
    "    )\n",
    "\n",
    "# Save to JSONL\n",
    "output_dir = Path(\"outs/rag\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "pipeline.save_documents(documents, \"outs/rag/clean_docs.jsonl\")\n",
    "\n",
    "print(\"\\nâœ… Smoke test completed successfully!\")\n",
    "print(\"ğŸ“ Output saved to: outs/rag/clean_docs.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5644a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 9: Key Parameters & Configuration\n",
    "# ============================================================================\n",
    "\n",
    "print(\"ğŸ”§ Key Parameters & Configuration:\")\n",
    "print(\n",
    "    \"\"\"\n",
    "DocumentPipeline ä¸»è¦åƒæ•¸ï¼š\n",
    "â”œâ”€â”€ convert_to_simplified: bool = False    # T2S ç¹ç°¡è½‰æ›\n",
    "â”œâ”€â”€ convert_to_traditional: bool = False   # S2T ç¹ç°¡è½‰æ›\n",
    "â”œâ”€â”€ dedup_threshold: float = 0.95          # å»é‡ç›¸ä¼¼åº¦é–€æª»\n",
    "\n",
    "ChineseTextNormalizer åŠŸèƒ½ï¼š\n",
    "â”œâ”€â”€ normalize_whitespace()                 # ç©ºç™½å­—ç¬¦æ­£è¦åŒ–\n",
    "â”œâ”€â”€ normalize_punctuation()               # ä¸­æ–‡æ¨™é»æ­£è¦åŒ–\n",
    "â””â”€â”€ full_normalize()                      # å®Œæ•´æ­£è¦åŒ–æµç¨‹\n",
    "\n",
    "æ”¯æ´æ ¼å¼ï¼š\n",
    "â”œâ”€â”€ .pdf    â†’ åˆ†é è™•ç†ï¼Œä¿ç•™é ç¢¼è³‡è¨Š\n",
    "â”œâ”€â”€ .html   â†’ trafilatura æŠ½å–ï¼Œå»é™¤æ¨™ç±¤\n",
    "â”œâ”€â”€ .md     â†’ Markdown èªæ³•æ¸…ç†\n",
    "â””â”€â”€ .txt    â†’ ç´”æ–‡å­—æ­£è¦åŒ–\n",
    "\n",
    "DocumentMeta è¿½è¹¤æ¬„ä½ï¼š\n",
    "â”œâ”€â”€ source_id: ä¾†æºè­˜åˆ¥ID\n",
    "â”œâ”€â”€ uri: æª”æ¡ˆæˆ–ç¶²å€è·¯å¾‘\n",
    "â”œâ”€â”€ page: é ç¢¼ï¼ˆPDFé©ç”¨ï¼‰\n",
    "â”œâ”€â”€ content_type: æ–‡æª”é¡å‹\n",
    "â””â”€â”€ title: æ¨™é¡Œï¼ˆå¦‚æœæœ‰ï¼‰\n",
    "\n",
    "Low-VRAM å»ºè­°ï¼š\n",
    "- å¤§æª”æ¡ˆåˆ†æ‰¹è™•ç†ï¼ˆbatch_size < 100ï¼‰\n",
    "- å•Ÿç”¨ incremental processing\n",
    "- é™åˆ¶ dedup_threshold é™ä½è¨˜æ†¶é«”ä½¿ç”¨\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0f88ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 10: When to Use This & Next Steps\n",
    "# ============================================================================\n",
    "\n",
    "print(\"ğŸ¯ When to Use This:\")\n",
    "print(\n",
    "    \"\"\"\n",
    "é©ç”¨å ´æ™¯ï¼š\n",
    "âœ… ä¸­æ–‡æ–‡æª”RAGç³»çµ±çš„è³‡æ–™é è™•ç†éšæ®µ\n",
    "âœ… å¤šæ ¼å¼æ–‡æª”çµ±ä¸€æ¸…ç†èˆ‡æ­£è¦åŒ–\n",
    "âœ… éœ€è¦ç¹ç°¡è½‰æ›çš„è·¨åœ°å€æ–‡æœ¬è™•ç†\n",
    "âœ… å»ºç«‹ä¹¾æ·¨ã€å¯è¿½è¹¤ä¾†æºçš„æ–‡æª”èªæ–™åº«\n",
    "\n",
    "ä¸é©ç”¨ï¼š\n",
    "âŒ éœ€è¦ä¿ç•™æ ¼å¼è³‡è¨Šçš„å ´æ™¯ï¼ˆå¦‚è¡¨æ ¼çµæ§‹ï¼‰\n",
    "âŒ å³æ™‚ç·šä¸Šæ–‡æª”è™•ç†ï¼ˆæ‰¹æ¬¡è™•ç†è¨­è¨ˆï¼‰\n",
    "âŒ éæ–‡å­—å…§å®¹æå–ï¼ˆåœ–ç‰‡ã€å½±éŸ³ï¼‰\n",
    "\n",
    "Next Steps (nb11-nb19)ï¼š\n",
    "â†’ nb11: ä¸­æ–‡åˆ†æ®µç­–ç•¥ï¼ˆchunk_size/overlap èª¿å„ªï¼‰\n",
    "â†’ nb12: bge-m3 åµŒå…¥å‘é‡åŒ–\n",
    "â†’ nb13: FAISS ç´¢å¼•å»ºæ§‹èˆ‡å„²å­˜\n",
    "â†’ nb14: æŸ¥è©¢èˆ‡å¼•ç”¨ç³»çµ±\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ” Pitfalls & Tips:\")\n",
    "print(\n",
    "    \"\"\"\n",
    "å¸¸è¦‹é™·é˜±ï¼š\n",
    "1. encoding å•é¡Œ â†’ çµ±ä¸€ä½¿ç”¨ utf-8\n",
    "2. å¤§æª”æ¡ˆ OOM â†’ åˆ†æ‰¹è¼‰å…¥ï¼Œæ§åˆ¶è¨˜æ†¶é«”ä½¿ç”¨\n",
    "3. trafilatura å¤±æ•ˆ â†’ æä¾› bare_extraction å‚™ç”¨æ–¹æ¡ˆ\n",
    "4. éåº¦å»é‡ â†’ threshold éä½æœƒèª¤åˆªæœ‰ç”¨å…§å®¹\n",
    "\n",
    "Reproducibility Tipsï¼š\n",
    "- å›ºå®š dedup_threshold å’Œè½‰æ›è¨­å®š\n",
    "- ä¿å­˜ pipeline é…ç½®åˆ° configs/rag.yaml\n",
    "- è¨˜éŒ„è™•ç†çµ±è¨ˆè³‡è¨Šï¼ˆæ–‡æª”æ•¸ã€å»é‡æ•¸ã€å­—ç¬¦æ•¸ï¼‰\n",
    "\"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
