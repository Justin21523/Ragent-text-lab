{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0756c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb63_perf_latency_tokens_vram.ipynb\n",
    "# Stage 7: Performance Monitoring & Resource Observability\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (複製到每本 notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d692f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 2: Performance Profiler Class\n",
    "# ============================================================================\n",
    "\n",
    "import time\n",
    "import json\n",
    "import psutil\n",
    "import gc\n",
    "from typing import Dict, Any, Optional, List\n",
    "from dataclasses import dataclass, asdict\n",
    "from contextlib import contextmanager\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PerfMetrics:\n",
    "    \"\"\"Performance metrics container\"\"\"\n",
    "\n",
    "    operation: str\n",
    "    latency_ms: float\n",
    "    tokens_per_sec: float\n",
    "    vram_peak_mb: float\n",
    "    cpu_percent: float\n",
    "    memory_mb: float\n",
    "    input_tokens: int\n",
    "    output_tokens: int\n",
    "    backend: str\n",
    "    model_id: str\n",
    "    quantization: Optional[str] = None\n",
    "    timestamp: float = 0.0\n",
    "\n",
    "\n",
    "class PerfProfiler:\n",
    "    \"\"\"Lightweight performance profiler for LLM operations\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.metrics_history: List[PerfMetrics] = []\n",
    "        self.baseline_path = \"outs/baseline.json\"\n",
    "        pathlib.Path(\"outs\").mkdir(exist_ok=True)\n",
    "\n",
    "    def get_vram_usage(self) -> float:\n",
    "        \"\"\"Get current VRAM usage in MB\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.cuda.memory_allocated() / 1024 / 1024\n",
    "        return 0.0\n",
    "\n",
    "    def get_vram_peak(self) -> float:\n",
    "        \"\"\"Get peak VRAM usage in MB since last reset\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.cuda.max_memory_allocated() / 1024 / 1024\n",
    "        return 0.0\n",
    "\n",
    "    def reset_vram_peak(self):\n",
    "        \"\"\"Reset VRAM peak counter\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    @contextmanager\n",
    "    def profile_operation(\n",
    "        self,\n",
    "        operation: str,\n",
    "        model_id: str,\n",
    "        backend: str,\n",
    "        input_tokens: int = 0,\n",
    "        quantization: str = None,\n",
    "    ):\n",
    "        \"\"\"Context manager for profiling operations\"\"\"\n",
    "        # Setup\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        self.reset_vram_peak()\n",
    "\n",
    "        start_time = time.time()\n",
    "        start_vram = self.get_vram_usage()\n",
    "        process = psutil.Process()\n",
    "        start_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
    "\n",
    "        try:\n",
    "            yield self\n",
    "\n",
    "        finally:\n",
    "            # Measurements\n",
    "            end_time = time.time()\n",
    "            latency_ms = (end_time - start_time) * 1000\n",
    "            peak_vram = self.get_vram_peak()\n",
    "            cpu_percent = process.cpu_percent()\n",
    "            end_memory = process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "            # Create metrics (tokens_per_sec will be updated externally)\n",
    "            metrics = PerfMetrics(\n",
    "                operation=operation,\n",
    "                latency_ms=latency_ms,\n",
    "                tokens_per_sec=0.0,  # To be filled\n",
    "                vram_peak_mb=peak_vram,\n",
    "                cpu_percent=cpu_percent,\n",
    "                memory_mb=end_memory - start_memory,\n",
    "                input_tokens=input_tokens,\n",
    "                output_tokens=0,  # To be filled\n",
    "                backend=backend,\n",
    "                model_id=model_id,\n",
    "                quantization=quantization,\n",
    "                timestamp=end_time,\n",
    "            )\n",
    "\n",
    "            self.current_metrics = metrics\n",
    "\n",
    "    def finalize_metrics(self, output_tokens: int):\n",
    "        \"\"\"Update current metrics with output token count and calculate tokens/sec\"\"\"\n",
    "        if hasattr(self, \"current_metrics\"):\n",
    "            self.current_metrics.output_tokens = output_tokens\n",
    "            if self.current_metrics.latency_ms > 0:\n",
    "                self.current_metrics.tokens_per_sec = (\n",
    "                    output_tokens * 1000\n",
    "                ) / self.current_metrics.latency_ms\n",
    "            self.metrics_history.append(self.current_metrics)\n",
    "\n",
    "    def save_baseline(self):\n",
    "        \"\"\"Save performance baseline to JSON\"\"\"\n",
    "        baseline_data = {\n",
    "            \"timestamp\": time.time(),\n",
    "            \"gpu_info\": {\n",
    "                \"available\": torch.cuda.is_available(),\n",
    "                \"name\": (\n",
    "                    torch.cuda.get_device_name(0)\n",
    "                    if torch.cuda.is_available()\n",
    "                    else \"CPU\"\n",
    "                ),\n",
    "                \"memory_gb\": (\n",
    "                    torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "                    if torch.cuda.is_available()\n",
    "                    else 0\n",
    "                ),\n",
    "            },\n",
    "            \"metrics\": [asdict(m) for m in self.metrics_history],\n",
    "        }\n",
    "\n",
    "        with open(self.baseline_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(baseline_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"✅ Baseline saved to {self.baseline_path}\")\n",
    "        return baseline_data\n",
    "\n",
    "    def print_summary(self):\n",
    "        \"\"\"Print performance summary\"\"\"\n",
    "        if not self.metrics_history:\n",
    "            print(\"❌ No metrics recorded\")\n",
    "            return\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"🚀 PERFORMANCE SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        for backend in set(m.backend for m in self.metrics_history):\n",
    "            backend_metrics = [m for m in self.metrics_history if m.backend == backend]\n",
    "            print(f\"\\n📊 Backend: {backend}\")\n",
    "\n",
    "            for metrics in backend_metrics:\n",
    "                print(f\"  {metrics.operation}:\")\n",
    "                print(f\"    ⏱️  Latency: {metrics.latency_ms:.1f}ms\")\n",
    "                print(f\"    🔥 Tokens/sec: {metrics.tokens_per_sec:.1f}\")\n",
    "                print(f\"    💾 VRAM Peak: {metrics.vram_peak_mb:.1f}MB\")\n",
    "                print(f\"    🎯 Tokens: {metrics.input_tokens}→{metrics.output_tokens}\")\n",
    "                if metrics.quantization:\n",
    "                    print(f\"    ⚡ Quant: {metrics.quantization}\")\n",
    "\n",
    "\n",
    "# Initialize global profiler\n",
    "profiler = PerfProfiler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9fea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 3: LLM Inference Performance Test\n",
    "# ============================================================================\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "class LLMPerfTester:\n",
    "    \"\"\"Test LLM inference performance across different configurations\"\"\"\n",
    "\n",
    "    def __init__(self, profiler: PerfProfiler):\n",
    "        self.profiler = profiler\n",
    "        self.test_prompts = [\n",
    "            \"請解釋什麼是檢索增強生成(RAG)？\",\n",
    "            \"寫一個Python函數來計算斐波那契數列\",\n",
    "            \"分析人工智慧在教育領域的應用前景與挑戰\",\n",
    "        ]\n",
    "\n",
    "    def test_transformers_inference(\n",
    "        self, model_id: str = \"Qwen/Qwen2.5-7B-Instruct\", use_4bit: bool = True\n",
    "    ):\n",
    "        \"\"\"Test Transformers backend performance\"\"\"\n",
    "        print(f\"🧪 Testing Transformers: {model_id}\")\n",
    "\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        # Configure quantization\n",
    "        quant_config = None\n",
    "        quant_str = None\n",
    "        if use_4bit and torch.cuda.is_available():\n",
    "            quant_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "            )\n",
    "            quant_str = \"4bit\"\n",
    "\n",
    "        # Load model\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            quantization_config=quant_config,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        # Test inference\n",
    "        for i, prompt in enumerate(self.test_prompts):\n",
    "            input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "            input_tokens = input_ids.shape[1]\n",
    "\n",
    "            with self.profiler.profile_operation(\n",
    "                f\"transformers_inference_{i+1}\",\n",
    "                model_id,\n",
    "                \"transformers\",\n",
    "                input_tokens,\n",
    "                quant_str,\n",
    "            ):\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        input_ids.to(model.device),\n",
    "                        max_new_tokens=100,\n",
    "                        do_sample=True,\n",
    "                        temperature=0.7,\n",
    "                        pad_token_id=tokenizer.eos_token_id,\n",
    "                    )\n",
    "\n",
    "            output_tokens = outputs.shape[1] - input_tokens\n",
    "            self.profiler.finalize_metrics(output_tokens)\n",
    "\n",
    "            # Print sample output\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            print(f\"  📝 Sample {i+1}: {response[-100:]}...\")\n",
    "\n",
    "        # Cleanup\n",
    "        del model\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        gc.collect()\n",
    "\n",
    "    def test_llamacpp_inference(self, model_path: str = None):\n",
    "        \"\"\"Test llama.cpp backend performance (if available)\"\"\"\n",
    "        try:\n",
    "            from llama_cpp import Llama\n",
    "\n",
    "            if not model_path:\n",
    "                print(\"⏭️  Skipping llama.cpp test (no model path provided)\")\n",
    "                return\n",
    "\n",
    "            print(f\"🧪 Testing llama.cpp: {model_path}\")\n",
    "\n",
    "            # Load model\n",
    "            llm = Llama(\n",
    "                model_path=model_path,\n",
    "                n_ctx=2048,\n",
    "                n_gpu_layers=32 if torch.cuda.is_available() else 0,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            # Test inference\n",
    "            for i, prompt in enumerate(self.test_prompts[:1]):  # Test one prompt\n",
    "                input_tokens = len(prompt.split()) * 1.3  # Rough estimate\n",
    "\n",
    "                with self.profiler.profile_operation(\n",
    "                    f\"llamacpp_inference_{i+1}\",\n",
    "                    model_path,\n",
    "                    \"llama.cpp\",\n",
    "                    int(input_tokens),\n",
    "                    \"GGUF\",\n",
    "                ):\n",
    "                    output = llm(prompt, max_tokens=100, temperature=0.7)\n",
    "\n",
    "                output_tokens = len(output[\"choices\"][0][\"text\"].split()) * 1.3\n",
    "                self.profiler.finalize_metrics(int(output_tokens))\n",
    "\n",
    "                print(f\"  📝 Sample: {output['choices'][0]['text'][:100]}...\")\n",
    "\n",
    "            del llm\n",
    "\n",
    "        except ImportError:\n",
    "            print(\"⏭️  Skipping llama.cpp test (library not installed)\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ llama.cpp test failed: {e}\")\n",
    "\n",
    "\n",
    "# Run LLM performance tests\n",
    "llm_tester = LLMPerfTester(profiler)\n",
    "\n",
    "# Test with quantization\n",
    "llm_tester.test_transformers_inference(use_4bit=True)\n",
    "\n",
    "# Test llama.cpp if available\n",
    "# llm_tester.test_llamacpp_inference(\"/path/to/model.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f94c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 4: RAG Retrieval Performance Test\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "class RAGPerfTester:\n",
    "    \"\"\"Test RAG retrieval performance\"\"\"\n",
    "\n",
    "    def __init__(self, profiler: PerfProfiler):\n",
    "        self.profiler = profiler\n",
    "        self.test_queries = [\n",
    "            \"什麼是機器學習？\",\n",
    "            \"如何實作神經網路？\",\n",
    "            \"深度學習的應用領域有哪些？\",\n",
    "        ]\n",
    "\n",
    "    def setup_test_index(self, n_docs: int = 1000):\n",
    "        \"\"\"Create test index with synthetic documents\"\"\"\n",
    "        print(f\"🔧 Setting up test index with {n_docs} documents...\")\n",
    "\n",
    "        # Generate synthetic documents\n",
    "        doc_templates = [\n",
    "            \"機器學習是人工智慧的重要分支，專注於讓電腦系統自動學習和改進。\",\n",
    "            \"神經網路是模仿人腦神經元結構設計的計算模型，用於解決複雜問題。\",\n",
    "            \"深度學習使用多層神經網路來學習數據的抽象表示和特徵。\",\n",
    "            \"自然語言處理結合語言學和機器學習來理解和生成人類語言。\",\n",
    "            \"電腦視覺使用深度學習技術來識別和理解圖像內容。\",\n",
    "        ]\n",
    "\n",
    "        docs = []\n",
    "        for i in range(n_docs):\n",
    "            base_doc = doc_templates[i % len(doc_templates)]\n",
    "            docs.append(f\"{base_doc} 文檔編號: {i+1}\")\n",
    "\n",
    "        return docs\n",
    "\n",
    "    def test_embedding_performance(self, model_name: str = \"BAAI/bge-m3\"):\n",
    "        \"\"\"Test embedding model performance\"\"\"\n",
    "        print(f\"🧪 Testing Embedding: {model_name}\")\n",
    "\n",
    "        # Load embedding model\n",
    "        embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "        # Setup test documents\n",
    "        docs = self.setup_test_index(500)\n",
    "\n",
    "        # Test document embedding\n",
    "        with self.profiler.profile_operation(\n",
    "            \"embedding_documents\",\n",
    "            model_name,\n",
    "            \"sentence-transformers\",\n",
    "            len(\" \".join(docs).split()),\n",
    "            None,\n",
    "        ):\n",
    "            doc_embeddings = embedding_model.encode(\n",
    "                docs, normalize_embeddings=True, show_progress_bar=False\n",
    "            )\n",
    "\n",
    "        # Estimate tokens processed (rough)\n",
    "        total_tokens = sum(len(doc.split()) for doc in docs) * 1.3\n",
    "        self.profiler.finalize_metrics(int(total_tokens))\n",
    "\n",
    "        # Test query embedding\n",
    "        for i, query in enumerate(self.test_queries):\n",
    "            query_tokens = len(query.split()) * 1.3\n",
    "\n",
    "            with self.profiler.profile_operation(\n",
    "                f\"embedding_query_{i+1}\",\n",
    "                model_name,\n",
    "                \"sentence-transformers\",\n",
    "                int(query_tokens),\n",
    "                None,\n",
    "            ):\n",
    "                query_embedding = embedding_model.encode(\n",
    "                    [query], normalize_embeddings=True\n",
    "                )\n",
    "\n",
    "            self.profiler.finalize_metrics(int(query_tokens))\n",
    "\n",
    "        return doc_embeddings, embedding_model\n",
    "\n",
    "    def test_faiss_performance(self, embeddings: np.ndarray):\n",
    "        \"\"\"Test FAISS index performance\"\"\"\n",
    "        print(f\"🧪 Testing FAISS Index: {embeddings.shape}\")\n",
    "\n",
    "        # Build FAISS index\n",
    "        with self.profiler.profile_operation(\n",
    "            \"faiss_index_build\", f\"IndexFlatIP-{embeddings.shape[0]}\", \"faiss\", 0, None\n",
    "        ):\n",
    "            index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "            index.add(embeddings.astype(\"float32\"))\n",
    "\n",
    "        self.profiler.finalize_metrics(embeddings.shape[0])\n",
    "\n",
    "        # Test search performance\n",
    "        embedding_model = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "\n",
    "        for i, query in enumerate(self.test_queries):\n",
    "            query_embedding = embedding_model.encode([query], normalize_embeddings=True)\n",
    "\n",
    "            with self.profiler.profile_operation(\n",
    "                f\"faiss_search_{i+1}\",\n",
    "                f\"IndexFlatIP-{embeddings.shape[0]}\",\n",
    "                \"faiss\",\n",
    "                1,\n",
    "                None,\n",
    "            ):\n",
    "                scores, indices = index.search(query_embedding.astype(\"float32\"), k=10)\n",
    "\n",
    "            self.profiler.finalize_metrics(10)  # Retrieved 10 docs\n",
    "            print(f\"  🎯 Query {i+1}: Top score = {scores[0][0]:.4f}\")\n",
    "\n",
    "        return index\n",
    "\n",
    "\n",
    "# Run RAG performance tests\n",
    "rag_tester = RAGPerfTester(profiler)\n",
    "\n",
    "# Test embedding performance\n",
    "doc_embeddings, emb_model = rag_tester.test_embedding_performance()\n",
    "\n",
    "# Test FAISS performance\n",
    "faiss_index = rag_tester.test_faiss_performance(doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2fd543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 5: End-to-End RAG Performance Test\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class EndToEndRAGTester:\n",
    "    \"\"\"Test complete RAG pipeline performance\"\"\"\n",
    "\n",
    "    def __init__(self, profiler: PerfProfiler, embedding_model, faiss_index, docs):\n",
    "        self.profiler = profiler\n",
    "        self.embedding_model = embedding_model\n",
    "        self.index = faiss_index\n",
    "        self.docs = docs\n",
    "        self.test_questions = [\"什麼是機器學習的基本概念？\", \"神經網路如何進行訓練？\"]\n",
    "\n",
    "    def rag_retrieve_and_answer(self, query: str, llm_model, tokenizer, k: int = 5):\n",
    "        \"\"\"Complete RAG pipeline: retrieve + generate\"\"\"\n",
    "\n",
    "        # Step 1: Query embedding\n",
    "        with self.profiler.profile_operation(\n",
    "            \"rag_query_embedding\",\n",
    "            \"BAAI/bge-m3\",\n",
    "            \"sentence-transformers\",\n",
    "            len(query.split()) * 1.3,\n",
    "            None,\n",
    "        ):\n",
    "            query_embedding = self.embedding_model.encode(\n",
    "                [query], normalize_embeddings=True\n",
    "            )\n",
    "\n",
    "        self.profiler.finalize_metrics(len(query.split()))\n",
    "\n",
    "        # Step 2: Retrieval\n",
    "        with self.profiler.profile_operation(\n",
    "            \"rag_retrieval\", f\"FAISS-{len(self.docs)}\", \"faiss\", 1, None\n",
    "        ):\n",
    "            scores, indices = self.index.search(query_embedding.astype(\"float32\"), k=k)\n",
    "\n",
    "        self.profiler.finalize_metrics(k)\n",
    "\n",
    "        # Step 3: Context preparation\n",
    "        retrieved_docs = [self.docs[idx] for idx in indices[0]]\n",
    "        context = \"\\n\".join([f\"[{i+1}] {doc}\" for i, doc in enumerate(retrieved_docs)])\n",
    "\n",
    "        prompt = f\"\"\"基於以下資料回答問題：\n",
    "\n",
    "資料：\n",
    "{context}\n",
    "\n",
    "問題：{query}\n",
    "\n",
    "回答：\"\"\"\n",
    "\n",
    "        # Step 4: Generation\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        input_tokens = input_ids.shape[1]\n",
    "\n",
    "        with self.profiler.profile_operation(\n",
    "            \"rag_generation\", \"Qwen2.5-7B\", \"transformers\", input_tokens, \"4bit\"\n",
    "        ):\n",
    "            with torch.no_grad():\n",
    "                outputs = llm_model.generate(\n",
    "                    input_ids.to(llm_model.device),\n",
    "                    max_new_tokens=200,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "\n",
    "        output_tokens = outputs.shape[1] - input_tokens\n",
    "        self.profiler.finalize_metrics(output_tokens)\n",
    "\n",
    "        # Decode response\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        answer = response[len(prompt) :].strip()\n",
    "\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"retrieved_docs\": retrieved_docs,\n",
    "            \"answer\": answer,\n",
    "            \"scores\": scores[0].tolist(),\n",
    "        }\n",
    "\n",
    "    def test_e2e_performance(self):\n",
    "        \"\"\"Test end-to-end RAG performance\"\"\"\n",
    "        print(\"🧪 Testing End-to-End RAG Pipeline\")\n",
    "\n",
    "        # Load LLM for generation\n",
    "        model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        # Load with 4-bit quantization\n",
    "        quant_config = (\n",
    "            BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "            )\n",
    "            if torch.cuda.is_available()\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            quantization_config=quant_config,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        # Test each question\n",
    "        results = []\n",
    "        for i, question in enumerate(self.test_questions):\n",
    "            print(f\"  📋 Question {i+1}: {question}\")\n",
    "\n",
    "            result = self.rag_retrieve_and_answer(question, llm_model, tokenizer)\n",
    "            results.append(result)\n",
    "\n",
    "            print(f\"  ✅ Answer preview: {result['answer'][:100]}...\")\n",
    "            print(f\"  📊 Top retrieval score: {result['scores'][0]:.4f}\")\n",
    "\n",
    "        # Cleanup\n",
    "        del llm_model\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        gc.collect()\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# Create test documents for E2E testing\n",
    "test_docs = rag_tester.setup_test_index(200)\n",
    "\n",
    "# Run end-to-end test\n",
    "e2e_tester = EndToEndRAGTester(profiler, emb_model, faiss_index, test_docs)\n",
    "e2e_results = e2e_tester.test_e2e_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56866672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 6: VRAM Peak Tracking & Memory Optimization\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class VRAMAnalyzer:\n",
    "    \"\"\"Analyze VRAM usage patterns and suggest optimizations\"\"\"\n",
    "\n",
    "    def __init__(self, profiler: PerfProfiler):\n",
    "        self.profiler = profiler\n",
    "\n",
    "    def analyze_vram_usage(self):\n",
    "        \"\"\"Analyze VRAM usage from metrics history\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"⚠️  CUDA not available - skipping VRAM analysis\")\n",
    "            return\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"💾 VRAM USAGE ANALYSIS\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # GPU info\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        total_vram = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"🖥️  GPU: {gpu_name}\")\n",
    "        print(f\"📏 Total VRAM: {total_vram:.1f} GB\")\n",
    "\n",
    "        # Analyze metrics\n",
    "        vram_metrics = [\n",
    "            (m.operation, m.vram_peak_mb) for m in self.profiler.metrics_history\n",
    "        ]\n",
    "\n",
    "        if not vram_metrics:\n",
    "            print(\"❌ No VRAM metrics available\")\n",
    "            return\n",
    "\n",
    "        # Sort by VRAM usage\n",
    "        vram_metrics.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        print(\"\\n📊 VRAM Usage by Operation:\")\n",
    "        for op, vram_mb in vram_metrics[:10]:  # Top 10\n",
    "            vram_gb = vram_mb / 1024\n",
    "            usage_pct = (vram_gb / total_vram) * 100\n",
    "            print(f\"  {op:<30} {vram_mb:>8.1f} MB ({usage_pct:>5.1f}%)\")\n",
    "\n",
    "        # Recommendations\n",
    "        max_vram = max(vram_mb for _, vram_mb in vram_metrics)\n",
    "        max_vram_gb = max_vram / 1024\n",
    "\n",
    "        print(f\"\\n🎯 Peak VRAM Usage: {max_vram:.1f} MB ({max_vram_gb:.2f} GB)\")\n",
    "\n",
    "        if max_vram_gb > total_vram * 0.9:\n",
    "            print(\"⚠️  HIGH VRAM USAGE - Consider optimizations:\")\n",
    "            print(\"   • Use smaller models or more aggressive quantization\")\n",
    "            print(\"   • Reduce batch sizes\")\n",
    "            print(\"   • Enable gradient checkpointing\")\n",
    "            print(\"   • Use model sharding or offloading\")\n",
    "        elif max_vram_gb > total_vram * 0.7:\n",
    "            print(\"⚡ MODERATE VRAM USAGE - Optimizations available:\")\n",
    "            print(\"   • 8-bit quantization could reduce usage\")\n",
    "            print(\"   • Consider larger batch sizes for efficiency\")\n",
    "        else:\n",
    "            print(\"✅ EFFICIENT VRAM USAGE - Well optimized!\")\n",
    "            print(\"   • Consider larger models or batch sizes\")\n",
    "\n",
    "    def memory_optimization_test(self):\n",
    "        \"\"\"Test different memory optimization strategies\"\"\"\n",
    "        print(\"\\n🧪 Testing Memory Optimizations...\")\n",
    "\n",
    "        test_text = \"請詳細解釋深度學習的基本原理和應用場景。\" * 5\n",
    "\n",
    "        # Test different configurations\n",
    "        configs = [\n",
    "            {\"name\": \"Baseline FP16\", \"dtype\": torch.float16, \"quant\": None},\n",
    "            {\"name\": \"4-bit Quantization\", \"dtype\": torch.float16, \"quant\": \"4bit\"},\n",
    "        ]\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            for config in configs:\n",
    "                print(f\"\\n  🔧 Testing: {config['name']}\")\n",
    "\n",
    "                try:\n",
    "                    # Reset VRAM tracking\n",
    "                    torch.cuda.empty_cache()\n",
    "                    self.profiler.reset_vram_peak()\n",
    "\n",
    "                    # Simulate model loading\n",
    "                    if config[\"quant\"] == \"4bit\":\n",
    "                        # Simulate 4-bit loading (reduced VRAM)\n",
    "                        dummy_tensor = torch.randn(\n",
    "                            1000, 1000, dtype=config[\"dtype\"]\n",
    "                        ).cuda()\n",
    "                        dummy_tensor = dummy_tensor * 0.5  # Simulate quantization\n",
    "                    else:\n",
    "                        dummy_tensor = torch.randn(\n",
    "                            2000, 2000, dtype=config[\"dtype\"]\n",
    "                        ).cuda()\n",
    "\n",
    "                    peak_vram = self.profiler.get_vram_peak()\n",
    "                    print(f\"    💾 Peak VRAM: {peak_vram:.1f} MB\")\n",
    "\n",
    "                    # Cleanup\n",
    "                    del dummy_tensor\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"    ❌ Failed: {e}\")\n",
    "\n",
    "\n",
    "# Run VRAM analysis\n",
    "vram_analyzer = VRAMAnalyzer(profiler)\n",
    "vram_analyzer.analyze_vram_usage()\n",
    "vram_analyzer.memory_optimization_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea868d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 7: Performance Report Generation\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def generate_performance_report():\n",
    "    \"\"\"Generate comprehensive performance report\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"📋 GENERATING PERFORMANCE REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Print summary\n",
    "    profiler.print_summary()\n",
    "\n",
    "    # Save baseline\n",
    "    baseline_data = profiler.save_baseline()\n",
    "\n",
    "    # Generate recommendations\n",
    "    print(\"\\n🎯 PERFORMANCE RECOMMENDATIONS:\")\n",
    "\n",
    "    # Analyze latency\n",
    "    latencies = [m.latency_ms for m in profiler.metrics_history]\n",
    "    if latencies:\n",
    "        avg_latency = sum(latencies) / len(latencies)\n",
    "        max_latency = max(latencies)\n",
    "\n",
    "        print(f\"   ⏱️  Average Latency: {avg_latency:.1f}ms\")\n",
    "        print(f\"   ⏱️  Max Latency: {max_latency:.1f}ms\")\n",
    "\n",
    "        if avg_latency > 5000:  # 5 seconds\n",
    "            print(\"   ⚠️  High latency detected - consider:\")\n",
    "            print(\"      • Smaller models or more aggressive quantization\")\n",
    "            print(\"      • GPU acceleration if using CPU\")\n",
    "            print(\"      • Batch processing for multiple requests\")\n",
    "\n",
    "    # Analyze throughput\n",
    "    token_rates = [\n",
    "        m.tokens_per_sec for m in profiler.metrics_history if m.tokens_per_sec > 0\n",
    "    ]\n",
    "    if token_rates:\n",
    "        avg_tokens_sec = sum(token_rates) / len(token_rates)\n",
    "        print(f\"   🔥 Average Tokens/sec: {avg_tokens_sec:.1f}\")\n",
    "\n",
    "        if avg_tokens_sec < 10:\n",
    "            print(\"   ⚠️  Low throughput detected - consider:\")\n",
    "            print(\"      • GPU acceleration or better hardware\")\n",
    "            print(\"      • Model quantization to reduce memory pressure\")\n",
    "            print(\"      • Batch inference for better GPU utilization\")\n",
    "        elif avg_tokens_sec > 50:\n",
    "            print(\"   ✅ Excellent throughput - well optimized!\")\n",
    "\n",
    "    # Hardware utilization\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        total_vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"   🖥️  Hardware: {gpu_name} ({total_vram_gb:.1f}GB)\")\n",
    "\n",
    "    return baseline_data\n",
    "\n",
    "\n",
    "# Generate the report\n",
    "report_data = generate_performance_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3ec7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 8: Smoke Test - Quick Performance Check\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def smoke_test_performance():\n",
    "    \"\"\"Quick smoke test for performance monitoring\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"🔥 SMOKE TEST - Performance Monitoring\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Test basic profiler functionality\n",
    "    test_profiler = PerfProfiler()\n",
    "\n",
    "    # Simulate a quick operation\n",
    "    with test_profiler.profile_operation(\n",
    "        \"smoke_test\", \"test-model\", \"test-backend\", 50, \"test-quant\"\n",
    "    ):\n",
    "        # Simulate some work\n",
    "        time.sleep(0.1)\n",
    "        if torch.cuda.is_available():\n",
    "            dummy = torch.randn(100, 100).cuda()\n",
    "            result = torch.matmul(dummy, dummy)\n",
    "            del dummy, result\n",
    "\n",
    "    test_profiler.finalize_metrics(25)\n",
    "\n",
    "    # Check results\n",
    "    assert len(test_profiler.metrics_history) == 1, \"❌ Metrics not recorded\"\n",
    "\n",
    "    metrics = test_profiler.metrics_history[0]\n",
    "    assert metrics.latency_ms > 50, f\"❌ Unexpected latency: {metrics.latency_ms}ms\"\n",
    "    assert (\n",
    "        metrics.tokens_per_sec > 0\n",
    "    ), f\"❌ Invalid tokens/sec: {metrics.tokens_per_sec}\"\n",
    "    assert metrics.input_tokens == 50, f\"❌ Wrong input tokens: {metrics.input_tokens}\"\n",
    "    assert (\n",
    "        metrics.output_tokens == 25\n",
    "    ), f\"❌ Wrong output tokens: {metrics.output_tokens}\"\n",
    "\n",
    "    print(\"✅ Basic profiler functionality working\")\n",
    "\n",
    "    # Test VRAM tracking\n",
    "    if torch.cuda.is_available():\n",
    "        test_profiler.reset_vram_peak()\n",
    "        dummy = torch.randn(1000, 1000).cuda()\n",
    "        peak = test_profiler.get_vram_peak()\n",
    "        del dummy\n",
    "\n",
    "        assert peak > 0, f\"❌ VRAM tracking failed: {peak}MB\"\n",
    "        print(f\"✅ VRAM tracking working: {peak:.1f}MB peak\")\n",
    "    else:\n",
    "        print(\"⏭️  VRAM tracking skipped (no CUDA)\")\n",
    "\n",
    "    # Test baseline saving\n",
    "    test_baseline_path = \"outs/smoke_baseline.json\"\n",
    "    test_profiler.baseline_path = test_baseline_path\n",
    "    test_profiler.save_baseline()\n",
    "\n",
    "    assert pathlib.Path(test_baseline_path).exists(), \"❌ Baseline file not created\"\n",
    "\n",
    "    with open(test_baseline_path, \"r\") as f:\n",
    "        baseline = json.load(f)\n",
    "\n",
    "    assert \"metrics\" in baseline, \"❌ Invalid baseline format\"\n",
    "    assert len(baseline[\"metrics\"]) == 1, \"❌ Wrong number of metrics in baseline\"\n",
    "\n",
    "    print(\"✅ Baseline saving working\")\n",
    "\n",
    "    # Cleanup\n",
    "    pathlib.Path(test_baseline_path).unlink()\n",
    "\n",
    "    print(\"\\n🎉 All smoke tests passed!\")\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 9: Summary & Optimization Recommendations\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def print_stage_summary():\n",
    "    \"\"\"Print Stage 7 notebook summary and next steps\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"📋 STAGE 7 - NOTEBOOK 63 SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(\"\\n✅ COMPLETED:\")\n",
    "    print(\"   • 效能監控框架 (PerfProfiler) 建立\")\n",
    "    print(\"   • LLM推理效能測試 (Transformers + 量化)\")\n",
    "    print(\"   • RAG檢索效能測試 (Embedding + FAISS)\")\n",
    "    print(\"   • 端到端RAG流程效能測試\")\n",
    "    print(\"   • VRAM使用分析與優化建議\")\n",
    "    print(\"   • 效能基準報表生成 (baseline.json)\")\n",
    "    print(\"   • 延遲、吞吐量、記憶體使用追蹤\")\n",
    "\n",
    "    print(\"\\n🎯 CORE CONCEPTS:\")\n",
    "    print(\"   • Performance Profiling: 使用context manager追蹤延遲與資源\")\n",
    "    print(\"   • VRAM Monitoring: torch.cuda.memory API追蹤峰值使用\")\n",
    "    print(\"   • Tokens/sec Calculation: 輸出token數除以延遲時間\")\n",
    "    print(\"   • Multi-backend Comparison: 對比不同後端效能差異\")\n",
    "    print(\"   • Memory Optimization: 量化、批次大小、模型分片策略\")\n",
    "\n",
    "    print(\"\\n⚠️  PITFALLS:\")\n",
    "    print(\"   • VRAM測量不準確: 需在操作前reset_peak_memory_stats()\")\n",
    "    print(\"   • Token計算估算: 不同tokenizer的token數差異很大\")\n",
    "    print(\"   • 冷啟動效應: 首次推理較慢，需多次測量取平均\")\n",
    "    print(\"   • 記憶體洩漏: 測試後要明確del模型並empty_cache()\")\n",
    "    print(\"   • 並發干擾: 多GPU或其他程序會影響效能測量\")\n",
    "\n",
    "    print(\"\\n🚀 NEXT ACTIONS:\")\n",
    "    print(\"   • nb64: 整合所有評估指標產生綜合報表\")\n",
    "    print(\"   • 建立效能回歸檢測CI\")\n",
    "    print(\"   • 加入更多模型與硬體配置的基準\")\n",
    "    print(\"   • 實作自動效能優化建議系統\")\n",
    "\n",
    "    print(\"\\n📊 CURRENT BASELINE:\")\n",
    "    if pathlib.Path(\"outs/baseline.json\").exists():\n",
    "        with open(\"outs/baseline.json\", \"r\") as f:\n",
    "            baseline = json.load(f)\n",
    "\n",
    "        print(f\"   📅 Generated: {time.ctime(baseline['timestamp'])}\")\n",
    "        print(f\"   🖥️  Hardware: {baseline['gpu_info']['name']}\")\n",
    "        print(f\"   📈 Metrics: {len(baseline['metrics'])} operations recorded\")\n",
    "\n",
    "        # Key stats\n",
    "        latencies = [m[\"latency_ms\"] for m in baseline[\"metrics\"]]\n",
    "        token_rates = [\n",
    "            m[\"tokens_per_sec\"] for m in baseline[\"metrics\"] if m[\"tokens_per_sec\"] > 0\n",
    "        ]\n",
    "\n",
    "        if latencies:\n",
    "            print(f\"   ⏱️  Avg Latency: {sum(latencies)/len(latencies):.1f}ms\")\n",
    "        if token_rates:\n",
    "            print(f\"   🔥 Avg Tokens/sec: {sum(token_rates)/len(token_rates):.1f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "\n",
    "# Print final summary\n",
    "print_stage_summary()\n",
    "\n",
    "# Final cleanup\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n🎉 Notebook 63 completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d253c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在 nb63 最後執行這個 cell 驗證功能\n",
    "def quick_verification():\n",
    "    \"\"\"3分鐘內驗證核心功能\"\"\"\n",
    "    print(\"🔥 Quick Verification (3 min)\")\n",
    "\n",
    "    # 1. 檢查 baseline.json 是否生成\n",
    "    assert pathlib.Path(\"outs/baseline.json\").exists()\n",
    "    print(\"✅ Baseline file created\")\n",
    "\n",
    "    # 2. 檢查是否記錄了效能指標\n",
    "    assert len(profiler.metrics_history) > 0\n",
    "    print(f\"✅ {len(profiler.metrics_history)} metrics recorded\")\n",
    "\n",
    "    # 3. 檢查關鍵指標存在\n",
    "    for m in profiler.metrics_history[:3]:\n",
    "        assert m.latency_ms > 0\n",
    "        assert m.vram_peak_mb >= 0\n",
    "        print(f\"✅ {m.operation}: {m.latency_ms:.1f}ms, {m.vram_peak_mb:.1f}MB\")\n",
    "\n",
    "    print(\"🎉 All verifications passed!\")\n",
    "\n",
    "\n",
    "quick_verification()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
