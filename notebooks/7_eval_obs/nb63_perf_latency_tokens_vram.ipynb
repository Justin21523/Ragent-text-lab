{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0756c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb63_perf_latency_tokens_vram.ipynb\n",
    "# Stage 7: Performance Monitoring & Resource Observability\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (è¤‡è£½åˆ°æ¯æœ¬ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d692f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 2: Performance Profiler Class\n",
    "# ============================================================================\n",
    "\n",
    "import time\n",
    "import json\n",
    "import psutil\n",
    "import gc\n",
    "from typing import Dict, Any, Optional, List\n",
    "from dataclasses import dataclass, asdict\n",
    "from contextlib import contextmanager\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PerfMetrics:\n",
    "    \"\"\"Performance metrics container\"\"\"\n",
    "\n",
    "    operation: str\n",
    "    latency_ms: float\n",
    "    tokens_per_sec: float\n",
    "    vram_peak_mb: float\n",
    "    cpu_percent: float\n",
    "    memory_mb: float\n",
    "    input_tokens: int\n",
    "    output_tokens: int\n",
    "    backend: str\n",
    "    model_id: str\n",
    "    quantization: Optional[str] = None\n",
    "    timestamp: float = 0.0\n",
    "\n",
    "\n",
    "class PerfProfiler:\n",
    "    \"\"\"Lightweight performance profiler for LLM operations\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.metrics_history: List[PerfMetrics] = []\n",
    "        self.baseline_path = \"outs/baseline.json\"\n",
    "        pathlib.Path(\"outs\").mkdir(exist_ok=True)\n",
    "\n",
    "    def get_vram_usage(self) -> float:\n",
    "        \"\"\"Get current VRAM usage in MB\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.cuda.memory_allocated() / 1024 / 1024\n",
    "        return 0.0\n",
    "\n",
    "    def get_vram_peak(self) -> float:\n",
    "        \"\"\"Get peak VRAM usage in MB since last reset\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.cuda.max_memory_allocated() / 1024 / 1024\n",
    "        return 0.0\n",
    "\n",
    "    def reset_vram_peak(self):\n",
    "        \"\"\"Reset VRAM peak counter\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    @contextmanager\n",
    "    def profile_operation(\n",
    "        self,\n",
    "        operation: str,\n",
    "        model_id: str,\n",
    "        backend: str,\n",
    "        input_tokens: int = 0,\n",
    "        quantization: str = None,\n",
    "    ):\n",
    "        \"\"\"Context manager for profiling operations\"\"\"\n",
    "        # Setup\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        self.reset_vram_peak()\n",
    "\n",
    "        start_time = time.time()\n",
    "        start_vram = self.get_vram_usage()\n",
    "        process = psutil.Process()\n",
    "        start_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
    "\n",
    "        try:\n",
    "            yield self\n",
    "\n",
    "        finally:\n",
    "            # Measurements\n",
    "            end_time = time.time()\n",
    "            latency_ms = (end_time - start_time) * 1000\n",
    "            peak_vram = self.get_vram_peak()\n",
    "            cpu_percent = process.cpu_percent()\n",
    "            end_memory = process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "            # Create metrics (tokens_per_sec will be updated externally)\n",
    "            metrics = PerfMetrics(\n",
    "                operation=operation,\n",
    "                latency_ms=latency_ms,\n",
    "                tokens_per_sec=0.0,  # To be filled\n",
    "                vram_peak_mb=peak_vram,\n",
    "                cpu_percent=cpu_percent,\n",
    "                memory_mb=end_memory - start_memory,\n",
    "                input_tokens=input_tokens,\n",
    "                output_tokens=0,  # To be filled\n",
    "                backend=backend,\n",
    "                model_id=model_id,\n",
    "                quantization=quantization,\n",
    "                timestamp=end_time,\n",
    "            )\n",
    "\n",
    "            self.current_metrics = metrics\n",
    "\n",
    "    def finalize_metrics(self, output_tokens: int):\n",
    "        \"\"\"Update current metrics with output token count and calculate tokens/sec\"\"\"\n",
    "        if hasattr(self, \"current_metrics\"):\n",
    "            self.current_metrics.output_tokens = output_tokens\n",
    "            if self.current_metrics.latency_ms > 0:\n",
    "                self.current_metrics.tokens_per_sec = (\n",
    "                    output_tokens * 1000\n",
    "                ) / self.current_metrics.latency_ms\n",
    "            self.metrics_history.append(self.current_metrics)\n",
    "\n",
    "    def save_baseline(self):\n",
    "        \"\"\"Save performance baseline to JSON\"\"\"\n",
    "        baseline_data = {\n",
    "            \"timestamp\": time.time(),\n",
    "            \"gpu_info\": {\n",
    "                \"available\": torch.cuda.is_available(),\n",
    "                \"name\": (\n",
    "                    torch.cuda.get_device_name(0)\n",
    "                    if torch.cuda.is_available()\n",
    "                    else \"CPU\"\n",
    "                ),\n",
    "                \"memory_gb\": (\n",
    "                    torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "                    if torch.cuda.is_available()\n",
    "                    else 0\n",
    "                ),\n",
    "            },\n",
    "            \"metrics\": [asdict(m) for m in self.metrics_history],\n",
    "        }\n",
    "\n",
    "        with open(self.baseline_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(baseline_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"âœ… Baseline saved to {self.baseline_path}\")\n",
    "        return baseline_data\n",
    "\n",
    "    def print_summary(self):\n",
    "        \"\"\"Print performance summary\"\"\"\n",
    "        if not self.metrics_history:\n",
    "            print(\"âŒ No metrics recorded\")\n",
    "            return\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸš€ PERFORMANCE SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        for backend in set(m.backend for m in self.metrics_history):\n",
    "            backend_metrics = [m for m in self.metrics_history if m.backend == backend]\n",
    "            print(f\"\\nğŸ“Š Backend: {backend}\")\n",
    "\n",
    "            for metrics in backend_metrics:\n",
    "                print(f\"  {metrics.operation}:\")\n",
    "                print(f\"    â±ï¸  Latency: {metrics.latency_ms:.1f}ms\")\n",
    "                print(f\"    ğŸ”¥ Tokens/sec: {metrics.tokens_per_sec:.1f}\")\n",
    "                print(f\"    ğŸ’¾ VRAM Peak: {metrics.vram_peak_mb:.1f}MB\")\n",
    "                print(f\"    ğŸ¯ Tokens: {metrics.input_tokens}â†’{metrics.output_tokens}\")\n",
    "                if metrics.quantization:\n",
    "                    print(f\"    âš¡ Quant: {metrics.quantization}\")\n",
    "\n",
    "\n",
    "# Initialize global profiler\n",
    "profiler = PerfProfiler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9fea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 3: LLM Inference Performance Test\n",
    "# ============================================================================\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "class LLMPerfTester:\n",
    "    \"\"\"Test LLM inference performance across different configurations\"\"\"\n",
    "\n",
    "    def __init__(self, profiler: PerfProfiler):\n",
    "        self.profiler = profiler\n",
    "        self.test_prompts = [\n",
    "            \"è«‹è§£é‡‹ä»€éº¼æ˜¯æª¢ç´¢å¢å¼·ç”Ÿæˆ(RAG)ï¼Ÿ\",\n",
    "            \"å¯«ä¸€å€‹Pythonå‡½æ•¸ä¾†è¨ˆç®—æ–æ³¢é‚£å¥‘æ•¸åˆ—\",\n",
    "            \"åˆ†æäººå·¥æ™ºæ…§åœ¨æ•™è‚²é ˜åŸŸçš„æ‡‰ç”¨å‰æ™¯èˆ‡æŒ‘æˆ°\",\n",
    "        ]\n",
    "\n",
    "    def test_transformers_inference(\n",
    "        self, model_id: str = \"Qwen/Qwen2.5-7B-Instruct\", use_4bit: bool = True\n",
    "    ):\n",
    "        \"\"\"Test Transformers backend performance\"\"\"\n",
    "        print(f\"ğŸ§ª Testing Transformers: {model_id}\")\n",
    "\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        # Configure quantization\n",
    "        quant_config = None\n",
    "        quant_str = None\n",
    "        if use_4bit and torch.cuda.is_available():\n",
    "            quant_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "            )\n",
    "            quant_str = \"4bit\"\n",
    "\n",
    "        # Load model\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            quantization_config=quant_config,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        # Test inference\n",
    "        for i, prompt in enumerate(self.test_prompts):\n",
    "            input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "            input_tokens = input_ids.shape[1]\n",
    "\n",
    "            with self.profiler.profile_operation(\n",
    "                f\"transformers_inference_{i+1}\",\n",
    "                model_id,\n",
    "                \"transformers\",\n",
    "                input_tokens,\n",
    "                quant_str,\n",
    "            ):\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        input_ids.to(model.device),\n",
    "                        max_new_tokens=100,\n",
    "                        do_sample=True,\n",
    "                        temperature=0.7,\n",
    "                        pad_token_id=tokenizer.eos_token_id,\n",
    "                    )\n",
    "\n",
    "            output_tokens = outputs.shape[1] - input_tokens\n",
    "            self.profiler.finalize_metrics(output_tokens)\n",
    "\n",
    "            # Print sample output\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            print(f\"  ğŸ“ Sample {i+1}: {response[-100:]}...\")\n",
    "\n",
    "        # Cleanup\n",
    "        del model\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        gc.collect()\n",
    "\n",
    "    def test_llamacpp_inference(self, model_path: str = None):\n",
    "        \"\"\"Test llama.cpp backend performance (if available)\"\"\"\n",
    "        try:\n",
    "            from llama_cpp import Llama\n",
    "\n",
    "            if not model_path:\n",
    "                print(\"â­ï¸  Skipping llama.cpp test (no model path provided)\")\n",
    "                return\n",
    "\n",
    "            print(f\"ğŸ§ª Testing llama.cpp: {model_path}\")\n",
    "\n",
    "            # Load model\n",
    "            llm = Llama(\n",
    "                model_path=model_path,\n",
    "                n_ctx=2048,\n",
    "                n_gpu_layers=32 if torch.cuda.is_available() else 0,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            # Test inference\n",
    "            for i, prompt in enumerate(self.test_prompts[:1]):  # Test one prompt\n",
    "                input_tokens = len(prompt.split()) * 1.3  # Rough estimate\n",
    "\n",
    "                with self.profiler.profile_operation(\n",
    "                    f\"llamacpp_inference_{i+1}\",\n",
    "                    model_path,\n",
    "                    \"llama.cpp\",\n",
    "                    int(input_tokens),\n",
    "                    \"GGUF\",\n",
    "                ):\n",
    "                    output = llm(prompt, max_tokens=100, temperature=0.7)\n",
    "\n",
    "                output_tokens = len(output[\"choices\"][0][\"text\"].split()) * 1.3\n",
    "                self.profiler.finalize_metrics(int(output_tokens))\n",
    "\n",
    "                print(f\"  ğŸ“ Sample: {output['choices'][0]['text'][:100]}...\")\n",
    "\n",
    "            del llm\n",
    "\n",
    "        except ImportError:\n",
    "            print(\"â­ï¸  Skipping llama.cpp test (library not installed)\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ llama.cpp test failed: {e}\")\n",
    "\n",
    "\n",
    "# Run LLM performance tests\n",
    "llm_tester = LLMPerfTester(profiler)\n",
    "\n",
    "# Test with quantization\n",
    "llm_tester.test_transformers_inference(use_4bit=True)\n",
    "\n",
    "# Test llama.cpp if available\n",
    "# llm_tester.test_llamacpp_inference(\"/path/to/model.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f94c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 4: RAG Retrieval Performance Test\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "class RAGPerfTester:\n",
    "    \"\"\"Test RAG retrieval performance\"\"\"\n",
    "\n",
    "    def __init__(self, profiler: PerfProfiler):\n",
    "        self.profiler = profiler\n",
    "        self.test_queries = [\n",
    "            \"ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ\",\n",
    "            \"å¦‚ä½•å¯¦ä½œç¥ç¶“ç¶²è·¯ï¼Ÿ\",\n",
    "            \"æ·±åº¦å­¸ç¿’çš„æ‡‰ç”¨é ˜åŸŸæœ‰å“ªäº›ï¼Ÿ\",\n",
    "        ]\n",
    "\n",
    "    def setup_test_index(self, n_docs: int = 1000):\n",
    "        \"\"\"Create test index with synthetic documents\"\"\"\n",
    "        print(f\"ğŸ”§ Setting up test index with {n_docs} documents...\")\n",
    "\n",
    "        # Generate synthetic documents\n",
    "        doc_templates = [\n",
    "            \"æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§çš„é‡è¦åˆ†æ”¯ï¼Œå°ˆæ³¨æ–¼è®“é›»è…¦ç³»çµ±è‡ªå‹•å­¸ç¿’å’Œæ”¹é€²ã€‚\",\n",
    "            \"ç¥ç¶“ç¶²è·¯æ˜¯æ¨¡ä»¿äººè…¦ç¥ç¶“å…ƒçµæ§‹è¨­è¨ˆçš„è¨ˆç®—æ¨¡å‹ï¼Œç”¨æ–¼è§£æ±ºè¤‡é›œå•é¡Œã€‚\",\n",
    "            \"æ·±åº¦å­¸ç¿’ä½¿ç”¨å¤šå±¤ç¥ç¶“ç¶²è·¯ä¾†å­¸ç¿’æ•¸æ“šçš„æŠ½è±¡è¡¨ç¤ºå’Œç‰¹å¾µã€‚\",\n",
    "            \"è‡ªç„¶èªè¨€è™•ç†çµåˆèªè¨€å­¸å’Œæ©Ÿå™¨å­¸ç¿’ä¾†ç†è§£å’Œç”Ÿæˆäººé¡èªè¨€ã€‚\",\n",
    "            \"é›»è…¦è¦–è¦ºä½¿ç”¨æ·±åº¦å­¸ç¿’æŠ€è¡“ä¾†è­˜åˆ¥å’Œç†è§£åœ–åƒå…§å®¹ã€‚\",\n",
    "        ]\n",
    "\n",
    "        docs = []\n",
    "        for i in range(n_docs):\n",
    "            base_doc = doc_templates[i % len(doc_templates)]\n",
    "            docs.append(f\"{base_doc} æ–‡æª”ç·¨è™Ÿ: {i+1}\")\n",
    "\n",
    "        return docs\n",
    "\n",
    "    def test_embedding_performance(self, model_name: str = \"BAAI/bge-m3\"):\n",
    "        \"\"\"Test embedding model performance\"\"\"\n",
    "        print(f\"ğŸ§ª Testing Embedding: {model_name}\")\n",
    "\n",
    "        # Load embedding model\n",
    "        embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "        # Setup test documents\n",
    "        docs = self.setup_test_index(500)\n",
    "\n",
    "        # Test document embedding\n",
    "        with self.profiler.profile_operation(\n",
    "            \"embedding_documents\",\n",
    "            model_name,\n",
    "            \"sentence-transformers\",\n",
    "            len(\" \".join(docs).split()),\n",
    "            None,\n",
    "        ):\n",
    "            doc_embeddings = embedding_model.encode(\n",
    "                docs, normalize_embeddings=True, show_progress_bar=False\n",
    "            )\n",
    "\n",
    "        # Estimate tokens processed (rough)\n",
    "        total_tokens = sum(len(doc.split()) for doc in docs) * 1.3\n",
    "        self.profiler.finalize_metrics(int(total_tokens))\n",
    "\n",
    "        # Test query embedding\n",
    "        for i, query in enumerate(self.test_queries):\n",
    "            query_tokens = len(query.split()) * 1.3\n",
    "\n",
    "            with self.profiler.profile_operation(\n",
    "                f\"embedding_query_{i+1}\",\n",
    "                model_name,\n",
    "                \"sentence-transformers\",\n",
    "                int(query_tokens),\n",
    "                None,\n",
    "            ):\n",
    "                query_embedding = embedding_model.encode(\n",
    "                    [query], normalize_embeddings=True\n",
    "                )\n",
    "\n",
    "            self.profiler.finalize_metrics(int(query_tokens))\n",
    "\n",
    "        return doc_embeddings, embedding_model\n",
    "\n",
    "    def test_faiss_performance(self, embeddings: np.ndarray):\n",
    "        \"\"\"Test FAISS index performance\"\"\"\n",
    "        print(f\"ğŸ§ª Testing FAISS Index: {embeddings.shape}\")\n",
    "\n",
    "        # Build FAISS index\n",
    "        with self.profiler.profile_operation(\n",
    "            \"faiss_index_build\", f\"IndexFlatIP-{embeddings.shape[0]}\", \"faiss\", 0, None\n",
    "        ):\n",
    "            index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "            index.add(embeddings.astype(\"float32\"))\n",
    "\n",
    "        self.profiler.finalize_metrics(embeddings.shape[0])\n",
    "\n",
    "        # Test search performance\n",
    "        embedding_model = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "\n",
    "        for i, query in enumerate(self.test_queries):\n",
    "            query_embedding = embedding_model.encode([query], normalize_embeddings=True)\n",
    "\n",
    "            with self.profiler.profile_operation(\n",
    "                f\"faiss_search_{i+1}\",\n",
    "                f\"IndexFlatIP-{embeddings.shape[0]}\",\n",
    "                \"faiss\",\n",
    "                1,\n",
    "                None,\n",
    "            ):\n",
    "                scores, indices = index.search(query_embedding.astype(\"float32\"), k=10)\n",
    "\n",
    "            self.profiler.finalize_metrics(10)  # Retrieved 10 docs\n",
    "            print(f\"  ğŸ¯ Query {i+1}: Top score = {scores[0][0]:.4f}\")\n",
    "\n",
    "        return index\n",
    "\n",
    "\n",
    "# Run RAG performance tests\n",
    "rag_tester = RAGPerfTester(profiler)\n",
    "\n",
    "# Test embedding performance\n",
    "doc_embeddings, emb_model = rag_tester.test_embedding_performance()\n",
    "\n",
    "# Test FAISS performance\n",
    "faiss_index = rag_tester.test_faiss_performance(doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2fd543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 5: End-to-End RAG Performance Test\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class EndToEndRAGTester:\n",
    "    \"\"\"Test complete RAG pipeline performance\"\"\"\n",
    "\n",
    "    def __init__(self, profiler: PerfProfiler, embedding_model, faiss_index, docs):\n",
    "        self.profiler = profiler\n",
    "        self.embedding_model = embedding_model\n",
    "        self.index = faiss_index\n",
    "        self.docs = docs\n",
    "        self.test_questions = [\"ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’çš„åŸºæœ¬æ¦‚å¿µï¼Ÿ\", \"ç¥ç¶“ç¶²è·¯å¦‚ä½•é€²è¡Œè¨“ç·´ï¼Ÿ\"]\n",
    "\n",
    "    def rag_retrieve_and_answer(self, query: str, llm_model, tokenizer, k: int = 5):\n",
    "        \"\"\"Complete RAG pipeline: retrieve + generate\"\"\"\n",
    "\n",
    "        # Step 1: Query embedding\n",
    "        with self.profiler.profile_operation(\n",
    "            \"rag_query_embedding\",\n",
    "            \"BAAI/bge-m3\",\n",
    "            \"sentence-transformers\",\n",
    "            len(query.split()) * 1.3,\n",
    "            None,\n",
    "        ):\n",
    "            query_embedding = self.embedding_model.encode(\n",
    "                [query], normalize_embeddings=True\n",
    "            )\n",
    "\n",
    "        self.profiler.finalize_metrics(len(query.split()))\n",
    "\n",
    "        # Step 2: Retrieval\n",
    "        with self.profiler.profile_operation(\n",
    "            \"rag_retrieval\", f\"FAISS-{len(self.docs)}\", \"faiss\", 1, None\n",
    "        ):\n",
    "            scores, indices = self.index.search(query_embedding.astype(\"float32\"), k=k)\n",
    "\n",
    "        self.profiler.finalize_metrics(k)\n",
    "\n",
    "        # Step 3: Context preparation\n",
    "        retrieved_docs = [self.docs[idx] for idx in indices[0]]\n",
    "        context = \"\\n\".join([f\"[{i+1}] {doc}\" for i, doc in enumerate(retrieved_docs)])\n",
    "\n",
    "        prompt = f\"\"\"åŸºæ–¼ä»¥ä¸‹è³‡æ–™å›ç­”å•é¡Œï¼š\n",
    "\n",
    "è³‡æ–™ï¼š\n",
    "{context}\n",
    "\n",
    "å•é¡Œï¼š{query}\n",
    "\n",
    "å›ç­”ï¼š\"\"\"\n",
    "\n",
    "        # Step 4: Generation\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        input_tokens = input_ids.shape[1]\n",
    "\n",
    "        with self.profiler.profile_operation(\n",
    "            \"rag_generation\", \"Qwen2.5-7B\", \"transformers\", input_tokens, \"4bit\"\n",
    "        ):\n",
    "            with torch.no_grad():\n",
    "                outputs = llm_model.generate(\n",
    "                    input_ids.to(llm_model.device),\n",
    "                    max_new_tokens=200,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "\n",
    "        output_tokens = outputs.shape[1] - input_tokens\n",
    "        self.profiler.finalize_metrics(output_tokens)\n",
    "\n",
    "        # Decode response\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        answer = response[len(prompt) :].strip()\n",
    "\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"retrieved_docs\": retrieved_docs,\n",
    "            \"answer\": answer,\n",
    "            \"scores\": scores[0].tolist(),\n",
    "        }\n",
    "\n",
    "    def test_e2e_performance(self):\n",
    "        \"\"\"Test end-to-end RAG performance\"\"\"\n",
    "        print(\"ğŸ§ª Testing End-to-End RAG Pipeline\")\n",
    "\n",
    "        # Load LLM for generation\n",
    "        model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        # Load with 4-bit quantization\n",
    "        quant_config = (\n",
    "            BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "            )\n",
    "            if torch.cuda.is_available()\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            quantization_config=quant_config,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        # Test each question\n",
    "        results = []\n",
    "        for i, question in enumerate(self.test_questions):\n",
    "            print(f\"  ğŸ“‹ Question {i+1}: {question}\")\n",
    "\n",
    "            result = self.rag_retrieve_and_answer(question, llm_model, tokenizer)\n",
    "            results.append(result)\n",
    "\n",
    "            print(f\"  âœ… Answer preview: {result['answer'][:100]}...\")\n",
    "            print(f\"  ğŸ“Š Top retrieval score: {result['scores'][0]:.4f}\")\n",
    "\n",
    "        # Cleanup\n",
    "        del llm_model\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        gc.collect()\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# Create test documents for E2E testing\n",
    "test_docs = rag_tester.setup_test_index(200)\n",
    "\n",
    "# Run end-to-end test\n",
    "e2e_tester = EndToEndRAGTester(profiler, emb_model, faiss_index, test_docs)\n",
    "e2e_results = e2e_tester.test_e2e_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56866672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 6: VRAM Peak Tracking & Memory Optimization\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class VRAMAnalyzer:\n",
    "    \"\"\"Analyze VRAM usage patterns and suggest optimizations\"\"\"\n",
    "\n",
    "    def __init__(self, profiler: PerfProfiler):\n",
    "        self.profiler = profiler\n",
    "\n",
    "    def analyze_vram_usage(self):\n",
    "        \"\"\"Analyze VRAM usage from metrics history\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"âš ï¸  CUDA not available - skipping VRAM analysis\")\n",
    "            return\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"ğŸ’¾ VRAM USAGE ANALYSIS\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # GPU info\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        total_vram = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"ğŸ–¥ï¸  GPU: {gpu_name}\")\n",
    "        print(f\"ğŸ“ Total VRAM: {total_vram:.1f} GB\")\n",
    "\n",
    "        # Analyze metrics\n",
    "        vram_metrics = [\n",
    "            (m.operation, m.vram_peak_mb) for m in self.profiler.metrics_history\n",
    "        ]\n",
    "\n",
    "        if not vram_metrics:\n",
    "            print(\"âŒ No VRAM metrics available\")\n",
    "            return\n",
    "\n",
    "        # Sort by VRAM usage\n",
    "        vram_metrics.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        print(\"\\nğŸ“Š VRAM Usage by Operation:\")\n",
    "        for op, vram_mb in vram_metrics[:10]:  # Top 10\n",
    "            vram_gb = vram_mb / 1024\n",
    "            usage_pct = (vram_gb / total_vram) * 100\n",
    "            print(f\"  {op:<30} {vram_mb:>8.1f} MB ({usage_pct:>5.1f}%)\")\n",
    "\n",
    "        # Recommendations\n",
    "        max_vram = max(vram_mb for _, vram_mb in vram_metrics)\n",
    "        max_vram_gb = max_vram / 1024\n",
    "\n",
    "        print(f\"\\nğŸ¯ Peak VRAM Usage: {max_vram:.1f} MB ({max_vram_gb:.2f} GB)\")\n",
    "\n",
    "        if max_vram_gb > total_vram * 0.9:\n",
    "            print(\"âš ï¸  HIGH VRAM USAGE - Consider optimizations:\")\n",
    "            print(\"   â€¢ Use smaller models or more aggressive quantization\")\n",
    "            print(\"   â€¢ Reduce batch sizes\")\n",
    "            print(\"   â€¢ Enable gradient checkpointing\")\n",
    "            print(\"   â€¢ Use model sharding or offloading\")\n",
    "        elif max_vram_gb > total_vram * 0.7:\n",
    "            print(\"âš¡ MODERATE VRAM USAGE - Optimizations available:\")\n",
    "            print(\"   â€¢ 8-bit quantization could reduce usage\")\n",
    "            print(\"   â€¢ Consider larger batch sizes for efficiency\")\n",
    "        else:\n",
    "            print(\"âœ… EFFICIENT VRAM USAGE - Well optimized!\")\n",
    "            print(\"   â€¢ Consider larger models or batch sizes\")\n",
    "\n",
    "    def memory_optimization_test(self):\n",
    "        \"\"\"Test different memory optimization strategies\"\"\"\n",
    "        print(\"\\nğŸ§ª Testing Memory Optimizations...\")\n",
    "\n",
    "        test_text = \"è«‹è©³ç´°è§£é‡‹æ·±åº¦å­¸ç¿’çš„åŸºæœ¬åŸç†å’Œæ‡‰ç”¨å ´æ™¯ã€‚\" * 5\n",
    "\n",
    "        # Test different configurations\n",
    "        configs = [\n",
    "            {\"name\": \"Baseline FP16\", \"dtype\": torch.float16, \"quant\": None},\n",
    "            {\"name\": \"4-bit Quantization\", \"dtype\": torch.float16, \"quant\": \"4bit\"},\n",
    "        ]\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            for config in configs:\n",
    "                print(f\"\\n  ğŸ”§ Testing: {config['name']}\")\n",
    "\n",
    "                try:\n",
    "                    # Reset VRAM tracking\n",
    "                    torch.cuda.empty_cache()\n",
    "                    self.profiler.reset_vram_peak()\n",
    "\n",
    "                    # Simulate model loading\n",
    "                    if config[\"quant\"] == \"4bit\":\n",
    "                        # Simulate 4-bit loading (reduced VRAM)\n",
    "                        dummy_tensor = torch.randn(\n",
    "                            1000, 1000, dtype=config[\"dtype\"]\n",
    "                        ).cuda()\n",
    "                        dummy_tensor = dummy_tensor * 0.5  # Simulate quantization\n",
    "                    else:\n",
    "                        dummy_tensor = torch.randn(\n",
    "                            2000, 2000, dtype=config[\"dtype\"]\n",
    "                        ).cuda()\n",
    "\n",
    "                    peak_vram = self.profiler.get_vram_peak()\n",
    "                    print(f\"    ğŸ’¾ Peak VRAM: {peak_vram:.1f} MB\")\n",
    "\n",
    "                    # Cleanup\n",
    "                    del dummy_tensor\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"    âŒ Failed: {e}\")\n",
    "\n",
    "\n",
    "# Run VRAM analysis\n",
    "vram_analyzer = VRAMAnalyzer(profiler)\n",
    "vram_analyzer.analyze_vram_usage()\n",
    "vram_analyzer.memory_optimization_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea868d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 7: Performance Report Generation\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def generate_performance_report():\n",
    "    \"\"\"Generate comprehensive performance report\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ“‹ GENERATING PERFORMANCE REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Print summary\n",
    "    profiler.print_summary()\n",
    "\n",
    "    # Save baseline\n",
    "    baseline_data = profiler.save_baseline()\n",
    "\n",
    "    # Generate recommendations\n",
    "    print(\"\\nğŸ¯ PERFORMANCE RECOMMENDATIONS:\")\n",
    "\n",
    "    # Analyze latency\n",
    "    latencies = [m.latency_ms for m in profiler.metrics_history]\n",
    "    if latencies:\n",
    "        avg_latency = sum(latencies) / len(latencies)\n",
    "        max_latency = max(latencies)\n",
    "\n",
    "        print(f\"   â±ï¸  Average Latency: {avg_latency:.1f}ms\")\n",
    "        print(f\"   â±ï¸  Max Latency: {max_latency:.1f}ms\")\n",
    "\n",
    "        if avg_latency > 5000:  # 5 seconds\n",
    "            print(\"   âš ï¸  High latency detected - consider:\")\n",
    "            print(\"      â€¢ Smaller models or more aggressive quantization\")\n",
    "            print(\"      â€¢ GPU acceleration if using CPU\")\n",
    "            print(\"      â€¢ Batch processing for multiple requests\")\n",
    "\n",
    "    # Analyze throughput\n",
    "    token_rates = [\n",
    "        m.tokens_per_sec for m in profiler.metrics_history if m.tokens_per_sec > 0\n",
    "    ]\n",
    "    if token_rates:\n",
    "        avg_tokens_sec = sum(token_rates) / len(token_rates)\n",
    "        print(f\"   ğŸ”¥ Average Tokens/sec: {avg_tokens_sec:.1f}\")\n",
    "\n",
    "        if avg_tokens_sec < 10:\n",
    "            print(\"   âš ï¸  Low throughput detected - consider:\")\n",
    "            print(\"      â€¢ GPU acceleration or better hardware\")\n",
    "            print(\"      â€¢ Model quantization to reduce memory pressure\")\n",
    "            print(\"      â€¢ Batch inference for better GPU utilization\")\n",
    "        elif avg_tokens_sec > 50:\n",
    "            print(\"   âœ… Excellent throughput - well optimized!\")\n",
    "\n",
    "    # Hardware utilization\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        total_vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"   ğŸ–¥ï¸  Hardware: {gpu_name} ({total_vram_gb:.1f}GB)\")\n",
    "\n",
    "    return baseline_data\n",
    "\n",
    "\n",
    "# Generate the report\n",
    "report_data = generate_performance_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3ec7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 8: Smoke Test - Quick Performance Check\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def smoke_test_performance():\n",
    "    \"\"\"Quick smoke test for performance monitoring\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ğŸ”¥ SMOKE TEST - Performance Monitoring\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Test basic profiler functionality\n",
    "    test_profiler = PerfProfiler()\n",
    "\n",
    "    # Simulate a quick operation\n",
    "    with test_profiler.profile_operation(\n",
    "        \"smoke_test\", \"test-model\", \"test-backend\", 50, \"test-quant\"\n",
    "    ):\n",
    "        # Simulate some work\n",
    "        time.sleep(0.1)\n",
    "        if torch.cuda.is_available():\n",
    "            dummy = torch.randn(100, 100).cuda()\n",
    "            result = torch.matmul(dummy, dummy)\n",
    "            del dummy, result\n",
    "\n",
    "    test_profiler.finalize_metrics(25)\n",
    "\n",
    "    # Check results\n",
    "    assert len(test_profiler.metrics_history) == 1, \"âŒ Metrics not recorded\"\n",
    "\n",
    "    metrics = test_profiler.metrics_history[0]\n",
    "    assert metrics.latency_ms > 50, f\"âŒ Unexpected latency: {metrics.latency_ms}ms\"\n",
    "    assert (\n",
    "        metrics.tokens_per_sec > 0\n",
    "    ), f\"âŒ Invalid tokens/sec: {metrics.tokens_per_sec}\"\n",
    "    assert metrics.input_tokens == 50, f\"âŒ Wrong input tokens: {metrics.input_tokens}\"\n",
    "    assert (\n",
    "        metrics.output_tokens == 25\n",
    "    ), f\"âŒ Wrong output tokens: {metrics.output_tokens}\"\n",
    "\n",
    "    print(\"âœ… Basic profiler functionality working\")\n",
    "\n",
    "    # Test VRAM tracking\n",
    "    if torch.cuda.is_available():\n",
    "        test_profiler.reset_vram_peak()\n",
    "        dummy = torch.randn(1000, 1000).cuda()\n",
    "        peak = test_profiler.get_vram_peak()\n",
    "        del dummy\n",
    "\n",
    "        assert peak > 0, f\"âŒ VRAM tracking failed: {peak}MB\"\n",
    "        print(f\"âœ… VRAM tracking working: {peak:.1f}MB peak\")\n",
    "    else:\n",
    "        print(\"â­ï¸  VRAM tracking skipped (no CUDA)\")\n",
    "\n",
    "    # Test baseline saving\n",
    "    test_baseline_path = \"outs/smoke_baseline.json\"\n",
    "    test_profiler.baseline_path = test_baseline_path\n",
    "    test_profiler.save_baseline()\n",
    "\n",
    "    assert pathlib.Path(test_baseline_path).exists(), \"âŒ Baseline file not created\"\n",
    "\n",
    "    with open(test_baseline_path, \"r\") as f:\n",
    "        baseline = json.load(f)\n",
    "\n",
    "    assert \"metrics\" in baseline, \"âŒ Invalid baseline format\"\n",
    "    assert len(baseline[\"metrics\"]) == 1, \"âŒ Wrong number of metrics in baseline\"\n",
    "\n",
    "    print(\"âœ… Baseline saving working\")\n",
    "\n",
    "    # Cleanup\n",
    "    pathlib.Path(test_baseline_path).unlink()\n",
    "\n",
    "    print(\"\\nğŸ‰ All smoke tests passed!\")\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 9: Summary & Optimization Recommendations\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def print_stage_summary():\n",
    "    \"\"\"Print Stage 7 notebook summary and next steps\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ“‹ STAGE 7 - NOTEBOOK 63 SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(\"\\nâœ… COMPLETED:\")\n",
    "    print(\"   â€¢ æ•ˆèƒ½ç›£æ§æ¡†æ¶ (PerfProfiler) å»ºç«‹\")\n",
    "    print(\"   â€¢ LLMæ¨ç†æ•ˆèƒ½æ¸¬è©¦ (Transformers + é‡åŒ–)\")\n",
    "    print(\"   â€¢ RAGæª¢ç´¢æ•ˆèƒ½æ¸¬è©¦ (Embedding + FAISS)\")\n",
    "    print(\"   â€¢ ç«¯åˆ°ç«¯RAGæµç¨‹æ•ˆèƒ½æ¸¬è©¦\")\n",
    "    print(\"   â€¢ VRAMä½¿ç”¨åˆ†æèˆ‡å„ªåŒ–å»ºè­°\")\n",
    "    print(\"   â€¢ æ•ˆèƒ½åŸºæº–å ±è¡¨ç”Ÿæˆ (baseline.json)\")\n",
    "    print(\"   â€¢ å»¶é²ã€ååé‡ã€è¨˜æ†¶é«”ä½¿ç”¨è¿½è¹¤\")\n",
    "\n",
    "    print(\"\\nğŸ¯ CORE CONCEPTS:\")\n",
    "    print(\"   â€¢ Performance Profiling: ä½¿ç”¨context managerè¿½è¹¤å»¶é²èˆ‡è³‡æº\")\n",
    "    print(\"   â€¢ VRAM Monitoring: torch.cuda.memory APIè¿½è¹¤å³°å€¼ä½¿ç”¨\")\n",
    "    print(\"   â€¢ Tokens/sec Calculation: è¼¸å‡ºtokenæ•¸é™¤ä»¥å»¶é²æ™‚é–“\")\n",
    "    print(\"   â€¢ Multi-backend Comparison: å°æ¯”ä¸åŒå¾Œç«¯æ•ˆèƒ½å·®ç•°\")\n",
    "    print(\"   â€¢ Memory Optimization: é‡åŒ–ã€æ‰¹æ¬¡å¤§å°ã€æ¨¡å‹åˆ†ç‰‡ç­–ç•¥\")\n",
    "\n",
    "    print(\"\\nâš ï¸  PITFALLS:\")\n",
    "    print(\"   â€¢ VRAMæ¸¬é‡ä¸æº–ç¢º: éœ€åœ¨æ“ä½œå‰reset_peak_memory_stats()\")\n",
    "    print(\"   â€¢ Tokenè¨ˆç®—ä¼°ç®—: ä¸åŒtokenizerçš„tokenæ•¸å·®ç•°å¾ˆå¤§\")\n",
    "    print(\"   â€¢ å†·å•Ÿå‹•æ•ˆæ‡‰: é¦–æ¬¡æ¨ç†è¼ƒæ…¢ï¼Œéœ€å¤šæ¬¡æ¸¬é‡å–å¹³å‡\")\n",
    "    print(\"   â€¢ è¨˜æ†¶é«”æ´©æ¼: æ¸¬è©¦å¾Œè¦æ˜ç¢ºdelæ¨¡å‹ä¸¦empty_cache()\")\n",
    "    print(\"   â€¢ ä¸¦ç™¼å¹²æ“¾: å¤šGPUæˆ–å…¶ä»–ç¨‹åºæœƒå½±éŸ¿æ•ˆèƒ½æ¸¬é‡\")\n",
    "\n",
    "    print(\"\\nğŸš€ NEXT ACTIONS:\")\n",
    "    print(\"   â€¢ nb64: æ•´åˆæ‰€æœ‰è©•ä¼°æŒ‡æ¨™ç”¢ç”Ÿç¶œåˆå ±è¡¨\")\n",
    "    print(\"   â€¢ å»ºç«‹æ•ˆèƒ½å›æ­¸æª¢æ¸¬CI\")\n",
    "    print(\"   â€¢ åŠ å…¥æ›´å¤šæ¨¡å‹èˆ‡ç¡¬é«”é…ç½®çš„åŸºæº–\")\n",
    "    print(\"   â€¢ å¯¦ä½œè‡ªå‹•æ•ˆèƒ½å„ªåŒ–å»ºè­°ç³»çµ±\")\n",
    "\n",
    "    print(\"\\nğŸ“Š CURRENT BASELINE:\")\n",
    "    if pathlib.Path(\"outs/baseline.json\").exists():\n",
    "        with open(\"outs/baseline.json\", \"r\") as f:\n",
    "            baseline = json.load(f)\n",
    "\n",
    "        print(f\"   ğŸ“… Generated: {time.ctime(baseline['timestamp'])}\")\n",
    "        print(f\"   ğŸ–¥ï¸  Hardware: {baseline['gpu_info']['name']}\")\n",
    "        print(f\"   ğŸ“ˆ Metrics: {len(baseline['metrics'])} operations recorded\")\n",
    "\n",
    "        # Key stats\n",
    "        latencies = [m[\"latency_ms\"] for m in baseline[\"metrics\"]]\n",
    "        token_rates = [\n",
    "            m[\"tokens_per_sec\"] for m in baseline[\"metrics\"] if m[\"tokens_per_sec\"] > 0\n",
    "        ]\n",
    "\n",
    "        if latencies:\n",
    "            print(f\"   â±ï¸  Avg Latency: {sum(latencies)/len(latencies):.1f}ms\")\n",
    "        if token_rates:\n",
    "            print(f\"   ğŸ”¥ Avg Tokens/sec: {sum(token_rates)/len(token_rates):.1f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "\n",
    "# Print final summary\n",
    "print_stage_summary()\n",
    "\n",
    "# Final cleanup\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nğŸ‰ Notebook 63 completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d253c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ¨ nb63 æœ€å¾ŒåŸ·è¡Œé€™å€‹ cell é©—è­‰åŠŸèƒ½\n",
    "def quick_verification():\n",
    "    \"\"\"3åˆ†é˜å…§é©—è­‰æ ¸å¿ƒåŠŸèƒ½\"\"\"\n",
    "    print(\"ğŸ”¥ Quick Verification (3 min)\")\n",
    "\n",
    "    # 1. æª¢æŸ¥ baseline.json æ˜¯å¦ç”Ÿæˆ\n",
    "    assert pathlib.Path(\"outs/baseline.json\").exists()\n",
    "    print(\"âœ… Baseline file created\")\n",
    "\n",
    "    # 2. æª¢æŸ¥æ˜¯å¦è¨˜éŒ„äº†æ•ˆèƒ½æŒ‡æ¨™\n",
    "    assert len(profiler.metrics_history) > 0\n",
    "    print(f\"âœ… {len(profiler.metrics_history)} metrics recorded\")\n",
    "\n",
    "    # 3. æª¢æŸ¥é—œéµæŒ‡æ¨™å­˜åœ¨\n",
    "    for m in profiler.metrics_history[:3]:\n",
    "        assert m.latency_ms > 0\n",
    "        assert m.vram_peak_mb >= 0\n",
    "        print(f\"âœ… {m.operation}: {m.latency_ms:.1f}ms, {m.vram_peak_mb:.1f}MB\")\n",
    "\n",
    "    print(\"ğŸ‰ All verifications passed!\")\n",
    "\n",
    "\n",
    "quick_verification()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
