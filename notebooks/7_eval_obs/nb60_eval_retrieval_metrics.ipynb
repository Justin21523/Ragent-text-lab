{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931655fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (è¤‡è£½åˆ°æ¯æœ¬ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36ae92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 2: Dependencies & Imports\n",
    "# =============================================================================\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Set, Any\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Create output directories\n",
    "Path(\"outs/eval\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"outs/charts\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94548afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 3: Evaluation Dataset Setup\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def create_eval_dataset():\n",
    "    \"\"\"Create a small evaluation dataset with queries and relevant document IDs\"\"\"\n",
    "\n",
    "    # Sample Chinese queries with known relevant documents\n",
    "    eval_data = [\n",
    "        {\n",
    "            \"query_id\": \"q001\",\n",
    "            \"query\": \"ä»€éº¼æ˜¯ RAG æª¢ç´¢å¢žå¼·ç”Ÿæˆï¼Ÿ\",\n",
    "            \"relevant_doc_ids\": {\"doc_001\", \"doc_002\", \"doc_005\"},\n",
    "            \"category\": \"concept\",\n",
    "        },\n",
    "        {\n",
    "            \"query_id\": \"q002\",\n",
    "            \"query\": \"å¦‚ä½•å»ºç«‹ FAISS å‘é‡ç´¢å¼•ï¼Ÿ\",\n",
    "            \"relevant_doc_ids\": {\"doc_003\", \"doc_007\", \"doc_012\"},\n",
    "            \"category\": \"technical\",\n",
    "        },\n",
    "        {\n",
    "            \"query_id\": \"q003\",\n",
    "            \"query\": \"bge-m3 åµŒå…¥æ¨¡åž‹çš„ç‰¹é»ž\",\n",
    "            \"relevant_doc_ids\": {\"doc_004\", \"doc_008\", \"doc_010\"},\n",
    "            \"category\": \"model\",\n",
    "        },\n",
    "        {\n",
    "            \"query_id\": \"q004\",\n",
    "            \"query\": \"ä¸­æ–‡æ–‡æœ¬åˆ†æ®µç­–ç•¥\",\n",
    "            \"relevant_doc_ids\": {\"doc_006\", \"doc_009\", \"doc_011\"},\n",
    "            \"category\": \"preprocessing\",\n",
    "        },\n",
    "        {\n",
    "            \"query_id\": \"q005\",\n",
    "            \"query\": \"é‡æŽ’å™¨å¦‚ä½•æå‡æª¢ç´¢æ•ˆæžœï¼Ÿ\",\n",
    "            \"relevant_doc_ids\": {\"doc_002\", \"doc_013\", \"doc_015\"},\n",
    "            \"category\": \"reranking\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Sample document corpus\n",
    "    documents = [\n",
    "        {\n",
    "            \"doc_id\": \"doc_001\",\n",
    "            \"text\": \"RAGï¼ˆæª¢ç´¢å¢žå¼·ç”Ÿæˆï¼‰æ˜¯ä¸€ç¨®çµåˆæª¢ç´¢å’Œç”Ÿæˆçš„ AI æŠ€è¡“...\",\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"doc_002\",\n",
    "            \"text\": \"æª¢ç´¢å¢žå¼·ç”Ÿæˆé€éŽå¤–éƒ¨çŸ¥è­˜åº«æå‡æ¨¡åž‹å›žç­”çš„æº–ç¢ºæ€§...\",\n",
    "        },\n",
    "        {\"doc_id\": \"doc_003\", \"text\": \"FAISS æ˜¯ Facebook é–‹ç™¼çš„å‘é‡ç›¸ä¼¼åº¦æœå°‹åº«...\"},\n",
    "        {\"doc_id\": \"doc_004\", \"text\": \"bge-m3 æ˜¯ä¸­è‹±æ–‡å¤šèªžè¨€åµŒå…¥æ¨¡åž‹ï¼Œæ”¯æ´å¯†é›†æª¢ç´¢...\"},\n",
    "        {\"doc_id\": \"doc_005\", \"text\": \"ç”Ÿæˆå¼ AI çµåˆæª¢ç´¢æŠ€è¡“å¯ä»¥æœ‰æ•ˆæ¸›å°‘å¹»è¦ºå•é¡Œ...\"},\n",
    "        {\"doc_id\": \"doc_006\", \"text\": \"ä¸­æ–‡åˆ†æ®µéœ€è¦è€ƒæ…®æ¨™é»žç¬¦è™Ÿå’Œèªžæ„å®Œæ•´æ€§...\"},\n",
    "        {\"doc_id\": \"doc_007\", \"text\": \"å»ºç«‹ FAISS ç´¢å¼•éœ€è¦å…ˆå°‡æ–‡æœ¬è½‰æ›ç‚ºå‘é‡è¡¨ç¤º...\"},\n",
    "        {\"doc_id\": \"doc_008\", \"text\": \"å¤šèªžè¨€åµŒå…¥æ¨¡åž‹åœ¨è·¨èªžè¨€æª¢ç´¢ä¸­è¡¨ç¾å„ªç•°...\"},\n",
    "        {\"doc_id\": \"doc_009\", \"text\": \"éžæ­¸å¼æ–‡æœ¬åˆ†å‰²å™¨å¯ä»¥ä¿æŒèªžå¢ƒå®Œæ•´æ€§...\"},\n",
    "        {\"doc_id\": \"doc_010\", \"text\": \"bge ç³»åˆ—æ¨¡åž‹åœ¨ä¸­æ–‡èªžç¾©åŒ¹é…ä»»å‹™ä¸Šæ•ˆæžœé¡¯è‘—...\"},\n",
    "        {\"doc_id\": \"doc_011\", \"text\": \"æ–‡æœ¬é è™•ç†åŒ…æ‹¬æ¸…ç†ã€åˆ†æ®µå’ŒåŽ»é‡ç­‰æ­¥é©Ÿ...\"},\n",
    "        {\"doc_id\": \"doc_012\", \"text\": \"å‘é‡ç´¢å¼•çš„é¸æ“‡å½±éŸ¿æª¢ç´¢é€Ÿåº¦å’Œæº–ç¢ºçŽ‡...\"},\n",
    "        {\"doc_id\": \"doc_013\", \"text\": \"é‡æŽ’å™¨ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›æ©Ÿåˆ¶é€²è¡Œç²¾ç¢ºåŒ¹é…...\"},\n",
    "        {\"doc_id\": \"doc_014\", \"text\": \"æ··åˆæª¢ç´¢çµåˆé—œéµå­—å’Œèªžç¾©æœå°‹çš„å„ªå‹¢...\"},\n",
    "        {\"doc_id\": \"doc_015\", \"text\": \"é›™éšŽæ®µæª¢ç´¢å…ˆå¬å›žå€™é¸å†é‡æŽ’å¯æå‡æ•ˆæžœ...\"},\n",
    "    ]\n",
    "\n",
    "    return eval_data, documents\n",
    "\n",
    "\n",
    "# Create evaluation dataset\n",
    "eval_queries, doc_corpus = create_eval_dataset()\n",
    "print(f\"Created evaluation dataset:\")\n",
    "print(f\"- Queries: {len(eval_queries)}\")\n",
    "print(f\"- Documents: {len(doc_corpus)}\")\n",
    "print(f\"- Sample query: {eval_queries[0]['query']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b54ee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 4: Load RAG Components\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class SimpleRetriever:\n",
    "    \"\"\"Simplified retriever for evaluation purposes\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model=\"BAAI/bge-m3\"):\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.documents = []\n",
    "        self.doc_embeddings = None\n",
    "        self.index = None\n",
    "\n",
    "    def build_index(self, documents: List[Dict]):\n",
    "        \"\"\"Build FAISS index from documents\"\"\"\n",
    "        self.documents = documents\n",
    "        texts = [doc[\"text\"] for doc in documents]\n",
    "\n",
    "        # Generate embeddings\n",
    "        print(\"Generating embeddings...\")\n",
    "        self.doc_embeddings = self.embedding_model.encode(\n",
    "            texts, normalize_embeddings=True, show_progress_bar=True\n",
    "        ).astype(\"float32\")\n",
    "\n",
    "        # Build FAISS index\n",
    "        dimension = self.doc_embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(\n",
    "            dimension\n",
    "        )  # Inner product for normalized vectors\n",
    "        self.index.add(self.doc_embeddings)\n",
    "\n",
    "        print(f\"Built index with {self.index.ntotal} documents\")\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 10) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Retrieve top-k documents for query\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not built. Call build_index() first.\")\n",
    "\n",
    "        # Encode query\n",
    "        query_vector = self.embedding_model.encode(\n",
    "            [query], normalize_embeddings=True\n",
    "        ).astype(\"float32\")\n",
    "\n",
    "        # Search\n",
    "        scores, indices = self.index.search(query_vector, k)\n",
    "\n",
    "        # Return (doc_id, score) pairs\n",
    "        results = []\n",
    "        for i, (idx, score) in enumerate(zip(indices[0], scores[0])):\n",
    "            if idx < len(self.documents):\n",
    "                doc_id = self.documents[idx][\"doc_id\"]\n",
    "                results.append((doc_id, float(score)))\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# Initialize retriever and build index\n",
    "retriever = SimpleRetriever()\n",
    "retriever.build_index(doc_corpus)\n",
    "\n",
    "print(\"RAG components loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f883ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 5: Retrieval Metrics Implementation\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def recall_at_k(retrieved_docs: List[str], relevant_docs: Set[str], k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Recall@k: proportion of relevant docs retrieved in top-k\n",
    "    \"\"\"\n",
    "    if not relevant_docs:\n",
    "        return 0.0\n",
    "\n",
    "    retrieved_k = set(retrieved_docs[:k])\n",
    "    relevant_retrieved = retrieved_k.intersection(relevant_docs)\n",
    "\n",
    "    return len(relevant_retrieved) / len(relevant_docs)\n",
    "\n",
    "\n",
    "def precision_at_k(retrieved_docs: List[str], relevant_docs: Set[str], k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Precision@k: proportion of retrieved docs that are relevant\n",
    "    \"\"\"\n",
    "    if k == 0:\n",
    "        return 0.0\n",
    "\n",
    "    retrieved_k = set(retrieved_docs[:k])\n",
    "    relevant_retrieved = retrieved_k.intersection(relevant_docs)\n",
    "\n",
    "    return len(relevant_retrieved) / min(k, len(retrieved_docs))\n",
    "\n",
    "\n",
    "def average_precision(retrieved_docs: List[str], relevant_docs: Set[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Average Precision (AP)\n",
    "    \"\"\"\n",
    "    if not relevant_docs:\n",
    "        return 0.0\n",
    "\n",
    "    ap_sum = 0.0\n",
    "    relevant_count = 0\n",
    "\n",
    "    for i, doc_id in enumerate(retrieved_docs):\n",
    "        if doc_id in relevant_docs:\n",
    "            relevant_count += 1\n",
    "            precision_at_i = relevant_count / (i + 1)\n",
    "            ap_sum += precision_at_i\n",
    "\n",
    "    return ap_sum / len(relevant_docs) if relevant_docs else 0.0\n",
    "\n",
    "\n",
    "def ndcg_at_k(retrieved_docs: List[str], relevant_docs: Set[str], k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain at k\n",
    "    Simplified version: relevant=1, irrelevant=0\n",
    "    \"\"\"\n",
    "    if not relevant_docs or k == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate DCG@k\n",
    "    dcg = 0.0\n",
    "    for i, doc_id in enumerate(retrieved_docs[:k]):\n",
    "        if doc_id in relevant_docs:\n",
    "            dcg += 1.0 / np.log2(i + 2)  # i+2 because log2(1) is undefined\n",
    "\n",
    "    # Calculate IDCG@k (ideal DCG)\n",
    "    idcg = 0.0\n",
    "    for i in range(min(k, len(relevant_docs))):\n",
    "        idcg += 1.0 / np.log2(i + 2)\n",
    "\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "\n",
    "def coverage(\n",
    "    all_retrieved_docs: List[List[str]], total_relevant_docs: Set[str]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate coverage: proportion of all relevant docs retrieved across all queries\n",
    "    \"\"\"\n",
    "    if not total_relevant_docs:\n",
    "        return 0.0\n",
    "\n",
    "    all_retrieved = set()\n",
    "    for retrieved_list in all_retrieved_docs:\n",
    "        all_retrieved.update(retrieved_list)\n",
    "\n",
    "    covered_relevant = all_retrieved.intersection(total_relevant_docs)\n",
    "    return len(covered_relevant) / len(total_relevant_docs)\n",
    "\n",
    "\n",
    "class RetrievalEvaluator:\n",
    "    \"\"\"Main evaluation class\"\"\"\n",
    "\n",
    "    def __init__(self, retriever, eval_queries, k_values=[1, 3, 5, 10]):\n",
    "        self.retriever = retriever\n",
    "        self.eval_queries = eval_queries\n",
    "        self.k_values = k_values\n",
    "\n",
    "    def evaluate(self, max_k=10) -> Dict[str, Any]:\n",
    "        \"\"\"Run complete evaluation\"\"\"\n",
    "        results = {\"query_results\": [], \"aggregate_metrics\": {}, \"by_category\": {}}\n",
    "\n",
    "        all_retrieved = []\n",
    "        all_relevant = set()\n",
    "\n",
    "        print(\"Running retrieval evaluation...\")\n",
    "\n",
    "        for query_data in self.eval_queries:\n",
    "            query_id = query_data[\"query_id\"]\n",
    "            query = query_data[\"query\"]\n",
    "            relevant_docs = query_data[\"relevant_doc_ids\"]\n",
    "            category = query_data[\"category\"]\n",
    "\n",
    "            # Collect all relevant docs\n",
    "            all_relevant.update(relevant_docs)\n",
    "\n",
    "            # Retrieve documents\n",
    "            retrieved_results = self.retriever.retrieve(query, k=max_k)\n",
    "            retrieved_doc_ids = [doc_id for doc_id, score in retrieved_results]\n",
    "            all_retrieved.append(retrieved_doc_ids)\n",
    "\n",
    "            # Calculate metrics for this query\n",
    "            query_metrics = {\n",
    "                \"query_id\": query_id,\n",
    "                \"category\": category,\n",
    "                \"num_relevant\": len(relevant_docs),\n",
    "            }\n",
    "\n",
    "            # Recall@k, Precision@k, nDCG@k for different k values\n",
    "            for k in self.k_values:\n",
    "                query_metrics[f\"recall@{k}\"] = recall_at_k(\n",
    "                    retrieved_doc_ids, relevant_docs, k\n",
    "                )\n",
    "                query_metrics[f\"precision@{k}\"] = precision_at_k(\n",
    "                    retrieved_doc_ids, relevant_docs, k\n",
    "                )\n",
    "                query_metrics[f\"ndcg@{k}\"] = ndcg_at_k(\n",
    "                    retrieved_doc_ids, relevant_docs, k\n",
    "                )\n",
    "\n",
    "            # Average Precision\n",
    "            query_metrics[\"average_precision\"] = average_precision(\n",
    "                retrieved_doc_ids, relevant_docs\n",
    "            )\n",
    "\n",
    "            results[\"query_results\"].append(query_metrics)\n",
    "\n",
    "        # Calculate aggregate metrics\n",
    "        df = pd.DataFrame(results[\"query_results\"])\n",
    "\n",
    "        aggregate = {}\n",
    "        for k in self.k_values:\n",
    "            aggregate[f\"mean_recall@{k}\"] = df[f\"recall@{k}\"].mean()\n",
    "            aggregate[f\"mean_precision@{k}\"] = df[f\"precision@{k}\"].mean()\n",
    "            aggregate[f\"mean_ndcg@{k}\"] = df[f\"ndcg@{k}\"].mean()\n",
    "\n",
    "        aggregate[\"mean_average_precision\"] = df[\"average_precision\"].mean()\n",
    "        aggregate[\"coverage\"] = coverage(all_retrieved, all_relevant)\n",
    "\n",
    "        results[\"aggregate_metrics\"] = aggregate\n",
    "\n",
    "        # By category analysis\n",
    "        by_category = {}\n",
    "        for category in df[\"category\"].unique():\n",
    "            cat_df = df[df[\"category\"] == category]\n",
    "            by_category[category] = {\n",
    "                f\"mean_recall@{k}\": cat_df[f\"recall@{k}\"].mean() for k in self.k_values\n",
    "            }\n",
    "\n",
    "        results[\"by_category\"] = by_category\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "print(\"Retrieval metrics implementation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5eb2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 6: Baseline Retrieval Evaluation\n",
    "# =============================================================================\n",
    "\n",
    "# Run baseline evaluation\n",
    "evaluator = RetrievalEvaluator(retriever, eval_queries)\n",
    "baseline_results = evaluator.evaluate()\n",
    "\n",
    "# Display results\n",
    "print(\"=== Baseline Retrieval Results ===\")\n",
    "print(\n",
    "    f\"Mean Average Precision: {baseline_results['aggregate_metrics']['mean_average_precision']:.3f}\"\n",
    ")\n",
    "print(f\"Coverage: {baseline_results['aggregate_metrics']['coverage']:.3f}\")\n",
    "print()\n",
    "\n",
    "print(\"Recall@k:\")\n",
    "for k in [1, 3, 5, 10]:\n",
    "    recall = baseline_results[\"aggregate_metrics\"][f\"mean_recall@{k}\"]\n",
    "    print(f\"  Recall@{k}: {recall:.3f}\")\n",
    "\n",
    "print(\"\\nPrecision@k:\")\n",
    "for k in [1, 3, 5, 10]:\n",
    "    precision = baseline_results[\"aggregate_metrics\"][f\"mean_precision@{k}\"]\n",
    "    print(f\"  Precision@{k}: {precision:.3f}\")\n",
    "\n",
    "print(\"\\nnDCG@k:\")\n",
    "for k in [1, 3, 5, 10]:\n",
    "    ndcg = baseline_results[\"aggregate_metrics\"][f\"mean_ndcg@{k}\"]\n",
    "    print(f\"  nDCG@{k}: {ndcg:.3f}\")\n",
    "\n",
    "# Query-level results\n",
    "query_df = pd.DataFrame(baseline_results[\"query_results\"])\n",
    "print(f\"\\nQuery-level results saved to DataFrame with {len(query_df)} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c58f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 7: Hybrid & Reranker Comparison (Simplified)\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def simulate_hybrid_retrieval(query: str, k: int = 10) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Simulate hybrid retrieval (BM25 + Vector)\n",
    "    For demo purposes, we'll add some noise to simulate BM25 contribution\n",
    "    \"\"\"\n",
    "    # Get baseline vector results\n",
    "    vector_results = retriever.retrieve(query, k=k * 2)  # Get more candidates\n",
    "\n",
    "    # Simulate BM25 scores (keyword matching simulation)\n",
    "    import random\n",
    "\n",
    "    random.seed(42)  # For reproducibility\n",
    "\n",
    "    hybrid_results = []\n",
    "    for doc_id, vector_score in vector_results:\n",
    "        # Simulate BM25 component\n",
    "        bm25_sim = (\n",
    "            random.uniform(0.1, 0.8)\n",
    "            if any(\n",
    "                word\n",
    "                in retriever.documents[\n",
    "                    next(\n",
    "                        i\n",
    "                        for i, d in enumerate(retriever.documents)\n",
    "                        if d[\"doc_id\"] == doc_id\n",
    "                    )\n",
    "                ][\"text\"].lower()\n",
    "                for word in query.lower().split()\n",
    "            )\n",
    "            else random.uniform(0.0, 0.3)\n",
    "        )\n",
    "\n",
    "        # Combine scores (alpha=0.7 for vector, 0.3 for BM25)\n",
    "        hybrid_score = 0.7 * vector_score + 0.3 * bm25_sim\n",
    "        hybrid_results.append((doc_id, hybrid_score))\n",
    "\n",
    "    # Sort by hybrid score and return top-k\n",
    "    hybrid_results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return hybrid_results[:k]\n",
    "\n",
    "\n",
    "def simulate_reranked_retrieval(query: str, k: int = 10) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Simulate reranked retrieval\n",
    "    For demo purposes, we'll boost scores of truly relevant documents\n",
    "    \"\"\"\n",
    "    # Get baseline results\n",
    "    vector_results = retriever.retrieve(query, k=k * 2)\n",
    "\n",
    "    # Find which query this is to get relevant docs\n",
    "    relevant_docs = set()\n",
    "    for q_data in eval_queries:\n",
    "        if q_data[\"query\"] == query:\n",
    "            relevant_docs = q_data[\"relevant_doc_ids\"]\n",
    "            break\n",
    "\n",
    "    # Simulate reranker boosting relevant documents\n",
    "    reranked_results = []\n",
    "    for doc_id, score in vector_results:\n",
    "        if doc_id in relevant_docs:\n",
    "            # Boost relevant documents\n",
    "            boosted_score = min(1.0, score * 1.2 + 0.1)\n",
    "        else:\n",
    "            # Slightly penalize irrelevant documents\n",
    "            boosted_score = score * 0.95\n",
    "        reranked_results.append((doc_id, boosted_score))\n",
    "\n",
    "    # Sort and return top-k\n",
    "    reranked_results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return reranked_results[:k]\n",
    "\n",
    "\n",
    "# Create comparison retrievers\n",
    "class HybridRetriever:\n",
    "    def retrieve(self, query: str, k: int = 10):\n",
    "        return simulate_hybrid_retrieval(query, k)\n",
    "\n",
    "\n",
    "class RerankedRetriever:\n",
    "    def retrieve(self, query: str, k: int = 10):\n",
    "        return simulate_reranked_retrieval(query, k)\n",
    "\n",
    "\n",
    "# Evaluate different retrieval strategies\n",
    "strategies = {\n",
    "    \"Vector\": retriever,\n",
    "    \"Hybrid\": HybridRetriever(),\n",
    "    \"Reranked\": RerankedRetriever(),\n",
    "}\n",
    "\n",
    "comparison_results = {}\n",
    "for strategy_name, strategy_retriever in strategies.items():\n",
    "    print(f\"Evaluating {strategy_name} retrieval...\")\n",
    "    evaluator = RetrievalEvaluator(strategy_retriever, eval_queries)\n",
    "    comparison_results[strategy_name] = evaluator.evaluate()\n",
    "\n",
    "# Display comparison\n",
    "print(\"\\n=== Retrieval Strategy Comparison ===\")\n",
    "metrics_to_compare = [\"mean_recall@5\", \"mean_precision@5\", \"mean_ndcg@5\", \"coverage\"]\n",
    "\n",
    "comparison_df = pd.DataFrame(\n",
    "    {\n",
    "        strategy: [\n",
    "            results[\"aggregate_metrics\"][metric] for metric in metrics_to_compare\n",
    "        ]\n",
    "        for strategy, results in comparison_results.items()\n",
    "    },\n",
    "    index=metrics_to_compare,\n",
    ")\n",
    "\n",
    "print(comparison_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb4fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 8: Results Visualization & Export\n",
    "# =============================================================================\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle(\"Retrieval Metrics Comparison\", fontsize=16)\n",
    "\n",
    "# 1. Recall@k comparison\n",
    "k_values = [1, 3, 5, 10]\n",
    "for strategy in strategies.keys():\n",
    "    recall_values = [\n",
    "        comparison_results[strategy][\"aggregate_metrics\"][f\"mean_recall@{k}\"]\n",
    "        for k in k_values\n",
    "    ]\n",
    "    axes[0, 0].plot(k_values, recall_values, marker=\"o\", label=strategy)\n",
    "axes[0, 0].set_title(\"Recall@k\")\n",
    "axes[0, 0].set_xlabel(\"k\")\n",
    "axes[0, 0].set_ylabel(\"Recall\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Precision@k comparison\n",
    "for strategy in strategies.keys():\n",
    "    precision_values = [\n",
    "        comparison_results[strategy][\"aggregate_metrics\"][f\"mean_precision@{k}\"]\n",
    "        for k in k_values\n",
    "    ]\n",
    "    axes[0, 1].plot(k_values, precision_values, marker=\"s\", label=strategy)\n",
    "axes[0, 1].set_title(\"Precision@k\")\n",
    "axes[0, 1].set_xlabel(\"k\")\n",
    "axes[0, 1].set_ylabel(\"Precision\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. nDCG@k comparison\n",
    "for strategy in strategies.keys():\n",
    "    ndcg_values = [\n",
    "        comparison_results[strategy][\"aggregate_metrics\"][f\"mean_ndcg@{k}\"]\n",
    "        for k in k_values\n",
    "    ]\n",
    "    axes[1, 0].plot(k_values, ndcg_values, marker=\"^\", label=strategy)\n",
    "axes[1, 0].set_title(\"nDCG@k\")\n",
    "axes[1, 0].set_xlabel(\"k\")\n",
    "axes[1, 0].set_ylabel(\"nDCG\")\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Overall metrics bar chart\n",
    "overall_metrics = [\"mean_average_precision\", \"coverage\"]\n",
    "strategy_names = list(strategies.keys())\n",
    "x = np.arange(len(overall_metrics))\n",
    "width = 0.25\n",
    "\n",
    "for i, strategy in enumerate(strategy_names):\n",
    "    values = [\n",
    "        comparison_results[strategy][\"aggregate_metrics\"][metric]\n",
    "        for metric in overall_metrics\n",
    "    ]\n",
    "    axes[1, 1].bar(x + i * width, values, width, label=strategy)\n",
    "\n",
    "axes[1, 1].set_title(\"Overall Metrics\")\n",
    "axes[1, 1].set_xlabel(\"Metrics\")\n",
    "axes[1, 1].set_ylabel(\"Score\")\n",
    "axes[1, 1].set_xticks(x + width)\n",
    "axes[1, 1].set_xticklabels([\"MAP\", \"Coverage\"])\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    \"outs/charts/retrieval_metrics_comparison.png\", dpi=300, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Export detailed results to CSV\n",
    "detailed_results = []\n",
    "for strategy_name, results in comparison_results.items():\n",
    "    for query_result in results[\"query_results\"]:\n",
    "        row = {\"strategy\": strategy_name}\n",
    "        row.update(query_result)\n",
    "        detailed_results.append(row)\n",
    "\n",
    "detailed_df = pd.DataFrame(detailed_results)\n",
    "detailed_df.to_csv(\"outs/eval/retrieval_metrics_detailed.csv\", index=False)\n",
    "\n",
    "# Export aggregate results\n",
    "aggregate_df = pd.DataFrame(\n",
    "    {\n",
    "        strategy: results[\"aggregate_metrics\"]\n",
    "        for strategy, results in comparison_results.items()\n",
    "    }\n",
    ").T\n",
    "\n",
    "aggregate_df.to_csv(\"outs/eval/retrieval_metrics_aggregate.csv\")\n",
    "\n",
    "# Export category analysis\n",
    "category_data = []\n",
    "for strategy_name, results in comparison_results.items():\n",
    "    for category, metrics in results[\"by_category\"].items():\n",
    "        row = {\"strategy\": strategy_name, \"category\": category}\n",
    "        row.update(metrics)\n",
    "        category_data.append(row)\n",
    "\n",
    "category_df = pd.DataFrame(category_data)\n",
    "category_df.to_csv(\"outs/eval/retrieval_metrics_by_category.csv\", index=False)\n",
    "\n",
    "print(\"Results exported to:\")\n",
    "print(\"- outs/eval/retrieval_metrics_detailed.csv\")\n",
    "print(\"- outs/eval/retrieval_metrics_aggregate.csv\")\n",
    "print(\"- outs/eval/retrieval_metrics_by_category.csv\")\n",
    "print(\"- outs/charts/retrieval_metrics_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8108c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 9: Smoke Test\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def smoke_test_metrics():\n",
    "    \"\"\"Quick smoke test to verify metrics calculations\"\"\"\n",
    "    print(\"=== Smoke Test: Metrics Calculation ===\")\n",
    "\n",
    "    # Test data\n",
    "    retrieved = [\"doc1\", \"doc2\", \"doc3\", \"doc4\", \"doc5\"]\n",
    "    relevant = {\"doc1\", \"doc3\", \"doc5\"}\n",
    "\n",
    "    # Test Recall@k\n",
    "    recall_3 = recall_at_k(retrieved, relevant, 3)\n",
    "    expected_recall_3 = 2 / 3  # doc1, doc3 found out of 3 relevant\n",
    "    assert (\n",
    "        abs(recall_3 - expected_recall_3) < 1e-6\n",
    "    ), f\"Recall@3 failed: {recall_3} != {expected_recall_3}\"\n",
    "\n",
    "    # Test Precision@k\n",
    "    precision_3 = precision_at_k(retrieved, relevant, 3)\n",
    "    expected_precision_3 = 2 / 3  # 2 relevant out of 3 retrieved\n",
    "    assert (\n",
    "        abs(precision_3 - expected_precision_3) < 1e-6\n",
    "    ), f\"Precision@3 failed: {precision_3} != {expected_precision_3}\"\n",
    "\n",
    "    # Test nDCG@k\n",
    "    ndcg_5 = ndcg_at_k(retrieved, relevant, 5)\n",
    "    assert 0 <= ndcg_5 <= 1, f\"nDCG@5 out of range: {ndcg_5}\"\n",
    "\n",
    "    # Test AP\n",
    "    ap = average_precision(retrieved, relevant)\n",
    "    assert 0 <= ap <= 1, f\"AP out of range: {ap}\"\n",
    "\n",
    "    print(\"âœ“ Recall@k calculation correct\")\n",
    "    print(\"âœ“ Precision@k calculation correct\")\n",
    "    print(\"âœ“ nDCG@k calculation correct\")\n",
    "    print(\"âœ“ Average Precision calculation correct\")\n",
    "    print(\"âœ“ All metrics smoke tests passed!\")\n",
    "\n",
    "    # Test with actual evaluation data\n",
    "    print(f\"\\n=== Live Test Results ===\")\n",
    "    sample_query = eval_queries[0]\n",
    "    sample_results = retriever.retrieve(sample_query[\"query\"], k=5)\n",
    "    sample_retrieved = [doc_id for doc_id, _ in sample_results]\n",
    "    sample_relevant = sample_query[\"relevant_doc_ids\"]\n",
    "\n",
    "    print(f\"Query: {sample_query['query']}\")\n",
    "    print(f\"Retrieved: {sample_retrieved}\")\n",
    "    print(f\"Relevant: {sample_relevant}\")\n",
    "    print(f\"Recall@5: {recall_at_k(sample_retrieved, sample_relevant, 5):.3f}\")\n",
    "    print(f\"Precision@5: {precision_at_k(sample_retrieved, sample_relevant, 5):.3f}\")\n",
    "    print(f\"nDCG@5: {ndcg_at_k(sample_retrieved, sample_relevant, 5):.3f}\")\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_metrics()\n",
    "\n",
    "print(\"\\nðŸŽ‰ nb60: Retrieval Metrics Evaluation completed successfully!\")\n",
    "print(f\"ðŸ“Š Results exported to outs/eval/ and outs/charts/\")\n",
    "print(\n",
    "    f\"ðŸ“ˆ Baseline Mean Recall@5: {baseline_results['aggregate_metrics']['mean_recall@5']:.3f}\"\n",
    ")\n",
    "print(\n",
    "    f\"ðŸ“ˆ Best strategy Recall@5: {max(comparison_results[s]['aggregate_metrics']['mean_recall@5'] for s in strategies):.3f}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
