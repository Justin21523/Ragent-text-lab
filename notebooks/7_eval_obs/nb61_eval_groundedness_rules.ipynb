{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1908dddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb61_eval_groundedness_rules.ipynb\n",
    "# Stage 7: Evaluation & Observability\n",
    "# Goal: Rule-based + semantic groundedness evaluation for RAG answers\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (è¤‡è£½åˆ°æ¯æœ¬ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cc9053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 2: Import & Setup\n",
    "# =============================================================================\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import jieba\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Setup output directory\n",
    "pathlib.Path(\"outs/eval\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GroundednessResult:\n",
    "    \"\"\"Container for groundedness evaluation results\"\"\"\n",
    "\n",
    "    query: str\n",
    "    answer: str\n",
    "    sources: List[str]\n",
    "    jaccard_score: float\n",
    "    containment_score: float\n",
    "    semantic_score: float\n",
    "    combined_score: float\n",
    "    is_grounded: bool\n",
    "    details: Dict\n",
    "\n",
    "\n",
    "print(\"âœ… Imports and setup complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05b01a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 3: Sample RAG Q&A Dataset Preparation\n",
    "# =============================================================================\n",
    "\n",
    "# Create sample RAG evaluation dataset\n",
    "sample_qa_data = [\n",
    "    {\n",
    "        \"query\": \"ä»€éº¼æ˜¯ Transformer æž¶æ§‹çš„æ ¸å¿ƒå‰µæ–°ï¼Ÿ\",\n",
    "        \"answer\": \"Transformer æž¶æ§‹çš„æ ¸å¿ƒå‰µæ–°æ˜¯è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶ï¼ˆself-attention mechanismï¼‰ï¼Œå®ƒè®“æ¨¡åž‹èƒ½å¤ åŒæ™‚é—œæ³¨åºåˆ—ä¸­çš„æ‰€æœ‰ä½ç½®ï¼Œæ•æ‰é•·è·é›¢ä¾è³´é—œä¿‚ã€‚[1][2]\",\n",
    "        \"sources\": [\n",
    "            \"Transformer æ˜¯åŸºæ–¼æ³¨æ„åŠ›æ©Ÿåˆ¶çš„ç¥žç¶“ç¶²è·¯æž¶æ§‹ï¼Œç”± Vaswani ç­‰äººåœ¨ 2017 å¹´æå‡ºã€‚å…¶æ ¸å¿ƒæ˜¯è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶ï¼Œèƒ½å¤ è¨ˆç®—åºåˆ—ä¸­ä»»æ„å…©å€‹ä½ç½®ä¹‹é–“çš„ä¾è³´é—œä¿‚ã€‚\",\n",
    "            \"è‡ªæ³¨æ„åŠ›å…è¨±æ¨¡åž‹åœ¨è™•ç†æ¯å€‹è©žæ™‚ï¼ŒåŒæ™‚è€ƒæ…®æ•´å€‹åºåˆ—çš„ä¿¡æ¯ï¼Œé€™ä½¿å¾— Transformer èƒ½å¤ æ•æ‰é•·è·é›¢çš„èªžç¾©ä¾è³´ã€‚\",\n",
    "            \"ç›¸æ¯”æ–¼ RNN å’Œ CNNï¼ŒTransformer çš„ä¸¦è¡Œè¨ˆç®—èƒ½åŠ›æ›´å¼·ï¼Œè¨“ç·´æ•ˆçŽ‡æ›´é«˜ã€‚\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"RAG ç³»çµ±çš„ä¸»è¦çµ„ä»¶æœ‰å“ªäº›ï¼Ÿ\",\n",
    "        \"answer\": \"RAG ç³»çµ±ä¸»è¦åŒ…å«æª¢ç´¢å™¨ï¼ˆretrieverï¼‰ã€ç”Ÿæˆå™¨ï¼ˆgeneratorï¼‰å’ŒçŸ¥è­˜åº«ä¸‰å€‹æ ¸å¿ƒçµ„ä»¶ã€‚æª¢ç´¢å™¨è² è²¬å¾žçŸ¥è­˜åº«ä¸­æ‰¾åˆ°ç›¸é—œæ–‡æª”ï¼Œç”Ÿæˆå™¨å‰‡åŸºæ–¼æª¢ç´¢åˆ°çš„ä¿¡æ¯ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆã€‚[1]\",\n",
    "        \"sources\": [\n",
    "            \"æª¢ç´¢å¢žå¼·ç”Ÿæˆï¼ˆRAGï¼‰ç³»çµ±çµåˆäº†æª¢ç´¢å’Œç”Ÿæˆå…©å€‹æ­¥é©Ÿã€‚é¦–å…ˆç”¨æª¢ç´¢å™¨å¾žå¤§åž‹çŸ¥è­˜åº«ä¸­æ‰¾åˆ°ç›¸é—œæ–‡æª”ç‰‡æ®µã€‚\",\n",
    "            \"ç„¶å¾Œå°‡æª¢ç´¢åˆ°çš„æ–‡æª”èˆ‡åŽŸå§‹æŸ¥è©¢ä¸€èµ·è¼¸å…¥åˆ°ç”Ÿæˆæ¨¡åž‹ä¸­ï¼Œç”Ÿæˆæœ€çµ‚çš„å›žç­”ã€‚\",\n",
    "            \"RAG çš„å„ªå‹¢åœ¨æ–¼èƒ½å¤ åˆ©ç”¨å¤–éƒ¨çŸ¥è­˜ï¼Œæä¾›æ›´æº–ç¢ºå’Œæ™‚æ•ˆæ€§æ›´å¼·çš„ç­”æ¡ˆã€‚\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"æ·±åº¦å­¸ç¿’çš„ç™¼å±•æ­·å²å¦‚ä½•ï¼Ÿ\",\n",
    "        \"answer\": \"æ·±åº¦å­¸ç¿’èµ·æºæ–¼ 1940 å¹´ä»£çš„äººå·¥ç¥žç¶“ç¶²è·¯ç ”ç©¶ï¼Œç¶“æ­·äº†å¤šæ¬¡èµ·ä¼ã€‚2006 å¹´ Hinton æå‡ºæ·±åº¦ä¿¡å¿µç¶²è·¯ï¼Œ2012 å¹´ AlexNet åœ¨ ImageNet ä¸Šçš„çªç ´æ€§è¡¨ç¾æ¨™èªŒè‘—æ·±åº¦å­¸ç¿’æ™‚ä»£çš„åˆ°ä¾†ã€‚\",\n",
    "        \"sources\": [\n",
    "            \"Transformer æž¶æ§‹åœ¨è‡ªç„¶èªžè¨€è™•ç†é ˜åŸŸå–å¾—äº†é‡å¤§çªç ´ï¼ŒBERT å’Œ GPT ç³»åˆ—æ¨¡åž‹éƒ½åŸºæ–¼é€™ä¸€æž¶æ§‹ã€‚\",\n",
    "            \"å·ç©ç¥žç¶“ç¶²è·¯ï¼ˆCNNï¼‰åœ¨è¨ˆç®—æ©Ÿè¦–è¦ºä»»å‹™ä¸­è¡¨ç¾å„ªç•°ï¼ŒResNet è§£æ±ºäº†æ·±å±¤ç¶²è·¯çš„æ¢¯åº¦æ¶ˆå¤±å•é¡Œã€‚\",\n",
    "            \"å¾ªç’°ç¥žç¶“ç¶²è·¯ï¼ˆRNNï¼‰é©åˆè™•ç†åºåˆ—æ•¸æ“šï¼ŒLSTM å’Œ GRU æ˜¯å…¶é‡è¦è®Šç¨®ã€‚\",\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"âœ… Prepared {len(sample_qa_data)} sample Q&A pairs for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74b07bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 4: Rule-based Groundedness Metrics\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class RuleBasedGroundedness:\n",
    "    \"\"\"Rule-based groundedness evaluation using lexical overlap\"\"\"\n",
    "\n",
    "    def __init__(self, language=\"zh\"):\n",
    "        self.language = language\n",
    "\n",
    "    def tokenize_chinese(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize Chinese text using jieba\"\"\"\n",
    "        # Remove citations [1], [2] etc.\n",
    "        text = re.sub(r\"\\[\\d+\\]\", \"\", text)\n",
    "        # Remove punctuation and normalize\n",
    "        text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip().lower()\n",
    "\n",
    "        if self.language == \"zh\":\n",
    "            tokens = list(jieba.cut(text))\n",
    "            # Filter out single characters and stopwords\n",
    "            tokens = [t for t in tokens if len(t) > 1 and t.strip()]\n",
    "        else:\n",
    "            tokens = text.split()\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def jaccard_similarity(\n",
    "        self, answer_tokens: List[str], source_tokens: List[str]\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate Jaccard similarity between answer and source tokens\"\"\"\n",
    "        set_a = set(answer_tokens)\n",
    "        set_s = set(source_tokens)\n",
    "\n",
    "        if not set_a and not set_s:\n",
    "            return 1.0\n",
    "        if not set_a or not set_s:\n",
    "            return 0.0\n",
    "\n",
    "        intersection = len(set_a & set_s)\n",
    "        union = len(set_a | set_s)\n",
    "\n",
    "        return intersection / union if union > 0 else 0.0\n",
    "\n",
    "    def containment_score(\n",
    "        self, answer_tokens: List[str], source_tokens: List[str]\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate what percentage of answer tokens are contained in sources\"\"\"\n",
    "        if not answer_tokens:\n",
    "            return 1.0\n",
    "\n",
    "        set_a = set(answer_tokens)\n",
    "        set_s = set(source_tokens)\n",
    "\n",
    "        contained = len(set_a & set_s)\n",
    "        return contained / len(set_a) if len(set_a) > 0 else 0.0\n",
    "\n",
    "    def evaluate_against_sources(self, answer: str, sources: List[str]) -> Dict:\n",
    "        \"\"\"Evaluate answer groundedness against multiple sources\"\"\"\n",
    "        answer_tokens = self.tokenize_chinese(answer)\n",
    "        all_source_tokens = []\n",
    "\n",
    "        # Combine all source tokens\n",
    "        for source in sources:\n",
    "            source_tokens = self.tokenize_chinese(source)\n",
    "            all_source_tokens.extend(source_tokens)\n",
    "\n",
    "        # Calculate metrics\n",
    "        jaccard = self.jaccard_similarity(answer_tokens, all_source_tokens)\n",
    "        containment = self.containment_score(answer_tokens, all_source_tokens)\n",
    "\n",
    "        # Per-source analysis\n",
    "        source_scores = []\n",
    "        for i, source in enumerate(sources):\n",
    "            source_tokens = self.tokenize_chinese(source)\n",
    "            src_jaccard = self.jaccard_similarity(answer_tokens, source_tokens)\n",
    "            src_containment = self.containment_score(answer_tokens, source_tokens)\n",
    "            source_scores.append(\n",
    "                {\"source_id\": i, \"jaccard\": src_jaccard, \"containment\": src_containment}\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"jaccard_score\": jaccard,\n",
    "            \"containment_score\": containment,\n",
    "            \"answer_tokens\": answer_tokens,\n",
    "            \"source_tokens\": all_source_tokens,\n",
    "            \"source_scores\": source_scores,\n",
    "            \"num_answer_tokens\": len(answer_tokens),\n",
    "            \"num_source_tokens\": len(all_source_tokens),\n",
    "        }\n",
    "\n",
    "\n",
    "# Test rule-based evaluation\n",
    "rule_eval = RuleBasedGroundedness()\n",
    "\n",
    "print(\"ðŸ§ª Testing rule-based groundedness on sample data:\")\n",
    "for i, item in enumerate(sample_qa_data[:2]):\n",
    "    result = rule_eval.evaluate_against_sources(item[\"answer\"], item[\"sources\"])\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Jaccard: {result['jaccard_score']:.3f}\")\n",
    "    print(f\"  Containment: {result['containment_score']:.3f}\")\n",
    "    print(f\"  Answer tokens: {result['num_answer_tokens']}\")\n",
    "    print(f\"  Source tokens: {result['num_source_tokens']}\")\n",
    "\n",
    "print(\"âœ… Rule-based groundedness evaluation ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2966752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 5: Semantic Similarity Groundedness (BGE-M3)\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class SemanticGroundedness:\n",
    "    \"\"\"Semantic groundedness evaluation using embeddings\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"BAAI/bge-m3\", device: str = \"auto\"):\n",
    "        print(f\"Loading embedding model: {model_name}\")\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "        print(f\"âœ… Model loaded on device: {self.model.device}\")\n",
    "\n",
    "    def split_into_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into sentences for fine-grained comparison\"\"\"\n",
    "        # Remove citations\n",
    "        text = re.sub(r\"\\[\\d+\\]\", \"\", text)\n",
    "        # Split by Chinese punctuation\n",
    "        sentences = re.split(r\"[ã€‚ï¼ï¼Ÿï¼›]\", text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        return sentences\n",
    "\n",
    "    def calculate_semantic_similarity(self, answer: str, sources: List[str]) -> Dict:\n",
    "        \"\"\"Calculate semantic similarity between answer and sources\"\"\"\n",
    "        # Split answer into sentences\n",
    "        answer_sentences = self.split_into_sentences(answer)\n",
    "        if not answer_sentences:\n",
    "            return {\"overall_score\": 0.0, \"details\": []}\n",
    "\n",
    "        # Combine all sources\n",
    "        all_sources_text = \" \".join(sources)\n",
    "        source_sentences = self.split_into_sentences(all_sources_text)\n",
    "\n",
    "        if not source_sentences:\n",
    "            return {\"overall_score\": 0.0, \"details\": []}\n",
    "\n",
    "        # Encode sentences\n",
    "        answer_embeddings = self.model.encode(\n",
    "            answer_sentences, normalize_embeddings=True\n",
    "        )\n",
    "        source_embeddings = self.model.encode(\n",
    "            source_sentences, normalize_embeddings=True\n",
    "        )\n",
    "\n",
    "        # Calculate similarities\n",
    "        sentence_scores = []\n",
    "        for i, ans_emb in enumerate(answer_embeddings):\n",
    "            # Find best matching source sentence\n",
    "            similarities = np.dot(source_embeddings, ans_emb)\n",
    "            best_match_idx = np.argmax(similarities)\n",
    "            best_score = similarities[best_match_idx]\n",
    "\n",
    "            sentence_scores.append(\n",
    "                {\n",
    "                    \"answer_sentence\": answer_sentences[i],\n",
    "                    \"best_match_source\": source_sentences[best_match_idx],\n",
    "                    \"similarity\": float(best_score),\n",
    "                    \"answer_sent_id\": i,\n",
    "                    \"source_sent_id\": best_match_idx,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Overall score: average of all sentence similarities\n",
    "        overall_score = np.mean([s[\"similarity\"] for s in sentence_scores])\n",
    "\n",
    "        return {\n",
    "            \"overall_score\": float(overall_score),\n",
    "            \"sentence_details\": sentence_scores,\n",
    "            \"num_answer_sentences\": len(answer_sentences),\n",
    "            \"num_source_sentences\": len(source_sentences),\n",
    "        }\n",
    "\n",
    "\n",
    "# Load semantic evaluation model (low VRAM mode)\n",
    "try:\n",
    "    semantic_eval = SemanticGroundedness(\"BAAI/bge-m3\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ GPU model loading failed: {e}\")\n",
    "    print(\"ðŸ”„ Fallback to CPU mode...\")\n",
    "    semantic_eval = SemanticGroundedness(\"BAAI/bge-small-zh-v1.5\", device=\"cpu\")\n",
    "\n",
    "print(\"ðŸ§ª Testing semantic groundedness:\")\n",
    "for i, item in enumerate(sample_qa_data[:2]):\n",
    "    result = semantic_eval.calculate_semantic_similarity(\n",
    "        item[\"answer\"], item[\"sources\"]\n",
    "    )\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Semantic score: {result['overall_score']:.3f}\")\n",
    "    print(f\"  Answer sentences: {result['num_answer_sentences']}\")\n",
    "    print(f\"  Source sentences: {result['num_source_sentences']}\")\n",
    "\n",
    "print(\"âœ… Semantic groundedness evaluation ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a554579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 6: Combined Groundedness Score\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class GroundednessEvaluator:\n",
    "    \"\"\"Combined groundedness evaluator with configurable weights\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        rule_weight: float = 0.4,\n",
    "        semantic_weight: float = 0.6,\n",
    "        threshold: float = 0.6,\n",
    "    ):\n",
    "        self.rule_evaluator = RuleBasedGroundedness()\n",
    "        self.semantic_evaluator = semantic_eval  # Use loaded model\n",
    "        self.rule_weight = rule_weight\n",
    "        self.semantic_weight = semantic_weight\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def evaluate(\n",
    "        self, query: str, answer: str, sources: List[str]\n",
    "    ) -> GroundednessResult:\n",
    "        \"\"\"Comprehensive groundedness evaluation\"\"\"\n",
    "\n",
    "        # Rule-based evaluation\n",
    "        rule_result = self.rule_evaluator.evaluate_against_sources(answer, sources)\n",
    "\n",
    "        # Semantic evaluation\n",
    "        semantic_result = self.semantic_evaluator.calculate_semantic_similarity(\n",
    "            answer, sources\n",
    "        )\n",
    "\n",
    "        # Combined score calculation\n",
    "        # Use average of jaccard and containment for rule score\n",
    "        rule_score = (\n",
    "            rule_result[\"jaccard_score\"] + rule_result[\"containment_score\"]\n",
    "        ) / 2\n",
    "        semantic_score = semantic_result[\"overall_score\"]\n",
    "\n",
    "        combined_score = (\n",
    "            self.rule_weight * rule_score + self.semantic_weight * semantic_score\n",
    "        )\n",
    "\n",
    "        is_grounded = combined_score >= self.threshold\n",
    "\n",
    "        # Detailed results\n",
    "        details = {\n",
    "            \"rule_based\": rule_result,\n",
    "            \"semantic\": semantic_result,\n",
    "            \"weights\": {\n",
    "                \"rule_weight\": self.rule_weight,\n",
    "                \"semantic_weight\": self.semantic_weight,\n",
    "            },\n",
    "            \"component_scores\": {\n",
    "                \"rule_score\": rule_score,\n",
    "                \"semantic_score\": semantic_score,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        return GroundednessResult(\n",
    "            query=query,\n",
    "            answer=answer,\n",
    "            sources=sources,\n",
    "            jaccard_score=rule_result[\"jaccard_score\"],\n",
    "            containment_score=rule_result[\"containment_score\"],\n",
    "            semantic_score=semantic_score,\n",
    "            combined_score=combined_score,\n",
    "            is_grounded=is_grounded,\n",
    "            details=details,\n",
    "        )\n",
    "\n",
    "\n",
    "# Initialize combined evaluator\n",
    "evaluator = GroundednessEvaluator(rule_weight=0.4, semantic_weight=0.6, threshold=0.6)\n",
    "\n",
    "print(\"ðŸ§ª Testing combined groundedness evaluation:\")\n",
    "for i, item in enumerate(sample_qa_data):\n",
    "    result = evaluator.evaluate(item[\"query\"], item[\"answer\"], item[\"sources\"])\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Query: {item['query'][:50]}...\")\n",
    "    print(f\"  Jaccard: {result.jaccard_score:.3f}\")\n",
    "    print(f\"  Containment: {result.containment_score:.3f}\")\n",
    "    print(f\"  Semantic: {result.semantic_score:.3f}\")\n",
    "    print(f\"  Combined: {result.combined_score:.3f}\")\n",
    "    print(f\"  Grounded: {'âœ…' if result.is_grounded else 'âŒ'}\")\n",
    "\n",
    "print(\"âœ… Combined groundedness evaluator ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d55286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 7: Batch Evaluation & Analysis\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def run_batch_evaluation(\n",
    "    evaluator: GroundednessEvaluator, qa_data: List[Dict]\n",
    ") -> List[GroundednessResult]:\n",
    "    \"\"\"Run groundedness evaluation on batch of Q&A pairs\"\"\"\n",
    "    results = []\n",
    "\n",
    "    print(f\"ðŸ”„ Running batch evaluation on {len(qa_data)} samples...\")\n",
    "\n",
    "    for i, item in enumerate(qa_data):\n",
    "        try:\n",
    "            result = evaluator.evaluate(item[\"query\"], item[\"answer\"], item[\"sources\"])\n",
    "            results.append(result)\n",
    "\n",
    "            if (i + 1) % 5 == 0:\n",
    "                print(f\"  Processed {i + 1}/{len(qa_data)} samples\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error processing sample {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def analyze_results(results: List[GroundednessResult]) -> Dict:\n",
    "    \"\"\"Analyze batch evaluation results\"\"\"\n",
    "    if not results:\n",
    "        return {}\n",
    "\n",
    "    scores = {\n",
    "        \"jaccard\": [r.jaccard_score for r in results],\n",
    "        \"containment\": [r.containment_score for r in results],\n",
    "        \"semantic\": [r.semantic_score for r in results],\n",
    "        \"combined\": [r.combined_score for r in results],\n",
    "    }\n",
    "\n",
    "    grounded_count = sum(1 for r in results if r.is_grounded)\n",
    "    grounded_rate = grounded_count / len(results)\n",
    "\n",
    "    analysis = {\n",
    "        \"total_samples\": len(results),\n",
    "        \"grounded_count\": grounded_count,\n",
    "        \"grounded_rate\": grounded_rate,\n",
    "        \"score_stats\": {},\n",
    "    }\n",
    "\n",
    "    for metric, values in scores.items():\n",
    "        analysis[\"score_stats\"][metric] = {\n",
    "            \"mean\": np.mean(values),\n",
    "            \"std\": np.std(values),\n",
    "            \"min\": np.min(values),\n",
    "            \"max\": np.max(values),\n",
    "            \"median\": np.median(values),\n",
    "        }\n",
    "\n",
    "    return analysis\n",
    "\n",
    "\n",
    "# Run batch evaluation\n",
    "batch_results = run_batch_evaluation(evaluator, sample_qa_data)\n",
    "analysis = analyze_results(batch_results)\n",
    "\n",
    "print(f\"\\nðŸ“Š Batch Evaluation Results:\")\n",
    "print(f\"  Total samples: {analysis['total_samples']}\")\n",
    "print(f\"  Grounded samples: {analysis['grounded_count']}\")\n",
    "print(f\"  Grounded rate: {analysis['grounded_rate']:.1%}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Score Statistics:\")\n",
    "for metric, stats in analysis[\"score_stats\"].items():\n",
    "    print(\n",
    "        f\"  {metric.capitalize()}: Î¼={stats['mean']:.3f}, Ïƒ={stats['std']:.3f}, range=[{stats['min']:.3f}, {stats['max']:.3f}]\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0491ff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 8: Visualization & Low-score Case Study\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def plot_groundedness_distribution(results: List[GroundednessResult]):\n",
    "    \"\"\"Create visualization of groundedness scores\"\"\"\n",
    "\n",
    "    # Extract scores\n",
    "    scores_df = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"Jaccard\": r.jaccard_score,\n",
    "                \"Containment\": r.containment_score,\n",
    "                \"Semantic\": r.semantic_score,\n",
    "                \"Combined\": r.combined_score,\n",
    "                \"Grounded\": r.is_grounded,\n",
    "            }\n",
    "            for r in results\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle(\"Groundedness Score Distributions\", fontsize=16)\n",
    "\n",
    "    # Score distributions\n",
    "    metrics = [\"Jaccard\", \"Containment\", \"Semantic\", \"Combined\"]\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i // 2, i % 2]\n",
    "\n",
    "        # Histogram\n",
    "        ax.hist(\n",
    "            scores_df[metric], bins=15, alpha=0.7, color=\"skyblue\", edgecolor=\"black\"\n",
    "        )\n",
    "        ax.axvline(\n",
    "            scores_df[metric].mean(),\n",
    "            color=\"red\",\n",
    "            linestyle=\"--\",\n",
    "            label=f\"Mean: {scores_df[metric].mean():.3f}\",\n",
    "        )\n",
    "\n",
    "        if metric == \"Combined\":\n",
    "            ax.axvline(\n",
    "                0.6, color=\"orange\", linestyle=\"-\", linewidth=2, label=\"Threshold: 0.6\"\n",
    "            )\n",
    "\n",
    "        ax.set_title(f\"{metric} Score Distribution\")\n",
    "        ax.set_xlabel(\"Score\")\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"outs/eval/groundedness_distribution.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # Correlation heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    correlation = scores_df[metrics].corr()\n",
    "    sns.heatmap(\n",
    "        correlation, annot=True, cmap=\"coolwarm\", center=0, square=True, fmt=\".3f\"\n",
    "    )\n",
    "    plt.title(\"Groundedness Metrics Correlation\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"outs/eval/groundedness_correlation.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def identify_low_score_cases(\n",
    "    results: List[GroundednessResult], threshold: float = 0.5\n",
    ") -> List[GroundednessResult]:\n",
    "    \"\"\"Identify and analyze low groundedness score cases\"\"\"\n",
    "\n",
    "    low_score_cases = [r for r in results if r.combined_score < threshold]\n",
    "\n",
    "    print(f\"\\nðŸ” Low Score Case Analysis (threshold < {threshold}):\")\n",
    "    print(f\"  Found {len(low_score_cases)} low-score cases out of {len(results)} total\")\n",
    "\n",
    "    for i, case in enumerate(low_score_cases):\n",
    "        print(f\"\\n--- Case {i+1} ---\")\n",
    "        print(f\"Query: {case.query}\")\n",
    "        print(f\"Answer: {case.answer[:100]}...\")\n",
    "        print(f\"Combined Score: {case.combined_score:.3f}\")\n",
    "        print(f\"  - Jaccard: {case.jaccard_score:.3f}\")\n",
    "        print(f\"  - Containment: {case.containment_score:.3f}\")\n",
    "        print(f\"  - Semantic: {case.semantic_score:.3f}\")\n",
    "\n",
    "        # Show problematic elements\n",
    "        rule_details = case.details[\"rule_based\"]\n",
    "        print(f\"Answer tokens: {rule_details['num_answer_tokens']}\")\n",
    "        print(f\"Source tokens: {rule_details['num_source_tokens']}\")\n",
    "\n",
    "    return low_score_cases\n",
    "\n",
    "\n",
    "# Create visualizations\n",
    "if batch_results:\n",
    "    plot_groundedness_distribution(batch_results)\n",
    "    low_cases = identify_low_score_cases(batch_results, threshold=0.4)\n",
    "\n",
    "print(\"âœ… Visualization and case analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791475e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 9: Integration with RAG Pipeline\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class RAGWithGroundednessCheck:\n",
    "    \"\"\"RAG pipeline with integrated groundedness checking\"\"\"\n",
    "\n",
    "    def __init__(self, groundedness_evaluator: GroundednessEvaluator):\n",
    "        self.evaluator = groundedness_evaluator\n",
    "        self.quality_log = []\n",
    "\n",
    "    def generate_with_check(\n",
    "        self,\n",
    "        query: str,\n",
    "        retrieved_sources: List[str],\n",
    "        generated_answer: str,\n",
    "        min_groundedness: float = 0.5,\n",
    "    ) -> Dict:\n",
    "        \"\"\"Generate answer with groundedness checking\"\"\"\n",
    "\n",
    "        # Evaluate groundedness\n",
    "        result = self.evaluator.evaluate(query, generated_answer, retrieved_sources)\n",
    "\n",
    "        # Determine action based on groundedness\n",
    "        if result.combined_score >= min_groundedness:\n",
    "            action = \"accept\"\n",
    "            final_answer = generated_answer\n",
    "        else:\n",
    "            action = \"reject\"\n",
    "            final_answer = f\"âš ï¸ å›žç­”å¯ä¿¡åº¦ä¸è¶³ (åˆ†æ•¸: {result.combined_score:.2f}), å»ºè­°é‡æ–°æª¢ç´¢æˆ–äººå·¥æ ¸å¯¦ã€‚\"\n",
    "\n",
    "        # Log quality metrics\n",
    "        quality_entry = {\n",
    "            \"query\": query,\n",
    "            \"groundedness_score\": result.combined_score,\n",
    "            \"action\": action,\n",
    "            \"timestamp\": pd.Timestamp.now(),\n",
    "            \"component_scores\": {\n",
    "                \"jaccard\": result.jaccard_score,\n",
    "                \"containment\": result.containment_score,\n",
    "                \"semantic\": result.semantic_score,\n",
    "            },\n",
    "        }\n",
    "        self.quality_log.append(quality_entry)\n",
    "\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"original_answer\": generated_answer,\n",
    "            \"final_answer\": final_answer,\n",
    "            \"groundedness_result\": result,\n",
    "            \"action\": action,\n",
    "            \"quality_score\": result.combined_score,\n",
    "        }\n",
    "\n",
    "    def get_quality_summary(self) -> Dict:\n",
    "        \"\"\"Get summary of quality metrics\"\"\"\n",
    "        if not self.quality_log:\n",
    "            return {}\n",
    "\n",
    "        df = pd.DataFrame(self.quality_log)\n",
    "\n",
    "        return {\n",
    "            \"total_queries\": len(df),\n",
    "            \"accepted_rate\": (df[\"action\"] == \"accept\").mean(),\n",
    "            \"mean_groundedness\": df[\"groundedness_score\"].mean(),\n",
    "            \"low_quality_count\": (df[\"groundedness_score\"] < 0.5).sum(),\n",
    "            \"quality_trend\": df.groupby(df[\"timestamp\"].dt.hour)[\"groundedness_score\"]\n",
    "            .mean()\n",
    "            .to_dict(),\n",
    "        }\n",
    "\n",
    "\n",
    "# Simulate RAG pipeline with groundedness checking\n",
    "rag_pipeline = RAGWithGroundednessCheck(evaluator)\n",
    "\n",
    "print(\"ðŸ§ª Testing RAG pipeline with groundedness checking:\")\n",
    "\n",
    "# Simulate some RAG responses\n",
    "test_cases = [\n",
    "    {\n",
    "        \"query\": \"ä»€éº¼æ˜¯æ·±åº¦å­¸ç¿’ï¼Ÿ\",\n",
    "        \"sources\": [\n",
    "            \"æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’çš„å­é ˜åŸŸï¼Œä½¿ç”¨å¤šå±¤ç¥žç¶“ç¶²è·¯ä¾†å­¸ç¿’æ•¸æ“šè¡¨ç¤ºã€‚\",\n",
    "            \"æ·±åº¦å­¸ç¿’åœ¨åœ–åƒè­˜åˆ¥ã€è‡ªç„¶èªžè¨€è™•ç†ç­‰é ˜åŸŸå–å¾—çªç ´ã€‚\",\n",
    "        ],\n",
    "        \"answer\": \"æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’çš„é‡è¦åˆ†æ”¯ï¼Œé€šéŽå¤šå±¤ç¥žç¶“ç¶²è·¯ä¾†è‡ªå‹•å­¸ç¿’æ•¸æ“šçš„è¤‡é›œè¡¨ç¤ºå’Œæ¨¡å¼ã€‚[1]\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"å€å¡Šéˆçš„æ‡‰ç”¨é ˜åŸŸï¼Ÿ\",\n",
    "        \"sources\": [\n",
    "            \"Transformer æž¶æ§‹revolutionizedè‡ªç„¶èªžè¨€è™•ç†\",\n",
    "            \"BERT å’Œ GPT éƒ½åŸºæ–¼ Transformer\",\n",
    "        ],\n",
    "        \"answer\": \"å€å¡Šéˆå»£æ³›æ‡‰ç”¨æ–¼é‡‘èžã€ä¾›æ‡‰éˆç®¡ç†ã€æ•¸å­—èº«ä»½èªè­‰ç­‰å¤šå€‹é ˜åŸŸï¼Œå…·æœ‰åŽ»ä¸­å¿ƒåŒ–å’Œä¸å¯ç¯¡æ”¹çš„ç‰¹æ€§ã€‚\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for i, case in enumerate(test_cases):\n",
    "    result = rag_pipeline.generate_with_check(\n",
    "        case[\"query\"], case[\"sources\"], case[\"answer\"], min_groundedness=0.6\n",
    "    )\n",
    "\n",
    "    print(f\"\\n--- Test Case {i+1} ---\")\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(f\"Quality Score: {result['quality_score']:.3f}\")\n",
    "    print(f\"Action: {result['action']}\")\n",
    "    print(f\"Final Answer: {result['final_answer'][:100]}...\")\n",
    "\n",
    "# Quality summary\n",
    "quality_summary = rag_pipeline.get_quality_summary()\n",
    "print(f\"\\nðŸ“Š Pipeline Quality Summary:\")\n",
    "print(f\"  Total queries: {quality_summary['total_queries']}\")\n",
    "print(f\"  Acceptance rate: {quality_summary['accepted_rate']:.1%}\")\n",
    "print(f\"  Mean groundedness: {quality_summary['mean_groundedness']:.3f}\")\n",
    "\n",
    "print(\"âœ… RAG pipeline integration complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68bb42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 10: Smoke Test & Export Results\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def export_evaluation_results(\n",
    "    results: List[GroundednessResult],\n",
    "    analysis: Dict,\n",
    "    output_path: str = \"outs/eval/groundedness_results.json\",\n",
    "):\n",
    "    \"\"\"Export evaluation results to JSON\"\"\"\n",
    "\n",
    "    export_data = {\n",
    "        \"metadata\": {\n",
    "            \"evaluation_timestamp\": pd.Timestamp.now().isoformat(),\n",
    "            \"total_samples\": len(results),\n",
    "            \"evaluator_config\": {\n",
    "                \"rule_weight\": evaluator.rule_weight,\n",
    "                \"semantic_weight\": evaluator.semantic_weight,\n",
    "                \"threshold\": evaluator.threshold,\n",
    "            },\n",
    "        },\n",
    "        \"summary\": analysis,\n",
    "        \"detailed_results\": [],\n",
    "    }\n",
    "\n",
    "    # Add detailed results\n",
    "    for result in results:\n",
    "        export_data[\"detailed_results\"].append(\n",
    "            {\n",
    "                \"query\": result.query,\n",
    "                \"answer\": result.answer,\n",
    "                \"sources\": result.sources,\n",
    "                \"scores\": {\n",
    "                    \"jaccard\": result.jaccard_score,\n",
    "                    \"containment\": result.containment_score,\n",
    "                    \"semantic\": result.semantic_score,\n",
    "                    \"combined\": result.combined_score,\n",
    "                },\n",
    "                \"is_grounded\": result.is_grounded,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Write to file\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(export_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"âœ… Results exported to {output_path}\")\n",
    "    return export_data\n",
    "\n",
    "\n",
    "def create_groundedness_report(\n",
    "    results: List[GroundednessResult],\n",
    "    analysis: Dict,\n",
    "    output_path: str = \"outs/eval/groundedness_report.md\",\n",
    "):\n",
    "    \"\"\"Create markdown report of groundedness evaluation\"\"\"\n",
    "\n",
    "    report = f\"\"\"# Groundedness Evaluation Report\n",
    "\n",
    "Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Total Samples**: {analysis['total_samples']}\n",
    "- **Grounded Samples**: {analysis['grounded_count']} ({analysis['grounded_rate']:.1%})\n",
    "- **Mean Combined Score**: {analysis['score_stats']['combined']['mean']:.3f}\n",
    "\n",
    "## Score Statistics\n",
    "\n",
    "| Metric | Mean | Std | Min | Max | Median |\n",
    "|--------|------|-----|-----|-----|--------|\n",
    "\"\"\"\n",
    "\n",
    "    for metric, stats in analysis[\"score_stats\"].items():\n",
    "        report += f\"| {metric.capitalize()} | {stats['mean']:.3f} | {stats['std']:.3f} | {stats['min']:.3f} | {stats['max']:.3f} | {stats['median']:.3f} |\\n\"\n",
    "\n",
    "    report += f\"\"\"\n",
    "## Configuration\n",
    "\n",
    "- **Rule Weight**: {evaluator.rule_weight}\n",
    "- **Semantic Weight**: {evaluator.semantic_weight}\n",
    "- **Threshold**: {evaluator.threshold}\n",
    "\n",
    "## Low Groundedness Cases\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    low_cases = [r for r in results if r.combined_score < 0.5]\n",
    "    for i, case in enumerate(low_cases[:3]):  # Show top 3 low cases\n",
    "        report += f\"\"\"\n",
    "### Case {i+1} (Score: {case.combined_score:.3f})\n",
    "\n",
    "**Query**: {case.query}\n",
    "\n",
    "**Answer**: {case.answer[:200]}...\n",
    "\n",
    "**Scores**: Jaccard={case.jaccard_score:.3f}, Containment={case.containment_score:.3f}, Semantic={case.semantic_score:.3f}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    report += \"\"\"\n",
    "## Recommendations\n",
    "\n",
    "1. **Rule-based vs Semantic**: Consider adjusting weights based on your use case\n",
    "2. **Threshold Tuning**: Current threshold may need adjustment based on domain requirements\n",
    "3. **Low Score Investigation**: Review cases below 0.5 for common patterns\n",
    "4. **Integration**: Consider adding groundedness checks to production RAG pipeline\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "    print(f\"âœ… Report generated: {output_path}\")\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "print(\"ðŸ§ª Running final smoke test...\")\n",
    "\n",
    "# Test single evaluation\n",
    "smoke_query = \"ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ\"\n",
    "smoke_answer = \"æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§çš„åˆ†æ”¯ï¼Œè®“é›»è…¦å¾žæ•¸æ“šä¸­è‡ªå‹•å­¸ç¿’æ¨¡å¼ã€‚[1]\"\n",
    "smoke_sources = [\"æ©Ÿå™¨å­¸ç¿’æ˜¯AIçš„é‡è¦çµ„æˆéƒ¨åˆ†ï¼Œé€šéŽç®—æ³•è®“æ©Ÿå™¨å¾žæ•¸æ“šä¸­å­¸ç¿’ä¸¦åšå‡ºé æ¸¬ã€‚\"]\n",
    "\n",
    "smoke_result = evaluator.evaluate(smoke_query, smoke_answer, smoke_sources)\n",
    "\n",
    "assert smoke_result.combined_score > 0, \"Smoke test failed: No score generated\"\n",
    "assert 0 <= smoke_result.combined_score <= 1, \"Smoke test failed: Score out of range\"\n",
    "assert isinstance(\n",
    "    smoke_result.is_grounded, bool\n",
    "), \"Smoke test failed: Invalid grounded flag\"\n",
    "\n",
    "print(f\"âœ… Smoke test passed!\")\n",
    "print(f\"  Sample score: {smoke_result.combined_score:.3f}\")\n",
    "print(f\"  Is grounded: {smoke_result.is_grounded}\")\n",
    "\n",
    "# Export results\n",
    "if batch_results and analysis:\n",
    "    export_data = export_evaluation_results(batch_results, analysis)\n",
    "    create_groundedness_report(batch_results, analysis)\n",
    "\n",
    "    # Create CSV for easy analysis\n",
    "    results_df = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"query\": r.query,\n",
    "                \"jaccard_score\": r.jaccard_score,\n",
    "                \"containment_score\": r.containment_score,\n",
    "                \"semantic_score\": r.semantic_score,\n",
    "                \"combined_score\": r.combined_score,\n",
    "                \"is_grounded\": r.is_grounded,\n",
    "                \"answer_length\": len(r.answer),\n",
    "                \"num_sources\": len(r.sources),\n",
    "            }\n",
    "            for r in batch_results\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    results_df.to_csv(\n",
    "        \"outs/eval/groundedness_scores.csv\", index=False, encoding=\"utf-8\"\n",
    "    )\n",
    "    print(\"âœ… CSV export complete: outs/eval/groundedness_scores.csv\")\n",
    "\n",
    "print(\n",
    "    \"\"\"\n",
    "ðŸŽ¯ Key Takeaways:\n",
    "1. Rule-based metrics (Jaccard, containment) catch lexical overlap\n",
    "2. Semantic similarity captures meaning alignment even without exact matches\n",
    "3. Combined scoring balances precision and recall\n",
    "4. Integration with RAG pipeline enables quality gating\n",
    "5. Threshold tuning critical for production deployment\n",
    "\n",
    "âš ï¸ Pitfalls to avoid:\n",
    "- Over-relying on lexical overlap for technical content\n",
    "- Ignoring citation misalignment in answers\n",
    "- Not accounting for paraphrasing in semantic evaluation\n",
    "- Setting thresholds without domain-specific validation\n",
    "\n",
    "ðŸ”„ Next steps:\n",
    "- Tune weights and thresholds for your specific domain\n",
    "- Add human evaluation benchmark for validation\n",
    "- Implement real-time groundedness monitoring\n",
    "- Consider query-type specific evaluation strategies\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"âœ… nb61_eval_groundedness_rules.ipynb complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a2c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick smoke test - run this cell independently to verify setup\n",
    "print(\"ðŸ”¥ Groundedness Evaluation Smoke Test\")\n",
    "\n",
    "# Minimal test case\n",
    "test_query = \"ä»€éº¼æ˜¯ Transformerï¼Ÿ\"\n",
    "test_answer = \"Transformer æ˜¯ä¸€ç¨®åŸºæ–¼æ³¨æ„åŠ›æ©Ÿåˆ¶çš„ç¥žç¶“ç¶²è·¯æž¶æ§‹ï¼Œç”¨æ–¼è‡ªç„¶èªžè¨€è™•ç†ã€‚[1]\"\n",
    "test_sources = [\n",
    "    \"Transformer æ˜¯ Google åœ¨ 2017 å¹´æå‡ºçš„ç¥žç¶“ç¶²è·¯æž¶æ§‹ï¼Œæ ¸å¿ƒæ˜¯è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶ã€‚\"\n",
    "]\n",
    "\n",
    "# Quick rule-based check\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "\n",
    "def quick_jaccard(answer, sources):\n",
    "    # Simple tokenization\n",
    "    ans_tokens = set(re.findall(r\"[\\w]+\", answer.lower()))\n",
    "    src_tokens = set(re.findall(r\"[\\w]+\", \" \".join(sources).lower()))\n",
    "\n",
    "    if not ans_tokens or not src_tokens:\n",
    "        return 0.0\n",
    "\n",
    "    intersection = len(ans_tokens & src_tokens)\n",
    "    union = len(ans_tokens | src_tokens)\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "\n",
    "score = quick_jaccard(test_answer, test_sources)\n",
    "print(f\"âœ… Quick Jaccard score: {score:.3f}\")\n",
    "print(f\"âœ… Score validation: {'PASS' if 0 <= score <= 1 else 'FAIL'}\")\n",
    "print(\"ðŸŽ¯ Ready for full evaluation pipeline!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
