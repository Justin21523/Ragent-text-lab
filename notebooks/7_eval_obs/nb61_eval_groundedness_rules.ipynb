{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1908dddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb61_eval_groundedness_rules.ipynb\n",
    "# Stage 7: Evaluation & Observability\n",
    "# Goal: Rule-based + semantic groundedness evaluation for RAG answers\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (複製到每本 notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cc9053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 2: Import & Setup\n",
    "# =============================================================================\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import jieba\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Setup output directory\n",
    "pathlib.Path(\"outs/eval\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GroundednessResult:\n",
    "    \"\"\"Container for groundedness evaluation results\"\"\"\n",
    "\n",
    "    query: str\n",
    "    answer: str\n",
    "    sources: List[str]\n",
    "    jaccard_score: float\n",
    "    containment_score: float\n",
    "    semantic_score: float\n",
    "    combined_score: float\n",
    "    is_grounded: bool\n",
    "    details: Dict\n",
    "\n",
    "\n",
    "print(\"✅ Imports and setup complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05b01a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 3: Sample RAG Q&A Dataset Preparation\n",
    "# =============================================================================\n",
    "\n",
    "# Create sample RAG evaluation dataset\n",
    "sample_qa_data = [\n",
    "    {\n",
    "        \"query\": \"什麼是 Transformer 架構的核心創新？\",\n",
    "        \"answer\": \"Transformer 架構的核心創新是自注意力機制（self-attention mechanism），它讓模型能夠同時關注序列中的所有位置，捕捉長距離依賴關係。[1][2]\",\n",
    "        \"sources\": [\n",
    "            \"Transformer 是基於注意力機制的神經網路架構，由 Vaswani 等人在 2017 年提出。其核心是自注意力機制，能夠計算序列中任意兩個位置之間的依賴關係。\",\n",
    "            \"自注意力允許模型在處理每個詞時，同時考慮整個序列的信息，這使得 Transformer 能夠捕捉長距離的語義依賴。\",\n",
    "            \"相比於 RNN 和 CNN，Transformer 的並行計算能力更強，訓練效率更高。\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"RAG 系統的主要組件有哪些？\",\n",
    "        \"answer\": \"RAG 系統主要包含檢索器（retriever）、生成器（generator）和知識庫三個核心組件。檢索器負責從知識庫中找到相關文檔，生成器則基於檢索到的信息生成最終答案。[1]\",\n",
    "        \"sources\": [\n",
    "            \"檢索增強生成（RAG）系統結合了檢索和生成兩個步驟。首先用檢索器從大型知識庫中找到相關文檔片段。\",\n",
    "            \"然後將檢索到的文檔與原始查詢一起輸入到生成模型中，生成最終的回答。\",\n",
    "            \"RAG 的優勢在於能夠利用外部知識，提供更準確和時效性更強的答案。\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"深度學習的發展歷史如何？\",\n",
    "        \"answer\": \"深度學習起源於 1940 年代的人工神經網路研究，經歷了多次起伏。2006 年 Hinton 提出深度信念網路，2012 年 AlexNet 在 ImageNet 上的突破性表現標誌著深度學習時代的到來。\",\n",
    "        \"sources\": [\n",
    "            \"Transformer 架構在自然語言處理領域取得了重大突破，BERT 和 GPT 系列模型都基於這一架構。\",\n",
    "            \"卷積神經網路（CNN）在計算機視覺任務中表現優異，ResNet 解決了深層網路的梯度消失問題。\",\n",
    "            \"循環神經網路（RNN）適合處理序列數據，LSTM 和 GRU 是其重要變種。\",\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"✅ Prepared {len(sample_qa_data)} sample Q&A pairs for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74b07bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 4: Rule-based Groundedness Metrics\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class RuleBasedGroundedness:\n",
    "    \"\"\"Rule-based groundedness evaluation using lexical overlap\"\"\"\n",
    "\n",
    "    def __init__(self, language=\"zh\"):\n",
    "        self.language = language\n",
    "\n",
    "    def tokenize_chinese(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize Chinese text using jieba\"\"\"\n",
    "        # Remove citations [1], [2] etc.\n",
    "        text = re.sub(r\"\\[\\d+\\]\", \"\", text)\n",
    "        # Remove punctuation and normalize\n",
    "        text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip().lower()\n",
    "\n",
    "        if self.language == \"zh\":\n",
    "            tokens = list(jieba.cut(text))\n",
    "            # Filter out single characters and stopwords\n",
    "            tokens = [t for t in tokens if len(t) > 1 and t.strip()]\n",
    "        else:\n",
    "            tokens = text.split()\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def jaccard_similarity(\n",
    "        self, answer_tokens: List[str], source_tokens: List[str]\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate Jaccard similarity between answer and source tokens\"\"\"\n",
    "        set_a = set(answer_tokens)\n",
    "        set_s = set(source_tokens)\n",
    "\n",
    "        if not set_a and not set_s:\n",
    "            return 1.0\n",
    "        if not set_a or not set_s:\n",
    "            return 0.0\n",
    "\n",
    "        intersection = len(set_a & set_s)\n",
    "        union = len(set_a | set_s)\n",
    "\n",
    "        return intersection / union if union > 0 else 0.0\n",
    "\n",
    "    def containment_score(\n",
    "        self, answer_tokens: List[str], source_tokens: List[str]\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate what percentage of answer tokens are contained in sources\"\"\"\n",
    "        if not answer_tokens:\n",
    "            return 1.0\n",
    "\n",
    "        set_a = set(answer_tokens)\n",
    "        set_s = set(source_tokens)\n",
    "\n",
    "        contained = len(set_a & set_s)\n",
    "        return contained / len(set_a) if len(set_a) > 0 else 0.0\n",
    "\n",
    "    def evaluate_against_sources(self, answer: str, sources: List[str]) -> Dict:\n",
    "        \"\"\"Evaluate answer groundedness against multiple sources\"\"\"\n",
    "        answer_tokens = self.tokenize_chinese(answer)\n",
    "        all_source_tokens = []\n",
    "\n",
    "        # Combine all source tokens\n",
    "        for source in sources:\n",
    "            source_tokens = self.tokenize_chinese(source)\n",
    "            all_source_tokens.extend(source_tokens)\n",
    "\n",
    "        # Calculate metrics\n",
    "        jaccard = self.jaccard_similarity(answer_tokens, all_source_tokens)\n",
    "        containment = self.containment_score(answer_tokens, all_source_tokens)\n",
    "\n",
    "        # Per-source analysis\n",
    "        source_scores = []\n",
    "        for i, source in enumerate(sources):\n",
    "            source_tokens = self.tokenize_chinese(source)\n",
    "            src_jaccard = self.jaccard_similarity(answer_tokens, source_tokens)\n",
    "            src_containment = self.containment_score(answer_tokens, source_tokens)\n",
    "            source_scores.append(\n",
    "                {\"source_id\": i, \"jaccard\": src_jaccard, \"containment\": src_containment}\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"jaccard_score\": jaccard,\n",
    "            \"containment_score\": containment,\n",
    "            \"answer_tokens\": answer_tokens,\n",
    "            \"source_tokens\": all_source_tokens,\n",
    "            \"source_scores\": source_scores,\n",
    "            \"num_answer_tokens\": len(answer_tokens),\n",
    "            \"num_source_tokens\": len(all_source_tokens),\n",
    "        }\n",
    "\n",
    "\n",
    "# Test rule-based evaluation\n",
    "rule_eval = RuleBasedGroundedness()\n",
    "\n",
    "print(\"🧪 Testing rule-based groundedness on sample data:\")\n",
    "for i, item in enumerate(sample_qa_data[:2]):\n",
    "    result = rule_eval.evaluate_against_sources(item[\"answer\"], item[\"sources\"])\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Jaccard: {result['jaccard_score']:.3f}\")\n",
    "    print(f\"  Containment: {result['containment_score']:.3f}\")\n",
    "    print(f\"  Answer tokens: {result['num_answer_tokens']}\")\n",
    "    print(f\"  Source tokens: {result['num_source_tokens']}\")\n",
    "\n",
    "print(\"✅ Rule-based groundedness evaluation ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2966752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 5: Semantic Similarity Groundedness (BGE-M3)\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class SemanticGroundedness:\n",
    "    \"\"\"Semantic groundedness evaluation using embeddings\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"BAAI/bge-m3\", device: str = \"auto\"):\n",
    "        print(f\"Loading embedding model: {model_name}\")\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "        print(f\"✅ Model loaded on device: {self.model.device}\")\n",
    "\n",
    "    def split_into_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into sentences for fine-grained comparison\"\"\"\n",
    "        # Remove citations\n",
    "        text = re.sub(r\"\\[\\d+\\]\", \"\", text)\n",
    "        # Split by Chinese punctuation\n",
    "        sentences = re.split(r\"[。！？；]\", text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        return sentences\n",
    "\n",
    "    def calculate_semantic_similarity(self, answer: str, sources: List[str]) -> Dict:\n",
    "        \"\"\"Calculate semantic similarity between answer and sources\"\"\"\n",
    "        # Split answer into sentences\n",
    "        answer_sentences = self.split_into_sentences(answer)\n",
    "        if not answer_sentences:\n",
    "            return {\"overall_score\": 0.0, \"details\": []}\n",
    "\n",
    "        # Combine all sources\n",
    "        all_sources_text = \" \".join(sources)\n",
    "        source_sentences = self.split_into_sentences(all_sources_text)\n",
    "\n",
    "        if not source_sentences:\n",
    "            return {\"overall_score\": 0.0, \"details\": []}\n",
    "\n",
    "        # Encode sentences\n",
    "        answer_embeddings = self.model.encode(\n",
    "            answer_sentences, normalize_embeddings=True\n",
    "        )\n",
    "        source_embeddings = self.model.encode(\n",
    "            source_sentences, normalize_embeddings=True\n",
    "        )\n",
    "\n",
    "        # Calculate similarities\n",
    "        sentence_scores = []\n",
    "        for i, ans_emb in enumerate(answer_embeddings):\n",
    "            # Find best matching source sentence\n",
    "            similarities = np.dot(source_embeddings, ans_emb)\n",
    "            best_match_idx = np.argmax(similarities)\n",
    "            best_score = similarities[best_match_idx]\n",
    "\n",
    "            sentence_scores.append(\n",
    "                {\n",
    "                    \"answer_sentence\": answer_sentences[i],\n",
    "                    \"best_match_source\": source_sentences[best_match_idx],\n",
    "                    \"similarity\": float(best_score),\n",
    "                    \"answer_sent_id\": i,\n",
    "                    \"source_sent_id\": best_match_idx,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Overall score: average of all sentence similarities\n",
    "        overall_score = np.mean([s[\"similarity\"] for s in sentence_scores])\n",
    "\n",
    "        return {\n",
    "            \"overall_score\": float(overall_score),\n",
    "            \"sentence_details\": sentence_scores,\n",
    "            \"num_answer_sentences\": len(answer_sentences),\n",
    "            \"num_source_sentences\": len(source_sentences),\n",
    "        }\n",
    "\n",
    "\n",
    "# Load semantic evaluation model (low VRAM mode)\n",
    "try:\n",
    "    semantic_eval = SemanticGroundedness(\"BAAI/bge-m3\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ GPU model loading failed: {e}\")\n",
    "    print(\"🔄 Fallback to CPU mode...\")\n",
    "    semantic_eval = SemanticGroundedness(\"BAAI/bge-small-zh-v1.5\", device=\"cpu\")\n",
    "\n",
    "print(\"🧪 Testing semantic groundedness:\")\n",
    "for i, item in enumerate(sample_qa_data[:2]):\n",
    "    result = semantic_eval.calculate_semantic_similarity(\n",
    "        item[\"answer\"], item[\"sources\"]\n",
    "    )\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Semantic score: {result['overall_score']:.3f}\")\n",
    "    print(f\"  Answer sentences: {result['num_answer_sentences']}\")\n",
    "    print(f\"  Source sentences: {result['num_source_sentences']}\")\n",
    "\n",
    "print(\"✅ Semantic groundedness evaluation ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a554579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 6: Combined Groundedness Score\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class GroundednessEvaluator:\n",
    "    \"\"\"Combined groundedness evaluator with configurable weights\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        rule_weight: float = 0.4,\n",
    "        semantic_weight: float = 0.6,\n",
    "        threshold: float = 0.6,\n",
    "    ):\n",
    "        self.rule_evaluator = RuleBasedGroundedness()\n",
    "        self.semantic_evaluator = semantic_eval  # Use loaded model\n",
    "        self.rule_weight = rule_weight\n",
    "        self.semantic_weight = semantic_weight\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def evaluate(\n",
    "        self, query: str, answer: str, sources: List[str]\n",
    "    ) -> GroundednessResult:\n",
    "        \"\"\"Comprehensive groundedness evaluation\"\"\"\n",
    "\n",
    "        # Rule-based evaluation\n",
    "        rule_result = self.rule_evaluator.evaluate_against_sources(answer, sources)\n",
    "\n",
    "        # Semantic evaluation\n",
    "        semantic_result = self.semantic_evaluator.calculate_semantic_similarity(\n",
    "            answer, sources\n",
    "        )\n",
    "\n",
    "        # Combined score calculation\n",
    "        # Use average of jaccard and containment for rule score\n",
    "        rule_score = (\n",
    "            rule_result[\"jaccard_score\"] + rule_result[\"containment_score\"]\n",
    "        ) / 2\n",
    "        semantic_score = semantic_result[\"overall_score\"]\n",
    "\n",
    "        combined_score = (\n",
    "            self.rule_weight * rule_score + self.semantic_weight * semantic_score\n",
    "        )\n",
    "\n",
    "        is_grounded = combined_score >= self.threshold\n",
    "\n",
    "        # Detailed results\n",
    "        details = {\n",
    "            \"rule_based\": rule_result,\n",
    "            \"semantic\": semantic_result,\n",
    "            \"weights\": {\n",
    "                \"rule_weight\": self.rule_weight,\n",
    "                \"semantic_weight\": self.semantic_weight,\n",
    "            },\n",
    "            \"component_scores\": {\n",
    "                \"rule_score\": rule_score,\n",
    "                \"semantic_score\": semantic_score,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        return GroundednessResult(\n",
    "            query=query,\n",
    "            answer=answer,\n",
    "            sources=sources,\n",
    "            jaccard_score=rule_result[\"jaccard_score\"],\n",
    "            containment_score=rule_result[\"containment_score\"],\n",
    "            semantic_score=semantic_score,\n",
    "            combined_score=combined_score,\n",
    "            is_grounded=is_grounded,\n",
    "            details=details,\n",
    "        )\n",
    "\n",
    "\n",
    "# Initialize combined evaluator\n",
    "evaluator = GroundednessEvaluator(rule_weight=0.4, semantic_weight=0.6, threshold=0.6)\n",
    "\n",
    "print(\"🧪 Testing combined groundedness evaluation:\")\n",
    "for i, item in enumerate(sample_qa_data):\n",
    "    result = evaluator.evaluate(item[\"query\"], item[\"answer\"], item[\"sources\"])\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Query: {item['query'][:50]}...\")\n",
    "    print(f\"  Jaccard: {result.jaccard_score:.3f}\")\n",
    "    print(f\"  Containment: {result.containment_score:.3f}\")\n",
    "    print(f\"  Semantic: {result.semantic_score:.3f}\")\n",
    "    print(f\"  Combined: {result.combined_score:.3f}\")\n",
    "    print(f\"  Grounded: {'✅' if result.is_grounded else '❌'}\")\n",
    "\n",
    "print(\"✅ Combined groundedness evaluator ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d55286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 7: Batch Evaluation & Analysis\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def run_batch_evaluation(\n",
    "    evaluator: GroundednessEvaluator, qa_data: List[Dict]\n",
    ") -> List[GroundednessResult]:\n",
    "    \"\"\"Run groundedness evaluation on batch of Q&A pairs\"\"\"\n",
    "    results = []\n",
    "\n",
    "    print(f\"🔄 Running batch evaluation on {len(qa_data)} samples...\")\n",
    "\n",
    "    for i, item in enumerate(qa_data):\n",
    "        try:\n",
    "            result = evaluator.evaluate(item[\"query\"], item[\"answer\"], item[\"sources\"])\n",
    "            results.append(result)\n",
    "\n",
    "            if (i + 1) % 5 == 0:\n",
    "                print(f\"  Processed {i + 1}/{len(qa_data)} samples\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing sample {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def analyze_results(results: List[GroundednessResult]) -> Dict:\n",
    "    \"\"\"Analyze batch evaluation results\"\"\"\n",
    "    if not results:\n",
    "        return {}\n",
    "\n",
    "    scores = {\n",
    "        \"jaccard\": [r.jaccard_score for r in results],\n",
    "        \"containment\": [r.containment_score for r in results],\n",
    "        \"semantic\": [r.semantic_score for r in results],\n",
    "        \"combined\": [r.combined_score for r in results],\n",
    "    }\n",
    "\n",
    "    grounded_count = sum(1 for r in results if r.is_grounded)\n",
    "    grounded_rate = grounded_count / len(results)\n",
    "\n",
    "    analysis = {\n",
    "        \"total_samples\": len(results),\n",
    "        \"grounded_count\": grounded_count,\n",
    "        \"grounded_rate\": grounded_rate,\n",
    "        \"score_stats\": {},\n",
    "    }\n",
    "\n",
    "    for metric, values in scores.items():\n",
    "        analysis[\"score_stats\"][metric] = {\n",
    "            \"mean\": np.mean(values),\n",
    "            \"std\": np.std(values),\n",
    "            \"min\": np.min(values),\n",
    "            \"max\": np.max(values),\n",
    "            \"median\": np.median(values),\n",
    "        }\n",
    "\n",
    "    return analysis\n",
    "\n",
    "\n",
    "# Run batch evaluation\n",
    "batch_results = run_batch_evaluation(evaluator, sample_qa_data)\n",
    "analysis = analyze_results(batch_results)\n",
    "\n",
    "print(f\"\\n📊 Batch Evaluation Results:\")\n",
    "print(f\"  Total samples: {analysis['total_samples']}\")\n",
    "print(f\"  Grounded samples: {analysis['grounded_count']}\")\n",
    "print(f\"  Grounded rate: {analysis['grounded_rate']:.1%}\")\n",
    "\n",
    "print(f\"\\n📈 Score Statistics:\")\n",
    "for metric, stats in analysis[\"score_stats\"].items():\n",
    "    print(\n",
    "        f\"  {metric.capitalize()}: μ={stats['mean']:.3f}, σ={stats['std']:.3f}, range=[{stats['min']:.3f}, {stats['max']:.3f}]\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0491ff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 8: Visualization & Low-score Case Study\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def plot_groundedness_distribution(results: List[GroundednessResult]):\n",
    "    \"\"\"Create visualization of groundedness scores\"\"\"\n",
    "\n",
    "    # Extract scores\n",
    "    scores_df = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"Jaccard\": r.jaccard_score,\n",
    "                \"Containment\": r.containment_score,\n",
    "                \"Semantic\": r.semantic_score,\n",
    "                \"Combined\": r.combined_score,\n",
    "                \"Grounded\": r.is_grounded,\n",
    "            }\n",
    "            for r in results\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle(\"Groundedness Score Distributions\", fontsize=16)\n",
    "\n",
    "    # Score distributions\n",
    "    metrics = [\"Jaccard\", \"Containment\", \"Semantic\", \"Combined\"]\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i // 2, i % 2]\n",
    "\n",
    "        # Histogram\n",
    "        ax.hist(\n",
    "            scores_df[metric], bins=15, alpha=0.7, color=\"skyblue\", edgecolor=\"black\"\n",
    "        )\n",
    "        ax.axvline(\n",
    "            scores_df[metric].mean(),\n",
    "            color=\"red\",\n",
    "            linestyle=\"--\",\n",
    "            label=f\"Mean: {scores_df[metric].mean():.3f}\",\n",
    "        )\n",
    "\n",
    "        if metric == \"Combined\":\n",
    "            ax.axvline(\n",
    "                0.6, color=\"orange\", linestyle=\"-\", linewidth=2, label=\"Threshold: 0.6\"\n",
    "            )\n",
    "\n",
    "        ax.set_title(f\"{metric} Score Distribution\")\n",
    "        ax.set_xlabel(\"Score\")\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"outs/eval/groundedness_distribution.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # Correlation heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    correlation = scores_df[metrics].corr()\n",
    "    sns.heatmap(\n",
    "        correlation, annot=True, cmap=\"coolwarm\", center=0, square=True, fmt=\".3f\"\n",
    "    )\n",
    "    plt.title(\"Groundedness Metrics Correlation\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"outs/eval/groundedness_correlation.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def identify_low_score_cases(\n",
    "    results: List[GroundednessResult], threshold: float = 0.5\n",
    ") -> List[GroundednessResult]:\n",
    "    \"\"\"Identify and analyze low groundedness score cases\"\"\"\n",
    "\n",
    "    low_score_cases = [r for r in results if r.combined_score < threshold]\n",
    "\n",
    "    print(f\"\\n🔍 Low Score Case Analysis (threshold < {threshold}):\")\n",
    "    print(f\"  Found {len(low_score_cases)} low-score cases out of {len(results)} total\")\n",
    "\n",
    "    for i, case in enumerate(low_score_cases):\n",
    "        print(f\"\\n--- Case {i+1} ---\")\n",
    "        print(f\"Query: {case.query}\")\n",
    "        print(f\"Answer: {case.answer[:100]}...\")\n",
    "        print(f\"Combined Score: {case.combined_score:.3f}\")\n",
    "        print(f\"  - Jaccard: {case.jaccard_score:.3f}\")\n",
    "        print(f\"  - Containment: {case.containment_score:.3f}\")\n",
    "        print(f\"  - Semantic: {case.semantic_score:.3f}\")\n",
    "\n",
    "        # Show problematic elements\n",
    "        rule_details = case.details[\"rule_based\"]\n",
    "        print(f\"Answer tokens: {rule_details['num_answer_tokens']}\")\n",
    "        print(f\"Source tokens: {rule_details['num_source_tokens']}\")\n",
    "\n",
    "    return low_score_cases\n",
    "\n",
    "\n",
    "# Create visualizations\n",
    "if batch_results:\n",
    "    plot_groundedness_distribution(batch_results)\n",
    "    low_cases = identify_low_score_cases(batch_results, threshold=0.4)\n",
    "\n",
    "print(\"✅ Visualization and case analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791475e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 9: Integration with RAG Pipeline\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class RAGWithGroundednessCheck:\n",
    "    \"\"\"RAG pipeline with integrated groundedness checking\"\"\"\n",
    "\n",
    "    def __init__(self, groundedness_evaluator: GroundednessEvaluator):\n",
    "        self.evaluator = groundedness_evaluator\n",
    "        self.quality_log = []\n",
    "\n",
    "    def generate_with_check(\n",
    "        self,\n",
    "        query: str,\n",
    "        retrieved_sources: List[str],\n",
    "        generated_answer: str,\n",
    "        min_groundedness: float = 0.5,\n",
    "    ) -> Dict:\n",
    "        \"\"\"Generate answer with groundedness checking\"\"\"\n",
    "\n",
    "        # Evaluate groundedness\n",
    "        result = self.evaluator.evaluate(query, generated_answer, retrieved_sources)\n",
    "\n",
    "        # Determine action based on groundedness\n",
    "        if result.combined_score >= min_groundedness:\n",
    "            action = \"accept\"\n",
    "            final_answer = generated_answer\n",
    "        else:\n",
    "            action = \"reject\"\n",
    "            final_answer = f\"⚠️ 回答可信度不足 (分數: {result.combined_score:.2f}), 建議重新檢索或人工核實。\"\n",
    "\n",
    "        # Log quality metrics\n",
    "        quality_entry = {\n",
    "            \"query\": query,\n",
    "            \"groundedness_score\": result.combined_score,\n",
    "            \"action\": action,\n",
    "            \"timestamp\": pd.Timestamp.now(),\n",
    "            \"component_scores\": {\n",
    "                \"jaccard\": result.jaccard_score,\n",
    "                \"containment\": result.containment_score,\n",
    "                \"semantic\": result.semantic_score,\n",
    "            },\n",
    "        }\n",
    "        self.quality_log.append(quality_entry)\n",
    "\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"original_answer\": generated_answer,\n",
    "            \"final_answer\": final_answer,\n",
    "            \"groundedness_result\": result,\n",
    "            \"action\": action,\n",
    "            \"quality_score\": result.combined_score,\n",
    "        }\n",
    "\n",
    "    def get_quality_summary(self) -> Dict:\n",
    "        \"\"\"Get summary of quality metrics\"\"\"\n",
    "        if not self.quality_log:\n",
    "            return {}\n",
    "\n",
    "        df = pd.DataFrame(self.quality_log)\n",
    "\n",
    "        return {\n",
    "            \"total_queries\": len(df),\n",
    "            \"accepted_rate\": (df[\"action\"] == \"accept\").mean(),\n",
    "            \"mean_groundedness\": df[\"groundedness_score\"].mean(),\n",
    "            \"low_quality_count\": (df[\"groundedness_score\"] < 0.5).sum(),\n",
    "            \"quality_trend\": df.groupby(df[\"timestamp\"].dt.hour)[\"groundedness_score\"]\n",
    "            .mean()\n",
    "            .to_dict(),\n",
    "        }\n",
    "\n",
    "\n",
    "# Simulate RAG pipeline with groundedness checking\n",
    "rag_pipeline = RAGWithGroundednessCheck(evaluator)\n",
    "\n",
    "print(\"🧪 Testing RAG pipeline with groundedness checking:\")\n",
    "\n",
    "# Simulate some RAG responses\n",
    "test_cases = [\n",
    "    {\n",
    "        \"query\": \"什麼是深度學習？\",\n",
    "        \"sources\": [\n",
    "            \"深度學習是機器學習的子領域，使用多層神經網路來學習數據表示。\",\n",
    "            \"深度學習在圖像識別、自然語言處理等領域取得突破。\",\n",
    "        ],\n",
    "        \"answer\": \"深度學習是機器學習的重要分支，通過多層神經網路來自動學習數據的複雜表示和模式。[1]\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"區塊鏈的應用領域？\",\n",
    "        \"sources\": [\n",
    "            \"Transformer 架構revolutionized自然語言處理\",\n",
    "            \"BERT 和 GPT 都基於 Transformer\",\n",
    "        ],\n",
    "        \"answer\": \"區塊鏈廣泛應用於金融、供應鏈管理、數字身份認證等多個領域，具有去中心化和不可篡改的特性。\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for i, case in enumerate(test_cases):\n",
    "    result = rag_pipeline.generate_with_check(\n",
    "        case[\"query\"], case[\"sources\"], case[\"answer\"], min_groundedness=0.6\n",
    "    )\n",
    "\n",
    "    print(f\"\\n--- Test Case {i+1} ---\")\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(f\"Quality Score: {result['quality_score']:.3f}\")\n",
    "    print(f\"Action: {result['action']}\")\n",
    "    print(f\"Final Answer: {result['final_answer'][:100]}...\")\n",
    "\n",
    "# Quality summary\n",
    "quality_summary = rag_pipeline.get_quality_summary()\n",
    "print(f\"\\n📊 Pipeline Quality Summary:\")\n",
    "print(f\"  Total queries: {quality_summary['total_queries']}\")\n",
    "print(f\"  Acceptance rate: {quality_summary['accepted_rate']:.1%}\")\n",
    "print(f\"  Mean groundedness: {quality_summary['mean_groundedness']:.3f}\")\n",
    "\n",
    "print(\"✅ RAG pipeline integration complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68bb42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 10: Smoke Test & Export Results\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def export_evaluation_results(\n",
    "    results: List[GroundednessResult],\n",
    "    analysis: Dict,\n",
    "    output_path: str = \"outs/eval/groundedness_results.json\",\n",
    "):\n",
    "    \"\"\"Export evaluation results to JSON\"\"\"\n",
    "\n",
    "    export_data = {\n",
    "        \"metadata\": {\n",
    "            \"evaluation_timestamp\": pd.Timestamp.now().isoformat(),\n",
    "            \"total_samples\": len(results),\n",
    "            \"evaluator_config\": {\n",
    "                \"rule_weight\": evaluator.rule_weight,\n",
    "                \"semantic_weight\": evaluator.semantic_weight,\n",
    "                \"threshold\": evaluator.threshold,\n",
    "            },\n",
    "        },\n",
    "        \"summary\": analysis,\n",
    "        \"detailed_results\": [],\n",
    "    }\n",
    "\n",
    "    # Add detailed results\n",
    "    for result in results:\n",
    "        export_data[\"detailed_results\"].append(\n",
    "            {\n",
    "                \"query\": result.query,\n",
    "                \"answer\": result.answer,\n",
    "                \"sources\": result.sources,\n",
    "                \"scores\": {\n",
    "                    \"jaccard\": result.jaccard_score,\n",
    "                    \"containment\": result.containment_score,\n",
    "                    \"semantic\": result.semantic_score,\n",
    "                    \"combined\": result.combined_score,\n",
    "                },\n",
    "                \"is_grounded\": result.is_grounded,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Write to file\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(export_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✅ Results exported to {output_path}\")\n",
    "    return export_data\n",
    "\n",
    "\n",
    "def create_groundedness_report(\n",
    "    results: List[GroundednessResult],\n",
    "    analysis: Dict,\n",
    "    output_path: str = \"outs/eval/groundedness_report.md\",\n",
    "):\n",
    "    \"\"\"Create markdown report of groundedness evaluation\"\"\"\n",
    "\n",
    "    report = f\"\"\"# Groundedness Evaluation Report\n",
    "\n",
    "Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Total Samples**: {analysis['total_samples']}\n",
    "- **Grounded Samples**: {analysis['grounded_count']} ({analysis['grounded_rate']:.1%})\n",
    "- **Mean Combined Score**: {analysis['score_stats']['combined']['mean']:.3f}\n",
    "\n",
    "## Score Statistics\n",
    "\n",
    "| Metric | Mean | Std | Min | Max | Median |\n",
    "|--------|------|-----|-----|-----|--------|\n",
    "\"\"\"\n",
    "\n",
    "    for metric, stats in analysis[\"score_stats\"].items():\n",
    "        report += f\"| {metric.capitalize()} | {stats['mean']:.3f} | {stats['std']:.3f} | {stats['min']:.3f} | {stats['max']:.3f} | {stats['median']:.3f} |\\n\"\n",
    "\n",
    "    report += f\"\"\"\n",
    "## Configuration\n",
    "\n",
    "- **Rule Weight**: {evaluator.rule_weight}\n",
    "- **Semantic Weight**: {evaluator.semantic_weight}\n",
    "- **Threshold**: {evaluator.threshold}\n",
    "\n",
    "## Low Groundedness Cases\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    low_cases = [r for r in results if r.combined_score < 0.5]\n",
    "    for i, case in enumerate(low_cases[:3]):  # Show top 3 low cases\n",
    "        report += f\"\"\"\n",
    "### Case {i+1} (Score: {case.combined_score:.3f})\n",
    "\n",
    "**Query**: {case.query}\n",
    "\n",
    "**Answer**: {case.answer[:200]}...\n",
    "\n",
    "**Scores**: Jaccard={case.jaccard_score:.3f}, Containment={case.containment_score:.3f}, Semantic={case.semantic_score:.3f}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    report += \"\"\"\n",
    "## Recommendations\n",
    "\n",
    "1. **Rule-based vs Semantic**: Consider adjusting weights based on your use case\n",
    "2. **Threshold Tuning**: Current threshold may need adjustment based on domain requirements\n",
    "3. **Low Score Investigation**: Review cases below 0.5 for common patterns\n",
    "4. **Integration**: Consider adding groundedness checks to production RAG pipeline\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "    print(f\"✅ Report generated: {output_path}\")\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "print(\"🧪 Running final smoke test...\")\n",
    "\n",
    "# Test single evaluation\n",
    "smoke_query = \"什麼是機器學習？\"\n",
    "smoke_answer = \"機器學習是人工智慧的分支，讓電腦從數據中自動學習模式。[1]\"\n",
    "smoke_sources = [\"機器學習是AI的重要組成部分，通過算法讓機器從數據中學習並做出預測。\"]\n",
    "\n",
    "smoke_result = evaluator.evaluate(smoke_query, smoke_answer, smoke_sources)\n",
    "\n",
    "assert smoke_result.combined_score > 0, \"Smoke test failed: No score generated\"\n",
    "assert 0 <= smoke_result.combined_score <= 1, \"Smoke test failed: Score out of range\"\n",
    "assert isinstance(\n",
    "    smoke_result.is_grounded, bool\n",
    "), \"Smoke test failed: Invalid grounded flag\"\n",
    "\n",
    "print(f\"✅ Smoke test passed!\")\n",
    "print(f\"  Sample score: {smoke_result.combined_score:.3f}\")\n",
    "print(f\"  Is grounded: {smoke_result.is_grounded}\")\n",
    "\n",
    "# Export results\n",
    "if batch_results and analysis:\n",
    "    export_data = export_evaluation_results(batch_results, analysis)\n",
    "    create_groundedness_report(batch_results, analysis)\n",
    "\n",
    "    # Create CSV for easy analysis\n",
    "    results_df = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"query\": r.query,\n",
    "                \"jaccard_score\": r.jaccard_score,\n",
    "                \"containment_score\": r.containment_score,\n",
    "                \"semantic_score\": r.semantic_score,\n",
    "                \"combined_score\": r.combined_score,\n",
    "                \"is_grounded\": r.is_grounded,\n",
    "                \"answer_length\": len(r.answer),\n",
    "                \"num_sources\": len(r.sources),\n",
    "            }\n",
    "            for r in batch_results\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    results_df.to_csv(\n",
    "        \"outs/eval/groundedness_scores.csv\", index=False, encoding=\"utf-8\"\n",
    "    )\n",
    "    print(\"✅ CSV export complete: outs/eval/groundedness_scores.csv\")\n",
    "\n",
    "print(\n",
    "    \"\"\"\n",
    "🎯 Key Takeaways:\n",
    "1. Rule-based metrics (Jaccard, containment) catch lexical overlap\n",
    "2. Semantic similarity captures meaning alignment even without exact matches\n",
    "3. Combined scoring balances precision and recall\n",
    "4. Integration with RAG pipeline enables quality gating\n",
    "5. Threshold tuning critical for production deployment\n",
    "\n",
    "⚠️ Pitfalls to avoid:\n",
    "- Over-relying on lexical overlap for technical content\n",
    "- Ignoring citation misalignment in answers\n",
    "- Not accounting for paraphrasing in semantic evaluation\n",
    "- Setting thresholds without domain-specific validation\n",
    "\n",
    "🔄 Next steps:\n",
    "- Tune weights and thresholds for your specific domain\n",
    "- Add human evaluation benchmark for validation\n",
    "- Implement real-time groundedness monitoring\n",
    "- Consider query-type specific evaluation strategies\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"✅ nb61_eval_groundedness_rules.ipynb complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a2c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick smoke test - run this cell independently to verify setup\n",
    "print(\"🔥 Groundedness Evaluation Smoke Test\")\n",
    "\n",
    "# Minimal test case\n",
    "test_query = \"什麼是 Transformer？\"\n",
    "test_answer = \"Transformer 是一種基於注意力機制的神經網路架構，用於自然語言處理。[1]\"\n",
    "test_sources = [\n",
    "    \"Transformer 是 Google 在 2017 年提出的神經網路架構，核心是自注意力機制。\"\n",
    "]\n",
    "\n",
    "# Quick rule-based check\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "\n",
    "def quick_jaccard(answer, sources):\n",
    "    # Simple tokenization\n",
    "    ans_tokens = set(re.findall(r\"[\\w]+\", answer.lower()))\n",
    "    src_tokens = set(re.findall(r\"[\\w]+\", \" \".join(sources).lower()))\n",
    "\n",
    "    if not ans_tokens or not src_tokens:\n",
    "        return 0.0\n",
    "\n",
    "    intersection = len(ans_tokens & src_tokens)\n",
    "    union = len(ans_tokens | src_tokens)\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "\n",
    "score = quick_jaccard(test_answer, test_sources)\n",
    "print(f\"✅ Quick Jaccard score: {score:.3f}\")\n",
    "print(f\"✅ Score validation: {'PASS' if 0 <= score <= 1 else 'FAIL'}\")\n",
    "print(\"🎯 Ready for full evaluation pipeline!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
