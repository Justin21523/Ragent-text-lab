{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d60fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb64 | å½™ç¸½å ±è¡¨èˆ‡æ’è¡Œæ¦œç³»çµ±\n",
    "# Goal: æ•´åˆå¤šç¶­åº¦è©•ä¼°æŒ‡æ¨™ï¼Œç”Ÿæˆæ¨¡å‹æ¯”è¼ƒæ’è¡Œæ¦œèˆ‡è¦–è¦ºåŒ–å ±è¡¨\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (è¤‡è£½åˆ°æ¯æœ¬ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fffbea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: è¼‰å…¥è©•ä¼°çµæœèˆ‡ä¾è³´\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Create output directories\n",
    "outs_dir = Path(\"outs\")\n",
    "outs_dir.mkdir(exist_ok=True)\n",
    "(outs_dir / \"reports\").mkdir(exist_ok=True)\n",
    "(outs_dir / \"charts\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“Š è¼‰å…¥è©•ä¼°çµæœæ¨¡çµ„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e4e17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: å ±è¡¨è³‡æ–™çµæ§‹è¨­è¨ˆ\n",
    "class EvalReportBuilder:\n",
    "    \"\"\"çµ±ä¸€è©•ä¼°å ±è¡¨å»ºæ§‹å™¨\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.metrics = {}\n",
    "        self.models = {}\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    def load_retrieval_metrics(self, filepath=\"outs/retrieval_metrics.json\"):\n",
    "        \"\"\"è¼‰å…¥æª¢ç´¢æŒ‡æ¨™ (nb60)\"\"\"\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            self.metrics[\"retrieval\"] = data\n",
    "            print(f\"âœ… è¼‰å…¥æª¢ç´¢æŒ‡æ¨™: {len(data.get('results', []))} é …çµæœ\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"âš ï¸  æª”æ¡ˆä¸å­˜åœ¨: {filepath}\")\n",
    "            self.metrics[\"retrieval\"] = {}\n",
    "\n",
    "    def load_groundedness_metrics(self, filepath=\"outs/groundedness_metrics.json\"):\n",
    "        \"\"\"è¼‰å…¥ Groundedness æŒ‡æ¨™ (nb61)\"\"\"\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            self.metrics[\"groundedness\"] = data\n",
    "            print(f\"âœ… è¼‰å…¥çœŸå¯¦æ€§æŒ‡æ¨™: {len(data.get('results', []))} é …çµæœ\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"âš ï¸  æª”æ¡ˆä¸å­˜åœ¨: {filepath}\")\n",
    "            self.metrics[\"groundedness\"] = {}\n",
    "\n",
    "    def load_quality_metrics(self, filepath=\"outs/text_quality_metrics.json\"):\n",
    "        \"\"\"è¼‰å…¥æ–‡æœ¬å“è³ªæŒ‡æ¨™ (nb62)\"\"\"\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            self.metrics[\"quality\"] = data\n",
    "            print(f\"âœ… è¼‰å…¥å“è³ªæŒ‡æ¨™: {len(data.get('results', []))} é …çµæœ\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"âš ï¸  æª”æ¡ˆä¸å­˜åœ¨: {filepath}\")\n",
    "            self.metrics[\"quality\"] = {}\n",
    "\n",
    "    def load_performance_metrics(self, filepath=\"outs/performance_baseline.json\"):\n",
    "        \"\"\"è¼‰å…¥æ•ˆèƒ½æŒ‡æ¨™ (nb63)\"\"\"\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            self.metrics[\"performance\"] = data\n",
    "            print(f\"âœ… è¼‰å…¥æ•ˆèƒ½æŒ‡æ¨™: {len(data.get('models', []))} å€‹æ¨¡å‹\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"âš ï¸  æª”æ¡ˆä¸å­˜åœ¨: {filepath}\")\n",
    "            self.metrics[\"performance\"] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8960b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: æª¢ç´¢æŒ‡æ¨™å½™ç¸½\n",
    "def aggregate_retrieval_metrics(retrieval_data):\n",
    "    \"\"\"å½™ç¸½æª¢ç´¢æŒ‡æ¨™\"\"\"\n",
    "    if not retrieval_data:\n",
    "        return {}\n",
    "\n",
    "    results = retrieval_data.get(\"results\", [])\n",
    "    if not results:\n",
    "        return {}\n",
    "\n",
    "    # Extract metrics from results\n",
    "    recall_at_5 = []\n",
    "    recall_at_10 = []\n",
    "    ndcg_scores = []\n",
    "\n",
    "    for result in results:\n",
    "        metrics = result.get(\"metrics\", {})\n",
    "        recall_at_5.append(metrics.get(\"recall_at_5\", 0))\n",
    "        recall_at_10.append(metrics.get(\"recall_at_10\", 0))\n",
    "        ndcg_scores.append(metrics.get(\"ndcg_at_10\", 0))\n",
    "\n",
    "    return {\n",
    "        \"recall_at_5_mean\": np.mean(recall_at_5),\n",
    "        \"recall_at_5_std\": np.std(recall_at_5),\n",
    "        \"recall_at_10_mean\": np.mean(recall_at_10),\n",
    "        \"recall_at_10_std\": np.std(recall_at_10),\n",
    "        \"ndcg_at_10_mean\": np.mean(ndcg_scores),\n",
    "        \"ndcg_at_10_std\": np.std(ndcg_scores),\n",
    "        \"total_queries\": len(results),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34f91c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: å“è³ªæŒ‡æ¨™å½™ç¸½\n",
    "def aggregate_quality_metrics(groundedness_data, quality_data):\n",
    "    \"\"\"å½™ç¸½å“è³ªç›¸é—œæŒ‡æ¨™\"\"\"\n",
    "    metrics = {}\n",
    "\n",
    "    # Groundedness metrics\n",
    "    if groundedness_data:\n",
    "        ground_results = groundedness_data.get(\"results\", [])\n",
    "        if ground_results:\n",
    "            ground_scores = [r.get(\"groundedness_score\", 0) for r in ground_results]\n",
    "            metrics.update(\n",
    "                {\n",
    "                    \"groundedness_mean\": np.mean(ground_scores),\n",
    "                    \"groundedness_std\": np.std(ground_scores),\n",
    "                    \"groundedness_samples\": len(ground_scores),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Text quality metrics (Rouge-L, chrF++)\n",
    "    if quality_data:\n",
    "        qual_results = quality_data.get(\"results\", [])\n",
    "        if qual_results:\n",
    "            rouge_scores = [r.get(\"rouge_l\", 0) for r in qual_results]\n",
    "            chrf_scores = [r.get(\"chrf\", 0) for r in qual_results]\n",
    "\n",
    "            metrics.update(\n",
    "                {\n",
    "                    \"rouge_l_mean\": np.mean(rouge_scores),\n",
    "                    \"rouge_l_std\": np.std(rouge_scores),\n",
    "                    \"chrf_mean\": np.mean(chrf_scores),\n",
    "                    \"chrf_std\": np.std(chrf_scores),\n",
    "                    \"quality_samples\": len(qual_results),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8371f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: æ•ˆèƒ½æŒ‡æ¨™å½™ç¸½\n",
    "def aggregate_performance_metrics(performance_data):\n",
    "    \"\"\"å½™ç¸½æ•ˆèƒ½æŒ‡æ¨™\"\"\"\n",
    "    if not performance_data:\n",
    "        return {}\n",
    "\n",
    "    models = performance_data.get(\"models\", [])\n",
    "    if not models:\n",
    "        return {}\n",
    "\n",
    "    # Find best performing model\n",
    "    best_model = None\n",
    "    best_tokens_per_sec = 0\n",
    "\n",
    "    for model in models:\n",
    "        tps = model.get(\"tokens_per_second\", 0)\n",
    "        if tps > best_tokens_per_sec:\n",
    "            best_tokens_per_sec = tps\n",
    "            best_model = model\n",
    "\n",
    "    if not best_model:\n",
    "        return {}\n",
    "\n",
    "    return {\n",
    "        \"best_model_name\": best_model.get(\"model_name\", \"unknown\"),\n",
    "        \"best_tokens_per_sec\": best_model.get(\"tokens_per_second\", 0),\n",
    "        \"best_latency_ms\": best_model.get(\"avg_latency_ms\", 0),\n",
    "        \"best_vram_peak_gb\": best_model.get(\"vram_peak_gb\", 0),\n",
    "        \"total_models_tested\": len(models),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6956730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: ç¶œåˆæ’è¡Œæ¦œç”Ÿæˆ\n",
    "def generate_leaderboard(builder: EvalReportBuilder):\n",
    "    \"\"\"ç”Ÿæˆç¶œåˆæ’è¡Œæ¦œ\"\"\"\n",
    "\n",
    "    # Aggregate all metrics\n",
    "    retrieval_agg = aggregate_retrieval_metrics(builder.metrics.get(\"retrieval\", {}))\n",
    "    quality_agg = aggregate_quality_metrics(\n",
    "        builder.metrics.get(\"groundedness\", {}), builder.metrics.get(\"quality\", {})\n",
    "    )\n",
    "    perf_agg = aggregate_performance_metrics(builder.metrics.get(\"performance\", {}))\n",
    "\n",
    "    # Calculate composite score (weighted)\n",
    "    composite_score = 0\n",
    "    weights = {\"retrieval\": 0.4, \"quality\": 0.4, \"performance\": 0.2}\n",
    "\n",
    "    if retrieval_agg:\n",
    "        retrieval_score = (\n",
    "            retrieval_agg.get(\"recall_at_5_mean\", 0)\n",
    "            + retrieval_agg.get(\"ndcg_at_10_mean\", 0)\n",
    "        ) / 2\n",
    "        composite_score += weights[\"retrieval\"] * retrieval_score\n",
    "\n",
    "    if quality_agg:\n",
    "        quality_score = (\n",
    "            quality_agg.get(\"groundedness_mean\", 0) + quality_agg.get(\"rouge_l_mean\", 0)\n",
    "        ) / 2\n",
    "        composite_score += weights[\"quality\"] * quality_score\n",
    "\n",
    "    if perf_agg:\n",
    "        # Normalize performance (higher tokens/sec = better, lower latency = better)\n",
    "        perf_score = min(perf_agg.get(\"best_tokens_per_sec\", 0) / 100, 1.0)\n",
    "        composite_score += weights[\"performance\"] * perf_score\n",
    "\n",
    "    return {\n",
    "        \"composite_score\": composite_score,\n",
    "        \"retrieval\": retrieval_agg,\n",
    "        \"quality\": quality_agg,\n",
    "        \"performance\": perf_agg,\n",
    "        \"timestamp\": builder.timestamp,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2d1f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Markdown å ±è¡¨è¼¸å‡º\n",
    "def generate_markdown_report(\n",
    "    leaderboard_data, output_path=\"outs/reports/leaderboard.md\"\n",
    "):\n",
    "    \"\"\"ç”Ÿæˆ Markdown æ ¼å¼çš„æ’è¡Œæ¦œå ±è¡¨\"\"\"\n",
    "\n",
    "    md_content = f\"\"\"# RAG ç³»çµ±è©•ä¼°æ’è¡Œæ¦œ\n",
    "\n",
    "> ç”Ÿæˆæ™‚é–“ï¼š{leaderboard_data['timestamp']}\n",
    "> ç¶œåˆåˆ†æ•¸ï¼š{leaderboard_data['composite_score']:.3f} / 1.000\n",
    "\n",
    "## ğŸ“Š æ•´é«”è¡¨ç¾æ‘˜è¦\n",
    "\n",
    "| ç¶­åº¦ | ä¸»è¦æŒ‡æ¨™ | åˆ†æ•¸ | å‚™è¨» |\n",
    "|------|----------|------|------|\n",
    "| ğŸ” æª¢ç´¢æ•ˆèƒ½ | Recall@5 | {leaderboard_data['retrieval'].get('recall_at_5_mean', 0):.3f} | {leaderboard_data['retrieval'].get('total_queries', 0)} æ¢æŸ¥è©¢ |\n",
    "| ğŸ“ å“è³ªæŒ‡æ¨™ | Groundedness | {leaderboard_data['quality'].get('groundedness_mean', 0):.3f} | çœŸå¯¦æ€§è©•åˆ† |\n",
    "| âš¡ ç³»çµ±æ•ˆèƒ½ | Tokens/sec | {leaderboard_data['performance'].get('best_tokens_per_sec', 0):.1f} | {leaderboard_data['performance'].get('best_model_name', 'N/A')} |\n",
    "\n",
    "## ğŸ¯ è©³ç´°æŒ‡æ¨™\n",
    "\n",
    "### æª¢ç´¢ç³»çµ±\n",
    "\"\"\"\n",
    "\n",
    "    if leaderboard_data[\"retrieval\"]:\n",
    "        ret = leaderboard_data[\"retrieval\"]\n",
    "        md_content += f\"\"\"\n",
    "- **Recall@5**: {ret.get('recall_at_5_mean', 0):.3f} Â± {ret.get('recall_at_5_std', 0):.3f}\n",
    "- **Recall@10**: {ret.get('recall_at_10_mean', 0):.3f} Â± {ret.get('recall_at_10_std', 0):.3f}\n",
    "- **nDCG@10**: {ret.get('ndcg_at_10_mean', 0):.3f} Â± {ret.get('ndcg_at_10_std', 0):.3f}\n",
    "- **æ¸¬è©¦æŸ¥è©¢æ•¸**: {ret.get('total_queries', 0)}\n",
    "\"\"\"\n",
    "\n",
    "    md_content += \"\\n### ç”Ÿæˆå“è³ª\\n\"\n",
    "    if leaderboard_data[\"quality\"]:\n",
    "        qual = leaderboard_data[\"quality\"]\n",
    "        md_content += f\"\"\"\n",
    "- **Groundedness**: {qual.get('groundedness_mean', 0):.3f} Â± {qual.get('groundedness_std', 0):.3f}\n",
    "- **Rouge-L**: {qual.get('rouge_l_mean', 0):.3f} Â± {qual.get('rouge_l_std', 0):.3f}\n",
    "- **chrF++**: {qual.get('chrf_mean', 0):.3f} Â± {qual.get('chrf_std', 0):.3f}\n",
    "\"\"\"\n",
    "\n",
    "    md_content += \"\\n### ç³»çµ±æ•ˆèƒ½\\n\"\n",
    "    if leaderboard_data[\"performance\"]:\n",
    "        perf = leaderboard_data[\"performance\"]\n",
    "        md_content += f\"\"\"\n",
    "- **æœ€ä½³æ¨¡å‹**: {perf.get('best_model_name', 'N/A')}\n",
    "- **è¼¸å‡ºé€Ÿåº¦**: {perf.get('best_tokens_per_sec', 0):.1f} tokens/sec\n",
    "- **å¹³å‡å»¶é²**: {perf.get('best_latency_ms', 0):.1f} ms\n",
    "- **VRAM å³°å€¼**: {perf.get('best_vram_peak_gb', 0):.1f} GB\n",
    "\"\"\"\n",
    "\n",
    "    md_content += f\"\"\"\n",
    "\n",
    "## ğŸ“ˆ è¶¨å‹¢åˆ†æ\n",
    "\n",
    "> å»ºè­°æª¢è¦– `outs/charts/` ç›®éŒ„ä¸‹çš„è¦–è¦ºåŒ–åœ–è¡¨ä»¥ç²å¾—æ›´è©³ç´°çš„è¶¨å‹¢åˆ†æã€‚\n",
    "\n",
    "## ğŸ¯ æ”¹é€²å»ºè­°\n",
    "\n",
    "æ ¹æ“šç•¶å‰è©•ä¼°çµæœï¼š\n",
    "\n",
    "1. **æª¢ç´¢å„ªåŒ–**: {\"å¬å›ç‡åä½ï¼Œå»ºè­°èª¿æ•´ chunk ç­–ç•¥\" if leaderboard_data['retrieval'].get('recall_at_5_mean', 0) < 0.6 else \"æª¢ç´¢è¡¨ç¾è‰¯å¥½\"}\n",
    "2. **å“è³ªæå‡**: {\"éœ€è¦æ”¹å–„ç­”æ¡ˆçœŸå¯¦æ€§\" if leaderboard_data['quality'].get('groundedness_mean', 0) < 0.7 else \"ç­”æ¡ˆå“è³ªé”æ¨™\"}\n",
    "3. **æ•ˆèƒ½èª¿å„ª**: {\"å»ºè­°ä½¿ç”¨é‡åŒ–æˆ–æ›´é«˜æ•ˆæ¨¡å‹\" if leaderboard_data['performance'].get('best_tokens_per_sec', 0) < 20 else \"æ•ˆèƒ½è¡¨ç¾ä½³\"}\n",
    "\n",
    "---\n",
    "*æœ¬å ±è¡¨ç”± nb64 è‡ªå‹•ç”Ÿæˆ | ragent-text-lab*\n",
    "\"\"\"\n",
    "\n",
    "    # Write to file\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(md_content)\n",
    "\n",
    "    print(f\"ğŸ“„ æ’è¡Œæ¦œå ±è¡¨å·²ç”Ÿæˆ: {output_path}\")\n",
    "    return md_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20a5b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: è¦–è¦ºåŒ–åœ–è¡¨ç”Ÿæˆ\n",
    "def generate_charts(leaderboard_data, charts_dir=\"outs/charts\"):\n",
    "    \"\"\"ç”Ÿæˆè©•ä¼°æŒ‡æ¨™çš„è¦–è¦ºåŒ–åœ–è¡¨\"\"\"\n",
    "\n",
    "    charts_path = Path(charts_dir)\n",
    "    charts_path.mkdir(exist_ok=True)\n",
    "\n",
    "    # Set Chinese font for matplotlib\n",
    "    plt.rcParams[\"font.sans-serif\"] = [\"SimHei\", \"Arial Unicode MS\", \"DejaVu Sans\"]\n",
    "    plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "    # Chart 1: ç¶œåˆæŒ‡æ¨™é›·é”åœ–\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Metrics summary bar chart\n",
    "    metrics = []\n",
    "    values = []\n",
    "\n",
    "    if leaderboard_data[\"retrieval\"]:\n",
    "        metrics.append(\"Recall@5\")\n",
    "        values.append(leaderboard_data[\"retrieval\"].get(\"recall_at_5_mean\", 0))\n",
    "\n",
    "    if leaderboard_data[\"quality\"]:\n",
    "        metrics.append(\"Groundedness\")\n",
    "        values.append(leaderboard_data[\"quality\"].get(\"groundedness_mean\", 0))\n",
    "\n",
    "    metrics.append(\"Composite Score\")\n",
    "    values.append(leaderboard_data[\"composite_score\"])\n",
    "\n",
    "    if metrics and values:\n",
    "        bars = ax1.bar(metrics, values, color=[\"#3498db\", \"#e74c3c\", \"#f39c12\"])\n",
    "        ax1.set_title(\"Key Metrics Summary\", fontsize=14, fontweight=\"bold\")\n",
    "        ax1.set_ylabel(\"Score\")\n",
    "        ax1.set_ylim(0, 1)\n",
    "\n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, values):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(\n",
    "                bar.get_x() + bar.get_width() / 2.0,\n",
    "                height + 0.01,\n",
    "                f\"{value:.3f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "            )\n",
    "\n",
    "    # Performance timeline (mock data for demo)\n",
    "    if leaderboard_data[\"performance\"]:\n",
    "        perf_data = leaderboard_data[\"performance\"]\n",
    "        ax2.plot(\n",
    "            [1, 2, 3],\n",
    "            [10, 15, perf_data.get(\"best_tokens_per_sec\", 20)],\n",
    "            marker=\"o\",\n",
    "            linewidth=2,\n",
    "            markersize=6,\n",
    "        )\n",
    "        ax2.set_title(\"Performance Trend\", fontsize=14, fontweight=\"bold\")\n",
    "        ax2.set_xlabel(\"Evaluation Run\")\n",
    "        ax2.set_ylabel(\"Tokens/sec\")\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    chart_path = charts_path / \"metrics_summary.png\"\n",
    "    plt.savefig(chart_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"ğŸ“Š åœ–è¡¨å·²ç”Ÿæˆ: {chart_path}\")\n",
    "\n",
    "    return str(chart_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0107d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: ä¸»åŸ·è¡Œæµç¨‹èˆ‡ Smoke Test\n",
    "def run_eval_reports():\n",
    "    \"\"\"åŸ·è¡Œå®Œæ•´çš„è©•ä¼°å ±è¡¨ç”Ÿæˆæµç¨‹\"\"\"\n",
    "\n",
    "    print(\"ğŸš€ é–‹å§‹ç”Ÿæˆè©•ä¼°å ±è¡¨...\")\n",
    "\n",
    "    # Initialize report builder\n",
    "    builder = EvalReportBuilder()\n",
    "\n",
    "    # Create sample data if files don't exist (for demo)\n",
    "    sample_retrieval = {\n",
    "        \"results\": [\n",
    "            {\n",
    "                \"metrics\": {\n",
    "                    \"recall_at_5\": 0.75,\n",
    "                    \"recall_at_10\": 0.85,\n",
    "                    \"ndcg_at_10\": 0.72,\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"metrics\": {\n",
    "                    \"recall_at_5\": 0.68,\n",
    "                    \"recall_at_10\": 0.78,\n",
    "                    \"ndcg_at_10\": 0.69,\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"metrics\": {\n",
    "                    \"recall_at_5\": 0.72,\n",
    "                    \"recall_at_10\": 0.82,\n",
    "                    \"ndcg_at_10\": 0.71,\n",
    "                }\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    sample_groundedness = {\n",
    "        \"results\": [\n",
    "            {\"groundedness_score\": 0.82},\n",
    "            {\"groundedness_score\": 0.79},\n",
    "            {\"groundedness_score\": 0.85},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    sample_quality = {\n",
    "        \"results\": [\n",
    "            {\"rouge_l\": 0.65, \"chrf\": 0.58},\n",
    "            {\"rouge_l\": 0.62, \"chrf\": 0.55},\n",
    "            {\"rouge_l\": 0.68, \"chrf\": 0.61},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    sample_performance = {\n",
    "        \"models\": [\n",
    "            {\n",
    "                \"model_name\": \"Qwen2.5-7B-Instruct\",\n",
    "                \"tokens_per_second\": 28.5,\n",
    "                \"avg_latency_ms\": 85.2,\n",
    "                \"vram_peak_gb\": 8.4,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Load metrics (use samples if files don't exist)\n",
    "    builder.load_retrieval_metrics()\n",
    "    if not builder.metrics.get(\"retrieval\"):\n",
    "        builder.metrics[\"retrieval\"] = sample_retrieval\n",
    "        print(\"ğŸ“ ä½¿ç”¨ç¯„ä¾‹æª¢ç´¢è³‡æ–™\")\n",
    "\n",
    "    builder.load_groundedness_metrics()\n",
    "    if not builder.metrics.get(\"groundedness\"):\n",
    "        builder.metrics[\"groundedness\"] = sample_groundedness\n",
    "        print(\"ğŸ“ ä½¿ç”¨ç¯„ä¾‹ Groundedness è³‡æ–™\")\n",
    "\n",
    "    builder.load_quality_metrics()\n",
    "    if not builder.metrics.get(\"quality\"):\n",
    "        builder.metrics[\"quality\"] = sample_quality\n",
    "        print(\"ğŸ“ ä½¿ç”¨ç¯„ä¾‹å“è³ªè³‡æ–™\")\n",
    "\n",
    "    builder.load_performance_metrics()\n",
    "    if not builder.metrics.get(\"performance\"):\n",
    "        builder.metrics[\"performance\"] = sample_performance\n",
    "        print(\"ğŸ“ ä½¿ç”¨ç¯„ä¾‹æ•ˆèƒ½è³‡æ–™\")\n",
    "\n",
    "    # Generate leaderboard\n",
    "    leaderboard = generate_leaderboard(builder)\n",
    "\n",
    "    # Generate reports\n",
    "    md_report = generate_markdown_report(leaderboard)\n",
    "    chart_path = generate_charts(leaderboard)\n",
    "\n",
    "    # Save leaderboard data as JSON\n",
    "    json_path = \"outs/reports/leaderboard.json\"\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(leaderboard, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"ğŸ’¾ æ’è¡Œæ¦œè³‡æ–™å·²ä¿å­˜: {json_path}\")\n",
    "\n",
    "    return leaderboard\n",
    "\n",
    "\n",
    "# Smoke Test\n",
    "if __name__ == \"__main__\":\n",
    "    leaderboard_result = run_eval_reports()\n",
    "\n",
    "    # Verify results\n",
    "    assert leaderboard_result[\"composite_score\"] > 0, \"ç¶œåˆåˆ†æ•¸æ‡‰å¤§æ–¼ 0\"\n",
    "    assert \"retrieval\" in leaderboard_result, \"æ‡‰åŒ…å«æª¢ç´¢æŒ‡æ¨™\"\n",
    "    assert \"quality\" in leaderboard_result, \"æ‡‰åŒ…å«å“è³ªæŒ‡æ¨™\"\n",
    "    assert \"performance\" in leaderboard_result, \"æ‡‰åŒ…å«æ•ˆèƒ½æŒ‡æ¨™\"\n",
    "\n",
    "    print(\"âœ… Smoke Test é€šé\")\n",
    "    print(f\"ğŸ¯ ç¶œåˆåˆ†æ•¸: {leaderboard_result['composite_score']:.3f}\")\n",
    "    print(\"ğŸ“‹ å ±è¡¨ç”Ÿæˆå®Œæˆï¼Œè«‹æª¢æŸ¥ outs/reports/ ç›®éŒ„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaef8b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    \"retrieval\": 0.4,  # æª¢ç´¢æ•ˆèƒ½æ¬Šé‡\n",
    "    \"quality\": 0.4,  # å“è³ªæŒ‡æ¨™æ¬Šé‡\n",
    "    \"performance\": 0.2,  # ç³»çµ±æ•ˆèƒ½æ¬Šé‡\n",
    "}\n",
    "\n",
    "report_formats = {\n",
    "    \"markdown\": True,  # ç”Ÿæˆ Markdown å ±è¡¨\n",
    "    \"json\": True,  # ä¿å­˜ JSON è³‡æ–™\n",
    "    \"charts\": True,  # ç”Ÿæˆè¦–è¦ºåŒ–åœ–è¡¨\n",
    "    \"csv\": False,  # CSV æ ¼å¼ï¼ˆå¯é¸ï¼‰\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea232288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿«é€Ÿé©—è­‰è…³æœ¬\n",
    "def smoke_test_reports():\n",
    "    builder = EvalReportBuilder()\n",
    "\n",
    "    # æ¨¡æ“¬è¼‰å…¥åŸºæœ¬è³‡æ–™\n",
    "    sample_data = {\n",
    "        \"retrieval\": {\"results\": [{\"metrics\": {\"recall_at_5\": 0.7}}]},\n",
    "        \"performance\": {\"models\": [{\"model_name\": \"test\", \"tokens_per_second\": 25}]},\n",
    "    }\n",
    "\n",
    "    for key, value in sample_data.items():\n",
    "        builder.metrics[key] = value\n",
    "\n",
    "    leaderboard = generate_leaderboard(builder)\n",
    "\n",
    "    assert leaderboard[\"composite_score\"] > 0\n",
    "    assert \"timestamp\" in leaderboard\n",
    "\n",
    "    print(\"âœ… å ±è¡¨ç³»çµ±é©—è­‰é€šé\")\n",
    "\n",
    "\n",
    "smoke_test_reports()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
