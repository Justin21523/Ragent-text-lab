{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d60fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb64 | 彙總報表與排行榜系統\n",
    "# Goal: 整合多維度評估指標，生成模型比較排行榜與視覺化報表\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (複製到每本 notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fffbea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: 載入評估結果與依賴\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Create output directories\n",
    "outs_dir = Path(\"outs\")\n",
    "outs_dir.mkdir(exist_ok=True)\n",
    "(outs_dir / \"reports\").mkdir(exist_ok=True)\n",
    "(outs_dir / \"charts\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"📊 載入評估結果模組\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e4e17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: 報表資料結構設計\n",
    "class EvalReportBuilder:\n",
    "    \"\"\"統一評估報表建構器\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.metrics = {}\n",
    "        self.models = {}\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    def load_retrieval_metrics(self, filepath=\"outs/retrieval_metrics.json\"):\n",
    "        \"\"\"載入檢索指標 (nb60)\"\"\"\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            self.metrics[\"retrieval\"] = data\n",
    "            print(f\"✅ 載入檢索指標: {len(data.get('results', []))} 項結果\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"⚠️  檔案不存在: {filepath}\")\n",
    "            self.metrics[\"retrieval\"] = {}\n",
    "\n",
    "    def load_groundedness_metrics(self, filepath=\"outs/groundedness_metrics.json\"):\n",
    "        \"\"\"載入 Groundedness 指標 (nb61)\"\"\"\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            self.metrics[\"groundedness\"] = data\n",
    "            print(f\"✅ 載入真實性指標: {len(data.get('results', []))} 項結果\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"⚠️  檔案不存在: {filepath}\")\n",
    "            self.metrics[\"groundedness\"] = {}\n",
    "\n",
    "    def load_quality_metrics(self, filepath=\"outs/text_quality_metrics.json\"):\n",
    "        \"\"\"載入文本品質指標 (nb62)\"\"\"\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            self.metrics[\"quality\"] = data\n",
    "            print(f\"✅ 載入品質指標: {len(data.get('results', []))} 項結果\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"⚠️  檔案不存在: {filepath}\")\n",
    "            self.metrics[\"quality\"] = {}\n",
    "\n",
    "    def load_performance_metrics(self, filepath=\"outs/performance_baseline.json\"):\n",
    "        \"\"\"載入效能指標 (nb63)\"\"\"\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            self.metrics[\"performance\"] = data\n",
    "            print(f\"✅ 載入效能指標: {len(data.get('models', []))} 個模型\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"⚠️  檔案不存在: {filepath}\")\n",
    "            self.metrics[\"performance\"] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8960b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: 檢索指標彙總\n",
    "def aggregate_retrieval_metrics(retrieval_data):\n",
    "    \"\"\"彙總檢索指標\"\"\"\n",
    "    if not retrieval_data:\n",
    "        return {}\n",
    "\n",
    "    results = retrieval_data.get(\"results\", [])\n",
    "    if not results:\n",
    "        return {}\n",
    "\n",
    "    # Extract metrics from results\n",
    "    recall_at_5 = []\n",
    "    recall_at_10 = []\n",
    "    ndcg_scores = []\n",
    "\n",
    "    for result in results:\n",
    "        metrics = result.get(\"metrics\", {})\n",
    "        recall_at_5.append(metrics.get(\"recall_at_5\", 0))\n",
    "        recall_at_10.append(metrics.get(\"recall_at_10\", 0))\n",
    "        ndcg_scores.append(metrics.get(\"ndcg_at_10\", 0))\n",
    "\n",
    "    return {\n",
    "        \"recall_at_5_mean\": np.mean(recall_at_5),\n",
    "        \"recall_at_5_std\": np.std(recall_at_5),\n",
    "        \"recall_at_10_mean\": np.mean(recall_at_10),\n",
    "        \"recall_at_10_std\": np.std(recall_at_10),\n",
    "        \"ndcg_at_10_mean\": np.mean(ndcg_scores),\n",
    "        \"ndcg_at_10_std\": np.std(ndcg_scores),\n",
    "        \"total_queries\": len(results),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34f91c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: 品質指標彙總\n",
    "def aggregate_quality_metrics(groundedness_data, quality_data):\n",
    "    \"\"\"彙總品質相關指標\"\"\"\n",
    "    metrics = {}\n",
    "\n",
    "    # Groundedness metrics\n",
    "    if groundedness_data:\n",
    "        ground_results = groundedness_data.get(\"results\", [])\n",
    "        if ground_results:\n",
    "            ground_scores = [r.get(\"groundedness_score\", 0) for r in ground_results]\n",
    "            metrics.update(\n",
    "                {\n",
    "                    \"groundedness_mean\": np.mean(ground_scores),\n",
    "                    \"groundedness_std\": np.std(ground_scores),\n",
    "                    \"groundedness_samples\": len(ground_scores),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Text quality metrics (Rouge-L, chrF++)\n",
    "    if quality_data:\n",
    "        qual_results = quality_data.get(\"results\", [])\n",
    "        if qual_results:\n",
    "            rouge_scores = [r.get(\"rouge_l\", 0) for r in qual_results]\n",
    "            chrf_scores = [r.get(\"chrf\", 0) for r in qual_results]\n",
    "\n",
    "            metrics.update(\n",
    "                {\n",
    "                    \"rouge_l_mean\": np.mean(rouge_scores),\n",
    "                    \"rouge_l_std\": np.std(rouge_scores),\n",
    "                    \"chrf_mean\": np.mean(chrf_scores),\n",
    "                    \"chrf_std\": np.std(chrf_scores),\n",
    "                    \"quality_samples\": len(qual_results),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8371f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: 效能指標彙總\n",
    "def aggregate_performance_metrics(performance_data):\n",
    "    \"\"\"彙總效能指標\"\"\"\n",
    "    if not performance_data:\n",
    "        return {}\n",
    "\n",
    "    models = performance_data.get(\"models\", [])\n",
    "    if not models:\n",
    "        return {}\n",
    "\n",
    "    # Find best performing model\n",
    "    best_model = None\n",
    "    best_tokens_per_sec = 0\n",
    "\n",
    "    for model in models:\n",
    "        tps = model.get(\"tokens_per_second\", 0)\n",
    "        if tps > best_tokens_per_sec:\n",
    "            best_tokens_per_sec = tps\n",
    "            best_model = model\n",
    "\n",
    "    if not best_model:\n",
    "        return {}\n",
    "\n",
    "    return {\n",
    "        \"best_model_name\": best_model.get(\"model_name\", \"unknown\"),\n",
    "        \"best_tokens_per_sec\": best_model.get(\"tokens_per_second\", 0),\n",
    "        \"best_latency_ms\": best_model.get(\"avg_latency_ms\", 0),\n",
    "        \"best_vram_peak_gb\": best_model.get(\"vram_peak_gb\", 0),\n",
    "        \"total_models_tested\": len(models),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6956730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: 綜合排行榜生成\n",
    "def generate_leaderboard(builder: EvalReportBuilder):\n",
    "    \"\"\"生成綜合排行榜\"\"\"\n",
    "\n",
    "    # Aggregate all metrics\n",
    "    retrieval_agg = aggregate_retrieval_metrics(builder.metrics.get(\"retrieval\", {}))\n",
    "    quality_agg = aggregate_quality_metrics(\n",
    "        builder.metrics.get(\"groundedness\", {}), builder.metrics.get(\"quality\", {})\n",
    "    )\n",
    "    perf_agg = aggregate_performance_metrics(builder.metrics.get(\"performance\", {}))\n",
    "\n",
    "    # Calculate composite score (weighted)\n",
    "    composite_score = 0\n",
    "    weights = {\"retrieval\": 0.4, \"quality\": 0.4, \"performance\": 0.2}\n",
    "\n",
    "    if retrieval_agg:\n",
    "        retrieval_score = (\n",
    "            retrieval_agg.get(\"recall_at_5_mean\", 0)\n",
    "            + retrieval_agg.get(\"ndcg_at_10_mean\", 0)\n",
    "        ) / 2\n",
    "        composite_score += weights[\"retrieval\"] * retrieval_score\n",
    "\n",
    "    if quality_agg:\n",
    "        quality_score = (\n",
    "            quality_agg.get(\"groundedness_mean\", 0) + quality_agg.get(\"rouge_l_mean\", 0)\n",
    "        ) / 2\n",
    "        composite_score += weights[\"quality\"] * quality_score\n",
    "\n",
    "    if perf_agg:\n",
    "        # Normalize performance (higher tokens/sec = better, lower latency = better)\n",
    "        perf_score = min(perf_agg.get(\"best_tokens_per_sec\", 0) / 100, 1.0)\n",
    "        composite_score += weights[\"performance\"] * perf_score\n",
    "\n",
    "    return {\n",
    "        \"composite_score\": composite_score,\n",
    "        \"retrieval\": retrieval_agg,\n",
    "        \"quality\": quality_agg,\n",
    "        \"performance\": perf_agg,\n",
    "        \"timestamp\": builder.timestamp,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2d1f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Markdown 報表輸出\n",
    "def generate_markdown_report(\n",
    "    leaderboard_data, output_path=\"outs/reports/leaderboard.md\"\n",
    "):\n",
    "    \"\"\"生成 Markdown 格式的排行榜報表\"\"\"\n",
    "\n",
    "    md_content = f\"\"\"# RAG 系統評估排行榜\n",
    "\n",
    "> 生成時間：{leaderboard_data['timestamp']}\n",
    "> 綜合分數：{leaderboard_data['composite_score']:.3f} / 1.000\n",
    "\n",
    "## 📊 整體表現摘要\n",
    "\n",
    "| 維度 | 主要指標 | 分數 | 備註 |\n",
    "|------|----------|------|------|\n",
    "| 🔍 檢索效能 | Recall@5 | {leaderboard_data['retrieval'].get('recall_at_5_mean', 0):.3f} | {leaderboard_data['retrieval'].get('total_queries', 0)} 條查詢 |\n",
    "| 📝 品質指標 | Groundedness | {leaderboard_data['quality'].get('groundedness_mean', 0):.3f} | 真實性評分 |\n",
    "| ⚡ 系統效能 | Tokens/sec | {leaderboard_data['performance'].get('best_tokens_per_sec', 0):.1f} | {leaderboard_data['performance'].get('best_model_name', 'N/A')} |\n",
    "\n",
    "## 🎯 詳細指標\n",
    "\n",
    "### 檢索系統\n",
    "\"\"\"\n",
    "\n",
    "    if leaderboard_data[\"retrieval\"]:\n",
    "        ret = leaderboard_data[\"retrieval\"]\n",
    "        md_content += f\"\"\"\n",
    "- **Recall@5**: {ret.get('recall_at_5_mean', 0):.3f} ± {ret.get('recall_at_5_std', 0):.3f}\n",
    "- **Recall@10**: {ret.get('recall_at_10_mean', 0):.3f} ± {ret.get('recall_at_10_std', 0):.3f}\n",
    "- **nDCG@10**: {ret.get('ndcg_at_10_mean', 0):.3f} ± {ret.get('ndcg_at_10_std', 0):.3f}\n",
    "- **測試查詢數**: {ret.get('total_queries', 0)}\n",
    "\"\"\"\n",
    "\n",
    "    md_content += \"\\n### 生成品質\\n\"\n",
    "    if leaderboard_data[\"quality\"]:\n",
    "        qual = leaderboard_data[\"quality\"]\n",
    "        md_content += f\"\"\"\n",
    "- **Groundedness**: {qual.get('groundedness_mean', 0):.3f} ± {qual.get('groundedness_std', 0):.3f}\n",
    "- **Rouge-L**: {qual.get('rouge_l_mean', 0):.3f} ± {qual.get('rouge_l_std', 0):.3f}\n",
    "- **chrF++**: {qual.get('chrf_mean', 0):.3f} ± {qual.get('chrf_std', 0):.3f}\n",
    "\"\"\"\n",
    "\n",
    "    md_content += \"\\n### 系統效能\\n\"\n",
    "    if leaderboard_data[\"performance\"]:\n",
    "        perf = leaderboard_data[\"performance\"]\n",
    "        md_content += f\"\"\"\n",
    "- **最佳模型**: {perf.get('best_model_name', 'N/A')}\n",
    "- **輸出速度**: {perf.get('best_tokens_per_sec', 0):.1f} tokens/sec\n",
    "- **平均延遲**: {perf.get('best_latency_ms', 0):.1f} ms\n",
    "- **VRAM 峰值**: {perf.get('best_vram_peak_gb', 0):.1f} GB\n",
    "\"\"\"\n",
    "\n",
    "    md_content += f\"\"\"\n",
    "\n",
    "## 📈 趨勢分析\n",
    "\n",
    "> 建議檢視 `outs/charts/` 目錄下的視覺化圖表以獲得更詳細的趨勢分析。\n",
    "\n",
    "## 🎯 改進建議\n",
    "\n",
    "根據當前評估結果：\n",
    "\n",
    "1. **檢索優化**: {\"召回率偏低，建議調整 chunk 策略\" if leaderboard_data['retrieval'].get('recall_at_5_mean', 0) < 0.6 else \"檢索表現良好\"}\n",
    "2. **品質提升**: {\"需要改善答案真實性\" if leaderboard_data['quality'].get('groundedness_mean', 0) < 0.7 else \"答案品質達標\"}\n",
    "3. **效能調優**: {\"建議使用量化或更高效模型\" if leaderboard_data['performance'].get('best_tokens_per_sec', 0) < 20 else \"效能表現佳\"}\n",
    "\n",
    "---\n",
    "*本報表由 nb64 自動生成 | ragent-text-lab*\n",
    "\"\"\"\n",
    "\n",
    "    # Write to file\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(md_content)\n",
    "\n",
    "    print(f\"📄 排行榜報表已生成: {output_path}\")\n",
    "    return md_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20a5b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: 視覺化圖表生成\n",
    "def generate_charts(leaderboard_data, charts_dir=\"outs/charts\"):\n",
    "    \"\"\"生成評估指標的視覺化圖表\"\"\"\n",
    "\n",
    "    charts_path = Path(charts_dir)\n",
    "    charts_path.mkdir(exist_ok=True)\n",
    "\n",
    "    # Set Chinese font for matplotlib\n",
    "    plt.rcParams[\"font.sans-serif\"] = [\"SimHei\", \"Arial Unicode MS\", \"DejaVu Sans\"]\n",
    "    plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "    # Chart 1: 綜合指標雷達圖\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Metrics summary bar chart\n",
    "    metrics = []\n",
    "    values = []\n",
    "\n",
    "    if leaderboard_data[\"retrieval\"]:\n",
    "        metrics.append(\"Recall@5\")\n",
    "        values.append(leaderboard_data[\"retrieval\"].get(\"recall_at_5_mean\", 0))\n",
    "\n",
    "    if leaderboard_data[\"quality\"]:\n",
    "        metrics.append(\"Groundedness\")\n",
    "        values.append(leaderboard_data[\"quality\"].get(\"groundedness_mean\", 0))\n",
    "\n",
    "    metrics.append(\"Composite Score\")\n",
    "    values.append(leaderboard_data[\"composite_score\"])\n",
    "\n",
    "    if metrics and values:\n",
    "        bars = ax1.bar(metrics, values, color=[\"#3498db\", \"#e74c3c\", \"#f39c12\"])\n",
    "        ax1.set_title(\"Key Metrics Summary\", fontsize=14, fontweight=\"bold\")\n",
    "        ax1.set_ylabel(\"Score\")\n",
    "        ax1.set_ylim(0, 1)\n",
    "\n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, values):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(\n",
    "                bar.get_x() + bar.get_width() / 2.0,\n",
    "                height + 0.01,\n",
    "                f\"{value:.3f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "            )\n",
    "\n",
    "    # Performance timeline (mock data for demo)\n",
    "    if leaderboard_data[\"performance\"]:\n",
    "        perf_data = leaderboard_data[\"performance\"]\n",
    "        ax2.plot(\n",
    "            [1, 2, 3],\n",
    "            [10, 15, perf_data.get(\"best_tokens_per_sec\", 20)],\n",
    "            marker=\"o\",\n",
    "            linewidth=2,\n",
    "            markersize=6,\n",
    "        )\n",
    "        ax2.set_title(\"Performance Trend\", fontsize=14, fontweight=\"bold\")\n",
    "        ax2.set_xlabel(\"Evaluation Run\")\n",
    "        ax2.set_ylabel(\"Tokens/sec\")\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    chart_path = charts_path / \"metrics_summary.png\"\n",
    "    plt.savefig(chart_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"📊 圖表已生成: {chart_path}\")\n",
    "\n",
    "    return str(chart_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0107d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: 主執行流程與 Smoke Test\n",
    "def run_eval_reports():\n",
    "    \"\"\"執行完整的評估報表生成流程\"\"\"\n",
    "\n",
    "    print(\"🚀 開始生成評估報表...\")\n",
    "\n",
    "    # Initialize report builder\n",
    "    builder = EvalReportBuilder()\n",
    "\n",
    "    # Create sample data if files don't exist (for demo)\n",
    "    sample_retrieval = {\n",
    "        \"results\": [\n",
    "            {\n",
    "                \"metrics\": {\n",
    "                    \"recall_at_5\": 0.75,\n",
    "                    \"recall_at_10\": 0.85,\n",
    "                    \"ndcg_at_10\": 0.72,\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"metrics\": {\n",
    "                    \"recall_at_5\": 0.68,\n",
    "                    \"recall_at_10\": 0.78,\n",
    "                    \"ndcg_at_10\": 0.69,\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"metrics\": {\n",
    "                    \"recall_at_5\": 0.72,\n",
    "                    \"recall_at_10\": 0.82,\n",
    "                    \"ndcg_at_10\": 0.71,\n",
    "                }\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    sample_groundedness = {\n",
    "        \"results\": [\n",
    "            {\"groundedness_score\": 0.82},\n",
    "            {\"groundedness_score\": 0.79},\n",
    "            {\"groundedness_score\": 0.85},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    sample_quality = {\n",
    "        \"results\": [\n",
    "            {\"rouge_l\": 0.65, \"chrf\": 0.58},\n",
    "            {\"rouge_l\": 0.62, \"chrf\": 0.55},\n",
    "            {\"rouge_l\": 0.68, \"chrf\": 0.61},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    sample_performance = {\n",
    "        \"models\": [\n",
    "            {\n",
    "                \"model_name\": \"Qwen2.5-7B-Instruct\",\n",
    "                \"tokens_per_second\": 28.5,\n",
    "                \"avg_latency_ms\": 85.2,\n",
    "                \"vram_peak_gb\": 8.4,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Load metrics (use samples if files don't exist)\n",
    "    builder.load_retrieval_metrics()\n",
    "    if not builder.metrics.get(\"retrieval\"):\n",
    "        builder.metrics[\"retrieval\"] = sample_retrieval\n",
    "        print(\"📝 使用範例檢索資料\")\n",
    "\n",
    "    builder.load_groundedness_metrics()\n",
    "    if not builder.metrics.get(\"groundedness\"):\n",
    "        builder.metrics[\"groundedness\"] = sample_groundedness\n",
    "        print(\"📝 使用範例 Groundedness 資料\")\n",
    "\n",
    "    builder.load_quality_metrics()\n",
    "    if not builder.metrics.get(\"quality\"):\n",
    "        builder.metrics[\"quality\"] = sample_quality\n",
    "        print(\"📝 使用範例品質資料\")\n",
    "\n",
    "    builder.load_performance_metrics()\n",
    "    if not builder.metrics.get(\"performance\"):\n",
    "        builder.metrics[\"performance\"] = sample_performance\n",
    "        print(\"📝 使用範例效能資料\")\n",
    "\n",
    "    # Generate leaderboard\n",
    "    leaderboard = generate_leaderboard(builder)\n",
    "\n",
    "    # Generate reports\n",
    "    md_report = generate_markdown_report(leaderboard)\n",
    "    chart_path = generate_charts(leaderboard)\n",
    "\n",
    "    # Save leaderboard data as JSON\n",
    "    json_path = \"outs/reports/leaderboard.json\"\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(leaderboard, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"💾 排行榜資料已保存: {json_path}\")\n",
    "\n",
    "    return leaderboard\n",
    "\n",
    "\n",
    "# Smoke Test\n",
    "if __name__ == \"__main__\":\n",
    "    leaderboard_result = run_eval_reports()\n",
    "\n",
    "    # Verify results\n",
    "    assert leaderboard_result[\"composite_score\"] > 0, \"綜合分數應大於 0\"\n",
    "    assert \"retrieval\" in leaderboard_result, \"應包含檢索指標\"\n",
    "    assert \"quality\" in leaderboard_result, \"應包含品質指標\"\n",
    "    assert \"performance\" in leaderboard_result, \"應包含效能指標\"\n",
    "\n",
    "    print(\"✅ Smoke Test 通過\")\n",
    "    print(f\"🎯 綜合分數: {leaderboard_result['composite_score']:.3f}\")\n",
    "    print(\"📋 報表生成完成，請檢查 outs/reports/ 目錄\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaef8b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    \"retrieval\": 0.4,  # 檢索效能權重\n",
    "    \"quality\": 0.4,  # 品質指標權重\n",
    "    \"performance\": 0.2,  # 系統效能權重\n",
    "}\n",
    "\n",
    "report_formats = {\n",
    "    \"markdown\": True,  # 生成 Markdown 報表\n",
    "    \"json\": True,  # 保存 JSON 資料\n",
    "    \"charts\": True,  # 生成視覺化圖表\n",
    "    \"csv\": False,  # CSV 格式（可選）\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea232288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 快速驗證腳本\n",
    "def smoke_test_reports():\n",
    "    builder = EvalReportBuilder()\n",
    "\n",
    "    # 模擬載入基本資料\n",
    "    sample_data = {\n",
    "        \"retrieval\": {\"results\": [{\"metrics\": {\"recall_at_5\": 0.7}}]},\n",
    "        \"performance\": {\"models\": [{\"model_name\": \"test\", \"tokens_per_second\": 25}]},\n",
    "    }\n",
    "\n",
    "    for key, value in sample_data.items():\n",
    "        builder.metrics[key] = value\n",
    "\n",
    "    leaderboard = generate_leaderboard(builder)\n",
    "\n",
    "    assert leaderboard[\"composite_score\"] > 0\n",
    "    assert \"timestamp\" in leaderboard\n",
    "\n",
    "    print(\"✅ 報表系統驗證通過\")\n",
    "\n",
    "\n",
    "smoke_test_reports()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
