{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb17f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 7 - nb62 | Text Generation Quality Evaluation\n",
    "# Goals: Rouge-L, chrF++, human annotation, comparative analysis\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (è¤‡è£½åˆ°æ¯æœ¬ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4028b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Dependencies and Setup\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Text evaluation metrics\n",
    "try:\n",
    "    from rouge_score import rouge_scorer\n",
    "\n",
    "    print(\"âœ“ rouge-score available\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ Installing rouge-score...\")\n",
    "    os.system(\"pip install rouge-score\")\n",
    "    from rouge_score import rouge_scorer\n",
    "\n",
    "try:\n",
    "    import sacrebleu\n",
    "\n",
    "    print(\"âœ“ sacrebleu available\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ Installing sacrebleu...\")\n",
    "    os.system(\"pip install sacrebleu\")\n",
    "    import sacrebleu\n",
    "\n",
    "# Optional: Gradio for human evaluation interface\n",
    "try:\n",
    "    import gradio as gr\n",
    "\n",
    "    print(\"âœ“ gradio available\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ Installing gradio...\")\n",
    "    os.system(\"pip install gradio\")\n",
    "    import gradio as gr\n",
    "\n",
    "# Create output directories\n",
    "for dir_name in [\"outs/eval\", \"outs/reports\", \"data/eval\"]:\n",
    "    pathlib.Path(dir_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Dependencies loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc69d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Test Dataset and Reference Answers Setup\n",
    "@dataclass\n",
    "class EvalSample:\n",
    "    \"\"\"Single evaluation sample with question, reference, and generated answers\"\"\"\n",
    "\n",
    "    id: str\n",
    "    question: str\n",
    "    reference: str\n",
    "    context: str = \"\"\n",
    "    domain: str = \"general\"\n",
    "\n",
    "\n",
    "class QualityEvaluator:\n",
    "    \"\"\"Text generation quality evaluator with multiple metrics\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize Rouge scorer for Chinese/English\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=False)\n",
    "\n",
    "    def compute_rouge_l(self, generated: str, reference: str) -> float:\n",
    "        \"\"\"Compute Rouge-L F1 score\"\"\"\n",
    "        try:\n",
    "            scores = self.rouge_scorer.score(reference, generated)\n",
    "            return scores[\"rougeL\"].fmeasure\n",
    "        except Exception as e:\n",
    "            print(f\"Rouge-L error: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def compute_chrf(self, generated: str, reference: str) -> float:\n",
    "        \"\"\"Compute chrF++ score\"\"\"\n",
    "        try:\n",
    "            # chrF++ with word and character n-grams\n",
    "            score = sacrebleu.sentence_chrf(generated, [reference], word_order=2)\n",
    "            return score.score / 100.0  # Normalize to 0-1\n",
    "        except Exception as e:\n",
    "            print(f\"chrF++ error: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def compute_length_penalty(self, generated: str, reference: str) -> float:\n",
    "        \"\"\"Length-based penalty (closer to reference length is better)\"\"\"\n",
    "        if not reference.strip():\n",
    "            return 0.0\n",
    "\n",
    "        gen_len = len(generated.strip())\n",
    "        ref_len = len(reference.strip())\n",
    "\n",
    "        if gen_len == 0:\n",
    "            return 0.0\n",
    "\n",
    "        # Penalty for being too short or too long\n",
    "        ratio = min(gen_len, ref_len) / max(gen_len, ref_len)\n",
    "        return ratio\n",
    "\n",
    "    def evaluate_sample(self, sample: EvalSample, generated: str) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate a single generated text against reference\"\"\"\n",
    "        metrics = {\n",
    "            \"rouge_l\": self.compute_rouge_l(generated, sample.reference),\n",
    "            \"chrf_plus\": self.compute_chrf(generated, sample.reference),\n",
    "            \"length_penalty\": self.compute_length_penalty(generated, sample.reference),\n",
    "        }\n",
    "\n",
    "        # Composite score (weighted average)\n",
    "        metrics[\"composite\"] = (\n",
    "            0.4 * metrics[\"rouge_l\"]\n",
    "            + 0.4 * metrics[\"chrf_plus\"]\n",
    "            + 0.2 * metrics[\"length_penalty\"]\n",
    "        )\n",
    "\n",
    "        return metrics\n",
    "\n",
    "\n",
    "# Create sample test dataset\n",
    "def create_test_dataset() -> List[EvalSample]:\n",
    "    \"\"\"Create a small test dataset for evaluation\"\"\"\n",
    "    samples = [\n",
    "        EvalSample(\n",
    "            id=\"qa_001\",\n",
    "            question=\"ä»€éº¼æ˜¯æª¢ç´¢å¢å¼·ç”Ÿæˆï¼ˆRAGï¼‰ï¼Ÿ\",\n",
    "            reference=\"æª¢ç´¢å¢å¼·ç”Ÿæˆï¼ˆRAGï¼‰æ˜¯ä¸€ç¨®çµåˆè³‡è¨Šæª¢ç´¢å’Œæ–‡æœ¬ç”Ÿæˆçš„æŠ€è¡“ï¼Œé€šéå¾å¤–éƒ¨çŸ¥è­˜åº«æª¢ç´¢ç›¸é—œè³‡è¨Šä¾†å¢å¼·èªè¨€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œæé«˜å›ç­”çš„æº–ç¢ºæ€§å’Œæ™‚æ•ˆæ€§ã€‚\",\n",
    "            domain=\"tech\",\n",
    "        ),\n",
    "        EvalSample(\n",
    "            id=\"qa_002\",\n",
    "            question=\"è«‹è§£é‡‹æ©Ÿå™¨å­¸ç¿’ä¸­çš„éæ“¬åˆç¾è±¡ã€‚\",\n",
    "            reference=\"éæ“¬åˆæ˜¯æŒ‡æ¨¡å‹åœ¨è¨“ç·´æ•¸æ“šä¸Šè¡¨ç¾å¾ˆå¥½ï¼Œä½†åœ¨æ–°çš„ã€æœªè¦‹éçš„æ•¸æ“šä¸Šè¡¨ç¾è¼ƒå·®çš„ç¾è±¡ã€‚é€™é€šå¸¸æ˜¯å› ç‚ºæ¨¡å‹éæ–¼è¤‡é›œï¼Œå­¸ç¿’äº†è¨“ç·´æ•¸æ“šä¸­çš„å™ªéŸ³å’Œç‰¹æ®Šæƒ…æ³ï¼Œè€Œéé€šç”¨è¦å¾‹ã€‚\",\n",
    "            domain=\"tech\",\n",
    "        ),\n",
    "        EvalSample(\n",
    "            id=\"qa_003\",\n",
    "            question=\"å°ç£çš„åœ°ç†ä½ç½®æœ‰ä»€éº¼ç‰¹è‰²ï¼Ÿ\",\n",
    "            reference=\"å°ç£ä½æ–¼äºæ´²æ±éƒ¨ï¼Œè¥¿å¤ªå¹³æ´‹å³¶å¼§ä¸Šï¼Œæ±è‡¨å¤ªå¹³æ´‹ï¼Œè¥¿éš”å°ç£æµ·å³½èˆ‡ä¸­åœ‹å¤§é™¸ç›¸æœ›ã€‚åœ°è™•ç†±å¸¶èˆ‡äºç†±å¸¶äº¤ç•Œï¼Œåœ°å½¢å¤šå±±ï¼Œå¹³åŸä¸»è¦åˆ†å¸ƒåœ¨è¥¿éƒ¨æ²¿æµ·ã€‚\",\n",
    "            domain=\"general\",\n",
    "        ),\n",
    "        EvalSample(\n",
    "            id=\"qa_004\",\n",
    "            question=\"å¦‚ä½•æé«˜åœ˜éšŠæºé€šæ•ˆç‡ï¼Ÿ\",\n",
    "            reference=\"æé«˜åœ˜éšŠæºé€šæ•ˆç‡çš„æ–¹æ³•åŒ…æ‹¬ï¼šå»ºç«‹æ¸…æ™°çš„æºé€šæµç¨‹ã€ä½¿ç”¨åˆé©çš„æºé€šå·¥å…·ã€å®šæœŸèˆ‰è¡Œåœ˜éšŠæœƒè­°ã€é¼“å‹µé–‹æ”¾é€æ˜çš„å°è©±ã€ç¢ºä¿è³‡è¨ŠåŠæ™‚å…±äº«ã€åŸ¹é¤Šç©æ¥µå‚¾è½çš„ç¿’æ…£ã€‚\",\n",
    "            domain=\"general\",\n",
    "        ),\n",
    "        EvalSample(\n",
    "            id=\"qa_005\",\n",
    "            question=\"ä»€éº¼æ˜¯å€å¡ŠéˆæŠ€è¡“çš„æ ¸å¿ƒæ¦‚å¿µï¼Ÿ\",\n",
    "            reference=\"å€å¡Šéˆæ˜¯ä¸€ç¨®åˆ†æ•£å¼å¸³æœ¬æŠ€è¡“ï¼Œæ ¸å¿ƒæ¦‚å¿µåŒ…æ‹¬ï¼šå»ä¸­å¿ƒåŒ–ã€ä¸å¯ç¯¡æ”¹æ€§ã€é€æ˜æ€§å’Œå…±è­˜æ©Ÿåˆ¶ã€‚æ¯å€‹å€å¡ŠåŒ…å«äº¤æ˜“è¨˜éŒ„ï¼Œä¸¦é€šéå¯†ç¢¼å­¸æ–¹æ³•éˆæ¥ï¼Œå½¢æˆå®‰å…¨å¯ä¿¡çš„æ•¸æ“šå„²å­˜ç³»çµ±ã€‚\",\n",
    "            domain=\"tech\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "# Load test dataset\n",
    "test_samples = create_test_dataset()\n",
    "evaluator = QualityEvaluator()\n",
    "\n",
    "print(f\"Created test dataset with {len(test_samples)} samples\")\n",
    "for sample in test_samples[:2]:\n",
    "    print(f\"Sample {sample.id}: {sample.question[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4940791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Human Evaluation Interface (Gradio)\n",
    "class HumanEvaluator:\n",
    "    \"\"\"Simple human evaluation interface for subjective quality assessment\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.annotations = []\n",
    "        self.current_sample_idx = 0\n",
    "\n",
    "    def create_evaluation_interface(\n",
    "        self, samples: List[EvalSample], generated_texts: List[str]\n",
    "    ):\n",
    "        \"\"\"Create Gradio interface for human evaluation\"\"\"\n",
    "\n",
    "        def evaluate_text(\n",
    "            sample_idx: int,\n",
    "            generated: str,\n",
    "            relevance: int,\n",
    "            fluency: int,\n",
    "            informativeness: int,\n",
    "            overall: int,\n",
    "            comments: str,\n",
    "        ):\n",
    "            \"\"\"Save human evaluation\"\"\"\n",
    "            annotation = {\n",
    "                \"sample_id\": samples[sample_idx].id,\n",
    "                \"question\": samples[sample_idx].question,\n",
    "                \"reference\": samples[sample_idx].reference,\n",
    "                \"generated\": generated,\n",
    "                \"relevance\": relevance,\n",
    "                \"fluency\": fluency,\n",
    "                \"informativeness\": informativeness,\n",
    "                \"overall\": overall,\n",
    "                \"comments\": comments,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "            }\n",
    "            self.annotations.append(annotation)\n",
    "\n",
    "            # Save to file\n",
    "            annotations_file = \"outs/eval/human_annotations.jsonl\"\n",
    "            with open(annotations_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(annotation, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            return f\"è©•ä¼°å·²ä¿å­˜ï¼å·²å®Œæˆ {len(self.annotations)} å€‹æ¨£æœ¬\"\n",
    "\n",
    "        def load_sample(sample_idx: int):\n",
    "            \"\"\"Load sample for evaluation\"\"\"\n",
    "            if 0 <= sample_idx < len(samples):\n",
    "                sample = samples[sample_idx]\n",
    "                generated = (\n",
    "                    generated_texts[sample_idx]\n",
    "                    if sample_idx < len(generated_texts)\n",
    "                    else \"\"\n",
    "                )\n",
    "                return (\n",
    "                    sample.question,\n",
    "                    sample.reference,\n",
    "                    generated,\n",
    "                    f\"æ¨£æœ¬ {sample_idx + 1}/{len(samples)} - {sample.domain}\",\n",
    "                )\n",
    "            return \"\", \"\", \"\", \"ç„¡æ•ˆçš„æ¨£æœ¬ç´¢å¼•\"\n",
    "\n",
    "        # Create interface\n",
    "        with gr.Blocks(title=\"æ–‡æœ¬å“è³ªäººå·¥è©•ä¼°\") as demo:\n",
    "            gr.Markdown(\"# æ–‡æœ¬ç”Ÿæˆå“è³ªäººå·¥è©•ä¼°ä»‹é¢\")\n",
    "            gr.Markdown(\"è«‹æ ¹æ“šä»¥ä¸‹æ¨™æº–è©•ä¼°ç”Ÿæˆçš„æ–‡æœ¬å“è³ªï¼ˆ1-5åˆ†ï¼Œ5åˆ†æœ€é«˜ï¼‰\")\n",
    "\n",
    "            with gr.Row():\n",
    "                sample_idx = gr.Number(label=\"æ¨£æœ¬ç´¢å¼•\", value=0, precision=0)\n",
    "                load_btn = gr.Button(\"è¼‰å…¥æ¨£æœ¬\")\n",
    "\n",
    "            info_display = gr.Textbox(label=\"æ¨£æœ¬è³‡è¨Š\", interactive=False)\n",
    "            question_display = gr.Textbox(label=\"å•é¡Œ\", lines=2, interactive=False)\n",
    "            reference_display = gr.Textbox(label=\"åƒè€ƒç­”æ¡ˆ\", lines=3, interactive=False)\n",
    "            generated_display = gr.Textbox(label=\"ç”Ÿæˆæ–‡æœ¬\", lines=4, interactive=True)\n",
    "\n",
    "            with gr.Row():\n",
    "                relevance = gr.Slider(1, 5, value=3, step=1, label=\"ç›¸é—œæ€§ (1-5)\")\n",
    "                fluency = gr.Slider(1, 5, value=3, step=1, label=\"æµæš¢æ€§ (1-5)\")\n",
    "\n",
    "            with gr.Row():\n",
    "                informativeness = gr.Slider(1, 5, value=3, step=1, label=\"è³‡è¨Šæ€§ (1-5)\")\n",
    "                overall = gr.Slider(1, 5, value=3, step=1, label=\"æ•´é«”å“è³ª (1-5)\")\n",
    "\n",
    "            comments = gr.Textbox(\n",
    "                label=\"è©•è«–ï¼ˆå¯é¸ï¼‰\", lines=2, placeholder=\"å…¶ä»–æ„è¦‹æˆ–å»ºè­°...\"\n",
    "            )\n",
    "\n",
    "            with gr.Row():\n",
    "                submit_btn = gr.Button(\"æäº¤è©•ä¼°\", variant=\"primary\")\n",
    "                result_display = gr.Textbox(label=\"æäº¤çµæœ\", interactive=False)\n",
    "\n",
    "            # Event handlers\n",
    "            load_btn.click(\n",
    "                load_sample,\n",
    "                inputs=[sample_idx],\n",
    "                outputs=[\n",
    "                    question_display,\n",
    "                    reference_display,\n",
    "                    generated_display,\n",
    "                    info_display,\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            submit_btn.click(\n",
    "                evaluate_text,\n",
    "                inputs=[\n",
    "                    sample_idx,\n",
    "                    generated_display,\n",
    "                    relevance,\n",
    "                    fluency,\n",
    "                    informativeness,\n",
    "                    overall,\n",
    "                    comments,\n",
    "                ],\n",
    "                outputs=[result_display],\n",
    "            )\n",
    "\n",
    "        return demo\n",
    "\n",
    "    def load_annotations(\n",
    "        self, file_path: str = \"outs/eval/human_annotations.jsonl\"\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Load human annotations from file\"\"\"\n",
    "        annotations = []\n",
    "        if Path(file_path).exists():\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        annotations.append(json.loads(line.strip()))\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "        return annotations\n",
    "\n",
    "    def compute_human_metrics(self, annotations: List[Dict]) -> Dict[str, float]:\n",
    "        \"\"\"Compute aggregated human evaluation metrics\"\"\"\n",
    "        if not annotations:\n",
    "            return {}\n",
    "\n",
    "        df = pd.DataFrame(annotations)\n",
    "        metrics = {\n",
    "            \"avg_relevance\": df[\"relevance\"].mean(),\n",
    "            \"avg_fluency\": df[\"fluency\"].mean(),\n",
    "            \"avg_informativeness\": df[\"informativeness\"].mean(),\n",
    "            \"avg_overall\": df[\"overall\"].mean(),\n",
    "            \"std_overall\": df[\"overall\"].std(),\n",
    "            \"num_annotations\": len(annotations),\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "\n",
    "# Initialize human evaluator\n",
    "human_evaluator = HumanEvaluator()\n",
    "print(\"Human evaluation interface ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61826624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Multi-Model Comparative Evaluation\n",
    "class ComparativeEvaluator:\n",
    "    \"\"\"Compare multiple models/configurations on the same test set\"\"\"\n",
    "\n",
    "    def __init__(self, evaluator: QualityEvaluator):\n",
    "        self.evaluator = evaluator\n",
    "        self.results = []\n",
    "\n",
    "    def mock_model_generate(\n",
    "        self, model_name: str, question: str, context: str = \"\"\n",
    "    ) -> str:\n",
    "        \"\"\"Mock different model responses for demonstration\"\"\"\n",
    "        responses = {\n",
    "            \"baseline\": {\n",
    "                \"ä»€éº¼æ˜¯æª¢ç´¢å¢å¼·ç”Ÿæˆï¼ˆRAGï¼‰ï¼Ÿ\": \"RAGæ˜¯ä¸€ç¨®AIæŠ€è¡“ï¼Œçµåˆäº†æª¢ç´¢å’Œç”Ÿæˆã€‚\",\n",
    "                \"è«‹è§£é‡‹æ©Ÿå™¨å­¸ç¿’ä¸­çš„éæ“¬åˆç¾è±¡ã€‚\": \"éæ“¬åˆå°±æ˜¯æ¨¡å‹åœ¨è¨“ç·´æ•¸æ“šä¸Šæ•ˆæœå¥½ï¼Œä½†æ–°æ•¸æ“šä¸Šæ•ˆæœå·®ã€‚\",\n",
    "                \"å°ç£çš„åœ°ç†ä½ç½®æœ‰ä»€éº¼ç‰¹è‰²ï¼Ÿ\": \"å°ç£æ˜¯å€‹å³¶å¶¼ï¼Œåœ¨äºæ´²ã€‚\",\n",
    "                \"å¦‚ä½•æé«˜åœ˜éšŠæºé€šæ•ˆç‡ï¼Ÿ\": \"å¯ä»¥é–‹æœƒã€ç”¨å·¥å…·ã€‚\",\n",
    "                \"ä»€éº¼æ˜¯å€å¡ŠéˆæŠ€è¡“çš„æ ¸å¿ƒæ¦‚å¿µï¼Ÿ\": \"å€å¡Šéˆæ˜¯åˆ†æ•£å¼æŠ€è¡“ã€‚\",\n",
    "            },\n",
    "            \"improved\": {\n",
    "                \"ä»€éº¼æ˜¯æª¢ç´¢å¢å¼·ç”Ÿæˆï¼ˆRAGï¼‰ï¼Ÿ\": \"æª¢ç´¢å¢å¼·ç”Ÿæˆï¼ˆRAGï¼‰æ˜¯çµåˆè³‡è¨Šæª¢ç´¢èˆ‡æ–‡æœ¬ç”Ÿæˆçš„å…ˆé€²æŠ€è¡“ï¼Œèƒ½å¤ å¾å¤–éƒ¨çŸ¥è­˜åº«æª¢ç´¢ç›¸é—œè³‡è¨Šä¸¦å¢å¼·èªè¨€æ¨¡å‹çš„å›ç­”èƒ½åŠ›ï¼Œæå‡æº–ç¢ºæ€§ã€‚\",\n",
    "                \"è«‹è§£é‡‹æ©Ÿå™¨å­¸ç¿’ä¸­çš„éæ“¬åˆç¾è±¡ã€‚\": \"éæ“¬åˆæ˜¯æ©Ÿå™¨å­¸ç¿’ä¸­å¸¸è¦‹å•é¡Œï¼ŒæŒ‡æ¨¡å‹åœ¨è¨“ç·´æ•¸æ“šä¸Šè¡¨ç¾å„ªç•°ï¼Œä½†å°æ–°æ•¸æ“šçš„æ³›åŒ–èƒ½åŠ›è¼ƒå·®ã€‚é€™é€šå¸¸å› ç‚ºæ¨¡å‹éæ–¼è¤‡é›œï¼Œå­¸ç¿’äº†è¨“ç·´æ•¸æ“šçš„å™ªéŸ³ã€‚\",\n",
    "                \"å°ç£çš„åœ°ç†ä½ç½®æœ‰ä»€éº¼ç‰¹è‰²ï¼Ÿ\": \"å°ç£ä½æ–¼è¥¿å¤ªå¹³æ´‹å³¶å¼§ï¼Œæ±è‡¨å¤ªå¹³æ´‹ï¼Œè¥¿éš”å°ç£æµ·å³½èˆ‡å¤§é™¸ç›¸æœ›ï¼Œåœ°è™•ç†±å¸¶èˆ‡äºç†±å¸¶äº¤ç•Œï¼Œå…·æœ‰ç¨ç‰¹çš„åœ°ç†å„ªå‹¢ã€‚\",\n",
    "                \"å¦‚ä½•æé«˜åœ˜éšŠæºé€šæ•ˆç‡ï¼Ÿ\": \"æé«˜åœ˜éšŠæºé€šæ•ˆç‡éœ€è¦å»ºç«‹æ¸…æ™°æµç¨‹ã€é¸ç”¨é©ç•¶å·¥å…·ã€å®šæœŸæœƒè­°ã€ä¿ƒé€²é–‹æ”¾å°è©±ã€ç¢ºä¿è³‡è¨Šå…±äº«ï¼Œä¸¦åŸ¹é¤Šè‰¯å¥½çš„å‚¾è½ç¿’æ…£ã€‚\",\n",
    "                \"ä»€éº¼æ˜¯å€å¡ŠéˆæŠ€è¡“çš„æ ¸å¿ƒæ¦‚å¿µï¼Ÿ\": \"å€å¡Šéˆæ ¸å¿ƒæ¦‚å¿µåŒ…æ‹¬å»ä¸­å¿ƒåŒ–ã€ä¸å¯ç¯¡æ”¹æ€§ã€é€æ˜æ€§å’Œå…±è­˜æ©Ÿåˆ¶ã€‚é€éå¯†ç¢¼å­¸æ–¹æ³•å°‡äº¤æ˜“è¨˜éŒ„éˆæ¥æˆå®‰å…¨å¯ä¿¡çš„åˆ†æ•£å¼å¸³æœ¬ã€‚\",\n",
    "            },\n",
    "            \"advanced\": {\n",
    "                \"ä»€éº¼æ˜¯æª¢ç´¢å¢å¼·ç”Ÿæˆï¼ˆRAGï¼‰ï¼Ÿ\": \"æª¢ç´¢å¢å¼·ç”Ÿæˆï¼ˆRetrieval-Augmented Generation, RAGï¼‰æ˜¯ä¸€ç¨®å‰µæ–°çš„äººå·¥æ™ºæ…§æ¶æ§‹ï¼Œå°‡å‚³çµ±çš„è³‡è¨Šæª¢ç´¢ç³»çµ±èˆ‡å¤§å‹èªè¨€æ¨¡å‹æ·±åº¦æ•´åˆã€‚RAGç³»çµ±é¦–å…ˆå¾é¾å¤§çš„å¤–éƒ¨çŸ¥è­˜åº«ä¸­æª¢ç´¢èˆ‡æŸ¥è©¢ç›¸é—œçš„æ–‡æª”ç‰‡æ®µï¼Œç„¶å¾Œå°‡é€™äº›ä¸Šä¸‹æ–‡è³‡è¨Šèˆ‡åŸå§‹å•é¡Œä¸€èµ·è¼¸å…¥åˆ°ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤ ç”¢ç”Ÿæ›´æº–ç¢ºã€æ›´å…·æ™‚æ•ˆæ€§å’Œæ›´å…·å°ˆæ¥­æ€§çš„å›ç­”ã€‚é€™ç¨®æ–¹æ³•æœ‰æ•ˆè§£æ±ºäº†èªè¨€æ¨¡å‹çŸ¥è­˜æˆªæ­¢æ—¥æœŸçš„é™åˆ¶ï¼Œä¸¦å¤§å¹…æå‡äº†ç”Ÿæˆå…§å®¹çš„äº‹å¯¦æº–ç¢ºæ€§ã€‚\",\n",
    "                \"è«‹è§£é‡‹æ©Ÿå™¨å­¸ç¿’ä¸­çš„éæ“¬åˆç¾è±¡ã€‚\": \"éæ“¬åˆï¼ˆOverfittingï¼‰æ˜¯æ©Ÿå™¨å­¸ç¿’é ˜åŸŸçš„æ ¸å¿ƒæŒ‘æˆ°ä¹‹ä¸€ï¼Œè¡¨ç¾ç‚ºæ¨¡å‹åœ¨è¨“ç·´æ•¸æ“šé›†ä¸Šé”åˆ°æ¥µé«˜çš„æº–ç¢ºç‡ï¼Œå»åœ¨é©—è­‰é›†æˆ–æ¸¬è©¦é›†ä¸Šè¡¨ç¾é¡¯è‘—ä¸‹é™ã€‚é€™ç¨®ç¾è±¡çš„æ ¹æœ¬åŸå› åœ¨æ–¼æ¨¡å‹è¤‡é›œåº¦éé«˜ï¼Œå°è‡´å…¶ä¸åƒ…å­¸ç¿’äº†æ•¸æ“šä¸­çš„çœŸå¯¦æ¨¡å¼ï¼Œé‚„è¨˜æ†¶äº†è¨“ç·´æ¨£æœ¬ä¸­çš„éš¨æ©Ÿå™ªéŸ³ã€é›¢ç¾¤å€¼å’Œç‰¹å®šç´°ç¯€ã€‚éæ“¬åˆçš„æ¨¡å‹ç¼ºä¹æ³›åŒ–èƒ½åŠ›ï¼Œç„¡æ³•æœ‰æ•ˆè™•ç†æœªè¦‹éçš„æ–°æ•¸æ“šã€‚å¸¸è¦‹çš„ç·©è§£ç­–ç•¥åŒ…æ‹¬æ­£å‰‡åŒ–æŠ€è¡“ã€äº¤å‰é©—è­‰ã€æ—©åœæ³•ã€æ•¸æ“šæ“´å¢ä»¥åŠé™ä½æ¨¡å‹è¤‡é›œåº¦ç­‰æ–¹æ³•ã€‚\",\n",
    "                \"å°ç£çš„åœ°ç†ä½ç½®æœ‰ä»€éº¼ç‰¹è‰²ï¼Ÿ\": \"å°ç£å³¶åœ°ç†ä½ç½®æ¥µç‚ºç¨ç‰¹ä¸”æˆ°ç•¥é‡è¦ï¼Œä½æ–¼åŒ—ç·¯22Â°è‡³25Â°ã€æ±ç¶“120Â°è‡³122Â°ä¹‹é–“ï¼Œåº§è½åœ¨æ­äºå¤§é™¸æ¿å¡Šèˆ‡è²å¾‹è³“æµ·æ¿å¡Šçš„äº¤ç•Œè™•ã€‚å³¶å¶¼æ±è‡¨æµ©ç€šçš„å¤ªå¹³æ´‹ï¼Œè¥¿éš”å¯¬ç´„130å…¬é‡Œçš„å°ç£æµ·å³½èˆ‡ä¸­åœ‹å¤§é™¸ç¦å»ºçœç›¸æœ›ï¼Œå—ç«¯å·´å£«æµ·å³½é€£æ¥å—æµ·ï¼ŒåŒ—éƒ¨å‰‡é¢å‘æ±æµ·ã€‚å°ç£æ­£è™•æ–¼åŒ—å›æ­¸ç·šç©¿è¶Šçš„ç†±å¸¶èˆ‡äºç†±å¸¶æ°£å€™äº¤ç•Œå¸¶ï¼Œé€ å°±äº†è±å¯Œçš„ç”Ÿç‰©å¤šæ¨£æ€§ã€‚åœ°å½¢ä»¥å±±åœ°ç‚ºä¸»é«”ï¼Œä¸­å¤®å±±è„ˆç¸±è²«å—åŒ—ï¼Œå¹³åŸä¸»è¦åˆ†å¸ƒåœ¨è¥¿éƒ¨æ²¿æµ·ï¼Œå½¢æˆäº†ã€Œé«˜å±±æµ·å³¶ã€çš„ç¨ç‰¹åœ°ç†ç‰¹è‰²ã€‚\",\n",
    "                \"å¦‚ä½•æé«˜åœ˜éšŠæºé€šæ•ˆç‡ï¼Ÿ\": \"æå‡åœ˜éšŠæºé€šæ•ˆç‡æ˜¯ç¾ä»£çµ„ç¹”ç®¡ç†çš„é—œéµè­°é¡Œï¼Œéœ€è¦ç³»çµ±æ€§çš„ç­–ç•¥è¦åŠƒã€‚é¦–è¦æ­¥é©Ÿæ˜¯å»ºç«‹æ¨™æº–åŒ–çš„æºé€šæµç¨‹èˆ‡å”è­°ï¼Œæ˜ç¢ºä¸åŒæƒ…å¢ƒä¸‹çš„æºé€šæ–¹å¼ã€è²¬ä»»æ­¸å±¬å’Œæ±ºç­–æ¬Šé™ã€‚æŠ€è¡“å±¤é¢æ‡‰é¸æ“‡é©åˆåœ˜éšŠè¦æ¨¡èˆ‡å·¥ä½œæ€§è³ªçš„å”ä½œå·¥å…·ï¼Œå¦‚å³æ™‚é€šè¨Šå¹³å°ã€å°ˆæ¡ˆç®¡ç†ç³»çµ±å’Œè¦–è¨Šæœƒè­°è»Ÿé«”ã€‚çµ„ç¹”å±¤é¢éœ€è¦å»ºç«‹å®šæœŸçš„åœ˜éšŠæœƒè­°æ©Ÿåˆ¶ï¼ŒåŒ…æ‹¬æ—¥å¸¸ç«™æœƒã€é€±æœŸæ€§å›é¡§å’Œå°ˆæ¡ˆé‡Œç¨‹ç¢‘è¨è«–ã€‚æ–‡åŒ–å±¤é¢è¦åŸ¹é¤Šé–‹æ”¾é€æ˜çš„æºé€šæ°›åœï¼Œé¼“å‹µæˆå“¡ä¸»å‹•åˆ†äº«è³‡è¨Šã€æå‡ºç–‘å•å’Œå»ºè¨­æ€§æ„è¦‹ã€‚æ­¤å¤–ï¼Œé‚„éœ€è¦å¼·åŒ–ç©æ¥µå‚¾è½æŠ€å·§ã€æä¾›æºé€šæŠ€èƒ½åŸ¹è¨“ï¼Œä¸¦å»ºç«‹æœ‰æ•ˆçš„åé¥‹æ©Ÿåˆ¶ä»¥æŒçºŒå„ªåŒ–æºé€šå“è³ªã€‚\",\n",
    "                \"ä»€éº¼æ˜¯å€å¡ŠéˆæŠ€è¡“çš„æ ¸å¿ƒæ¦‚å¿µï¼Ÿ\": \"å€å¡Šéˆï¼ˆBlockchainï¼‰æŠ€è¡“çš„æ ¸å¿ƒæ¦‚å¿µå»ºç«‹åœ¨å››å¤§æ”¯æŸ±ä¹‹ä¸Šï¼šå»ä¸­å¿ƒåŒ–ã€ä¸å¯ç¯¡æ”¹æ€§ã€é€æ˜æ€§å’Œå…±è­˜æ©Ÿåˆ¶ã€‚å»ä¸­å¿ƒåŒ–æ„å‘³è‘—ç³»çµ±ä¸ä¾è³´å–®ä¸€çš„ä¸­å¤®æ©Ÿæ§‹æ§åˆ¶ï¼Œè€Œæ˜¯ç”±åˆ†æ•£åœ¨ç¶²è·¯ä¸­çš„å¤šå€‹ç¯€é»å…±åŒç¶­è­·ã€‚ä¸å¯ç¯¡æ”¹æ€§é€éå¯†ç¢¼å­¸é›œæ¹Šå‡½æ•¸å’Œéˆå¼çµæ§‹å¯¦ç¾ï¼Œæ¯å€‹å€å¡Šéƒ½åŒ…å«å‰ä¸€å€‹å€å¡Šçš„é›œæ¹Šå€¼ï¼Œå½¢æˆä¸å¯é€†çš„æ™‚é–“æˆ³è¨˜éŒ„éˆã€‚é€æ˜æ€§ç¢ºä¿æ‰€æœ‰äº¤æ˜“è¨˜éŒ„å°ç¶²è·¯åƒèˆ‡è€…å…¬é–‹å¯è¦‹ï¼Œæå‡ç³»çµ±çš„å¯å¯©è¨ˆæ€§ã€‚å…±è­˜æ©Ÿåˆ¶ï¼ˆå¦‚å·¥ä½œé‡è­‰æ˜ã€æ¬Šç›Šè­‰æ˜ç­‰ï¼‰ç¢ºä¿ç¶²è·¯ç¯€é»å°æ–°å€å¡Šçš„æœ‰æ•ˆæ€§é”æˆä¸€è‡´ï¼Œç¶­è­·åˆ†æ•£å¼å¸³æœ¬çš„å®Œæ•´æ€§ã€‚é€™äº›æ ¸å¿ƒç‰¹æ€§ä½¿å€å¡Šéˆæˆç‚ºæ§‹å»ºå¯ä¿¡ä»»æ•¸ä½ç¶“æ¿ŸåŸºç¤è¨­æ–½çš„é©å‘½æ€§æŠ€è¡“ï¼Œåœ¨é‡‘èæœå‹™ã€ä¾›æ‡‰éˆç®¡ç†ã€æ•¸ä½èº«ä»½é©—è­‰ç­‰é ˜åŸŸå±•ç¾å·¨å¤§æ½›åŠ›ã€‚\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Return mock response based on model and question\n",
    "        model_responses = responses.get(model_name, responses[\"baseline\"])\n",
    "        return model_responses.get(\n",
    "            question, f\"é€™æ˜¯{model_name}æ¨¡å‹å°æ–¼ã€Œ{question}ã€çš„å›ç­”ã€‚\"\n",
    "        )\n",
    "\n",
    "    def evaluate_models(\n",
    "        self, models: List[str], samples: List[EvalSample]\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Evaluate multiple models on test samples\"\"\"\n",
    "        results = []\n",
    "\n",
    "        for model_name in models:\n",
    "            print(f\"Evaluating model: {model_name}\")\n",
    "            model_results = []\n",
    "\n",
    "            for sample in samples:\n",
    "                # Generate response (mock)\n",
    "                generated = self.mock_model_generate(\n",
    "                    model_name, sample.question, sample.context\n",
    "                )\n",
    "\n",
    "                # Evaluate quality\n",
    "                metrics = self.evaluator.evaluate_sample(sample, generated)\n",
    "\n",
    "                result = {\n",
    "                    \"model\": model_name,\n",
    "                    \"sample_id\": sample.id,\n",
    "                    \"domain\": sample.domain,\n",
    "                    \"question\": sample.question,\n",
    "                    \"reference\": sample.reference,\n",
    "                    \"generated\": generated,\n",
    "                    **metrics,\n",
    "                }\n",
    "\n",
    "                model_results.append(result)\n",
    "\n",
    "            results.extend(model_results)\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def compute_model_summary(self, results_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Compute summary statistics for each model\"\"\"\n",
    "        summary = (\n",
    "            results_df.groupby(\"model\")\n",
    "            .agg(\n",
    "                {\n",
    "                    \"rouge_l\": [\"mean\", \"std\"],\n",
    "                    \"chrf_plus\": [\"mean\", \"std\"],\n",
    "                    \"length_penalty\": [\"mean\", \"std\"],\n",
    "                    \"composite\": [\"mean\", \"std\"],\n",
    "                    \"sample_id\": \"count\",\n",
    "                }\n",
    "            )\n",
    "            .round(4)\n",
    "        )\n",
    "\n",
    "        # Flatten column names\n",
    "        summary.columns = [\"_\".join(col).strip() for col in summary.columns]\n",
    "        summary = summary.rename(columns={\"sample_id_count\": \"num_samples\"})\n",
    "\n",
    "        return summary.reset_index()\n",
    "\n",
    "\n",
    "# Run comparative evaluation\n",
    "models_to_compare = [\"baseline\", \"improved\", \"advanced\"]\n",
    "comparative_evaluator = ComparativeEvaluator(evaluator)\n",
    "\n",
    "print(\"Running comparative evaluation...\")\n",
    "results_df = comparative_evaluator.evaluate_models(models_to_compare, test_samples)\n",
    "summary_df = comparative_evaluator.compute_model_summary(results_df)\n",
    "\n",
    "print(\"\\n=== Model Comparison Summary ===\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaab0dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Quality Score Aggregation and Reporting\n",
    "class QualityReporter:\n",
    "    \"\"\"Generate comprehensive quality evaluation reports\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    def generate_detailed_report(\n",
    "        self,\n",
    "        results_df: pd.DataFrame,\n",
    "        summary_df: pd.DataFrame,\n",
    "        human_metrics: Dict = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Generate detailed evaluation report\"\"\"\n",
    "\n",
    "        report_lines = [\n",
    "            \"# Text Generation Quality Evaluation Report\",\n",
    "            f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "            \"\",\n",
    "            \"## Executive Summary\",\n",
    "            f\"- Evaluated {len(results_df['model'].unique())} models\",\n",
    "            f\"- Tested on {len(results_df['sample_id'].unique())} samples\",\n",
    "            f\"- Domains: {', '.join(results_df['domain'].unique())}\",\n",
    "            \"\",\n",
    "            \"## Automatic Metrics Summary\",\n",
    "            \"| Model | Rouge-L | chrF++ | Length Penalty | Composite Score |\",\n",
    "            \"| --- | --- | --- | --- | --- |\",\n",
    "        ]\n",
    "\n",
    "        for _, row in summary_df.iterrows():\n",
    "            report_lines.append(\n",
    "                f\"| {row['model']} | \"\n",
    "                f\"{row['rouge_l_mean']:.3f}Â±{row['rouge_l_std']:.3f} | \"\n",
    "                f\"{row['chrf_plus_mean']:.3f}Â±{row['chrf_plus_std']:.3f} | \"\n",
    "                f\"{row['length_penalty_mean']:.3f}Â±{row['length_penalty_std']:.3f} | \"\n",
    "                f\"{row['composite_mean']:.3f}Â±{row['composite_std']:.3f} |\"\n",
    "            )\n",
    "\n",
    "        # Add human evaluation metrics if available\n",
    "        if human_metrics:\n",
    "            report_lines.extend(\n",
    "                [\n",
    "                    \"\",\n",
    "                    \"## Human Evaluation Metrics\",\n",
    "                    f\"- Number of annotations: {human_metrics.get('num_annotations', 0)}\",\n",
    "                    f\"- Average relevance: {human_metrics.get('avg_relevance', 0):.2f}/5\",\n",
    "                    f\"- Average fluency: {human_metrics.get('avg_fluency', 0):.2f}/5\",\n",
    "                    f\"- Average informativeness: {human_metrics.get('avg_informativeness', 0):.2f}/5\",\n",
    "                    f\"- Average overall quality: {human_metrics.get('avg_overall', 0):.2f}/5 (Â±{human_metrics.get('std_overall', 0):.2f})\",\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # Best performing model\n",
    "        best_model = summary_df.loc[summary_df[\"composite_mean\"].idxmax(), \"model\"]\n",
    "        best_score = summary_df.loc[\n",
    "            summary_df[\"composite_mean\"].idxmax(), \"composite_mean\"\n",
    "        ]\n",
    "\n",
    "        report_lines.extend(\n",
    "            [\n",
    "                \"\",\n",
    "                \"## Key Findings\",\n",
    "                f\"- **Best performing model**: {best_model} (composite score: {best_score:.3f})\",\n",
    "                \"- **Rouge-L**: Measures overlap of longest common subsequences\",\n",
    "                \"- **chrF++**: Character-level F-score with word order consideration\",\n",
    "                \"- **Length Penalty**: Penalizes responses too short or too long\",\n",
    "                \"\",\n",
    "                \"## Recommendations\",\n",
    "                \"1. Consider the trade-off between automatic metrics and human preferences\",\n",
    "                \"2. Evaluate on larger, domain-specific datasets for production use\",\n",
    "                \"3. Include task-specific metrics (e.g., factuality, coherence)\",\n",
    "                \"4. Regularly update evaluation benchmarks\",\n",
    "                \"\",\n",
    "                \"## Sample Outputs (Best vs Worst)\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Add sample comparisons\n",
    "        best_samples = results_df[results_df[\"model\"] == best_model].nlargest(\n",
    "            2, \"composite\"\n",
    "        )\n",
    "        for idx, (_, sample) in enumerate(best_samples.iterrows()):\n",
    "            report_lines.extend(\n",
    "                [\n",
    "                    f\"### Sample {idx+1} (Score: {sample['composite']:.3f})\",\n",
    "                    f\"**Question**: {sample['question']}\",\n",
    "                    f\"**Generated**: {sample['generated'][:200]}...\",\n",
    "                    \"\",\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        return \"\\n\".join(report_lines)\n",
    "\n",
    "    def save_results(\n",
    "        self,\n",
    "        results_df: pd.DataFrame,\n",
    "        summary_df: pd.DataFrame,\n",
    "        report_text: str,\n",
    "        human_metrics: Dict = None,\n",
    "    ):\n",
    "        \"\"\"Save all evaluation results to files\"\"\"\n",
    "\n",
    "        # Save detailed results\n",
    "        results_file = f\"outs/eval/quality_results_{self.timestamp}.csv\"\n",
    "        results_df.to_csv(results_file, index=False, encoding=\"utf-8\")\n",
    "        print(f\"âœ“ Detailed results saved to: {results_file}\")\n",
    "\n",
    "        # Save summary\n",
    "        summary_file = f\"outs/eval/quality_summary_{self.timestamp}.csv\"\n",
    "        summary_df.to_csv(summary_file, index=False, encoding=\"utf-8\")\n",
    "        print(f\"âœ“ Summary saved to: {summary_file}\")\n",
    "\n",
    "        # Save report\n",
    "        report_file = f\"outs/reports/quality_report_{self.timestamp}.md\"\n",
    "        with open(report_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(report_text)\n",
    "        print(f\"âœ“ Report saved to: {report_file}\")\n",
    "\n",
    "        # Save metrics JSON\n",
    "        metrics_data = {\n",
    "            \"timestamp\": self.timestamp,\n",
    "            \"num_models\": len(results_df[\"model\"].unique()),\n",
    "            \"num_samples\": len(results_df[\"sample_id\"].unique()),\n",
    "            \"summary\": summary_df.to_dict(\"records\"),\n",
    "            \"human_metrics\": human_metrics or {},\n",
    "        }\n",
    "\n",
    "        metrics_file = f\"outs/eval/quality_metrics_{self.timestamp}.json\"\n",
    "        with open(metrics_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metrics_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"âœ“ Metrics saved to: {metrics_file}\")\n",
    "\n",
    "        return {\n",
    "            \"results_file\": results_file,\n",
    "            \"summary_file\": summary_file,\n",
    "            \"report_file\": report_file,\n",
    "            \"metrics_file\": metrics_file,\n",
    "        }\n",
    "\n",
    "\n",
    "# Generate comprehensive report\n",
    "reporter = QualityReporter()\n",
    "\n",
    "# Load human annotations (if any exist)\n",
    "human_annotations = human_evaluator.load_annotations()\n",
    "human_metrics = (\n",
    "    human_evaluator.compute_human_metrics(human_annotations)\n",
    "    if human_annotations\n",
    "    else None\n",
    ")\n",
    "\n",
    "# Generate report\n",
    "report_text = reporter.generate_detailed_report(results_df, summary_df, human_metrics)\n",
    "\n",
    "# Save all results\n",
    "saved_files = reporter.save_results(results_df, summary_df, report_text, human_metrics)\n",
    "\n",
    "print(\"\\n=== Quality Evaluation Report Generated ===\")\n",
    "print(report_text[:1000] + \"...\" if len(report_text) > 1000 else report_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3f6eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Smoke Test - Run evaluation on subset\n",
    "def run_smoke_test():\n",
    "    \"\"\"Quick smoke test on a subset of samples\"\"\"\n",
    "    print(\"ğŸ”¥ Running smoke test...\")\n",
    "\n",
    "    # Test with first 3 samples only\n",
    "    smoke_samples = test_samples[:3]\n",
    "    smoke_models = [\"baseline\", \"improved\"]\n",
    "\n",
    "    print(f\"Testing {len(smoke_models)} models on {len(smoke_samples)} samples\")\n",
    "\n",
    "    # Run evaluation\n",
    "    smoke_evaluator = QualityEvaluator()\n",
    "    smoke_results = []\n",
    "\n",
    "    for model in smoke_models:\n",
    "        for sample in smoke_samples:\n",
    "            generated = comparative_evaluator.mock_model_generate(\n",
    "                model, sample.question\n",
    "            )\n",
    "            metrics = smoke_evaluator.evaluate_sample(sample, generated)\n",
    "\n",
    "            result = {\n",
    "                \"model\": model,\n",
    "                \"sample_id\": sample.id,\n",
    "                \"rouge_l\": metrics[\"rouge_l\"],\n",
    "                \"chrf_plus\": metrics[\"chrf_plus\"],\n",
    "                \"composite\": metrics[\"composite\"],\n",
    "            }\n",
    "            smoke_results.append(result)\n",
    "\n",
    "    smoke_df = pd.DataFrame(smoke_results)\n",
    "\n",
    "    # Quick summary\n",
    "    print(\"\\n=== Smoke Test Results ===\")\n",
    "    for model in smoke_models:\n",
    "        model_data = smoke_df[smoke_df[\"model\"] == model]\n",
    "        avg_composite = model_data[\"composite\"].mean()\n",
    "        print(f\"{model}: avg composite = {avg_composite:.3f}\")\n",
    "\n",
    "    # Verify all scores are reasonable (0-1 range)\n",
    "    all_scores_valid = (\n",
    "        (smoke_df[\"rouge_l\"] >= 0).all()\n",
    "        and (smoke_df[\"rouge_l\"] <= 1).all()\n",
    "        and (smoke_df[\"chrf_plus\"] >= 0).all()\n",
    "        and (smoke_df[\"chrf_plus\"] <= 1).all()\n",
    "        and (smoke_df[\"composite\"] >= 0).all()\n",
    "        and (smoke_df[\"composite\"] <= 1).all()\n",
    "    )\n",
    "\n",
    "    if all_scores_valid:\n",
    "        print(\"âœ… All metrics within expected range [0, 1]\")\n",
    "    else:\n",
    "        print(\"âŒ Some metrics outside expected range\")\n",
    "\n",
    "    return smoke_df\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_results = run_smoke_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7adfd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Advanced Quality Analysis\n",
    "class AdvancedQualityAnalyzer:\n",
    "    \"\"\"Advanced analysis of quality patterns and insights\"\"\"\n",
    "\n",
    "    def analyze_domain_performance(self, results_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Analyze performance by domain\"\"\"\n",
    "        domain_analysis = (\n",
    "            results_df.groupby([\"model\", \"domain\"])\n",
    "            .agg({\"rouge_l\": \"mean\", \"chrf_plus\": \"mean\", \"composite\": \"mean\"})\n",
    "            .round(3)\n",
    "        )\n",
    "\n",
    "        return domain_analysis.reset_index()\n",
    "\n",
    "    def identify_failure_cases(\n",
    "        self, results_df: pd.DataFrame, threshold: float = 0.3\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Identify samples with poor quality scores\"\"\"\n",
    "        failure_cases = results_df[results_df[\"composite\"] < threshold].copy()\n",
    "        failure_cases = failure_cases.sort_values(\"composite\")\n",
    "\n",
    "        return failure_cases[\n",
    "            [\"model\", \"sample_id\", \"domain\", \"question\", \"composite\", \"generated\"]\n",
    "        ]\n",
    "\n",
    "    def compute_consistency_metrics(self, results_df: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"Compute consistency metrics across samples\"\"\"\n",
    "        consistency = {}\n",
    "\n",
    "        for model in results_df[\"model\"].unique():\n",
    "            model_data = results_df[results_df[\"model\"] == model]\n",
    "\n",
    "            # Coefficient of variation (std/mean) for each metric\n",
    "            for metric in [\"rouge_l\", \"chrf_plus\", \"composite\"]:\n",
    "                cv = (\n",
    "                    model_data[metric].std() / model_data[metric].mean()\n",
    "                    if model_data[metric].mean() > 0\n",
    "                    else float(\"inf\")\n",
    "                )\n",
    "                consistency[f\"{model}_{metric}_cv\"] = cv\n",
    "\n",
    "        return consistency\n",
    "\n",
    "\n",
    "# Run advanced analysis\n",
    "analyzer = AdvancedQualityAnalyzer()\n",
    "\n",
    "domain_perf = analyzer.analyze_domain_performance(results_df)\n",
    "print(\"\\n=== Performance by Domain ===\")\n",
    "print(domain_perf.to_string(index=False))\n",
    "\n",
    "failure_cases = analyzer.identify_failure_cases(results_df, threshold=0.4)\n",
    "print(f\"\\n=== Failure Cases (composite < 0.4) ===\")\n",
    "print(f\"Found {len(failure_cases)} failure cases\")\n",
    "if not failure_cases.empty:\n",
    "    print(failure_cases[[\"model\", \"sample_id\", \"composite\"]].to_string(index=False))\n",
    "\n",
    "consistency = analyzer.compute_consistency_metrics(results_df)\n",
    "print(f\"\\n=== Consistency Metrics (Coefficient of Variation) ===\")\n",
    "for metric, value in consistency.items():\n",
    "    print(f\"{metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65e41f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Integration with RAG Groundedness (Optional)\n",
    "def integrate_groundedness_check(\n",
    "    results_df: pd.DataFrame, context_samples: List[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Add groundedness checking to quality evaluation\"\"\"\n",
    "\n",
    "    def simple_groundedness_score(generated: str, context: str) -> float:\n",
    "        \"\"\"Simple groundedness check based on keyword overlap\"\"\"\n",
    "        if not context or not generated:\n",
    "            return 0.0\n",
    "\n",
    "        # Tokenize (simple split for demo)\n",
    "        gen_words = set(generated.lower().split())\n",
    "        ctx_words = set(context.lower().split())\n",
    "\n",
    "        if not gen_words:\n",
    "            return 0.0\n",
    "\n",
    "        # Jaccard similarity\n",
    "        overlap = len(gen_words & ctx_words)\n",
    "        union = len(gen_words | ctx_words)\n",
    "\n",
    "        return overlap / union if union > 0 else 0.0\n",
    "\n",
    "    # Add groundedness scores (mock context for demo)\n",
    "    results_with_groundedness = results_df.copy()\n",
    "\n",
    "    if context_samples is None:\n",
    "        # Mock context for each sample\n",
    "        context_samples = [\n",
    "            \"æª¢ç´¢å¢å¼·ç”Ÿæˆæ˜¯çµåˆè³‡è¨Šæª¢ç´¢å’Œæ–‡æœ¬ç”Ÿæˆçš„æŠ€è¡“ï¼Œé€šéå¤–éƒ¨çŸ¥è­˜åº«å¢å¼·èªè¨€æ¨¡å‹èƒ½åŠ›ã€‚\",\n",
    "            \"éæ“¬åˆæ˜¯æ©Ÿå™¨å­¸ç¿’ä¸­æ¨¡å‹åœ¨è¨“ç·´æ•¸æ“šè¡¨ç¾å¥½ä½†åœ¨æ–°æ•¸æ“šè¡¨ç¾å·®çš„ç¾è±¡ã€‚\",\n",
    "            \"å°ç£ä½æ–¼äºæ´²æ±éƒ¨ï¼Œæ˜¯è¥¿å¤ªå¹³æ´‹ä¸Šçš„å³¶å¶¼ï¼Œå…·æœ‰ç¨ç‰¹åœ°ç†ä½ç½®ã€‚\",\n",
    "            \"åœ˜éšŠæºé€šæ•ˆç‡å¯é€éå»ºç«‹æµç¨‹ã€ä½¿ç”¨å·¥å…·ã€å®šæœŸæœƒè­°ç­‰æ–¹å¼æå‡ã€‚\",\n",
    "            \"å€å¡Šéˆæ˜¯åˆ†æ•£å¼å¸³æœ¬æŠ€è¡“ï¼Œå…·æœ‰å»ä¸­å¿ƒåŒ–ã€ä¸å¯ç¯¡æ”¹ç­‰ç‰¹æ€§ã€‚\",\n",
    "        ] * 3  # Repeat for all models\n",
    "\n",
    "    groundedness_scores = []\n",
    "    for idx, row in results_with_groundedness.iterrows():\n",
    "        sample_idx = list(test_samples).index(\n",
    "            next(s for s in test_samples if s.id == row[\"sample_id\"])\n",
    "        )\n",
    "        context = (\n",
    "            context_samples[sample_idx] if sample_idx < len(context_samples) else \"\"\n",
    "        )\n",
    "\n",
    "        groundedness = simple_groundedness_score(row[\"generated\"], context)\n",
    "        groundedness_scores.append(groundedness)\n",
    "\n",
    "    results_with_groundedness[\"groundedness\"] = groundedness_scores\n",
    "\n",
    "    # Update composite score to include groundedness\n",
    "    results_with_groundedness[\"composite_with_groundedness\"] = (\n",
    "        0.3 * results_with_groundedness[\"rouge_l\"]\n",
    "        + 0.3 * results_with_groundedness[\"chrf_plus\"]\n",
    "        + 0.2 * results_with_groundedness[\"length_penalty\"]\n",
    "        + 0.2 * results_with_groundedness[\"groundedness\"]\n",
    "    )\n",
    "\n",
    "    return results_with_groundedness\n",
    "\n",
    "\n",
    "# Add groundedness analysis\n",
    "results_with_groundedness = integrate_groundedness_check(results_df)\n",
    "\n",
    "print(\"\\n=== Updated Results with Groundedness ===\")\n",
    "groundedness_summary = (\n",
    "    results_with_groundedness.groupby(\"model\")[\n",
    "        [\"composite\", \"groundedness\", \"composite_with_groundedness\"]\n",
    "    ]\n",
    "    .mean()\n",
    "    .round(3)\n",
    ")\n",
    "print(groundedness_summary.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d4f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Summary and Next Steps\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š STAGE 7 - NB62 QUALITY EVALUATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nâœ… COMPLETED:\")\n",
    "print(\"â€¢ Rouge-L and chrF++ automatic metrics implementation\")\n",
    "print(\"â€¢ Human evaluation interface with Gradio\")\n",
    "print(\"â€¢ Multi-model comparative evaluation pipeline\")\n",
    "print(\"â€¢ Comprehensive reporting and CSV export\")\n",
    "print(\"â€¢ Groundedness integration for RAG applications\")\n",
    "print(\"â€¢ Domain-specific performance analysis\")\n",
    "print(\"â€¢ Failure case identification and consistency metrics\")\n",
    "\n",
    "print(\"\\nğŸ”‘ CORE CONCEPTS:\")\n",
    "print(\"â€¢ Rouge-L: Longest common subsequence F1 score\")\n",
    "print(\"â€¢ chrF++: Character-level F-score with word order\")\n",
    "print(\"â€¢ Human evaluation: Relevance, fluency, informativeness\")\n",
    "print(\"â€¢ Composite scoring: Weighted combination of metrics\")\n",
    "print(\"â€¢ Groundedness: Context-grounded generation quality\")\n",
    "print(\"â€¢ Consistency: Coefficient of variation across samples\")\n",
    "\n",
    "print(\"\\nâš ï¸ PITFALLS:\")\n",
    "print(\"â€¢ Automatic metrics may not correlate with human judgment\")\n",
    "print(\"â€¢ Small test sets can give misleading results\")\n",
    "print(\"â€¢ Domain-specific metrics often more valuable than general ones\")\n",
    "print(\"â€¢ Human annotation requires careful guideline design\")\n",
    "print(\"â€¢ Groundedness checking needs sophisticated NLI models\")\n",
    "\n",
    "print(\"\\nğŸ¯ NEXT STEPS:\")\n",
    "print(\"â€¢ Scale evaluation to larger, diverse datasets\")\n",
    "print(\"â€¢ Implement semantic similarity metrics (BERTScore)\")\n",
    "print(\"â€¢ Add task-specific metrics (factuality, coherence)\")\n",
    "print(\"â€¢ Integrate with continuous evaluation pipelines\")\n",
    "print(\"â€¢ Build evaluation leaderboards for model comparison\")\n",
    "\n",
    "print(\"\\nğŸ“ OUTPUT FILES:\")\n",
    "for file_type, file_path in saved_files.items():\n",
    "    print(f\"â€¢ {file_type}: {file_path}\")\n",
    "\n",
    "print(f\"\\nğŸ” KEY FINDINGS:\")\n",
    "best_model_row = summary_df.loc[summary_df[\"composite_mean\"].idxmax()]\n",
    "print(f\"â€¢ Best performing model: {best_model_row['model']}\")\n",
    "print(f\"â€¢ Best composite score: {best_model_row['composite_mean']:.3f}\")\n",
    "print(f\"â€¢ Total samples evaluated: {len(results_df)}\")\n",
    "print(f\"â€¢ Models compared: {len(results_df['model'].unique())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Quality evaluation framework ready for production use! ğŸš€\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
