{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb17f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 7 - nb62 | Text Generation Quality Evaluation\n",
    "# Goals: Rouge-L, chrF++, human annotation, comparative analysis\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (複製到每本 notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4028b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Dependencies and Setup\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Text evaluation metrics\n",
    "try:\n",
    "    from rouge_score import rouge_scorer\n",
    "\n",
    "    print(\"✓ rouge-score available\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ Installing rouge-score...\")\n",
    "    os.system(\"pip install rouge-score\")\n",
    "    from rouge_score import rouge_scorer\n",
    "\n",
    "try:\n",
    "    import sacrebleu\n",
    "\n",
    "    print(\"✓ sacrebleu available\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ Installing sacrebleu...\")\n",
    "    os.system(\"pip install sacrebleu\")\n",
    "    import sacrebleu\n",
    "\n",
    "# Optional: Gradio for human evaluation interface\n",
    "try:\n",
    "    import gradio as gr\n",
    "\n",
    "    print(\"✓ gradio available\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ Installing gradio...\")\n",
    "    os.system(\"pip install gradio\")\n",
    "    import gradio as gr\n",
    "\n",
    "# Create output directories\n",
    "for dir_name in [\"outs/eval\", \"outs/reports\", \"data/eval\"]:\n",
    "    pathlib.Path(dir_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Dependencies loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc69d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Test Dataset and Reference Answers Setup\n",
    "@dataclass\n",
    "class EvalSample:\n",
    "    \"\"\"Single evaluation sample with question, reference, and generated answers\"\"\"\n",
    "\n",
    "    id: str\n",
    "    question: str\n",
    "    reference: str\n",
    "    context: str = \"\"\n",
    "    domain: str = \"general\"\n",
    "\n",
    "\n",
    "class QualityEvaluator:\n",
    "    \"\"\"Text generation quality evaluator with multiple metrics\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize Rouge scorer for Chinese/English\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=False)\n",
    "\n",
    "    def compute_rouge_l(self, generated: str, reference: str) -> float:\n",
    "        \"\"\"Compute Rouge-L F1 score\"\"\"\n",
    "        try:\n",
    "            scores = self.rouge_scorer.score(reference, generated)\n",
    "            return scores[\"rougeL\"].fmeasure\n",
    "        except Exception as e:\n",
    "            print(f\"Rouge-L error: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def compute_chrf(self, generated: str, reference: str) -> float:\n",
    "        \"\"\"Compute chrF++ score\"\"\"\n",
    "        try:\n",
    "            # chrF++ with word and character n-grams\n",
    "            score = sacrebleu.sentence_chrf(generated, [reference], word_order=2)\n",
    "            return score.score / 100.0  # Normalize to 0-1\n",
    "        except Exception as e:\n",
    "            print(f\"chrF++ error: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def compute_length_penalty(self, generated: str, reference: str) -> float:\n",
    "        \"\"\"Length-based penalty (closer to reference length is better)\"\"\"\n",
    "        if not reference.strip():\n",
    "            return 0.0\n",
    "\n",
    "        gen_len = len(generated.strip())\n",
    "        ref_len = len(reference.strip())\n",
    "\n",
    "        if gen_len == 0:\n",
    "            return 0.0\n",
    "\n",
    "        # Penalty for being too short or too long\n",
    "        ratio = min(gen_len, ref_len) / max(gen_len, ref_len)\n",
    "        return ratio\n",
    "\n",
    "    def evaluate_sample(self, sample: EvalSample, generated: str) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate a single generated text against reference\"\"\"\n",
    "        metrics = {\n",
    "            \"rouge_l\": self.compute_rouge_l(generated, sample.reference),\n",
    "            \"chrf_plus\": self.compute_chrf(generated, sample.reference),\n",
    "            \"length_penalty\": self.compute_length_penalty(generated, sample.reference),\n",
    "        }\n",
    "\n",
    "        # Composite score (weighted average)\n",
    "        metrics[\"composite\"] = (\n",
    "            0.4 * metrics[\"rouge_l\"]\n",
    "            + 0.4 * metrics[\"chrf_plus\"]\n",
    "            + 0.2 * metrics[\"length_penalty\"]\n",
    "        )\n",
    "\n",
    "        return metrics\n",
    "\n",
    "\n",
    "# Create sample test dataset\n",
    "def create_test_dataset() -> List[EvalSample]:\n",
    "    \"\"\"Create a small test dataset for evaluation\"\"\"\n",
    "    samples = [\n",
    "        EvalSample(\n",
    "            id=\"qa_001\",\n",
    "            question=\"什麼是檢索增強生成（RAG）？\",\n",
    "            reference=\"檢索增強生成（RAG）是一種結合資訊檢索和文本生成的技術，通過從外部知識庫檢索相關資訊來增強語言模型的生成能力，提高回答的準確性和時效性。\",\n",
    "            domain=\"tech\",\n",
    "        ),\n",
    "        EvalSample(\n",
    "            id=\"qa_002\",\n",
    "            question=\"請解釋機器學習中的過擬合現象。\",\n",
    "            reference=\"過擬合是指模型在訓練數據上表現很好，但在新的、未見過的數據上表現較差的現象。這通常是因為模型過於複雜，學習了訓練數據中的噪音和特殊情況，而非通用規律。\",\n",
    "            domain=\"tech\",\n",
    "        ),\n",
    "        EvalSample(\n",
    "            id=\"qa_003\",\n",
    "            question=\"台灣的地理位置有什麼特色？\",\n",
    "            reference=\"台灣位於亞洲東部，西太平洋島弧上，東臨太平洋，西隔台灣海峽與中國大陸相望。地處熱帶與亞熱帶交界，地形多山，平原主要分布在西部沿海。\",\n",
    "            domain=\"general\",\n",
    "        ),\n",
    "        EvalSample(\n",
    "            id=\"qa_004\",\n",
    "            question=\"如何提高團隊溝通效率？\",\n",
    "            reference=\"提高團隊溝通效率的方法包括：建立清晰的溝通流程、使用合適的溝通工具、定期舉行團隊會議、鼓勵開放透明的對話、確保資訊及時共享、培養積極傾聽的習慣。\",\n",
    "            domain=\"general\",\n",
    "        ),\n",
    "        EvalSample(\n",
    "            id=\"qa_005\",\n",
    "            question=\"什麼是區塊鏈技術的核心概念？\",\n",
    "            reference=\"區塊鏈是一種分散式帳本技術，核心概念包括：去中心化、不可篡改性、透明性和共識機制。每個區塊包含交易記錄，並通過密碼學方法鏈接，形成安全可信的數據儲存系統。\",\n",
    "            domain=\"tech\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "# Load test dataset\n",
    "test_samples = create_test_dataset()\n",
    "evaluator = QualityEvaluator()\n",
    "\n",
    "print(f\"Created test dataset with {len(test_samples)} samples\")\n",
    "for sample in test_samples[:2]:\n",
    "    print(f\"Sample {sample.id}: {sample.question[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4940791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Human Evaluation Interface (Gradio)\n",
    "class HumanEvaluator:\n",
    "    \"\"\"Simple human evaluation interface for subjective quality assessment\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.annotations = []\n",
    "        self.current_sample_idx = 0\n",
    "\n",
    "    def create_evaluation_interface(\n",
    "        self, samples: List[EvalSample], generated_texts: List[str]\n",
    "    ):\n",
    "        \"\"\"Create Gradio interface for human evaluation\"\"\"\n",
    "\n",
    "        def evaluate_text(\n",
    "            sample_idx: int,\n",
    "            generated: str,\n",
    "            relevance: int,\n",
    "            fluency: int,\n",
    "            informativeness: int,\n",
    "            overall: int,\n",
    "            comments: str,\n",
    "        ):\n",
    "            \"\"\"Save human evaluation\"\"\"\n",
    "            annotation = {\n",
    "                \"sample_id\": samples[sample_idx].id,\n",
    "                \"question\": samples[sample_idx].question,\n",
    "                \"reference\": samples[sample_idx].reference,\n",
    "                \"generated\": generated,\n",
    "                \"relevance\": relevance,\n",
    "                \"fluency\": fluency,\n",
    "                \"informativeness\": informativeness,\n",
    "                \"overall\": overall,\n",
    "                \"comments\": comments,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "            }\n",
    "            self.annotations.append(annotation)\n",
    "\n",
    "            # Save to file\n",
    "            annotations_file = \"outs/eval/human_annotations.jsonl\"\n",
    "            with open(annotations_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(annotation, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            return f\"評估已保存！已完成 {len(self.annotations)} 個樣本\"\n",
    "\n",
    "        def load_sample(sample_idx: int):\n",
    "            \"\"\"Load sample for evaluation\"\"\"\n",
    "            if 0 <= sample_idx < len(samples):\n",
    "                sample = samples[sample_idx]\n",
    "                generated = (\n",
    "                    generated_texts[sample_idx]\n",
    "                    if sample_idx < len(generated_texts)\n",
    "                    else \"\"\n",
    "                )\n",
    "                return (\n",
    "                    sample.question,\n",
    "                    sample.reference,\n",
    "                    generated,\n",
    "                    f\"樣本 {sample_idx + 1}/{len(samples)} - {sample.domain}\",\n",
    "                )\n",
    "            return \"\", \"\", \"\", \"無效的樣本索引\"\n",
    "\n",
    "        # Create interface\n",
    "        with gr.Blocks(title=\"文本品質人工評估\") as demo:\n",
    "            gr.Markdown(\"# 文本生成品質人工評估介面\")\n",
    "            gr.Markdown(\"請根據以下標準評估生成的文本品質（1-5分，5分最高）\")\n",
    "\n",
    "            with gr.Row():\n",
    "                sample_idx = gr.Number(label=\"樣本索引\", value=0, precision=0)\n",
    "                load_btn = gr.Button(\"載入樣本\")\n",
    "\n",
    "            info_display = gr.Textbox(label=\"樣本資訊\", interactive=False)\n",
    "            question_display = gr.Textbox(label=\"問題\", lines=2, interactive=False)\n",
    "            reference_display = gr.Textbox(label=\"參考答案\", lines=3, interactive=False)\n",
    "            generated_display = gr.Textbox(label=\"生成文本\", lines=4, interactive=True)\n",
    "\n",
    "            with gr.Row():\n",
    "                relevance = gr.Slider(1, 5, value=3, step=1, label=\"相關性 (1-5)\")\n",
    "                fluency = gr.Slider(1, 5, value=3, step=1, label=\"流暢性 (1-5)\")\n",
    "\n",
    "            with gr.Row():\n",
    "                informativeness = gr.Slider(1, 5, value=3, step=1, label=\"資訊性 (1-5)\")\n",
    "                overall = gr.Slider(1, 5, value=3, step=1, label=\"整體品質 (1-5)\")\n",
    "\n",
    "            comments = gr.Textbox(\n",
    "                label=\"評論（可選）\", lines=2, placeholder=\"其他意見或建議...\"\n",
    "            )\n",
    "\n",
    "            with gr.Row():\n",
    "                submit_btn = gr.Button(\"提交評估\", variant=\"primary\")\n",
    "                result_display = gr.Textbox(label=\"提交結果\", interactive=False)\n",
    "\n",
    "            # Event handlers\n",
    "            load_btn.click(\n",
    "                load_sample,\n",
    "                inputs=[sample_idx],\n",
    "                outputs=[\n",
    "                    question_display,\n",
    "                    reference_display,\n",
    "                    generated_display,\n",
    "                    info_display,\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            submit_btn.click(\n",
    "                evaluate_text,\n",
    "                inputs=[\n",
    "                    sample_idx,\n",
    "                    generated_display,\n",
    "                    relevance,\n",
    "                    fluency,\n",
    "                    informativeness,\n",
    "                    overall,\n",
    "                    comments,\n",
    "                ],\n",
    "                outputs=[result_display],\n",
    "            )\n",
    "\n",
    "        return demo\n",
    "\n",
    "    def load_annotations(\n",
    "        self, file_path: str = \"outs/eval/human_annotations.jsonl\"\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Load human annotations from file\"\"\"\n",
    "        annotations = []\n",
    "        if Path(file_path).exists():\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        annotations.append(json.loads(line.strip()))\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "        return annotations\n",
    "\n",
    "    def compute_human_metrics(self, annotations: List[Dict]) -> Dict[str, float]:\n",
    "        \"\"\"Compute aggregated human evaluation metrics\"\"\"\n",
    "        if not annotations:\n",
    "            return {}\n",
    "\n",
    "        df = pd.DataFrame(annotations)\n",
    "        metrics = {\n",
    "            \"avg_relevance\": df[\"relevance\"].mean(),\n",
    "            \"avg_fluency\": df[\"fluency\"].mean(),\n",
    "            \"avg_informativeness\": df[\"informativeness\"].mean(),\n",
    "            \"avg_overall\": df[\"overall\"].mean(),\n",
    "            \"std_overall\": df[\"overall\"].std(),\n",
    "            \"num_annotations\": len(annotations),\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "\n",
    "# Initialize human evaluator\n",
    "human_evaluator = HumanEvaluator()\n",
    "print(\"Human evaluation interface ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61826624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Multi-Model Comparative Evaluation\n",
    "class ComparativeEvaluator:\n",
    "    \"\"\"Compare multiple models/configurations on the same test set\"\"\"\n",
    "\n",
    "    def __init__(self, evaluator: QualityEvaluator):\n",
    "        self.evaluator = evaluator\n",
    "        self.results = []\n",
    "\n",
    "    def mock_model_generate(\n",
    "        self, model_name: str, question: str, context: str = \"\"\n",
    "    ) -> str:\n",
    "        \"\"\"Mock different model responses for demonstration\"\"\"\n",
    "        responses = {\n",
    "            \"baseline\": {\n",
    "                \"什麼是檢索增強生成（RAG）？\": \"RAG是一種AI技術，結合了檢索和生成。\",\n",
    "                \"請解釋機器學習中的過擬合現象。\": \"過擬合就是模型在訓練數據上效果好，但新數據上效果差。\",\n",
    "                \"台灣的地理位置有什麼特色？\": \"台灣是個島嶼，在亞洲。\",\n",
    "                \"如何提高團隊溝通效率？\": \"可以開會、用工具。\",\n",
    "                \"什麼是區塊鏈技術的核心概念？\": \"區塊鏈是分散式技術。\",\n",
    "            },\n",
    "            \"improved\": {\n",
    "                \"什麼是檢索增強生成（RAG）？\": \"檢索增強生成（RAG）是結合資訊檢索與文本生成的先進技術，能夠從外部知識庫檢索相關資訊並增強語言模型的回答能力，提升準確性。\",\n",
    "                \"請解釋機器學習中的過擬合現象。\": \"過擬合是機器學習中常見問題，指模型在訓練數據上表現優異，但對新數據的泛化能力較差。這通常因為模型過於複雜，學習了訓練數據的噪音。\",\n",
    "                \"台灣的地理位置有什麼特色？\": \"台灣位於西太平洋島弧，東臨太平洋，西隔台灣海峽與大陸相望，地處熱帶與亞熱帶交界，具有獨特的地理優勢。\",\n",
    "                \"如何提高團隊溝通效率？\": \"提高團隊溝通效率需要建立清晰流程、選用適當工具、定期會議、促進開放對話、確保資訊共享，並培養良好的傾聽習慣。\",\n",
    "                \"什麼是區塊鏈技術的核心概念？\": \"區塊鏈核心概念包括去中心化、不可篡改性、透明性和共識機制。透過密碼學方法將交易記錄鏈接成安全可信的分散式帳本。\",\n",
    "            },\n",
    "            \"advanced\": {\n",
    "                \"什麼是檢索增強生成（RAG）？\": \"檢索增強生成（Retrieval-Augmented Generation, RAG）是一種創新的人工智慧架構，將傳統的資訊檢索系統與大型語言模型深度整合。RAG系統首先從龐大的外部知識庫中檢索與查詢相關的文檔片段，然後將這些上下文資訊與原始問題一起輸入到生成模型中，使模型能夠產生更準確、更具時效性和更具專業性的回答。這種方法有效解決了語言模型知識截止日期的限制，並大幅提升了生成內容的事實準確性。\",\n",
    "                \"請解釋機器學習中的過擬合現象。\": \"過擬合（Overfitting）是機器學習領域的核心挑戰之一，表現為模型在訓練數據集上達到極高的準確率，卻在驗證集或測試集上表現顯著下降。這種現象的根本原因在於模型複雜度過高，導致其不僅學習了數據中的真實模式，還記憶了訓練樣本中的隨機噪音、離群值和特定細節。過擬合的模型缺乏泛化能力，無法有效處理未見過的新數據。常見的緩解策略包括正則化技術、交叉驗證、早停法、數據擴增以及降低模型複雜度等方法。\",\n",
    "                \"台灣的地理位置有什麼特色？\": \"台灣島地理位置極為獨特且戰略重要，位於北緯22°至25°、東經120°至122°之間，座落在歐亞大陸板塊與菲律賓海板塊的交界處。島嶼東臨浩瀚的太平洋，西隔寬約130公里的台灣海峽與中國大陸福建省相望，南端巴士海峽連接南海，北部則面向東海。台灣正處於北回歸線穿越的熱帶與亞熱帶氣候交界帶，造就了豐富的生物多樣性。地形以山地為主體，中央山脈縱貫南北，平原主要分布在西部沿海，形成了「高山海島」的獨特地理特色。\",\n",
    "                \"如何提高團隊溝通效率？\": \"提升團隊溝通效率是現代組織管理的關鍵議題，需要系統性的策略規劃。首要步驟是建立標準化的溝通流程與協議，明確不同情境下的溝通方式、責任歸屬和決策權限。技術層面應選擇適合團隊規模與工作性質的協作工具，如即時通訊平台、專案管理系統和視訊會議軟體。組織層面需要建立定期的團隊會議機制，包括日常站會、週期性回顧和專案里程碑討論。文化層面要培養開放透明的溝通氛圍，鼓勵成員主動分享資訊、提出疑問和建設性意見。此外，還需要強化積極傾聽技巧、提供溝通技能培訓，並建立有效的反饋機制以持續優化溝通品質。\",\n",
    "                \"什麼是區塊鏈技術的核心概念？\": \"區塊鏈（Blockchain）技術的核心概念建立在四大支柱之上：去中心化、不可篡改性、透明性和共識機制。去中心化意味著系統不依賴單一的中央機構控制，而是由分散在網路中的多個節點共同維護。不可篡改性透過密碼學雜湊函數和鏈式結構實現，每個區塊都包含前一個區塊的雜湊值，形成不可逆的時間戳記錄鏈。透明性確保所有交易記錄對網路參與者公開可見，提升系統的可審計性。共識機制（如工作量證明、權益證明等）確保網路節點對新區塊的有效性達成一致，維護分散式帳本的完整性。這些核心特性使區塊鏈成為構建可信任數位經濟基礎設施的革命性技術，在金融服務、供應鏈管理、數位身份驗證等領域展現巨大潛力。\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Return mock response based on model and question\n",
    "        model_responses = responses.get(model_name, responses[\"baseline\"])\n",
    "        return model_responses.get(\n",
    "            question, f\"這是{model_name}模型對於「{question}」的回答。\"\n",
    "        )\n",
    "\n",
    "    def evaluate_models(\n",
    "        self, models: List[str], samples: List[EvalSample]\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Evaluate multiple models on test samples\"\"\"\n",
    "        results = []\n",
    "\n",
    "        for model_name in models:\n",
    "            print(f\"Evaluating model: {model_name}\")\n",
    "            model_results = []\n",
    "\n",
    "            for sample in samples:\n",
    "                # Generate response (mock)\n",
    "                generated = self.mock_model_generate(\n",
    "                    model_name, sample.question, sample.context\n",
    "                )\n",
    "\n",
    "                # Evaluate quality\n",
    "                metrics = self.evaluator.evaluate_sample(sample, generated)\n",
    "\n",
    "                result = {\n",
    "                    \"model\": model_name,\n",
    "                    \"sample_id\": sample.id,\n",
    "                    \"domain\": sample.domain,\n",
    "                    \"question\": sample.question,\n",
    "                    \"reference\": sample.reference,\n",
    "                    \"generated\": generated,\n",
    "                    **metrics,\n",
    "                }\n",
    "\n",
    "                model_results.append(result)\n",
    "\n",
    "            results.extend(model_results)\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def compute_model_summary(self, results_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Compute summary statistics for each model\"\"\"\n",
    "        summary = (\n",
    "            results_df.groupby(\"model\")\n",
    "            .agg(\n",
    "                {\n",
    "                    \"rouge_l\": [\"mean\", \"std\"],\n",
    "                    \"chrf_plus\": [\"mean\", \"std\"],\n",
    "                    \"length_penalty\": [\"mean\", \"std\"],\n",
    "                    \"composite\": [\"mean\", \"std\"],\n",
    "                    \"sample_id\": \"count\",\n",
    "                }\n",
    "            )\n",
    "            .round(4)\n",
    "        )\n",
    "\n",
    "        # Flatten column names\n",
    "        summary.columns = [\"_\".join(col).strip() for col in summary.columns]\n",
    "        summary = summary.rename(columns={\"sample_id_count\": \"num_samples\"})\n",
    "\n",
    "        return summary.reset_index()\n",
    "\n",
    "\n",
    "# Run comparative evaluation\n",
    "models_to_compare = [\"baseline\", \"improved\", \"advanced\"]\n",
    "comparative_evaluator = ComparativeEvaluator(evaluator)\n",
    "\n",
    "print(\"Running comparative evaluation...\")\n",
    "results_df = comparative_evaluator.evaluate_models(models_to_compare, test_samples)\n",
    "summary_df = comparative_evaluator.compute_model_summary(results_df)\n",
    "\n",
    "print(\"\\n=== Model Comparison Summary ===\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaab0dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Quality Score Aggregation and Reporting\n",
    "class QualityReporter:\n",
    "    \"\"\"Generate comprehensive quality evaluation reports\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    def generate_detailed_report(\n",
    "        self,\n",
    "        results_df: pd.DataFrame,\n",
    "        summary_df: pd.DataFrame,\n",
    "        human_metrics: Dict = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Generate detailed evaluation report\"\"\"\n",
    "\n",
    "        report_lines = [\n",
    "            \"# Text Generation Quality Evaluation Report\",\n",
    "            f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "            \"\",\n",
    "            \"## Executive Summary\",\n",
    "            f\"- Evaluated {len(results_df['model'].unique())} models\",\n",
    "            f\"- Tested on {len(results_df['sample_id'].unique())} samples\",\n",
    "            f\"- Domains: {', '.join(results_df['domain'].unique())}\",\n",
    "            \"\",\n",
    "            \"## Automatic Metrics Summary\",\n",
    "            \"| Model | Rouge-L | chrF++ | Length Penalty | Composite Score |\",\n",
    "            \"| --- | --- | --- | --- | --- |\",\n",
    "        ]\n",
    "\n",
    "        for _, row in summary_df.iterrows():\n",
    "            report_lines.append(\n",
    "                f\"| {row['model']} | \"\n",
    "                f\"{row['rouge_l_mean']:.3f}±{row['rouge_l_std']:.3f} | \"\n",
    "                f\"{row['chrf_plus_mean']:.3f}±{row['chrf_plus_std']:.3f} | \"\n",
    "                f\"{row['length_penalty_mean']:.3f}±{row['length_penalty_std']:.3f} | \"\n",
    "                f\"{row['composite_mean']:.3f}±{row['composite_std']:.3f} |\"\n",
    "            )\n",
    "\n",
    "        # Add human evaluation metrics if available\n",
    "        if human_metrics:\n",
    "            report_lines.extend(\n",
    "                [\n",
    "                    \"\",\n",
    "                    \"## Human Evaluation Metrics\",\n",
    "                    f\"- Number of annotations: {human_metrics.get('num_annotations', 0)}\",\n",
    "                    f\"- Average relevance: {human_metrics.get('avg_relevance', 0):.2f}/5\",\n",
    "                    f\"- Average fluency: {human_metrics.get('avg_fluency', 0):.2f}/5\",\n",
    "                    f\"- Average informativeness: {human_metrics.get('avg_informativeness', 0):.2f}/5\",\n",
    "                    f\"- Average overall quality: {human_metrics.get('avg_overall', 0):.2f}/5 (±{human_metrics.get('std_overall', 0):.2f})\",\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # Best performing model\n",
    "        best_model = summary_df.loc[summary_df[\"composite_mean\"].idxmax(), \"model\"]\n",
    "        best_score = summary_df.loc[\n",
    "            summary_df[\"composite_mean\"].idxmax(), \"composite_mean\"\n",
    "        ]\n",
    "\n",
    "        report_lines.extend(\n",
    "            [\n",
    "                \"\",\n",
    "                \"## Key Findings\",\n",
    "                f\"- **Best performing model**: {best_model} (composite score: {best_score:.3f})\",\n",
    "                \"- **Rouge-L**: Measures overlap of longest common subsequences\",\n",
    "                \"- **chrF++**: Character-level F-score with word order consideration\",\n",
    "                \"- **Length Penalty**: Penalizes responses too short or too long\",\n",
    "                \"\",\n",
    "                \"## Recommendations\",\n",
    "                \"1. Consider the trade-off between automatic metrics and human preferences\",\n",
    "                \"2. Evaluate on larger, domain-specific datasets for production use\",\n",
    "                \"3. Include task-specific metrics (e.g., factuality, coherence)\",\n",
    "                \"4. Regularly update evaluation benchmarks\",\n",
    "                \"\",\n",
    "                \"## Sample Outputs (Best vs Worst)\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Add sample comparisons\n",
    "        best_samples = results_df[results_df[\"model\"] == best_model].nlargest(\n",
    "            2, \"composite\"\n",
    "        )\n",
    "        for idx, (_, sample) in enumerate(best_samples.iterrows()):\n",
    "            report_lines.extend(\n",
    "                [\n",
    "                    f\"### Sample {idx+1} (Score: {sample['composite']:.3f})\",\n",
    "                    f\"**Question**: {sample['question']}\",\n",
    "                    f\"**Generated**: {sample['generated'][:200]}...\",\n",
    "                    \"\",\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        return \"\\n\".join(report_lines)\n",
    "\n",
    "    def save_results(\n",
    "        self,\n",
    "        results_df: pd.DataFrame,\n",
    "        summary_df: pd.DataFrame,\n",
    "        report_text: str,\n",
    "        human_metrics: Dict = None,\n",
    "    ):\n",
    "        \"\"\"Save all evaluation results to files\"\"\"\n",
    "\n",
    "        # Save detailed results\n",
    "        results_file = f\"outs/eval/quality_results_{self.timestamp}.csv\"\n",
    "        results_df.to_csv(results_file, index=False, encoding=\"utf-8\")\n",
    "        print(f\"✓ Detailed results saved to: {results_file}\")\n",
    "\n",
    "        # Save summary\n",
    "        summary_file = f\"outs/eval/quality_summary_{self.timestamp}.csv\"\n",
    "        summary_df.to_csv(summary_file, index=False, encoding=\"utf-8\")\n",
    "        print(f\"✓ Summary saved to: {summary_file}\")\n",
    "\n",
    "        # Save report\n",
    "        report_file = f\"outs/reports/quality_report_{self.timestamp}.md\"\n",
    "        with open(report_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(report_text)\n",
    "        print(f\"✓ Report saved to: {report_file}\")\n",
    "\n",
    "        # Save metrics JSON\n",
    "        metrics_data = {\n",
    "            \"timestamp\": self.timestamp,\n",
    "            \"num_models\": len(results_df[\"model\"].unique()),\n",
    "            \"num_samples\": len(results_df[\"sample_id\"].unique()),\n",
    "            \"summary\": summary_df.to_dict(\"records\"),\n",
    "            \"human_metrics\": human_metrics or {},\n",
    "        }\n",
    "\n",
    "        metrics_file = f\"outs/eval/quality_metrics_{self.timestamp}.json\"\n",
    "        with open(metrics_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metrics_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"✓ Metrics saved to: {metrics_file}\")\n",
    "\n",
    "        return {\n",
    "            \"results_file\": results_file,\n",
    "            \"summary_file\": summary_file,\n",
    "            \"report_file\": report_file,\n",
    "            \"metrics_file\": metrics_file,\n",
    "        }\n",
    "\n",
    "\n",
    "# Generate comprehensive report\n",
    "reporter = QualityReporter()\n",
    "\n",
    "# Load human annotations (if any exist)\n",
    "human_annotations = human_evaluator.load_annotations()\n",
    "human_metrics = (\n",
    "    human_evaluator.compute_human_metrics(human_annotations)\n",
    "    if human_annotations\n",
    "    else None\n",
    ")\n",
    "\n",
    "# Generate report\n",
    "report_text = reporter.generate_detailed_report(results_df, summary_df, human_metrics)\n",
    "\n",
    "# Save all results\n",
    "saved_files = reporter.save_results(results_df, summary_df, report_text, human_metrics)\n",
    "\n",
    "print(\"\\n=== Quality Evaluation Report Generated ===\")\n",
    "print(report_text[:1000] + \"...\" if len(report_text) > 1000 else report_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3f6eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Smoke Test - Run evaluation on subset\n",
    "def run_smoke_test():\n",
    "    \"\"\"Quick smoke test on a subset of samples\"\"\"\n",
    "    print(\"🔥 Running smoke test...\")\n",
    "\n",
    "    # Test with first 3 samples only\n",
    "    smoke_samples = test_samples[:3]\n",
    "    smoke_models = [\"baseline\", \"improved\"]\n",
    "\n",
    "    print(f\"Testing {len(smoke_models)} models on {len(smoke_samples)} samples\")\n",
    "\n",
    "    # Run evaluation\n",
    "    smoke_evaluator = QualityEvaluator()\n",
    "    smoke_results = []\n",
    "\n",
    "    for model in smoke_models:\n",
    "        for sample in smoke_samples:\n",
    "            generated = comparative_evaluator.mock_model_generate(\n",
    "                model, sample.question\n",
    "            )\n",
    "            metrics = smoke_evaluator.evaluate_sample(sample, generated)\n",
    "\n",
    "            result = {\n",
    "                \"model\": model,\n",
    "                \"sample_id\": sample.id,\n",
    "                \"rouge_l\": metrics[\"rouge_l\"],\n",
    "                \"chrf_plus\": metrics[\"chrf_plus\"],\n",
    "                \"composite\": metrics[\"composite\"],\n",
    "            }\n",
    "            smoke_results.append(result)\n",
    "\n",
    "    smoke_df = pd.DataFrame(smoke_results)\n",
    "\n",
    "    # Quick summary\n",
    "    print(\"\\n=== Smoke Test Results ===\")\n",
    "    for model in smoke_models:\n",
    "        model_data = smoke_df[smoke_df[\"model\"] == model]\n",
    "        avg_composite = model_data[\"composite\"].mean()\n",
    "        print(f\"{model}: avg composite = {avg_composite:.3f}\")\n",
    "\n",
    "    # Verify all scores are reasonable (0-1 range)\n",
    "    all_scores_valid = (\n",
    "        (smoke_df[\"rouge_l\"] >= 0).all()\n",
    "        and (smoke_df[\"rouge_l\"] <= 1).all()\n",
    "        and (smoke_df[\"chrf_plus\"] >= 0).all()\n",
    "        and (smoke_df[\"chrf_plus\"] <= 1).all()\n",
    "        and (smoke_df[\"composite\"] >= 0).all()\n",
    "        and (smoke_df[\"composite\"] <= 1).all()\n",
    "    )\n",
    "\n",
    "    if all_scores_valid:\n",
    "        print(\"✅ All metrics within expected range [0, 1]\")\n",
    "    else:\n",
    "        print(\"❌ Some metrics outside expected range\")\n",
    "\n",
    "    return smoke_df\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_results = run_smoke_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7adfd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Advanced Quality Analysis\n",
    "class AdvancedQualityAnalyzer:\n",
    "    \"\"\"Advanced analysis of quality patterns and insights\"\"\"\n",
    "\n",
    "    def analyze_domain_performance(self, results_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Analyze performance by domain\"\"\"\n",
    "        domain_analysis = (\n",
    "            results_df.groupby([\"model\", \"domain\"])\n",
    "            .agg({\"rouge_l\": \"mean\", \"chrf_plus\": \"mean\", \"composite\": \"mean\"})\n",
    "            .round(3)\n",
    "        )\n",
    "\n",
    "        return domain_analysis.reset_index()\n",
    "\n",
    "    def identify_failure_cases(\n",
    "        self, results_df: pd.DataFrame, threshold: float = 0.3\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Identify samples with poor quality scores\"\"\"\n",
    "        failure_cases = results_df[results_df[\"composite\"] < threshold].copy()\n",
    "        failure_cases = failure_cases.sort_values(\"composite\")\n",
    "\n",
    "        return failure_cases[\n",
    "            [\"model\", \"sample_id\", \"domain\", \"question\", \"composite\", \"generated\"]\n",
    "        ]\n",
    "\n",
    "    def compute_consistency_metrics(self, results_df: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"Compute consistency metrics across samples\"\"\"\n",
    "        consistency = {}\n",
    "\n",
    "        for model in results_df[\"model\"].unique():\n",
    "            model_data = results_df[results_df[\"model\"] == model]\n",
    "\n",
    "            # Coefficient of variation (std/mean) for each metric\n",
    "            for metric in [\"rouge_l\", \"chrf_plus\", \"composite\"]:\n",
    "                cv = (\n",
    "                    model_data[metric].std() / model_data[metric].mean()\n",
    "                    if model_data[metric].mean() > 0\n",
    "                    else float(\"inf\")\n",
    "                )\n",
    "                consistency[f\"{model}_{metric}_cv\"] = cv\n",
    "\n",
    "        return consistency\n",
    "\n",
    "\n",
    "# Run advanced analysis\n",
    "analyzer = AdvancedQualityAnalyzer()\n",
    "\n",
    "domain_perf = analyzer.analyze_domain_performance(results_df)\n",
    "print(\"\\n=== Performance by Domain ===\")\n",
    "print(domain_perf.to_string(index=False))\n",
    "\n",
    "failure_cases = analyzer.identify_failure_cases(results_df, threshold=0.4)\n",
    "print(f\"\\n=== Failure Cases (composite < 0.4) ===\")\n",
    "print(f\"Found {len(failure_cases)} failure cases\")\n",
    "if not failure_cases.empty:\n",
    "    print(failure_cases[[\"model\", \"sample_id\", \"composite\"]].to_string(index=False))\n",
    "\n",
    "consistency = analyzer.compute_consistency_metrics(results_df)\n",
    "print(f\"\\n=== Consistency Metrics (Coefficient of Variation) ===\")\n",
    "for metric, value in consistency.items():\n",
    "    print(f\"{metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65e41f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Integration with RAG Groundedness (Optional)\n",
    "def integrate_groundedness_check(\n",
    "    results_df: pd.DataFrame, context_samples: List[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Add groundedness checking to quality evaluation\"\"\"\n",
    "\n",
    "    def simple_groundedness_score(generated: str, context: str) -> float:\n",
    "        \"\"\"Simple groundedness check based on keyword overlap\"\"\"\n",
    "        if not context or not generated:\n",
    "            return 0.0\n",
    "\n",
    "        # Tokenize (simple split for demo)\n",
    "        gen_words = set(generated.lower().split())\n",
    "        ctx_words = set(context.lower().split())\n",
    "\n",
    "        if not gen_words:\n",
    "            return 0.0\n",
    "\n",
    "        # Jaccard similarity\n",
    "        overlap = len(gen_words & ctx_words)\n",
    "        union = len(gen_words | ctx_words)\n",
    "\n",
    "        return overlap / union if union > 0 else 0.0\n",
    "\n",
    "    # Add groundedness scores (mock context for demo)\n",
    "    results_with_groundedness = results_df.copy()\n",
    "\n",
    "    if context_samples is None:\n",
    "        # Mock context for each sample\n",
    "        context_samples = [\n",
    "            \"檢索增強生成是結合資訊檢索和文本生成的技術，通過外部知識庫增強語言模型能力。\",\n",
    "            \"過擬合是機器學習中模型在訓練數據表現好但在新數據表現差的現象。\",\n",
    "            \"台灣位於亞洲東部，是西太平洋上的島嶼，具有獨特地理位置。\",\n",
    "            \"團隊溝通效率可透過建立流程、使用工具、定期會議等方式提升。\",\n",
    "            \"區塊鏈是分散式帳本技術，具有去中心化、不可篡改等特性。\",\n",
    "        ] * 3  # Repeat for all models\n",
    "\n",
    "    groundedness_scores = []\n",
    "    for idx, row in results_with_groundedness.iterrows():\n",
    "        sample_idx = list(test_samples).index(\n",
    "            next(s for s in test_samples if s.id == row[\"sample_id\"])\n",
    "        )\n",
    "        context = (\n",
    "            context_samples[sample_idx] if sample_idx < len(context_samples) else \"\"\n",
    "        )\n",
    "\n",
    "        groundedness = simple_groundedness_score(row[\"generated\"], context)\n",
    "        groundedness_scores.append(groundedness)\n",
    "\n",
    "    results_with_groundedness[\"groundedness\"] = groundedness_scores\n",
    "\n",
    "    # Update composite score to include groundedness\n",
    "    results_with_groundedness[\"composite_with_groundedness\"] = (\n",
    "        0.3 * results_with_groundedness[\"rouge_l\"]\n",
    "        + 0.3 * results_with_groundedness[\"chrf_plus\"]\n",
    "        + 0.2 * results_with_groundedness[\"length_penalty\"]\n",
    "        + 0.2 * results_with_groundedness[\"groundedness\"]\n",
    "    )\n",
    "\n",
    "    return results_with_groundedness\n",
    "\n",
    "\n",
    "# Add groundedness analysis\n",
    "results_with_groundedness = integrate_groundedness_check(results_df)\n",
    "\n",
    "print(\"\\n=== Updated Results with Groundedness ===\")\n",
    "groundedness_summary = (\n",
    "    results_with_groundedness.groupby(\"model\")[\n",
    "        [\"composite\", \"groundedness\", \"composite_with_groundedness\"]\n",
    "    ]\n",
    "    .mean()\n",
    "    .round(3)\n",
    ")\n",
    "print(groundedness_summary.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d4f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Summary and Next Steps\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"📊 STAGE 7 - NB62 QUALITY EVALUATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n✅ COMPLETED:\")\n",
    "print(\"• Rouge-L and chrF++ automatic metrics implementation\")\n",
    "print(\"• Human evaluation interface with Gradio\")\n",
    "print(\"• Multi-model comparative evaluation pipeline\")\n",
    "print(\"• Comprehensive reporting and CSV export\")\n",
    "print(\"• Groundedness integration for RAG applications\")\n",
    "print(\"• Domain-specific performance analysis\")\n",
    "print(\"• Failure case identification and consistency metrics\")\n",
    "\n",
    "print(\"\\n🔑 CORE CONCEPTS:\")\n",
    "print(\"• Rouge-L: Longest common subsequence F1 score\")\n",
    "print(\"• chrF++: Character-level F-score with word order\")\n",
    "print(\"• Human evaluation: Relevance, fluency, informativeness\")\n",
    "print(\"• Composite scoring: Weighted combination of metrics\")\n",
    "print(\"• Groundedness: Context-grounded generation quality\")\n",
    "print(\"• Consistency: Coefficient of variation across samples\")\n",
    "\n",
    "print(\"\\n⚠️ PITFALLS:\")\n",
    "print(\"• Automatic metrics may not correlate with human judgment\")\n",
    "print(\"• Small test sets can give misleading results\")\n",
    "print(\"• Domain-specific metrics often more valuable than general ones\")\n",
    "print(\"• Human annotation requires careful guideline design\")\n",
    "print(\"• Groundedness checking needs sophisticated NLI models\")\n",
    "\n",
    "print(\"\\n🎯 NEXT STEPS:\")\n",
    "print(\"• Scale evaluation to larger, diverse datasets\")\n",
    "print(\"• Implement semantic similarity metrics (BERTScore)\")\n",
    "print(\"• Add task-specific metrics (factuality, coherence)\")\n",
    "print(\"• Integrate with continuous evaluation pipelines\")\n",
    "print(\"• Build evaluation leaderboards for model comparison\")\n",
    "\n",
    "print(\"\\n📁 OUTPUT FILES:\")\n",
    "for file_type, file_path in saved_files.items():\n",
    "    print(f\"• {file_type}: {file_path}\")\n",
    "\n",
    "print(f\"\\n🔍 KEY FINDINGS:\")\n",
    "best_model_row = summary_df.loc[summary_df[\"composite_mean\"].idxmax()]\n",
    "print(f\"• Best performing model: {best_model_row['model']}\")\n",
    "print(f\"• Best composite score: {best_model_row['composite_mean']:.3f}\")\n",
    "print(f\"• Total samples evaluated: {len(results_df)}\")\n",
    "print(f\"• Models compared: {len(results_df['model'].unique())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Quality evaluation framework ready for production use! 🚀\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
