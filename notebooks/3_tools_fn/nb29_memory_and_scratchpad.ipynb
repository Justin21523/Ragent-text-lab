{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017c6e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb29_memory_and_scratchpad.ipynb\n",
    "# Stage 3: Memory and Scratchpad for Multi-Step Reasoning\n",
    "\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "from enum import Enum\n",
    "import asyncio\n",
    "import logging\n",
    "\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (Ë§áË£ΩÂà∞ÊØèÊú¨ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb32eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 2: Import Dependencies\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "from collections import deque\n",
    "import re\n",
    "\n",
    "# For LLM (minimal transformers setup)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc14801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 3: Memory Entry Data Structure\n",
    "@dataclass\n",
    "class MemoryEntry:\n",
    "    \"\"\"Single memory entry with metadata\"\"\"\n",
    "\n",
    "    timestamp: str\n",
    "    step_id: int\n",
    "    action_type: str  # \"thought\", \"action\", \"observation\", \"note\"\n",
    "    content: str\n",
    "    importance: float = 0.5  # 0.0 to 1.0\n",
    "    tags: List[str] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.tags is None:\n",
    "            self.tags = []\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return asdict(self)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, data: Dict[str, Any]) -> \"MemoryEntry\":\n",
    "        return cls(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20bdd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 4: Scratchpad Memory Manager\n",
    "class ScratchpadMemory:\n",
    "    \"\"\"Manages short-term memory with capacity limits and importance scoring\"\"\"\n",
    "\n",
    "    def __init__(self, max_entries: int = 20, max_tokens: int = 1500):\n",
    "        self.max_entries = max_entries\n",
    "        self.max_tokens = max_tokens\n",
    "        self.entries: deque = deque(maxlen=max_entries)\n",
    "        self.current_step = 0\n",
    "\n",
    "    def add_entry(\n",
    "        self,\n",
    "        action_type: str,\n",
    "        content: str,\n",
    "        importance: float = 0.5,\n",
    "        tags: List[str] = None,\n",
    "    ) -> MemoryEntry:\n",
    "        \"\"\"Add new memory entry\"\"\"\n",
    "        entry = MemoryEntry(\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            step_id=self.current_step,\n",
    "            action_type=action_type,\n",
    "            content=content,\n",
    "            importance=importance,\n",
    "            tags=tags or [],\n",
    "        )\n",
    "        self.entries.append(entry)\n",
    "        self._manage_capacity()\n",
    "        return entry\n",
    "\n",
    "    def _manage_capacity(self):\n",
    "        \"\"\"Remove low-importance entries if over token limit\"\"\"\n",
    "        while len(self.entries) > 0 and self._estimate_tokens() > self.max_tokens:\n",
    "            # Remove least important entry\n",
    "            min_entry = min(self.entries, key=lambda x: x.importance)\n",
    "            self.entries.remove(min_entry)\n",
    "\n",
    "    def _estimate_tokens(self) -> int:\n",
    "        \"\"\"Rough token estimation for all entries\"\"\"\n",
    "        total_chars = sum(len(entry.content) for entry in self.entries)\n",
    "        return total_chars // 4  # Rough estimate: 4 chars per token\n",
    "\n",
    "    def get_context_summary(self, include_tags: List[str] = None) -> str:\n",
    "        \"\"\"Generate context summary for LLM prompt\"\"\"\n",
    "        if not self.entries:\n",
    "            return \"## Memory: Empty\\n\"\n",
    "\n",
    "        summary_lines = [\"## Recent Memory:\"]\n",
    "\n",
    "        # Filter by tags if specified\n",
    "        relevant_entries = list(self.entries)\n",
    "        if include_tags:\n",
    "            relevant_entries = [\n",
    "                e for e in self.entries if any(tag in e.tags for tag in include_tags)\n",
    "            ]\n",
    "\n",
    "        # Sort by importance (descending) and recency\n",
    "        relevant_entries.sort(key=lambda x: (x.importance, x.step_id), reverse=True)\n",
    "\n",
    "        for entry in relevant_entries[:10]:  # Top 10 most relevant\n",
    "            summary_lines.append(\n",
    "                f\"Step {entry.step_id} [{entry.action_type}]: {entry.content}\"\n",
    "            )\n",
    "\n",
    "        return \"\\n\".join(summary_lines) + \"\\n\"\n",
    "\n",
    "    def add_thought(self, thought: str, importance: float = 0.6):\n",
    "        return self.add_entry(\"thought\", thought, importance, [\"reasoning\"])\n",
    "\n",
    "    def add_action(self, action: str, importance: float = 0.7):\n",
    "        return self.add_entry(\"action\", action, importance, [\"execution\"])\n",
    "\n",
    "    def add_observation(self, observation: str, importance: float = 0.8):\n",
    "        return self.add_entry(\"observation\", observation, importance, [\"result\"])\n",
    "\n",
    "    def add_note(self, note: str, importance: float = 0.9):\n",
    "        return self.add_entry(\"note\", note, importance, [\"important\"])\n",
    "\n",
    "    def next_step(self):\n",
    "        \"\"\"Increment step counter\"\"\"\n",
    "        self.current_step += 1\n",
    "\n",
    "    def export_history(self) -> List[Dict]:\n",
    "        \"\"\"Export all entries as JSON-serializable list\"\"\"\n",
    "        return [entry.to_dict() for entry in self.entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d81803",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 5: Key Information Extractor\n",
    "class KeyInfoExtractor:\n",
    "    \"\"\"Extract important information from text for memory storage\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Patterns for detecting important information\n",
    "        self.important_patterns = [\n",
    "            r'result:\\s*(.+)',\n",
    "            r'answer:\\s*(.+)',\n",
    "            r'found:\\s*(.+)',\n",
    "            r'error:\\s*(.+)',\n",
    "            r'(\\d+\\.?\\d*)\\s*(dollars?|USD|\\$)',  # Money amounts\n",
    "            r'(\\d{4}-\\d{2}-\\d{2})',  # Dates\n",
    "            r'(\\w+@\\w+\\.\\w+)',  # Emails\n",
    "            r'(https?://\\S+)',  # URLs\n",
    "        ]\n",
    "\n",
    "    def extract_key_info(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract key information pieces from text\"\"\"\n",
    "        key_info = []\n",
    "\n",
    "        # Extract using patterns\n",
    "        for pattern in self.important_patterns:\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            if matches:\n",
    "                key_info.extend([str(match) for match in matches])\n",
    "\n",
    "        # Extract sentences with high-value keywords\n",
    "        high_value_keywords = [\n",
    "            'result', 'answer', 'found', 'discovered', 'calculated',\n",
    "            'error', 'failed', 'succeeded', 'completed', 'important'\n",
    "        ]\n",
    "\n",
    "        sentences = text.split('.')\n",
    "        for sentence in sentences:\n",
    "            if any(keyword in sentence.lower() for keyword in high_value_keywords):\n",
    "                key_info.append(sentence.strip())\n",
    "\n",
    "        return key_info[:5]  # Limit to top 5 pieces\n",
    "\n",
    "    def calculate_importance(self, text: str) -> float:\n",
    "        \"\"\"Calculate importance score based on content\"\"\"\n",
    "        importance = 0.5  # Base importance\n",
    "\n",
    "        # Boost for error/success indicators\n",
    "        if any(word in text.lower() for word in ['error', 'failed', 'exception']):\n",
    "            importance += 0.3\n",
    "        if any(word in text.lower() for word in ['success', 'completed', 'found']):\n",
    "            importance += 0.2\n",
    "\n",
    "        # Boost for numbers and specific data\n",
    "        if re.search(r'\\d+', text):\n",
    "            importance += 0.1\n",
    "\n",
    "        # Boost for longer, detailed content\n",
    "        if len(text) > 100:\n",
    "            importance += 0.1\n",
    "\n",
    "        return min(importance, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b69a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 6: Memory-Enhanced ReAct Agent\n",
    "class MemoryReActAgent:\n",
    "    \"\"\"ReAct agent with scratchpad memory\"\"\"\n",
    "\n",
    "    def __init__(self, model_id: str = \"Qwen/Qwen2.5-7B-Instruct\"):\n",
    "        # Load model with low-VRAM settings\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            load_in_8bit=True,  # For VRAM efficiency\n",
    "        )\n",
    "\n",
    "        self.memory = ScratchpadMemory(max_entries=25, max_tokens=2000)\n",
    "        self.extractor = KeyInfoExtractor()\n",
    "\n",
    "        # Available tools (simplified for demo)\n",
    "        self.tools = {\"calculator\": self._calculator, \"memory_note\": self._memory_note}\n",
    "\n",
    "    def _calculator(self, expression: str) -> str:\n",
    "        \"\"\"Safe calculator tool\"\"\"\n",
    "        try:\n",
    "            # Simple safety check\n",
    "            if any(char in expression for char in [\"import\", \"exec\", \"eval\", \"__\"]):\n",
    "                return \"Error: Unsafe expression\"\n",
    "\n",
    "            result = eval(expression)\n",
    "            return f\"Result: {result}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def _memory_note(self, note: str) -> str:\n",
    "        \"\"\"Add important note to memory\"\"\"\n",
    "        self.memory.add_note(note, importance=0.9)\n",
    "        return f\"Note saved to memory: {note}\"\n",
    "\n",
    "    def _generate_response(self, prompt: str, max_tokens: int = 150) -> str:\n",
    "        \"\"\"Generate response using local LLM\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            prompt, return_tensors=\"pt\", truncation=True, max_length=2048\n",
    "        )\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Extract only the new part\n",
    "        prompt_length = len(\n",
    "            self.tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
    "        )\n",
    "        return response[prompt_length:].strip()\n",
    "\n",
    "    def solve_task(self, task: str, max_iterations: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Solve task using memory-enhanced ReAct\"\"\"\n",
    "        self.memory.add_entry(\n",
    "            \"task\", f\"Starting task: {task}\", importance=1.0, tags=[\"task\"]\n",
    "        )\n",
    "\n",
    "        iteration = 0\n",
    "        final_answer = None\n",
    "\n",
    "        while iteration < max_iterations:\n",
    "            # Build prompt with memory context\n",
    "            memory_context = self.memory.get_context_summary()\n",
    "\n",
    "            prompt = f\"\"\"You are a helpful assistant with access to tools and memory.\n",
    "\n",
    "{memory_context}\n",
    "\n",
    "Current Task: {task}\n",
    "\n",
    "Available tools: calculator(expression), memory_note(note)\n",
    "\n",
    "Think step by step. Use this format:\n",
    "Thought: [your reasoning]\n",
    "Action: [tool_name(arguments) or FINISH]\n",
    "Observation: [tool result]\n",
    "\n",
    "Continue until you can provide a final answer.\"\"\"\n",
    "\n",
    "            # Generate thought/action\n",
    "            response = self._generate_response(prompt, max_tokens=200)\n",
    "\n",
    "            # Parse response\n",
    "            thought_match = re.search(\n",
    "                r\"Thought:\\s*(.+?)(?=Action:|$)\", response, re.DOTALL\n",
    "            )\n",
    "            action_match = re.search(\n",
    "                r\"Action:\\s*(.+?)(?=Observation:|$)\", response, re.DOTALL\n",
    "            )\n",
    "\n",
    "            if thought_match:\n",
    "                thought = thought_match.group(1).strip()\n",
    "                self.memory.add_thought(thought)\n",
    "                print(f\"üí≠ Thought: {thought}\")\n",
    "\n",
    "            if action_match:\n",
    "                action = action_match.group(1).strip()\n",
    "                self.memory.add_action(action)\n",
    "                print(f\"üîß Action: {action}\")\n",
    "\n",
    "                # Execute action\n",
    "                if action.upper() == \"FINISH\":\n",
    "                    final_answer = thought if thought_match else \"Task completed\"\n",
    "                    break\n",
    "\n",
    "                # Parse and execute tool call\n",
    "                observation = self._execute_action(action)\n",
    "                self.memory.add_observation(observation)\n",
    "                print(f\"üëÅÔ∏è Observation: {observation}\")\n",
    "\n",
    "                # Extract key information for memory\n",
    "                key_info = self.extractor.extract_key_info(observation)\n",
    "                for info in key_info:\n",
    "                    importance = self.extractor.calculate_importance(info)\n",
    "                    self.memory.add_entry(\"key_info\", info, importance, [\"extracted\"])\n",
    "\n",
    "            self.memory.next_step()\n",
    "            iteration += 1\n",
    "            print(f\"--- Step {iteration} completed ---\\n\")\n",
    "\n",
    "        return {\n",
    "            \"final_answer\": final_answer or \"Max iterations reached\",\n",
    "            \"iterations\": iteration,\n",
    "            \"memory_entries\": len(self.memory.entries),\n",
    "            \"memory_summary\": self.memory.get_context_summary(),\n",
    "            \"full_history\": self.memory.export_history(),\n",
    "        }\n",
    "\n",
    "    def _execute_action(self, action: str) -> str:\n",
    "        \"\"\"Execute tool action\"\"\"\n",
    "        # Parse tool call\n",
    "        tool_match = re.match(r\"(\\w+)\\((.+)\\)\", action)\n",
    "        if not tool_match:\n",
    "            return \"Error: Invalid action format\"\n",
    "\n",
    "        tool_name, args = tool_match.groups()\n",
    "\n",
    "        if tool_name not in self.tools:\n",
    "            return f\"Error: Unknown tool '{tool_name}'\"\n",
    "\n",
    "        try:\n",
    "            # Simple argument parsing (remove quotes)\n",
    "            args = args.strip(\"'\\\"\")\n",
    "            return self.tools[tool_name](args)\n",
    "        except Exception as e:\n",
    "            return f\"Error executing {tool_name}: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ccff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 7: Smoke Test - Basic Memory Operations\n",
    "print(\"=== Testing Basic Memory Operations ===\")\n",
    "\n",
    "# Test memory entry creation\n",
    "memory = ScratchpadMemory(max_entries=5, max_tokens=500)\n",
    "\n",
    "# Add different types of entries\n",
    "memory.add_thought(\"I need to solve a math problem\")\n",
    "memory.add_action(\"calculator(2 + 3)\")\n",
    "memory.add_observation(\"Result: 5\")\n",
    "memory.add_note(\"The answer is 5\")\n",
    "\n",
    "print(\"Memory entries:\", len(memory.entries))\n",
    "print(\"\\nMemory summary:\")\n",
    "print(memory.get_context_summary())\n",
    "\n",
    "# Test capacity management\n",
    "print(\"\\n=== Testing Capacity Management ===\")\n",
    "for i in range(10):\n",
    "    memory.add_entry(\"test\", f\"Long test entry {i} \" * 20, importance=0.1)\n",
    "\n",
    "print(\"Entries after capacity management:\", len(memory.entries))\n",
    "print(\"Estimated tokens:\", memory._estimate_tokens())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27515b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 8: Smoke Test - Key Information Extraction\n",
    "print(\"\\n=== Testing Key Information Extraction ===\")\n",
    "\n",
    "extractor = KeyInfoExtractor()\n",
    "\n",
    "test_text = \"\"\"\n",
    "I searched the web and found that the price is $29.99.\n",
    "The result was successful and the email contact is support@example.com.\n",
    "Error: Connection timeout occurred.\n",
    "The date is 2024-01-15 and the calculation result: 142.5\n",
    "\"\"\"\n",
    "\n",
    "key_info = extractor.extract_key_info(test_text)\n",
    "print(\"Key information extracted:\")\n",
    "for i, info in enumerate(key_info, 1):\n",
    "    importance = extractor.calculate_importance(info)\n",
    "    print(f\"{i}. {info} (importance: {importance:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f2a7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 9: Smoke Test - Memory-Enhanced ReAct\n",
    "print(\"\\n=== Testing Memory-Enhanced ReAct ===\")\n",
    "\n",
    "# Note: This is a minimal test. Full test requires actual model loading\n",
    "agent = MemoryReActAgent()\n",
    "\n",
    "# Test with a simple task\n",
    "task = \"Calculate 15 * 8 and remember the result\"\n",
    "\n",
    "print(f\"Task: {task}\")\n",
    "print(\"\\nStarting ReAct with memory...\")\n",
    "\n",
    "# For demo purposes, we'll simulate the process without actual LLM calls\n",
    "print(\"üí≠ Thought: I need to calculate 15 * 8\")\n",
    "agent.memory.add_thought(\"I need to calculate 15 * 8\")\n",
    "\n",
    "print(\"üîß Action: calculator(15 * 8)\")\n",
    "result = agent._calculator(\"15 * 8\")\n",
    "agent.memory.add_action(\"calculator(15 * 8)\")\n",
    "\n",
    "print(f\"üëÅÔ∏è Observation: {result}\")\n",
    "agent.memory.add_observation(result)\n",
    "\n",
    "print(\"üîß Action: memory_note(15 * 8 = 120)\")\n",
    "note_result = agent._memory_note(\"15 * 8 = 120\")\n",
    "print(f\"üëÅÔ∏è Observation: {note_result}\")\n",
    "\n",
    "print(\"\\nFinal memory summary:\")\n",
    "print(agent.memory.get_context_summary())\n",
    "\n",
    "print(\"\\n‚úÖ Smoke test completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd2f722",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 10: Advanced Features Demo\n",
    "print(\"\\n=== Advanced Memory Features ===\")\n",
    "\n",
    "# Test memory filtering by tags\n",
    "memory_advanced = ScratchpadMemory()\n",
    "\n",
    "# Add entries with different tags\n",
    "memory_advanced.add_entry(\n",
    "    \"action\", \"Searched for weather\", importance=0.6, tags=[\"search\", \"weather\"]\n",
    ")\n",
    "memory_advanced.add_entry(\n",
    "    \"result\", \"Temperature is 25¬∞C\", importance=0.8, tags=[\"weather\", \"data\"]\n",
    ")\n",
    "memory_advanced.add_entry(\n",
    "    \"action\", \"Calculated sum\", importance=0.5, tags=[\"math\", \"calculation\"]\n",
    ")\n",
    "memory_advanced.add_entry(\n",
    "    \"error\", \"Network timeout\", importance=0.9, tags=[\"error\", \"network\"]\n",
    ")\n",
    "\n",
    "print(\"All entries:\")\n",
    "print(memory_advanced.get_context_summary())\n",
    "\n",
    "print(\"\\nWeather-related entries only:\")\n",
    "print(memory_advanced.get_context_summary(include_tags=[\"weather\"]))\n",
    "\n",
    "print(\"\\nError entries only:\")\n",
    "print(memory_advanced.get_context_summary(include_tags=[\"error\"]))\n",
    "\n",
    "# Test memory export\n",
    "print(\"\\nMemory export (JSON):\")\n",
    "exported = memory_advanced.export_history()\n",
    "print(json.dumps(exported[0], indent=2))  # Show first entry\n",
    "\n",
    "print(\"\\n=== Memory and Scratchpad Demo Complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56f3faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 11: Key Parameters and Configuration\n",
    "print(\"\\n=== Key Parameters for Production Use ===\")\n",
    "\n",
    "config = {\n",
    "    \"memory\": {\n",
    "        \"max_entries\": 25,  # Adjust based on context window\n",
    "        \"max_tokens\": 2000,  # Token budget for memory\n",
    "        \"importance_threshold\": 0.3,  # Minimum importance to keep\n",
    "    },\n",
    "    \"extraction\": {\n",
    "        \"max_key_info\": 5,  # Max pieces per observation\n",
    "        \"importance_boost\": {\n",
    "            \"error\": 0.3,  # Boost for errors\n",
    "            \"success\": 0.2,  # Boost for success indicators\n",
    "            \"numbers\": 0.1,  # Boost for numerical data\n",
    "        },\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"max_new_tokens\": 200,  # For memory-aware generation\n",
    "        \"temperature\": 0.7,  # Balance creativity/consistency\n",
    "        \"load_in_8bit\": True,  # For VRAM efficiency\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Production configuration:\")\n",
    "print(json.dumps(config, indent=2))\n",
    "\n",
    "print(\"\\nüìù Low-VRAM options:\")\n",
    "print(\"- Use load_in_8bit or load_in_4bit\")\n",
    "print(\"- Reduce max_entries to 15-20\")\n",
    "print(\"- Lower max_tokens to 1000-1500\")\n",
    "print(\"- Use CPU offload for embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b1c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 12: When to Use This Pattern\n",
    "print(\"\\n=== When to Use Memory and Scratchpad ===\")\n",
    "\n",
    "use_cases = [\n",
    "    \"‚úÖ Multi-step calculations with intermediate results\",\n",
    "    \"‚úÖ Research tasks requiring information synthesis\",\n",
    "    \"‚úÖ Debugging scenarios where error tracking is crucial\",\n",
    "    \"‚úÖ Long conversations where context matters\",\n",
    "    \"‚úÖ Sequential problem-solving with dependencies\",\n",
    "    \"‚ùå Simple, single-step tasks\",\n",
    "    \"‚ùå Stateless API endpoints with no session\",\n",
    "    \"‚ùå Tasks with very limited context windows\",\n",
    "]\n",
    "\n",
    "print(\"Use cases:\")\n",
    "for case in use_cases:\n",
    "    print(f\"  {case}\")\n",
    "\n",
    "print(\"\\nüîß Integration notes:\")\n",
    "print(\"- Memory persists within a session/conversation\")\n",
    "print(\"- Export/import for session restoration\")\n",
    "print(\"- Combine with RAG for long-term knowledge\")\n",
    "print(\"- Use importance scoring to prioritize relevant info\")\n",
    "print(\"- Consider privacy: don't store sensitive data\")\n",
    "\n",
    "print(\"\\nüéØ Ready for Stage 4 multi-agent coordination!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1758b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Âü∫Êú¨Ë®òÊÜ∂Êìç‰Ωú\n",
    "memory = ScratchpadMemory(max_entries=5, max_tokens=500)\n",
    "memory.add_thought(\"ÈúÄË¶ÅËß£Ê±∫Êï∏Â≠∏ÂïèÈ°å\")\n",
    "memory.add_action(\"calculator(2 + 3)\")\n",
    "memory.add_observation(\"ÁµêÊûúÔºö5\")\n",
    "\n",
    "# ÂÆπÈáèÁÆ°ÁêÜÊ∏¨Ë©¶\n",
    "for i in range(10):\n",
    "    memory.add_entry(\"test\", f\"Èï∑Ê∏¨Ë©¶Ê¢ùÁõÆ {i}\", importance=0.1)\n",
    "\n",
    "# ÈóúÈçµË≥áË®äÊèêÂèñ\n",
    "extractor = KeyInfoExtractor()\n",
    "key_info = extractor.extract_key_info(\"ÁµêÊûúÔºö$29.99ÔºåÈåØË™§ÔºöÈÄ£Á∑öÈÄæÊôÇ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
