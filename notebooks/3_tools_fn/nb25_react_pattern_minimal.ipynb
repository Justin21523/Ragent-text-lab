{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f66bd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb25_react_pattern_minimal.ipynb\n",
    "# ReAct Pattern: Thought → Action → Observation → Answer\n",
    "\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (複製到每本 notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53251573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import & Setup\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from duckduckgo_search import DDGS\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Simple LLM Adapter\n",
    "class LLMAdapter:\n",
    "    def __init__(self, model_id=\"Qwen/Qwen2.5-7B-Instruct\"):\n",
    "        print(f\"Loading {model_id}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=\"auto\",\n",
    "            load_in_4bit=True,  # Low VRAM\n",
    "        )\n",
    "\n",
    "    def generate(self, prompt: str, max_new_tokens=512, temperature=0.7) -> str:\n",
    "        inputs = self.tokenizer(\n",
    "            prompt, return_tensors=\"pt\", truncation=True, max_length=2048\n",
    "        )\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Extract only the new generation\n",
    "        prompt_length = len(\n",
    "            self.tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
    "        )\n",
    "        return response[prompt_length:].strip()\n",
    "\n",
    "\n",
    "# Initialize LLM\n",
    "llm = LLMAdapter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6725de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Tool Registry & Schema\n",
    "class ToolRegistry:\n",
    "    def __init__(self):\n",
    "        self.tools = {}\n",
    "        self.register_default_tools()\n",
    "\n",
    "    def register_tool(self, name: str, func, description: str, schema: dict):\n",
    "        \"\"\"Register a tool with its function and schema\"\"\"\n",
    "        self.tools[name] = {\n",
    "            \"function\": func,\n",
    "            \"description\": description,\n",
    "            \"schema\": schema,\n",
    "        }\n",
    "\n",
    "    def get_tools_description(self) -> str:\n",
    "        \"\"\"Get formatted description of all available tools\"\"\"\n",
    "        desc = \"Available tools:\\n\"\n",
    "        for name, tool in self.tools.items():\n",
    "            desc += f\"- {name}: {tool['description']}\\n\"\n",
    "            desc += f\"  Schema: {json.dumps(tool['schema'])}\\n\"\n",
    "        return desc\n",
    "\n",
    "    def execute_tool(self, name: str, args: dict) -> str:\n",
    "        \"\"\"Execute a tool with given arguments\"\"\"\n",
    "        if name not in self.tools:\n",
    "            return f\"Error: Tool '{name}' not found\"\n",
    "\n",
    "        try:\n",
    "            return self.tools[name][\"function\"](**args)\n",
    "        except Exception as e:\n",
    "            return f\"Error executing {name}: {str(e)}\"\n",
    "\n",
    "    def register_default_tools(self):\n",
    "        \"\"\"Register default tools\"\"\"\n",
    "\n",
    "        # Calculator tool\n",
    "        def safe_calculator(expression: str) -> str:\n",
    "            \"\"\"Safe arithmetic calculator\"\"\"\n",
    "            try:\n",
    "                # Parse as AST to ensure safety\n",
    "                node = ast.parse(expression, mode=\"eval\")\n",
    "\n",
    "                # Only allow specific node types\n",
    "                allowed_nodes = (\n",
    "                    ast.Expression,\n",
    "                    ast.Num,\n",
    "                    ast.BinOp,\n",
    "                    ast.UnaryOp,\n",
    "                    ast.Add,\n",
    "                    ast.Sub,\n",
    "                    ast.Mult,\n",
    "                    ast.Div,\n",
    "                    ast.Pow,\n",
    "                    ast.USub,\n",
    "                    ast.UAdd,\n",
    "                    ast.Constant,\n",
    "                )\n",
    "\n",
    "                for node_item in ast.walk(node):\n",
    "                    if not isinstance(node_item, allowed_nodes):\n",
    "                        return f\"Error: Unsafe operation in expression\"\n",
    "\n",
    "                result = eval(compile(node, \"<string>\", \"eval\"))\n",
    "                return f\"Result: {result}\"\n",
    "            except Exception as e:\n",
    "                return f\"Error: {str(e)}\"\n",
    "\n",
    "        self.register_tool(\n",
    "            \"calculator\",\n",
    "            safe_calculator,\n",
    "            \"Evaluate arithmetic expressions safely\",\n",
    "            {\"expression\": \"string (e.g., '2+3*4')\"},\n",
    "        )\n",
    "\n",
    "        # Web search tool\n",
    "        def web_search(query: str, max_results: int = 3) -> str:\n",
    "            \"\"\"Search the web using DuckDuckGo\"\"\"\n",
    "            try:\n",
    "                with DDGS() as ddgs:\n",
    "                    results = list(ddgs.text(query, max_results=max_results))\n",
    "                    if not results:\n",
    "                        return \"No search results found\"\n",
    "\n",
    "                    output = f\"Search results for '{query}':\\n\"\n",
    "                    for i, result in enumerate(results, 1):\n",
    "                        output += f\"{i}. {result['title']}\\n\"\n",
    "                        output += f\"   {result['body'][:200]}...\\n\"\n",
    "                        output += f\"   URL: {result['href']}\\n\\n\"\n",
    "                    return output\n",
    "            except Exception as e:\n",
    "                return f\"Search error: {str(e)}\"\n",
    "\n",
    "        self.register_tool(\n",
    "            \"web_search\",\n",
    "            web_search,\n",
    "            \"Search the web for information\",\n",
    "            {\"query\": \"string\", \"max_results\": \"integer (optional, default 3)\"},\n",
    "        )\n",
    "\n",
    "        # File lookup tool\n",
    "        def file_lookup(filename: str) -> str:\n",
    "            \"\"\"Look up content in allowed directories\"\"\"\n",
    "            try:\n",
    "                # Whitelist allowed paths\n",
    "                allowed_dirs = [\"data\", \"outs\", \"configs\"]\n",
    "                file_path = Path(filename)\n",
    "\n",
    "                # Check if path is within allowed directories\n",
    "                allowed = any(str(file_path).startswith(d) for d in allowed_dirs)\n",
    "                if not allowed:\n",
    "                    return f\"Error: Access denied to {filename}\"\n",
    "\n",
    "                if file_path.exists() and file_path.is_file():\n",
    "                    content = file_path.read_text(encoding=\"utf-8\")[\n",
    "                        :1000\n",
    "                    ]  # Limit content\n",
    "                    return f\"File content:\\n{content}\"\n",
    "                else:\n",
    "                    return f\"Error: File {filename} not found\"\n",
    "            except Exception as e:\n",
    "                return f\"File lookup error: {str(e)}\"\n",
    "\n",
    "        self.register_tool(\n",
    "            \"file_lookup\",\n",
    "            file_lookup,\n",
    "            \"Look up file content in data/outs/configs directories\",\n",
    "            {\"filename\": \"string (relative path)\"},\n",
    "        )\n",
    "\n",
    "\n",
    "# Initialize tool registry\n",
    "tools = ToolRegistry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac322fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: ReAct Prompt Template\n",
    "REACT_PROMPT_TEMPLATE = \"\"\"You are a helpful AI assistant that can use tools to answer questions.\n",
    "You must follow this exact format for each step:\n",
    "\n",
    "Thought: [Your reasoning about what to do next]\n",
    "Action: {\"tool\": \"tool_name\", \"args\": {\"arg1\": \"value1\"}}\n",
    "Observation: [Tool output will be provided here]\n",
    "\n",
    "Continue this pattern until you have enough information to answer.\n",
    "When ready to answer, use:\n",
    "Answer: [Your final response]\n",
    "\n",
    "{tools_description}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Begin:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def parse_action(text: str) -> Optional[Tuple[str, dict]]:\n",
    "    \"\"\"Parse action JSON from LLM output\"\"\"\n",
    "    try:\n",
    "        # Look for JSON in the Action line\n",
    "        action_match = re.search(r\"Action:\\s*(\\{.*?\\})\", text, re.DOTALL)\n",
    "        if action_match:\n",
    "            action_json = action_match.group(1)\n",
    "            action_data = json.loads(action_json)\n",
    "            tool_name = action_data.get(\"tool\")\n",
    "            args = action_data.get(\"args\", {})\n",
    "            return tool_name, args\n",
    "    except (json.JSONDecodeError, AttributeError) as e:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_answer(text: str) -> Optional[str]:\n",
    "    \"\"\"Extract final answer from ReAct output\"\"\"\n",
    "    answer_match = re.search(r\"Answer:\\s*(.*?)(?:\\n|$)\", text, re.DOTALL)\n",
    "    if answer_match:\n",
    "        return answer_match.group(1).strip()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e917b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Core ReAct Loop\n",
    "class ReActAgent:\n",
    "    def __init__(self, llm: LLMAdapter, tools: ToolRegistry, max_iterations=5):\n",
    "        self.llm = llm\n",
    "        self.tools = tools\n",
    "        self.max_iterations = max_iterations\n",
    "\n",
    "    def run(self, question: str) -> str:\n",
    "        \"\"\"Run ReAct loop for a given question\"\"\"\n",
    "\n",
    "        # Initialize prompt\n",
    "        prompt = REACT_PROMPT_TEMPLATE.format(\n",
    "            tools_description=self.tools.get_tools_description(), question=question\n",
    "        )\n",
    "\n",
    "        conversation = prompt\n",
    "\n",
    "        for iteration in range(self.max_iterations):\n",
    "            print(f\"\\n--- Iteration {iteration + 1} ---\")\n",
    "\n",
    "            # Generate response from LLM\n",
    "            response = self.llm.generate(\n",
    "                conversation, max_new_tokens=256, temperature=0.3\n",
    "            )\n",
    "            conversation += response\n",
    "\n",
    "            print(f\"LLM Response: {response[:200]}...\")\n",
    "\n",
    "            # Check if we have a final answer\n",
    "            final_answer = extract_answer(response)\n",
    "            if final_answer:\n",
    "                print(f\"Final answer found: {final_answer}\")\n",
    "                return final_answer\n",
    "\n",
    "            # Parse action\n",
    "            action_result = parse_action(response)\n",
    "            if action_result:\n",
    "                tool_name, args = action_result\n",
    "                print(f\"Executing tool: {tool_name} with args: {args}\")\n",
    "\n",
    "                # Execute tool\n",
    "                observation = self.tools.execute_tool(tool_name, args)\n",
    "\n",
    "                # Add observation to conversation\n",
    "                conversation += f\"\\nObservation: {observation}\\n\"\n",
    "                print(f\"Observation: {observation[:100]}...\")\n",
    "            else:\n",
    "                # No valid action found, prompt for continuation\n",
    "                conversation += \"\\nPlease provide a valid Action in JSON format or your final Answer.\\n\"\n",
    "\n",
    "        return \"Sorry, I couldn't solve this problem within the maximum iterations.\"\n",
    "\n",
    "\n",
    "# Initialize agent\n",
    "agent = ReActAgent(llm, tools, max_iterations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6ee668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Error Handling & Retry\n",
    "class RobustReActAgent(ReActAgent):\n",
    "    def __init__(\n",
    "        self, llm: LLMAdapter, tools: ToolRegistry, max_iterations=5, max_retries=2\n",
    "    ):\n",
    "        super().__init__(llm, tools, max_iterations)\n",
    "        self.max_retries = max_retries\n",
    "\n",
    "    def run_with_retry(self, question: str) -> str:\n",
    "        \"\"\"Run ReAct with retry mechanism\"\"\"\n",
    "\n",
    "        for attempt in range(self.max_retries + 1):\n",
    "            try:\n",
    "                print(f\"\\n=== Attempt {attempt + 1} ===\")\n",
    "                result = self.run(question)\n",
    "\n",
    "                # Check if result is meaningful (not just error message)\n",
    "                if not result.startswith(\"Sorry, I couldn't\"):\n",
    "                    return result\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                if attempt == self.max_retries:\n",
    "                    return f\"Error: Maximum retries exceeded. Last error: {str(e)}\"\n",
    "\n",
    "        return \"Error: All attempts failed to produce a valid answer.\"\n",
    "\n",
    "\n",
    "# Initialize robust agent\n",
    "robust_agent = RobustReActAgent(llm, tools, max_iterations=3, max_retries=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7232b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Smoke Test - Multi-step Problem\n",
    "print(\"=== Smoke Test: Multi-step Problem ===\")\n",
    "\n",
    "# Test question that requires multiple tools\n",
    "test_question = (\n",
    "    \"What is 15 * 23, and can you search for information about the ReAct pattern in AI?\"\n",
    ")\n",
    "\n",
    "print(f\"Question: {test_question}\")\n",
    "print(\n",
    "    f\"Expected: Should use calculator for 15*23=345, then search for ReAct pattern info\"\n",
    ")\n",
    "\n",
    "# Run the test\n",
    "result = robust_agent.run_with_retry(test_question)\n",
    "print(f\"\\n=== Final Result ===\")\n",
    "print(result)\n",
    "\n",
    "# Simple validation\n",
    "print(f\"\\n=== Validation ===\")\n",
    "contains_calculation = \"345\" in result or \"15\" in result and \"23\" in result\n",
    "contains_search_info = any(\n",
    "    word in result.lower() for word in [\"react\", \"reasoning\", \"action\", \"thought\"]\n",
    ")\n",
    "\n",
    "print(f\"✓ Contains calculation result: {contains_calculation}\")\n",
    "print(f\"✓ Contains search information: {contains_search_info}\")\n",
    "print(f\"✓ Overall success: {contains_calculation or contains_search_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637d9fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: What we built / Next steps\n",
    "print(\n",
    "    \"\"\"\n",
    "=== What we built ===\n",
    "✓ Minimal ReAct Agent: Thought → Action → Observation → Answer\n",
    "✓ Tool Registry: Unified tool management and execution\n",
    "✓ Error Handling: Retry mechanism for failed attempts\n",
    "✓ JSON Parsing: Robust action extraction from LLM output\n",
    "✓ Safety: Whitelisted tools and safe execution\n",
    "\n",
    "=== Key Components ===\n",
    "• LLMAdapter: Simple wrapper for Qwen2.5-7B with 4-bit loading\n",
    "• ToolRegistry: Calculator, web search, file lookup\n",
    "• ReActAgent: Core reasoning loop with iteration limits\n",
    "• Action Parser: Extract tool calls from natural language\n",
    "\n",
    "=== Pitfalls ===\n",
    "• LLM may not follow JSON format exactly - need robust parsing\n",
    "• Tool execution can fail - always handle exceptions\n",
    "• Infinite loops possible - set max iterations\n",
    "• Context window overflow - manage conversation length\n",
    "\n",
    "=== Next Steps ===\n",
    "• Add more sophisticated tools (code execution, API calls)\n",
    "• Implement memory/context compression for long conversations\n",
    "• Add tool validation and security checks\n",
    "• Integrate with RAG for knowledge-based reasoning\n",
    "• Build multi-agent orchestration on this foundation\n",
    "\n",
    "=== When to use this ===\n",
    "• Multi-step problems requiring tool use\n",
    "• Situations where reasoning steps should be hidden from user\n",
    "• Building blocks for more complex agent systems\n",
    "• Prototyping tool-augmented AI applications\n",
    "\"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
