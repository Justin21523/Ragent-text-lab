{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d75ce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3 - Tools & Function Calling\n",
    "# nb23_content_extraction.ipynb\n",
    "# ç›®æ¨™ï¼štrafilatura å…§å®¹æŠ½å–ã€å» HTMLã€ä¸­æ–‡å‹å¥½è™•ç†\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (è¤‡è£½åˆ°æ¯æœ¬ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd84419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 2: Dependencies and Imports\n",
    "# ============================================================================\n",
    "\n",
    "# Install required packages (run once)\n",
    "# !pip install trafilatura requests beautifulsoup4 chardet opencc-python-reimplemented\n",
    "\n",
    "import trafilatura\n",
    "import requests\n",
    "import chardet\n",
    "from bs4 import BeautifulSoup\n",
    "from opencc import OpenCC\n",
    "import re\n",
    "import json\n",
    "import urllib.parse\n",
    "from typing import Dict, List, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "\n",
    "print(\"Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a81fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 3: Basic HTML Content Extraction\n",
    "# ============================================================================\n",
    "\n",
    "def extract_basic_content(html_content: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Basic content extraction using trafilatura\n",
    "\n",
    "    Args:\n",
    "        html_content: Raw HTML string\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with extracted content\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract main content\n",
    "    text = trafilatura.extract(html_content, include_links=False)\n",
    "\n",
    "    # Extract with metadata\n",
    "    metadata = trafilatura.extract_metadata(html_content)\n",
    "\n",
    "    # Extract with format preservation\n",
    "    formatted_text = trafilatura.extract(\n",
    "        html_content,\n",
    "        include_formatting=True,\n",
    "        include_links=False\n",
    "    )\n",
    "\n",
    "    result = {\n",
    "        \"text\": text or \"\",\n",
    "        \"formatted_text\": formatted_text or \"\",\n",
    "        \"title\": metadata.title if metadata else \"\",\n",
    "        \"author\": metadata.author if metadata else \"\",\n",
    "        \"date\": metadata.date if metadata else \"\",\n",
    "        \"description\": metadata.description if metadata else \"\",\n",
    "        \"language\": metadata.language if metadata else \"\"\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "# Test with sample HTML\n",
    "sample_html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>æ¸¬è©¦é é¢ - AI æŠ€è¡“åˆ†äº«</title>\n",
    "    <meta name=\"author\" content=\"å¼µä¸‰\">\n",
    "    <meta name=\"description\" content=\"é—œæ–¼å¤§å‹èªè¨€æ¨¡å‹çš„æŠ€è¡“æ–‡ç« \">\n",
    "</head>\n",
    "<body>\n",
    "    <header>\n",
    "        <nav>å°èˆªèœå–®</nav>\n",
    "    </header>\n",
    "    <main>\n",
    "        <h1>å¤§å‹èªè¨€æ¨¡å‹æŠ€è¡“è§£æ</h1>\n",
    "        <p>æœ¬æ–‡å°‡æ·±å…¥æ¢è¨ LLM çš„æ ¸å¿ƒåŸç†ã€‚</p>\n",
    "        <h2>Transformer æ¶æ§‹</h2>\n",
    "        <p>Transformer æ˜¯ç¾ä»£ LLM çš„åŸºç¤æ¶æ§‹ï¼ŒåŒ…å«æ³¨æ„åŠ›æ©Ÿåˆ¶ç­‰é—œéµçµ„ä»¶ã€‚</p>\n",
    "        <ul>\n",
    "            <li>è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶</li>\n",
    "            <li>ä½ç½®ç·¨ç¢¼</li>\n",
    "            <li>å‰é¥‹ç¶²çµ¡</li>\n",
    "        </ul>\n",
    "    </main>\n",
    "    <footer>ç‰ˆæ¬Šä¿¡æ¯</footer>\n",
    "    <script>console.log(\"ads\");</script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "extracted = extract_basic_content(sample_html)\n",
    "print(\"Extracted content:\")\n",
    "for key, value in extracted.items():\n",
    "    print(f\"{key}: {value[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134f5492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 4: Advanced Extraction Options\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def extract_advanced_content(html_content: str, config: Dict = None) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Advanced content extraction with configurable options\n",
    "\n",
    "    Args:\n",
    "        html_content: Raw HTML string\n",
    "        config: Configuration options\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with extracted content and metadata\n",
    "    \"\"\"\n",
    "\n",
    "    default_config = {\n",
    "        \"include_links\": True,\n",
    "        \"include_images\": False,\n",
    "        \"include_tables\": True,\n",
    "        \"include_formatting\": True,\n",
    "        \"deduplicate\": True,\n",
    "        \"favor_precision\": True,\n",
    "        \"favor_recall\": False,\n",
    "    }\n",
    "\n",
    "    if config:\n",
    "        default_config.update(config)\n",
    "\n",
    "    # Extract main content with options\n",
    "    text = trafilatura.extract(\n",
    "        html_content,\n",
    "        include_links=default_config[\"include_links\"],\n",
    "        include_images=default_config[\"include_images\"],\n",
    "        include_tables=default_config[\"include_tables\"],\n",
    "        include_formatting=default_config[\"include_formatting\"],\n",
    "        deduplicate=default_config[\"deduplicate\"],\n",
    "        favor_precision=default_config[\"favor_precision\"],\n",
    "        favor_recall=default_config[\"favor_recall\"],\n",
    "    )\n",
    "\n",
    "    # Extract metadata\n",
    "    metadata = trafilatura.extract_metadata(html_content)\n",
    "\n",
    "    # Extract links separately if needed\n",
    "    links = []\n",
    "    if default_config[\"include_links\"]:\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            links.append({\"text\": link.get_text().strip(), \"href\": link[\"href\"]})\n",
    "\n",
    "    result = {\n",
    "        \"text\": text or \"\",\n",
    "        \"title\": metadata.title if metadata else \"\",\n",
    "        \"author\": metadata.author if metadata else \"\",\n",
    "        \"date\": str(metadata.date) if metadata and metadata.date else \"\",\n",
    "        \"description\": metadata.description if metadata else \"\",\n",
    "        \"language\": metadata.language if metadata else \"\",\n",
    "        \"sitename\": metadata.sitename if metadata else \"\",\n",
    "        \"links\": links,\n",
    "        \"word_count\": len((text or \"\").split()),\n",
    "        \"char_count\": len(text or \"\"),\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test advanced extraction\n",
    "advanced_config = {\n",
    "    \"include_links\": True,\n",
    "    \"include_formatting\": True,\n",
    "    \"favor_precision\": True,\n",
    "}\n",
    "\n",
    "advanced_result = extract_advanced_content(sample_html, advanced_config)\n",
    "print(\"Advanced extraction results:\")\n",
    "print(f\"Word count: {advanced_result['word_count']}\")\n",
    "print(f\"Character count: {advanced_result['char_count']}\")\n",
    "print(f\"Links found: {len(advanced_result['links'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1863b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 5: Chinese Web Page Processing\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def detect_encoding(content: bytes) -> str:\n",
    "    \"\"\"Detect encoding of web content\"\"\"\n",
    "    detected = chardet.detect(content)\n",
    "    return detected[\"encoding\"] if detected[\"encoding\"] else \"utf-8\"\n",
    "\n",
    "\n",
    "def normalize_chinese_text(text: str, convert_variant: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Normalize Chinese text\n",
    "\n",
    "    Args:\n",
    "        text: Input text\n",
    "        convert_variant: 't2s' for traditional to simplified, 's2t' for reverse\n",
    "\n",
    "    Returns:\n",
    "        Normalized text\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = text.strip()\n",
    "\n",
    "    # Convert between traditional/simplified if requested\n",
    "    if convert_variant:\n",
    "        try:\n",
    "            cc = OpenCC(convert_variant)\n",
    "            text = cc.convert(text)\n",
    "        except:\n",
    "            print(f\"Warning: OpenCC conversion {convert_variant} failed\")\n",
    "\n",
    "    # Normalize punctuation\n",
    "    punctuation_map = {\n",
    "        \"ï¼ˆ\": \"(\",\n",
    "        \"ï¼‰\": \")\",\n",
    "        \"ï¼Ÿ\": \"?\",\n",
    "        \"ï¼\": \"!\",\n",
    "        \"ï¼Œ\": \",\",\n",
    "        \"ã€‚\": \".\",\n",
    "        \"ï¼›\": \";\",\n",
    "        \"ï¼š\": \":\",\n",
    "    }\n",
    "\n",
    "    for old, new in punctuation_map.items():\n",
    "        text = text.replace(old, new)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_chinese_content(\n",
    "    html_content: Union[str, bytes], url: str = \"\"\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Extract content from Chinese web pages with proper encoding handling\n",
    "\n",
    "    Args:\n",
    "        html_content: HTML content (string or bytes)\n",
    "        url: Source URL for context\n",
    "\n",
    "    Returns:\n",
    "        Extracted content dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle encoding if bytes\n",
    "    if isinstance(html_content, bytes):\n",
    "        encoding = detect_encoding(html_content)\n",
    "        try:\n",
    "            html_content = html_content.decode(encoding)\n",
    "        except:\n",
    "            html_content = html_content.decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    # Basic extraction\n",
    "    result = extract_advanced_content(html_content)\n",
    "\n",
    "    # Normalize Chinese text\n",
    "    if result[\"text\"]:\n",
    "        result[\"text_normalized\"] = normalize_chinese_text(result[\"text\"])\n",
    "        result[\"text_simplified\"] = normalize_chinese_text(result[\"text\"], \"t2s\")\n",
    "        result[\"text_traditional\"] = normalize_chinese_text(result[\"text\"], \"s2t\")\n",
    "\n",
    "    # Add source info\n",
    "    result[\"source_url\"] = url\n",
    "    result[\"extraction_time\"] = time.time()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test Chinese content processing\n",
    "chinese_html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"zh-TW\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>äººå·¥æ™ºæ…§æŠ€è¡“ç™¼å±•è¶¨å‹¢</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>ï¼’ï¼ï¼’ï¼”å¹´äººå·¥æ™ºæ…§æŠ€è¡“ç™¼å±•è¶¨å‹¢</h1>\n",
    "    <p>è¿‘å¹´ä¾†ï¼Œå¤§å‹èªè¨€æ¨¡å‹ï¼ˆï¼¬ï¼¬ï¼­ï¼‰æŠ€è¡“çªé£›çŒ›é€²ã€‚ä¸»è¦ç™¼å±•åŒ…æ‹¬ï¼š</p>\n",
    "    <ul>\n",
    "        <li>æ¨¡å‹è¦æ¨¡æŒçºŒæ“´å¤§</li>\n",
    "        <li>æ¨ç†èƒ½åŠ›é¡¯è‘—æå‡</li>\n",
    "        <li>å¤šæ¨¡æ…‹æ•´åˆåŠ é€Ÿ</li>\n",
    "    </ul>\n",
    "    <p>é€™äº›æŠ€è¡“å°‡å°å„è¡Œå„æ¥­ç”¢ç”Ÿæ·±é å½±éŸ¿ã€‚</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "chinese_result = extract_chinese_content(chinese_html, \"https://example.com/ai-trends\")\n",
    "print(\"Chinese content extraction:\")\n",
    "print(f\"Original: {chinese_result['text'][:100]}...\")\n",
    "print(f\"Normalized: {chinese_result['text_normalized'][:100]}...\")\n",
    "print(f\"Simplified: {chinese_result['text_simplified'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc106263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 6: Batch URL Processing Tool\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExtractionResult:\n",
    "    url: str\n",
    "    title: str\n",
    "    text: str\n",
    "    status: str\n",
    "    error: str = \"\"\n",
    "    metadata: Dict = None\n",
    "\n",
    "\n",
    "def fetch_and_extract(url: str, timeout: int = 30) -> ExtractionResult:\n",
    "    \"\"\"\n",
    "    Fetch and extract content from a single URL\n",
    "\n",
    "    Args:\n",
    "        url: Target URL\n",
    "        timeout: Request timeout in seconds\n",
    "\n",
    "    Returns:\n",
    "        ExtractionResult object\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Set headers to mimic browser\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
    "        }\n",
    "\n",
    "        # Fetch content\n",
    "        response = requests.get(url, headers=headers, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Extract content\n",
    "        extracted = extract_chinese_content(response.content, url)\n",
    "\n",
    "        return ExtractionResult(\n",
    "            url=url,\n",
    "            title=extracted.get(\"title\", \"\"),\n",
    "            text=extracted.get(\"text\", \"\"),\n",
    "            status=\"success\",\n",
    "            metadata=extracted,\n",
    "        )\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        return ExtractionResult(\n",
    "            url=url, title=\"\", text=\"\", status=\"network_error\", error=str(e)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return ExtractionResult(\n",
    "            url=url, title=\"\", text=\"\", status=\"extraction_error\", error=str(e)\n",
    "        )\n",
    "\n",
    "\n",
    "def batch_extract_urls(urls: List[str], delay: float = 1.0) -> List[ExtractionResult]:\n",
    "    \"\"\"\n",
    "    Extract content from multiple URLs with rate limiting\n",
    "\n",
    "    Args:\n",
    "        urls: List of URLs to process\n",
    "        delay: Delay between requests in seconds\n",
    "\n",
    "    Returns:\n",
    "        List of ExtractionResult objects\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, url in enumerate(urls):\n",
    "        print(f\"Processing {i+1}/{len(urls)}: {url}\")\n",
    "\n",
    "        result = fetch_and_extract(url)\n",
    "        results.append(result)\n",
    "\n",
    "        # Rate limiting\n",
    "        if i < len(urls) - 1:\n",
    "            time.sleep(delay)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Test batch processing with sample URLs\n",
    "sample_urls = [\n",
    "    \"https://httpbin.org/html\",  # Simple test page\n",
    "    \"https://example.com\",  # Basic example page\n",
    "]\n",
    "\n",
    "print(\"Batch URL processing test:\")\n",
    "batch_results = batch_extract_urls(\n",
    "    sample_urls[:1], delay=0.5\n",
    ")  # Test with first URL only\n",
    "\n",
    "for result in batch_results:\n",
    "    print(f\"URL: {result.url}\")\n",
    "    print(f\"Status: {result.status}\")\n",
    "    print(f\"Title: {result.title}\")\n",
    "    print(f\"Text length: {len(result.text)}\")\n",
    "    if result.error:\n",
    "        print(f\"Error: {result.error}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88f8880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 7: Safety Checks and Content Filtering\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def is_safe_url(url: str) -> bool:\n",
    "    \"\"\"\n",
    "    Basic URL safety check\n",
    "\n",
    "    Args:\n",
    "        url: URL to check\n",
    "\n",
    "    Returns:\n",
    "        True if URL appears safe\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse URL\n",
    "    try:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "    # Check scheme\n",
    "    if parsed.scheme not in [\"http\", \"https\"]:\n",
    "        return False\n",
    "\n",
    "    # Check for suspicious patterns\n",
    "    suspicious_patterns = [\n",
    "        \"javascript:\",\n",
    "        \"data:\",\n",
    "        \"file:\",\n",
    "        \"ftp:\",\n",
    "    ]\n",
    "\n",
    "    for pattern in suspicious_patterns:\n",
    "        if pattern in url.lower():\n",
    "            return False\n",
    "\n",
    "    # Check domain blacklist (basic example)\n",
    "    domain_blacklist = [\"malware.com\", \"phishing.site\"]\n",
    "\n",
    "    if parsed.netloc.lower() in domain_blacklist:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def filter_extracted_content(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Filter potentially harmful content from extracted text\n",
    "\n",
    "    Args:\n",
    "        content: Extracted text content\n",
    "\n",
    "    Returns:\n",
    "        Filtered content\n",
    "    \"\"\"\n",
    "\n",
    "    if not content:\n",
    "        return \"\"\n",
    "\n",
    "    # Remove script-like patterns\n",
    "    content = re.sub(\n",
    "        r\"<script.*?</script>\", \"\", content, flags=re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "    content = re.sub(r\"<style.*?</style>\", \"\", content, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # Remove suspicious URLs\n",
    "    content = re.sub(r\"javascript:[^\\\\s]*\", \"[FILTERED]\", content, flags=re.IGNORECASE)\n",
    "    content = re.sub(r\"data:[^\\\\s]*\", \"[FILTERED]\", content, flags=re.IGNORECASE)\n",
    "\n",
    "    # Limit excessive repetition\n",
    "    content = re.sub(r\"(.{10,}?)\\\\1{3,}\", r\"\\\\1[REPEATED]\", content)\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "def safe_extract_content(url_or_html: str, is_url: bool = True) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Safely extract content with security checks\n",
    "\n",
    "    Args:\n",
    "        url_or_html: URL to fetch or HTML content\n",
    "        is_url: True if input is URL, False if HTML content\n",
    "\n",
    "    Returns:\n",
    "        Extracted content with safety metadata\n",
    "    \"\"\"\n",
    "\n",
    "    result = {\"text\": \"\", \"title\": \"\", \"status\": \"error\", \"safety_warnings\": []}\n",
    "\n",
    "    try:\n",
    "        if is_url:\n",
    "            # Check URL safety\n",
    "            if not is_safe_url(url_or_html):\n",
    "                result[\"safety_warnings\"].append(\"Unsafe URL detected\")\n",
    "                result[\"status\"] = \"unsafe_url\"\n",
    "                return result\n",
    "\n",
    "            # Fetch content\n",
    "            extraction_result = fetch_and_extract(url_or_html)\n",
    "            if extraction_result.status != \"success\":\n",
    "                result[\"status\"] = extraction_result.status\n",
    "                result[\"error\"] = extraction_result.error\n",
    "                return result\n",
    "\n",
    "            content = extraction_result.text\n",
    "            title = extraction_result.title\n",
    "        else:\n",
    "            # Extract from HTML directly\n",
    "            extracted = extract_chinese_content(url_or_html)\n",
    "            content = extracted.get(\"text\", \"\")\n",
    "            title = extracted.get(\"title\", \"\")\n",
    "\n",
    "        # Filter content\n",
    "        filtered_content = filter_extracted_content(content)\n",
    "\n",
    "        # Check if significant content was filtered\n",
    "        if len(filtered_content) < len(content) * 0.8:\n",
    "            result[\"safety_warnings\"].append(\"Significant content filtered\")\n",
    "\n",
    "        result.update(\n",
    "            {\n",
    "                \"text\": filtered_content,\n",
    "                \"title\": title,\n",
    "                \"status\": \"success\",\n",
    "                \"original_length\": len(content),\n",
    "                \"filtered_length\": len(filtered_content),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        result[\"status\"] = \"extraction_error\"\n",
    "        result[\"error\"] = str(e)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test safety filtering\n",
    "malicious_html = \"\"\"\n",
    "<html>\n",
    "<head><title>Test Page</title></head>\n",
    "<body>\n",
    "    <h1>Normal Content</h1>\n",
    "    <p>This is normal text content.</p>\n",
    "    <script>alert('malicious code');</script>\n",
    "    <p>More normal content here.</p>\n",
    "    <a href=\"javascript:alert('xss')\">Suspicious Link</a>\n",
    "    <p>This sentence repeats. This sentence repeats. This sentence repeats. This sentence repeats.</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "safe_result = safe_extract_content(malicious_html, is_url=False)\n",
    "print(\"Safety filtering test:\")\n",
    "print(f\"Status: {safe_result['status']}\")\n",
    "print(f\"Warnings: {safe_result['safety_warnings']}\")\n",
    "print(f\"Original length: {safe_result.get('original_length', 0)}\")\n",
    "print(f\"Filtered length: {safe_result.get('filtered_length', 0)}\")\n",
    "print(f\"Filtered content: {safe_result['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1f60b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 8: Smoke Test - Real World Example\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def smoke_test_extraction():\n",
    "    \"\"\"\n",
    "    Comprehensive smoke test for content extraction\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=== Content Extraction Smoke Test ===\")\n",
    "\n",
    "    # Test 1: Basic HTML extraction\n",
    "    print(\"\\\\nTest 1: Basic HTML extraction\")\n",
    "    test_html = \"\"\"\n",
    "    <html>\n",
    "    <head><title>æ¸¬è©¦æ–‡ç« </title></head>\n",
    "    <body>\n",
    "        <h1>AI æŠ€è¡“ç™¼å±•</h1>\n",
    "        <p>äººå·¥æ™ºæ…§æŠ€è¡“æ­£åœ¨å¿«é€Ÿç™¼å±•ã€‚</p>\n",
    "        <p>ä¸»è¦åŒ…æ‹¬æ©Ÿå™¨å­¸ç¿’ã€æ·±åº¦å­¸ç¿’ç­‰é ˜åŸŸã€‚</p>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    result1 = extract_basic_content(test_html)\n",
    "    assert len(result1[\"text\"]) > 0, \"Basic extraction failed\"\n",
    "    assert \"AI æŠ€è¡“ç™¼å±•\" in result1[\"text\"], \"Title not extracted\"\n",
    "    print(\"âœ“ Basic extraction working\")\n",
    "\n",
    "    # Test 2: Chinese text normalization\n",
    "    print(\"\\\\nTest 2: Chinese text normalization\")\n",
    "    chinese_text = \"é€™æ˜¯æ¸¬è©¦æ–‡å­—ï¼ˆåŒ…å«å…¨å½¢æ‹¬è™Ÿï¼‰ã€‚\"\n",
    "    normalized = normalize_chinese_text(chinese_text)\n",
    "    assert \"(\" in normalized and \")\" in normalized, \"Punctuation normalization failed\"\n",
    "    print(\"âœ“ Chinese normalization working\")\n",
    "\n",
    "    # Test 3: Safety filtering\n",
    "    print(\"\\\\nTest 3: Safety filtering\")\n",
    "    unsafe_html = \"<script>alert('test')</script><p>Safe content</p>\"\n",
    "    safe_result = safe_extract_content(unsafe_html, is_url=False)\n",
    "    assert safe_result[\"status\"] == \"success\", \"Safety extraction failed\"\n",
    "    assert \"script\" not in safe_result[\"text\"].lower(), \"Script not filtered\"\n",
    "    print(\"âœ“ Safety filtering working\")\n",
    "\n",
    "    # Test 4: Metadata extraction\n",
    "    print(\"\\\\nTest 4: Metadata extraction\")\n",
    "    meta_html = \"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>æ¸¬è©¦æ¨™é¡Œ</title>\n",
    "        <meta name=\"author\" content=\"æ¸¬è©¦ä½œè€…\">\n",
    "        <meta name=\"description\" content=\"æ¸¬è©¦æè¿°\">\n",
    "    </head>\n",
    "    <body><p>å…§å®¹</p></body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    result4 = extract_advanced_content(meta_html)\n",
    "    assert result4[\"title\"] == \"æ¸¬è©¦æ¨™é¡Œ\", \"Title extraction failed\"\n",
    "    assert result4[\"author\"] == \"æ¸¬è©¦ä½œè€…\", \"Author extraction failed\"\n",
    "    print(\"âœ“ Metadata extraction working\")\n",
    "\n",
    "    print(\"\\\\nğŸ‰ All smoke tests passed!\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de760828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 9: Reusable Content Extractor Class\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class ContentExtractor:\n",
    "    \"\"\"\n",
    "    Reusable content extraction tool with configurable options\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Dict = None):\n",
    "        \"\"\"\n",
    "        Initialize extractor with configuration\n",
    "\n",
    "        Args:\n",
    "            config: Configuration dictionary\n",
    "        \"\"\"\n",
    "\n",
    "        self.config = {\n",
    "            \"include_links\": True,\n",
    "            \"include_formatting\": True,\n",
    "            \"chinese_normalization\": True,\n",
    "            \"safety_filtering\": True,\n",
    "            \"request_timeout\": 30,\n",
    "            \"request_delay\": 1.0,\n",
    "            \"convert_chinese\": None,  # 't2s', 's2t', or None\n",
    "            \"max_content_length\": 100000,\n",
    "            \"user_agent\": \"Mozilla/5.0 (compatible; ContentExtractor/1.0)\",\n",
    "        }\n",
    "\n",
    "        if config:\n",
    "            self.config.update(config)\n",
    "\n",
    "    def extract_from_url(self, url: str) -> Dict[str, any]:\n",
    "        \"\"\"Extract content from URL\"\"\"\n",
    "\n",
    "        if not is_safe_url(url):\n",
    "            return {\n",
    "                \"status\": \"unsafe_url\",\n",
    "                \"error\": \"URL failed safety check\",\n",
    "                \"text\": \"\",\n",
    "                \"metadata\": {},\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            headers = {\"User-Agent\": self.config[\"user_agent\"]}\n",
    "            response = requests.get(\n",
    "                url, headers=headers, timeout=self.config[\"request_timeout\"]\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "\n",
    "            return self.extract_from_html(response.content, source_url=url)\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"status\": \"error\", \"error\": str(e), \"text\": \"\", \"metadata\": {}}\n",
    "\n",
    "    def extract_from_html(\n",
    "        self, html_content: Union[str, bytes], source_url: str = \"\"\n",
    "    ) -> Dict[str, any]:\n",
    "        \"\"\"Extract content from HTML\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Handle encoding\n",
    "            if isinstance(html_content, bytes):\n",
    "                encoding = detect_encoding(html_content)\n",
    "                html_content = html_content.decode(encoding, errors=\"ignore\")\n",
    "\n",
    "            # Basic extraction\n",
    "            result = extract_advanced_content(html_content, self.config)\n",
    "\n",
    "            # Chinese processing\n",
    "            if self.config[\"chinese_normalization\"] and result[\"text\"]:\n",
    "                result[\"text\"] = normalize_chinese_text(\n",
    "                    result[\"text\"], self.config[\"convert_chinese\"]\n",
    "                )\n",
    "\n",
    "            # Safety filtering\n",
    "            if self.config[\"safety_filtering\"]:\n",
    "                result[\"text\"] = filter_extracted_content(result[\"text\"])\n",
    "\n",
    "            # Length limiting\n",
    "            max_len = self.config[\"max_content_length\"]\n",
    "            if len(result[\"text\"]) > max_len:\n",
    "                result[\"text\"] = result[\"text\"][:max_len] + \"...\"\n",
    "                result[\"truncated\"] = True\n",
    "\n",
    "            # Add extraction metadata\n",
    "            result.update(\n",
    "                {\n",
    "                    \"status\": \"success\",\n",
    "                    \"source_url\": source_url,\n",
    "                    \"extraction_time\": time.time(),\n",
    "                    \"extractor_version\": \"1.0\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"status\": \"error\", \"error\": str(e), \"text\": \"\", \"metadata\": {}}\n",
    "\n",
    "    def batch_extract(self, urls: List[str]) -> List[Dict[str, any]]:\n",
    "        \"\"\"Extract content from multiple URLs\"\"\"\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for i, url in enumerate(urls):\n",
    "            print(f\"Processing {i+1}/{len(urls)}: {url}\")\n",
    "\n",
    "            result = self.extract_from_url(url)\n",
    "            results.append(result)\n",
    "\n",
    "            # Rate limiting\n",
    "            if i < len(urls) - 1:\n",
    "                time.sleep(self.config[\"request_delay\"])\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# Test the extractor class\n",
    "extractor = ContentExtractor(\n",
    "    {\n",
    "        \"chinese_normalization\": True,\n",
    "        \"convert_chinese\": \"s2t\",  # Convert to traditional Chinese\n",
    "        \"safety_filtering\": True,\n",
    "        \"max_content_length\": 5000,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Test with HTML content\n",
    "test_html_final = \"\"\"\n",
    "<html>\n",
    "<head><title>RAG ç³»ç»Ÿå®ç°æŒ‡å—</title></head>\n",
    "<body>\n",
    "    <h1>æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ç³»ç»Ÿå®ç°</h1>\n",
    "    <p>æœ¬æ–‡ä»‹ç»å¦‚ä½•æ„å»ºä¸€ä¸ªå®Œæ•´çš„RAGç³»ç»Ÿã€‚</p>\n",
    "    <h2>æ ¸å¿ƒç»„ä»¶</h2>\n",
    "    <ul>\n",
    "        <li>æ–‡æ¡£å¤„ç†å™¨</li>\n",
    "        <li>å‘é‡æ•°æ®åº“</li>\n",
    "        <li>æ£€ç´¢æ¨¡å—</li>\n",
    "        <li>ç”Ÿæˆæ¨¡å—</li>\n",
    "    </ul>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "final_result = extractor.extract_from_html(test_html_final)\n",
    "print(\"\\\\nFinal extractor test:\")\n",
    "print(f\"Status: {final_result['status']}\")\n",
    "print(f\"Title: {final_result['title']}\")\n",
    "print(f\"Text preview: {final_result['text'][:150]}...\")\n",
    "print(f\"Word count: {final_result['word_count']}\")\n",
    "\n",
    "print(\"\\\\nâœ… Content extraction tool ready for use!\")\n",
    "\n",
    "# ============================================================================\n",
    "# Summary\n",
    "# ============================================================================\n",
    "\n",
    "print(\n",
    "    \"\"\"\n",
    "=== nb23 Content Extraction Summary ===\n",
    "\n",
    "âœ… Completed:\n",
    "- Basic HTML content extraction with trafilatura\n",
    "- Advanced extraction with metadata and formatting\n",
    "- Chinese text normalization and encoding handling\n",
    "- Batch URL processing with rate limiting\n",
    "- Safety filtering and content validation\n",
    "- Reusable ContentExtractor class\n",
    "\n",
    "ğŸ”§ Key Features:\n",
    "- Multi-format support (HTML, web pages)\n",
    "- Chinese-friendly text processing\n",
    "- Safety checks and content filtering\n",
    "- Configurable extraction options\n",
    "- Error handling and status reporting\n",
    "\n",
    "ğŸ“Š Performance:\n",
    "- Handles encoding detection automatically\n",
    "- Rate limiting for responsible web scraping\n",
    "- Memory-efficient processing\n",
    "- Robust error handling\n",
    "\n",
    "ğŸš€ Next Steps:\n",
    "- Integrate with RAG document pipeline\n",
    "- Add support for PDF extraction\n",
    "- Implement content caching\n",
    "- Add more sophisticated safety rules\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd666c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification\n",
    "def verify_extraction():\n",
    "    sample = \"<html><head><title>æ¸¬è©¦</title></head><body><p>é€™æ˜¯æ¸¬è©¦å…§å®¹ã€‚</p></body></html>\"\n",
    "    result = extract_basic_content(sample)\n",
    "    assert \"æ¸¬è©¦å…§å®¹\" in result[\"text\"]\n",
    "    assert result[\"title\"] == \"æ¸¬è©¦\"\n",
    "    print(\"âœ“ Content extraction verified\")\n",
    "\n",
    "\n",
    "verify_extraction()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
