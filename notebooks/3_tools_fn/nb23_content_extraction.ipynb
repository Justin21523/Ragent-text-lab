{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d75ce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3 - Tools & Function Calling\n",
    "# nb23_content_extraction.ipynb\n",
    "# 目標：trafilatura 內容抽取、去 HTML、中文友好處理\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (複製到每本 notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd84419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 2: Dependencies and Imports\n",
    "# ============================================================================\n",
    "\n",
    "# Install required packages (run once)\n",
    "# !pip install trafilatura requests beautifulsoup4 chardet opencc-python-reimplemented\n",
    "\n",
    "import trafilatura\n",
    "import requests\n",
    "import chardet\n",
    "from bs4 import BeautifulSoup\n",
    "from opencc import OpenCC\n",
    "import re\n",
    "import json\n",
    "import urllib.parse\n",
    "from typing import Dict, List, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "\n",
    "print(\"Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a81fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 3: Basic HTML Content Extraction\n",
    "# ============================================================================\n",
    "\n",
    "def extract_basic_content(html_content: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Basic content extraction using trafilatura\n",
    "\n",
    "    Args:\n",
    "        html_content: Raw HTML string\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with extracted content\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract main content\n",
    "    text = trafilatura.extract(html_content, include_links=False)\n",
    "\n",
    "    # Extract with metadata\n",
    "    metadata = trafilatura.extract_metadata(html_content)\n",
    "\n",
    "    # Extract with format preservation\n",
    "    formatted_text = trafilatura.extract(\n",
    "        html_content,\n",
    "        include_formatting=True,\n",
    "        include_links=False\n",
    "    )\n",
    "\n",
    "    result = {\n",
    "        \"text\": text or \"\",\n",
    "        \"formatted_text\": formatted_text or \"\",\n",
    "        \"title\": metadata.title if metadata else \"\",\n",
    "        \"author\": metadata.author if metadata else \"\",\n",
    "        \"date\": metadata.date if metadata else \"\",\n",
    "        \"description\": metadata.description if metadata else \"\",\n",
    "        \"language\": metadata.language if metadata else \"\"\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "# Test with sample HTML\n",
    "sample_html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>測試頁面 - AI 技術分享</title>\n",
    "    <meta name=\"author\" content=\"張三\">\n",
    "    <meta name=\"description\" content=\"關於大型語言模型的技術文章\">\n",
    "</head>\n",
    "<body>\n",
    "    <header>\n",
    "        <nav>導航菜單</nav>\n",
    "    </header>\n",
    "    <main>\n",
    "        <h1>大型語言模型技術解析</h1>\n",
    "        <p>本文將深入探討 LLM 的核心原理。</p>\n",
    "        <h2>Transformer 架構</h2>\n",
    "        <p>Transformer 是現代 LLM 的基礎架構，包含注意力機制等關鍵組件。</p>\n",
    "        <ul>\n",
    "            <li>自注意力機制</li>\n",
    "            <li>位置編碼</li>\n",
    "            <li>前饋網絡</li>\n",
    "        </ul>\n",
    "    </main>\n",
    "    <footer>版權信息</footer>\n",
    "    <script>console.log(\"ads\");</script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "extracted = extract_basic_content(sample_html)\n",
    "print(\"Extracted content:\")\n",
    "for key, value in extracted.items():\n",
    "    print(f\"{key}: {value[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134f5492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 4: Advanced Extraction Options\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def extract_advanced_content(html_content: str, config: Dict = None) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Advanced content extraction with configurable options\n",
    "\n",
    "    Args:\n",
    "        html_content: Raw HTML string\n",
    "        config: Configuration options\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with extracted content and metadata\n",
    "    \"\"\"\n",
    "\n",
    "    default_config = {\n",
    "        \"include_links\": True,\n",
    "        \"include_images\": False,\n",
    "        \"include_tables\": True,\n",
    "        \"include_formatting\": True,\n",
    "        \"deduplicate\": True,\n",
    "        \"favor_precision\": True,\n",
    "        \"favor_recall\": False,\n",
    "    }\n",
    "\n",
    "    if config:\n",
    "        default_config.update(config)\n",
    "\n",
    "    # Extract main content with options\n",
    "    text = trafilatura.extract(\n",
    "        html_content,\n",
    "        include_links=default_config[\"include_links\"],\n",
    "        include_images=default_config[\"include_images\"],\n",
    "        include_tables=default_config[\"include_tables\"],\n",
    "        include_formatting=default_config[\"include_formatting\"],\n",
    "        deduplicate=default_config[\"deduplicate\"],\n",
    "        favor_precision=default_config[\"favor_precision\"],\n",
    "        favor_recall=default_config[\"favor_recall\"],\n",
    "    )\n",
    "\n",
    "    # Extract metadata\n",
    "    metadata = trafilatura.extract_metadata(html_content)\n",
    "\n",
    "    # Extract links separately if needed\n",
    "    links = []\n",
    "    if default_config[\"include_links\"]:\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            links.append({\"text\": link.get_text().strip(), \"href\": link[\"href\"]})\n",
    "\n",
    "    result = {\n",
    "        \"text\": text or \"\",\n",
    "        \"title\": metadata.title if metadata else \"\",\n",
    "        \"author\": metadata.author if metadata else \"\",\n",
    "        \"date\": str(metadata.date) if metadata and metadata.date else \"\",\n",
    "        \"description\": metadata.description if metadata else \"\",\n",
    "        \"language\": metadata.language if metadata else \"\",\n",
    "        \"sitename\": metadata.sitename if metadata else \"\",\n",
    "        \"links\": links,\n",
    "        \"word_count\": len((text or \"\").split()),\n",
    "        \"char_count\": len(text or \"\"),\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test advanced extraction\n",
    "advanced_config = {\n",
    "    \"include_links\": True,\n",
    "    \"include_formatting\": True,\n",
    "    \"favor_precision\": True,\n",
    "}\n",
    "\n",
    "advanced_result = extract_advanced_content(sample_html, advanced_config)\n",
    "print(\"Advanced extraction results:\")\n",
    "print(f\"Word count: {advanced_result['word_count']}\")\n",
    "print(f\"Character count: {advanced_result['char_count']}\")\n",
    "print(f\"Links found: {len(advanced_result['links'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1863b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 5: Chinese Web Page Processing\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def detect_encoding(content: bytes) -> str:\n",
    "    \"\"\"Detect encoding of web content\"\"\"\n",
    "    detected = chardet.detect(content)\n",
    "    return detected[\"encoding\"] if detected[\"encoding\"] else \"utf-8\"\n",
    "\n",
    "\n",
    "def normalize_chinese_text(text: str, convert_variant: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Normalize Chinese text\n",
    "\n",
    "    Args:\n",
    "        text: Input text\n",
    "        convert_variant: 't2s' for traditional to simplified, 's2t' for reverse\n",
    "\n",
    "    Returns:\n",
    "        Normalized text\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = text.strip()\n",
    "\n",
    "    # Convert between traditional/simplified if requested\n",
    "    if convert_variant:\n",
    "        try:\n",
    "            cc = OpenCC(convert_variant)\n",
    "            text = cc.convert(text)\n",
    "        except:\n",
    "            print(f\"Warning: OpenCC conversion {convert_variant} failed\")\n",
    "\n",
    "    # Normalize punctuation\n",
    "    punctuation_map = {\n",
    "        \"（\": \"(\",\n",
    "        \"）\": \")\",\n",
    "        \"？\": \"?\",\n",
    "        \"！\": \"!\",\n",
    "        \"，\": \",\",\n",
    "        \"。\": \".\",\n",
    "        \"；\": \";\",\n",
    "        \"：\": \":\",\n",
    "    }\n",
    "\n",
    "    for old, new in punctuation_map.items():\n",
    "        text = text.replace(old, new)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_chinese_content(\n",
    "    html_content: Union[str, bytes], url: str = \"\"\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Extract content from Chinese web pages with proper encoding handling\n",
    "\n",
    "    Args:\n",
    "        html_content: HTML content (string or bytes)\n",
    "        url: Source URL for context\n",
    "\n",
    "    Returns:\n",
    "        Extracted content dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle encoding if bytes\n",
    "    if isinstance(html_content, bytes):\n",
    "        encoding = detect_encoding(html_content)\n",
    "        try:\n",
    "            html_content = html_content.decode(encoding)\n",
    "        except:\n",
    "            html_content = html_content.decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    # Basic extraction\n",
    "    result = extract_advanced_content(html_content)\n",
    "\n",
    "    # Normalize Chinese text\n",
    "    if result[\"text\"]:\n",
    "        result[\"text_normalized\"] = normalize_chinese_text(result[\"text\"])\n",
    "        result[\"text_simplified\"] = normalize_chinese_text(result[\"text\"], \"t2s\")\n",
    "        result[\"text_traditional\"] = normalize_chinese_text(result[\"text\"], \"s2t\")\n",
    "\n",
    "    # Add source info\n",
    "    result[\"source_url\"] = url\n",
    "    result[\"extraction_time\"] = time.time()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test Chinese content processing\n",
    "chinese_html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"zh-TW\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>人工智慧技術發展趨勢</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>２０２４年人工智慧技術發展趨勢</h1>\n",
    "    <p>近年來，大型語言模型（ＬＬＭ）技術突飛猛進。主要發展包括：</p>\n",
    "    <ul>\n",
    "        <li>模型規模持續擴大</li>\n",
    "        <li>推理能力顯著提升</li>\n",
    "        <li>多模態整合加速</li>\n",
    "    </ul>\n",
    "    <p>這些技術將對各行各業產生深遠影響。</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "chinese_result = extract_chinese_content(chinese_html, \"https://example.com/ai-trends\")\n",
    "print(\"Chinese content extraction:\")\n",
    "print(f\"Original: {chinese_result['text'][:100]}...\")\n",
    "print(f\"Normalized: {chinese_result['text_normalized'][:100]}...\")\n",
    "print(f\"Simplified: {chinese_result['text_simplified'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc106263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 6: Batch URL Processing Tool\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExtractionResult:\n",
    "    url: str\n",
    "    title: str\n",
    "    text: str\n",
    "    status: str\n",
    "    error: str = \"\"\n",
    "    metadata: Dict = None\n",
    "\n",
    "\n",
    "def fetch_and_extract(url: str, timeout: int = 30) -> ExtractionResult:\n",
    "    \"\"\"\n",
    "    Fetch and extract content from a single URL\n",
    "\n",
    "    Args:\n",
    "        url: Target URL\n",
    "        timeout: Request timeout in seconds\n",
    "\n",
    "    Returns:\n",
    "        ExtractionResult object\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Set headers to mimic browser\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
    "        }\n",
    "\n",
    "        # Fetch content\n",
    "        response = requests.get(url, headers=headers, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Extract content\n",
    "        extracted = extract_chinese_content(response.content, url)\n",
    "\n",
    "        return ExtractionResult(\n",
    "            url=url,\n",
    "            title=extracted.get(\"title\", \"\"),\n",
    "            text=extracted.get(\"text\", \"\"),\n",
    "            status=\"success\",\n",
    "            metadata=extracted,\n",
    "        )\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        return ExtractionResult(\n",
    "            url=url, title=\"\", text=\"\", status=\"network_error\", error=str(e)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return ExtractionResult(\n",
    "            url=url, title=\"\", text=\"\", status=\"extraction_error\", error=str(e)\n",
    "        )\n",
    "\n",
    "\n",
    "def batch_extract_urls(urls: List[str], delay: float = 1.0) -> List[ExtractionResult]:\n",
    "    \"\"\"\n",
    "    Extract content from multiple URLs with rate limiting\n",
    "\n",
    "    Args:\n",
    "        urls: List of URLs to process\n",
    "        delay: Delay between requests in seconds\n",
    "\n",
    "    Returns:\n",
    "        List of ExtractionResult objects\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, url in enumerate(urls):\n",
    "        print(f\"Processing {i+1}/{len(urls)}: {url}\")\n",
    "\n",
    "        result = fetch_and_extract(url)\n",
    "        results.append(result)\n",
    "\n",
    "        # Rate limiting\n",
    "        if i < len(urls) - 1:\n",
    "            time.sleep(delay)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Test batch processing with sample URLs\n",
    "sample_urls = [\n",
    "    \"https://httpbin.org/html\",  # Simple test page\n",
    "    \"https://example.com\",  # Basic example page\n",
    "]\n",
    "\n",
    "print(\"Batch URL processing test:\")\n",
    "batch_results = batch_extract_urls(\n",
    "    sample_urls[:1], delay=0.5\n",
    ")  # Test with first URL only\n",
    "\n",
    "for result in batch_results:\n",
    "    print(f\"URL: {result.url}\")\n",
    "    print(f\"Status: {result.status}\")\n",
    "    print(f\"Title: {result.title}\")\n",
    "    print(f\"Text length: {len(result.text)}\")\n",
    "    if result.error:\n",
    "        print(f\"Error: {result.error}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88f8880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 7: Safety Checks and Content Filtering\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def is_safe_url(url: str) -> bool:\n",
    "    \"\"\"\n",
    "    Basic URL safety check\n",
    "\n",
    "    Args:\n",
    "        url: URL to check\n",
    "\n",
    "    Returns:\n",
    "        True if URL appears safe\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse URL\n",
    "    try:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "    # Check scheme\n",
    "    if parsed.scheme not in [\"http\", \"https\"]:\n",
    "        return False\n",
    "\n",
    "    # Check for suspicious patterns\n",
    "    suspicious_patterns = [\n",
    "        \"javascript:\",\n",
    "        \"data:\",\n",
    "        \"file:\",\n",
    "        \"ftp:\",\n",
    "    ]\n",
    "\n",
    "    for pattern in suspicious_patterns:\n",
    "        if pattern in url.lower():\n",
    "            return False\n",
    "\n",
    "    # Check domain blacklist (basic example)\n",
    "    domain_blacklist = [\"malware.com\", \"phishing.site\"]\n",
    "\n",
    "    if parsed.netloc.lower() in domain_blacklist:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def filter_extracted_content(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Filter potentially harmful content from extracted text\n",
    "\n",
    "    Args:\n",
    "        content: Extracted text content\n",
    "\n",
    "    Returns:\n",
    "        Filtered content\n",
    "    \"\"\"\n",
    "\n",
    "    if not content:\n",
    "        return \"\"\n",
    "\n",
    "    # Remove script-like patterns\n",
    "    content = re.sub(\n",
    "        r\"<script.*?</script>\", \"\", content, flags=re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "    content = re.sub(r\"<style.*?</style>\", \"\", content, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # Remove suspicious URLs\n",
    "    content = re.sub(r\"javascript:[^\\\\s]*\", \"[FILTERED]\", content, flags=re.IGNORECASE)\n",
    "    content = re.sub(r\"data:[^\\\\s]*\", \"[FILTERED]\", content, flags=re.IGNORECASE)\n",
    "\n",
    "    # Limit excessive repetition\n",
    "    content = re.sub(r\"(.{10,}?)\\\\1{3,}\", r\"\\\\1[REPEATED]\", content)\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "def safe_extract_content(url_or_html: str, is_url: bool = True) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Safely extract content with security checks\n",
    "\n",
    "    Args:\n",
    "        url_or_html: URL to fetch or HTML content\n",
    "        is_url: True if input is URL, False if HTML content\n",
    "\n",
    "    Returns:\n",
    "        Extracted content with safety metadata\n",
    "    \"\"\"\n",
    "\n",
    "    result = {\"text\": \"\", \"title\": \"\", \"status\": \"error\", \"safety_warnings\": []}\n",
    "\n",
    "    try:\n",
    "        if is_url:\n",
    "            # Check URL safety\n",
    "            if not is_safe_url(url_or_html):\n",
    "                result[\"safety_warnings\"].append(\"Unsafe URL detected\")\n",
    "                result[\"status\"] = \"unsafe_url\"\n",
    "                return result\n",
    "\n",
    "            # Fetch content\n",
    "            extraction_result = fetch_and_extract(url_or_html)\n",
    "            if extraction_result.status != \"success\":\n",
    "                result[\"status\"] = extraction_result.status\n",
    "                result[\"error\"] = extraction_result.error\n",
    "                return result\n",
    "\n",
    "            content = extraction_result.text\n",
    "            title = extraction_result.title\n",
    "        else:\n",
    "            # Extract from HTML directly\n",
    "            extracted = extract_chinese_content(url_or_html)\n",
    "            content = extracted.get(\"text\", \"\")\n",
    "            title = extracted.get(\"title\", \"\")\n",
    "\n",
    "        # Filter content\n",
    "        filtered_content = filter_extracted_content(content)\n",
    "\n",
    "        # Check if significant content was filtered\n",
    "        if len(filtered_content) < len(content) * 0.8:\n",
    "            result[\"safety_warnings\"].append(\"Significant content filtered\")\n",
    "\n",
    "        result.update(\n",
    "            {\n",
    "                \"text\": filtered_content,\n",
    "                \"title\": title,\n",
    "                \"status\": \"success\",\n",
    "                \"original_length\": len(content),\n",
    "                \"filtered_length\": len(filtered_content),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        result[\"status\"] = \"extraction_error\"\n",
    "        result[\"error\"] = str(e)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test safety filtering\n",
    "malicious_html = \"\"\"\n",
    "<html>\n",
    "<head><title>Test Page</title></head>\n",
    "<body>\n",
    "    <h1>Normal Content</h1>\n",
    "    <p>This is normal text content.</p>\n",
    "    <script>alert('malicious code');</script>\n",
    "    <p>More normal content here.</p>\n",
    "    <a href=\"javascript:alert('xss')\">Suspicious Link</a>\n",
    "    <p>This sentence repeats. This sentence repeats. This sentence repeats. This sentence repeats.</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "safe_result = safe_extract_content(malicious_html, is_url=False)\n",
    "print(\"Safety filtering test:\")\n",
    "print(f\"Status: {safe_result['status']}\")\n",
    "print(f\"Warnings: {safe_result['safety_warnings']}\")\n",
    "print(f\"Original length: {safe_result.get('original_length', 0)}\")\n",
    "print(f\"Filtered length: {safe_result.get('filtered_length', 0)}\")\n",
    "print(f\"Filtered content: {safe_result['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1f60b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 8: Smoke Test - Real World Example\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def smoke_test_extraction():\n",
    "    \"\"\"\n",
    "    Comprehensive smoke test for content extraction\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=== Content Extraction Smoke Test ===\")\n",
    "\n",
    "    # Test 1: Basic HTML extraction\n",
    "    print(\"\\\\nTest 1: Basic HTML extraction\")\n",
    "    test_html = \"\"\"\n",
    "    <html>\n",
    "    <head><title>測試文章</title></head>\n",
    "    <body>\n",
    "        <h1>AI 技術發展</h1>\n",
    "        <p>人工智慧技術正在快速發展。</p>\n",
    "        <p>主要包括機器學習、深度學習等領域。</p>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    result1 = extract_basic_content(test_html)\n",
    "    assert len(result1[\"text\"]) > 0, \"Basic extraction failed\"\n",
    "    assert \"AI 技術發展\" in result1[\"text\"], \"Title not extracted\"\n",
    "    print(\"✓ Basic extraction working\")\n",
    "\n",
    "    # Test 2: Chinese text normalization\n",
    "    print(\"\\\\nTest 2: Chinese text normalization\")\n",
    "    chinese_text = \"這是測試文字（包含全形括號）。\"\n",
    "    normalized = normalize_chinese_text(chinese_text)\n",
    "    assert \"(\" in normalized and \")\" in normalized, \"Punctuation normalization failed\"\n",
    "    print(\"✓ Chinese normalization working\")\n",
    "\n",
    "    # Test 3: Safety filtering\n",
    "    print(\"\\\\nTest 3: Safety filtering\")\n",
    "    unsafe_html = \"<script>alert('test')</script><p>Safe content</p>\"\n",
    "    safe_result = safe_extract_content(unsafe_html, is_url=False)\n",
    "    assert safe_result[\"status\"] == \"success\", \"Safety extraction failed\"\n",
    "    assert \"script\" not in safe_result[\"text\"].lower(), \"Script not filtered\"\n",
    "    print(\"✓ Safety filtering working\")\n",
    "\n",
    "    # Test 4: Metadata extraction\n",
    "    print(\"\\\\nTest 4: Metadata extraction\")\n",
    "    meta_html = \"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>測試標題</title>\n",
    "        <meta name=\"author\" content=\"測試作者\">\n",
    "        <meta name=\"description\" content=\"測試描述\">\n",
    "    </head>\n",
    "    <body><p>內容</p></body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    result4 = extract_advanced_content(meta_html)\n",
    "    assert result4[\"title\"] == \"測試標題\", \"Title extraction failed\"\n",
    "    assert result4[\"author\"] == \"測試作者\", \"Author extraction failed\"\n",
    "    print(\"✓ Metadata extraction working\")\n",
    "\n",
    "    print(\"\\\\n🎉 All smoke tests passed!\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de760828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 9: Reusable Content Extractor Class\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class ContentExtractor:\n",
    "    \"\"\"\n",
    "    Reusable content extraction tool with configurable options\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Dict = None):\n",
    "        \"\"\"\n",
    "        Initialize extractor with configuration\n",
    "\n",
    "        Args:\n",
    "            config: Configuration dictionary\n",
    "        \"\"\"\n",
    "\n",
    "        self.config = {\n",
    "            \"include_links\": True,\n",
    "            \"include_formatting\": True,\n",
    "            \"chinese_normalization\": True,\n",
    "            \"safety_filtering\": True,\n",
    "            \"request_timeout\": 30,\n",
    "            \"request_delay\": 1.0,\n",
    "            \"convert_chinese\": None,  # 't2s', 's2t', or None\n",
    "            \"max_content_length\": 100000,\n",
    "            \"user_agent\": \"Mozilla/5.0 (compatible; ContentExtractor/1.0)\",\n",
    "        }\n",
    "\n",
    "        if config:\n",
    "            self.config.update(config)\n",
    "\n",
    "    def extract_from_url(self, url: str) -> Dict[str, any]:\n",
    "        \"\"\"Extract content from URL\"\"\"\n",
    "\n",
    "        if not is_safe_url(url):\n",
    "            return {\n",
    "                \"status\": \"unsafe_url\",\n",
    "                \"error\": \"URL failed safety check\",\n",
    "                \"text\": \"\",\n",
    "                \"metadata\": {},\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            headers = {\"User-Agent\": self.config[\"user_agent\"]}\n",
    "            response = requests.get(\n",
    "                url, headers=headers, timeout=self.config[\"request_timeout\"]\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "\n",
    "            return self.extract_from_html(response.content, source_url=url)\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"status\": \"error\", \"error\": str(e), \"text\": \"\", \"metadata\": {}}\n",
    "\n",
    "    def extract_from_html(\n",
    "        self, html_content: Union[str, bytes], source_url: str = \"\"\n",
    "    ) -> Dict[str, any]:\n",
    "        \"\"\"Extract content from HTML\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Handle encoding\n",
    "            if isinstance(html_content, bytes):\n",
    "                encoding = detect_encoding(html_content)\n",
    "                html_content = html_content.decode(encoding, errors=\"ignore\")\n",
    "\n",
    "            # Basic extraction\n",
    "            result = extract_advanced_content(html_content, self.config)\n",
    "\n",
    "            # Chinese processing\n",
    "            if self.config[\"chinese_normalization\"] and result[\"text\"]:\n",
    "                result[\"text\"] = normalize_chinese_text(\n",
    "                    result[\"text\"], self.config[\"convert_chinese\"]\n",
    "                )\n",
    "\n",
    "            # Safety filtering\n",
    "            if self.config[\"safety_filtering\"]:\n",
    "                result[\"text\"] = filter_extracted_content(result[\"text\"])\n",
    "\n",
    "            # Length limiting\n",
    "            max_len = self.config[\"max_content_length\"]\n",
    "            if len(result[\"text\"]) > max_len:\n",
    "                result[\"text\"] = result[\"text\"][:max_len] + \"...\"\n",
    "                result[\"truncated\"] = True\n",
    "\n",
    "            # Add extraction metadata\n",
    "            result.update(\n",
    "                {\n",
    "                    \"status\": \"success\",\n",
    "                    \"source_url\": source_url,\n",
    "                    \"extraction_time\": time.time(),\n",
    "                    \"extractor_version\": \"1.0\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"status\": \"error\", \"error\": str(e), \"text\": \"\", \"metadata\": {}}\n",
    "\n",
    "    def batch_extract(self, urls: List[str]) -> List[Dict[str, any]]:\n",
    "        \"\"\"Extract content from multiple URLs\"\"\"\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for i, url in enumerate(urls):\n",
    "            print(f\"Processing {i+1}/{len(urls)}: {url}\")\n",
    "\n",
    "            result = self.extract_from_url(url)\n",
    "            results.append(result)\n",
    "\n",
    "            # Rate limiting\n",
    "            if i < len(urls) - 1:\n",
    "                time.sleep(self.config[\"request_delay\"])\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# Test the extractor class\n",
    "extractor = ContentExtractor(\n",
    "    {\n",
    "        \"chinese_normalization\": True,\n",
    "        \"convert_chinese\": \"s2t\",  # Convert to traditional Chinese\n",
    "        \"safety_filtering\": True,\n",
    "        \"max_content_length\": 5000,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Test with HTML content\n",
    "test_html_final = \"\"\"\n",
    "<html>\n",
    "<head><title>RAG 系统实现指南</title></head>\n",
    "<body>\n",
    "    <h1>检索增强生成(RAG)系统实现</h1>\n",
    "    <p>本文介绍如何构建一个完整的RAG系统。</p>\n",
    "    <h2>核心组件</h2>\n",
    "    <ul>\n",
    "        <li>文档处理器</li>\n",
    "        <li>向量数据库</li>\n",
    "        <li>检索模块</li>\n",
    "        <li>生成模块</li>\n",
    "    </ul>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "final_result = extractor.extract_from_html(test_html_final)\n",
    "print(\"\\\\nFinal extractor test:\")\n",
    "print(f\"Status: {final_result['status']}\")\n",
    "print(f\"Title: {final_result['title']}\")\n",
    "print(f\"Text preview: {final_result['text'][:150]}...\")\n",
    "print(f\"Word count: {final_result['word_count']}\")\n",
    "\n",
    "print(\"\\\\n✅ Content extraction tool ready for use!\")\n",
    "\n",
    "# ============================================================================\n",
    "# Summary\n",
    "# ============================================================================\n",
    "\n",
    "print(\n",
    "    \"\"\"\n",
    "=== nb23 Content Extraction Summary ===\n",
    "\n",
    "✅ Completed:\n",
    "- Basic HTML content extraction with trafilatura\n",
    "- Advanced extraction with metadata and formatting\n",
    "- Chinese text normalization and encoding handling\n",
    "- Batch URL processing with rate limiting\n",
    "- Safety filtering and content validation\n",
    "- Reusable ContentExtractor class\n",
    "\n",
    "🔧 Key Features:\n",
    "- Multi-format support (HTML, web pages)\n",
    "- Chinese-friendly text processing\n",
    "- Safety checks and content filtering\n",
    "- Configurable extraction options\n",
    "- Error handling and status reporting\n",
    "\n",
    "📊 Performance:\n",
    "- Handles encoding detection automatically\n",
    "- Rate limiting for responsible web scraping\n",
    "- Memory-efficient processing\n",
    "- Robust error handling\n",
    "\n",
    "🚀 Next Steps:\n",
    "- Integrate with RAG document pipeline\n",
    "- Add support for PDF extraction\n",
    "- Implement content caching\n",
    "- Add more sophisticated safety rules\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd666c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification\n",
    "def verify_extraction():\n",
    "    sample = \"<html><head><title>測試</title></head><body><p>這是測試內容。</p></body></html>\"\n",
    "    result = extract_basic_content(sample)\n",
    "    assert \"測試內容\" in result[\"text\"]\n",
    "    assert result[\"title\"] == \"測試\"\n",
    "    print(\"✓ Content extraction verified\")\n",
    "\n",
    "\n",
    "verify_extraction()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
