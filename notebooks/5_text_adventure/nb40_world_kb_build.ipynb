{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327da18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (è¤‡è£½åˆ°æ¯æœ¬ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e380e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: World KB Schema and Sample Data\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from opencc import OpenCC\n",
    "\n",
    "\n",
    "# Game world schema\n",
    "class Character(BaseModel):\n",
    "    name: str\n",
    "    title: Optional[str] = None\n",
    "    description: str\n",
    "    attributes: Dict[str, int] = Field(default_factory=dict)\n",
    "    background: str\n",
    "    relationships: List[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class Location(BaseModel):\n",
    "    name: str\n",
    "    type: str  # city, dungeon, wilderness, etc.\n",
    "    description: str\n",
    "    connections: List[str] = Field(default_factory=list)\n",
    "    features: List[str] = Field(default_factory=list)\n",
    "    dangers: List[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class Item(BaseModel):\n",
    "    name: str\n",
    "    type: str  # weapon, armor, consumable, quest, etc.\n",
    "    description: str\n",
    "    properties: Dict[str, str] = Field(default_factory=dict)\n",
    "    rarity: str = \"common\"\n",
    "\n",
    "\n",
    "class Event(BaseModel):\n",
    "    name: str\n",
    "    type: str  # story, random, quest, etc.\n",
    "    description: str\n",
    "    triggers: List[str] = Field(default_factory=list)\n",
    "    outcomes: List[str] = Field(default_factory=list)\n",
    "    requirements: Dict[str, str] = Field(default_factory=dict)\n",
    "\n",
    "\n",
    "# Create sample world data\n",
    "def create_sample_world_data():\n",
    "    \"\"\"Create sample world KB data for testing\"\"\"\n",
    "\n",
    "    # Characters\n",
    "    characters = [\n",
    "        Character(\n",
    "            name=\"è‰¾è‰äº\",\n",
    "            title=\"æ˜Ÿç©ºæ³•å¸«\",\n",
    "            description=\"æŒæ¡å¤è€æ˜Ÿè±¡é­”æ³•çš„å¹´è¼•æ³•å¸«ï¼Œæ“æœ‰é è¦‹æœªä¾†çš„èƒ½åŠ›ã€‚\",\n",
    "            attributes={\"æ™ºåŠ›\": 18, \"é­”åŠ›\": 16, \"é«”åŠ›\": 12},\n",
    "            background=\"å‡ºèº«æ–¼å¤è€çš„æ˜Ÿè±¡æ³•å¸«å®¶æ—ï¼Œè‡ªå¹¼ç¿’å¾—è§€æ˜Ÿå åœä¹‹è¡“ã€‚åœ¨ä¸€æ¬¡æ„å¤–ä¸­ç²å¾—äº†é è¦‹æœªä¾†çš„èƒ½åŠ›ï¼Œä½†ä»£åƒ¹æ˜¯æ¯æ¬¡ä½¿ç”¨éƒ½æœƒç¸®çŸ­å£½å‘½ã€‚\",\n",
    "            relationships=[\"å°å¸«ï¼šè³¢è€…è¬å¾·\", \"å®¿æ•µï¼šæš—å½±é ˜ä¸»\"],\n",
    "        ),\n",
    "        Character(\n",
    "            name=\"é›·å…‹æ–¯\",\n",
    "            title=\"é‹¼éµé¨å£«\",\n",
    "            description=\"èº«ç©¿é‡ç”²çš„æ­£ç¾©é¨å£«ï¼Œä»¥ä¿è­·ç„¡è¾œç‚ºå·±ä»»ã€‚\",\n",
    "            attributes={\"åŠ›é‡\": 17, \"é«”åŠ›\": 19, \"æ„å¿—\": 15},\n",
    "            background=\"æ›¾ç¶“æ˜¯ç‹åœ‹ç²¾éŠ³é¨å£«åœ˜çš„éšŠé•·ï¼Œåœ¨ä¸€å ´æ”¿è®Šä¸­å¤±å»äº†åœ‹ç‹çš„ä¿¡ä»»ã€‚ç¾åœ¨ä½œç‚ºç¨è¡Œä¿ æ¸¸èµ°å„åœ°ï¼Œå°‹æ‰¾é‡æ–°è­‰æ˜è‡ªå·±çš„æ©Ÿæœƒã€‚\",\n",
    "            relationships=[\"èˆŠå‹ï¼šå®®å»·ä¾è¡›é•·\", \"ä»‡æ•µï¼šå›è®Šå…¬çˆµ\"],\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Locations\n",
    "    locations = [\n",
    "        Location(\n",
    "            name=\"ç¿¡ç¿ æ£®æ—\",\n",
    "            type=\"wilderness\",\n",
    "            description=\"å¤è€è€Œç¥ç§˜çš„æ£®æ—ï¼Œæ“šèªªæ˜¯ç²¾éˆæ—çš„æ•…é„‰ã€‚æ¨¹æœ¨é«˜è³å…¥äº‘ï¼Œé™½å…‰é€éæ¨¹è‘‰ç‘ä¸‹æ–‘é§çš„å…‰å½±ã€‚\",\n",
    "            connections=[\"éŠ€æœˆåŸ\", \"å¤å¢“è¿·å®®\", \"æ°´æ™¶æ¹–\"],\n",
    "            features=[\"ç²¾éˆéºè·¡\", \"å¤è€æ™ºæ…§æ¨¹\", \"é­”æ³•æ³‰æ°´\"],\n",
    "            dangers=[\"è¿·è·¯é¢¨éšª\", \"é‡ç”Ÿé­”ç¸\", \"ç²¾éˆé™·é˜±\"],\n",
    "        ),\n",
    "        Location(\n",
    "            name=\"éŠ€æœˆåŸ\",\n",
    "            type=\"city\",\n",
    "            description=\"å»ºç«‹åœ¨é«˜å±±ä¸Šçš„é›„å‰åŸå¸‚ï¼Œä»¥å…¶éŠ€ç™½è‰²çš„åŸç‰†å’Œå°–å¡”èåã€‚åŸä¸­å¿ƒæœ‰ä¸€åº§å¤è€çš„é­”æ³•å¡”ã€‚\",\n",
    "            connections=[\"ç¿¡ç¿ æ£®æ—\", \"è’é‡å¹³åŸ\", \"åœ°ä¸‹åŸ\"],\n",
    "            features=[\"é­”æ³•å­¸é™¢\", \"å†’éšªè€…å…¬æœƒ\", \"çš‡å®¶åœ–æ›¸é¤¨\"],\n",
    "            dangers=[\"æ”¿æ²»é™°è¬€\", \"ç›œè³Šå…¬æœƒ\", \"é­”æ³•å¯¦é©—æ„å¤–\"],\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Items\n",
    "    items = [\n",
    "        Item(\n",
    "            name=\"æ˜Ÿè¾°æ³•æ–\",\n",
    "            type=\"weapon\",\n",
    "            description=\"é‘²åµŒè‘—å¤è€æ˜ŸçŸ³çš„æ³•æ–ï¼Œèƒ½å¤ å¼•å°æ˜Ÿç©ºçš„åŠ›é‡ã€‚åœ¨å¤œæ™šä½¿ç”¨æ™‚å¨åŠ›å€å¢ã€‚\",\n",
    "            properties={\"é­”æ³•æ”»æ“Š\": \"+15\", \"æ˜Ÿè±¡é­”æ³•\": \"+3\", \"å¤œæ™šåŠ æˆ\": \"é›™å€å¨åŠ›\"},\n",
    "            rarity=\"legendary\",\n",
    "        ),\n",
    "        Item(\n",
    "            name=\"é‹¼éµæ„å¿—è­·ç¬¦\",\n",
    "            type=\"accessory\",\n",
    "            description=\"å¤è€é¨å£«ç•™ä¸‹çš„è­·ç¬¦ï¼Œèƒ½å¤ å¢å¼·ä½©æˆ´è€…çš„æ„å¿—åŠ›å’Œå‹‡æ°£ã€‚\",\n",
    "            properties={\"æ„å¿—\": \"+2\", \"ææ‡¼æŠ—æ€§\": \"å…ç–«\", \"æ­£ç¾©æ„Ÿ\": \"+1\"},\n",
    "            rarity=\"rare\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Events\n",
    "    events = [\n",
    "        Event(\n",
    "            name=\"å¤è€é è¨€çš„å¯¦ç¾\",\n",
    "            type=\"story\",\n",
    "            description=\"æ˜Ÿç©ºä¸­å‡ºç¾äº†å¤è€é è¨€è¨˜è¼‰çš„æ˜Ÿè±¡ï¼Œé ç¤ºè‘—é‡å¤§è®ŠåŒ–å³å°‡ä¾†è‡¨ã€‚\",\n",
    "            triggers=[\"é€²å…¥ç¿¡ç¿ æ£®æ—\", \"èˆ‡è‰¾è‰äºå°è©±\", \"å¤œæ™šæ™‚åˆ»\"],\n",
    "            outcomes=[\"ç²å¾—é è¨€ç·šç´¢\", \"è§£é–éš±è—ä»»å‹™\", \"æå‡é­”æ³•èƒ½åŠ›\"],\n",
    "            requirements={\"æ™ºåŠ›\": \"15+\", \"é­”æ³•è¦ªå’Œ\": \"ä¸­ç­‰ä»¥ä¸Š\"},\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"characters\": [c.dict() for c in characters],\n",
    "        \"locations\": [l.dict() for l in locations],\n",
    "        \"items\": [i.dict() for i in items],\n",
    "        \"events\": [e.dict() for e in events],\n",
    "    }\n",
    "\n",
    "\n",
    "# Create and save sample data\n",
    "sample_data = create_sample_world_data()\n",
    "Path(\"data/world_kb\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(\"data/world_kb/world_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sample_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ“ Created sample world KB data\")\n",
    "print(f\"Characters: {len(sample_data['characters'])}\")\n",
    "print(f\"Locations: {len(sample_data['locations'])}\")\n",
    "print(f\"Items: {len(sample_data['items'])}\")\n",
    "print(f\"Events: {len(sample_data['events'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a0e94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: World KB Document Processor\n",
    "class WorldKBProcessor:\n",
    "    \"\"\"Process world KB documents into chunks for RAG indexing\"\"\"\n",
    "\n",
    "    def __init__(self, language=\"zh-tw\"):\n",
    "        self.language = language\n",
    "        self.cc = OpenCC(\"s2t\" if language == \"zh-tw\" else \"t2s\")\n",
    "\n",
    "    def process_character(self, char_data: Dict) -> List[Dict]:\n",
    "        \"\"\"Convert character data to searchable chunks\"\"\"\n",
    "        chunks = []\n",
    "\n",
    "        # Basic info chunk\n",
    "        basic_info = f\"è§’è‰²ï¼š{char_data['name']}\\n\"\n",
    "        if char_data.get(\"title\"):\n",
    "            basic_info += f\"ç¨±è™Ÿï¼š{char_data['title']}\\n\"\n",
    "        basic_info += f\"æè¿°ï¼š{char_data['description']}\\n\"\n",
    "\n",
    "        # Attributes chunk\n",
    "        if char_data.get(\"attributes\"):\n",
    "            attrs = \"å±¬æ€§ï¼š\" + \"ã€\".join(\n",
    "                [f\"{k}{v}\" for k, v in char_data[\"attributes\"].items()]\n",
    "            )\n",
    "            basic_info += attrs + \"\\n\"\n",
    "\n",
    "        chunks.append(\n",
    "            {\n",
    "                \"text\": basic_info.strip(),\n",
    "                \"meta\": {\n",
    "                    \"type\": \"character\",\n",
    "                    \"name\": char_data[\"name\"],\n",
    "                    \"category\": \"basic_info\",\n",
    "                    \"source\": \"world_kb\",\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Background chunk\n",
    "        if char_data.get(\"background\"):\n",
    "            chunks.append(\n",
    "                {\n",
    "                    \"text\": f\"è§’è‰²èƒŒæ™¯ï¼š{char_data['name']}\\n{char_data['background']}\",\n",
    "                    \"meta\": {\n",
    "                        \"type\": \"character\",\n",
    "                        \"name\": char_data[\"name\"],\n",
    "                        \"category\": \"background\",\n",
    "                        \"source\": \"world_kb\",\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Relationships chunk\n",
    "        if char_data.get(\"relationships\"):\n",
    "            rel_text = f\"äººç‰©é—œä¿‚ï¼š{char_data['name']}\\n\" + \"\\n\".join(\n",
    "                char_data[\"relationships\"]\n",
    "            )\n",
    "            chunks.append(\n",
    "                {\n",
    "                    \"text\": rel_text,\n",
    "                    \"meta\": {\n",
    "                        \"type\": \"character\",\n",
    "                        \"name\": char_data[\"name\"],\n",
    "                        \"category\": \"relationships\",\n",
    "                        \"source\": \"world_kb\",\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def process_location(self, loc_data: Dict) -> List[Dict]:\n",
    "        \"\"\"Convert location data to searchable chunks\"\"\"\n",
    "        chunks = []\n",
    "\n",
    "        # Basic info chunk\n",
    "        basic_info = f\"åœ°é»ï¼š{loc_data['name']}\\n\"\n",
    "        basic_info += f\"é¡å‹ï¼š{loc_data['type']}\\n\"\n",
    "        basic_info += f\"æè¿°ï¼š{loc_data['description']}\\n\"\n",
    "\n",
    "        chunks.append(\n",
    "            {\n",
    "                \"text\": basic_info.strip(),\n",
    "                \"meta\": {\n",
    "                    \"type\": \"location\",\n",
    "                    \"name\": loc_data[\"name\"],\n",
    "                    \"category\": \"basic_info\",\n",
    "                    \"source\": \"world_kb\",\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Connections chunk\n",
    "        if loc_data.get(\"connections\"):\n",
    "            conn_text = f\"åœ°é»é€£æ¥ï¼š{loc_data['name']}\\nå¯å‰å¾€ï¼š\" + \"ã€\".join(\n",
    "                loc_data[\"connections\"]\n",
    "            )\n",
    "            chunks.append(\n",
    "                {\n",
    "                    \"text\": conn_text,\n",
    "                    \"meta\": {\n",
    "                        \"type\": \"location\",\n",
    "                        \"name\": loc_data[\"name\"],\n",
    "                        \"category\": \"connections\",\n",
    "                        \"source\": \"world_kb\",\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Features and dangers chunk\n",
    "        features_text = f\"åœ°é»ç‰¹è‰²ï¼š{loc_data['name']}\\n\"\n",
    "        if loc_data.get(\"features\"):\n",
    "            features_text += \"ç‰¹è‰²ï¼š\" + \"ã€\".join(loc_data[\"features\"]) + \"\\n\"\n",
    "        if loc_data.get(\"dangers\"):\n",
    "            features_text += \"å±éšªï¼š\" + \"ã€\".join(loc_data[\"dangers\"])\n",
    "\n",
    "        if len(features_text.strip()) > len(f\"åœ°é»ç‰¹è‰²ï¼š{loc_data['name']}\"):\n",
    "            chunks.append(\n",
    "                {\n",
    "                    \"text\": features_text.strip(),\n",
    "                    \"meta\": {\n",
    "                        \"type\": \"location\",\n",
    "                        \"name\": loc_data[\"name\"],\n",
    "                        \"category\": \"features\",\n",
    "                        \"source\": \"world_kb\",\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def process_item(self, item_data: Dict) -> List[Dict]:\n",
    "        \"\"\"Convert item data to searchable chunks\"\"\"\n",
    "        item_text = f\"ç‰©å“ï¼š{item_data['name']}\\n\"\n",
    "        item_text += f\"é¡å‹ï¼š{item_data['type']}\\n\"\n",
    "        item_text += f\"ç¨€æœ‰åº¦ï¼š{item_data['rarity']}\\n\"\n",
    "        item_text += f\"æè¿°ï¼š{item_data['description']}\\n\"\n",
    "\n",
    "        if item_data.get(\"properties\"):\n",
    "            props = \"å±¬æ€§ï¼š\" + \"ã€\".join(\n",
    "                [f\"{k}:{v}\" for k, v in item_data[\"properties\"].items()]\n",
    "            )\n",
    "            item_text += props\n",
    "\n",
    "        return [\n",
    "            {\n",
    "                \"text\": item_text.strip(),\n",
    "                \"meta\": {\n",
    "                    \"type\": \"item\",\n",
    "                    \"name\": item_data[\"name\"],\n",
    "                    \"category\": \"full_info\",\n",
    "                    \"source\": \"world_kb\",\n",
    "                },\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    def process_event(self, event_data: Dict) -> List[Dict]:\n",
    "        \"\"\"Convert event data to searchable chunks\"\"\"\n",
    "        event_text = f\"äº‹ä»¶ï¼š{event_data['name']}\\n\"\n",
    "        event_text += f\"é¡å‹ï¼š{event_data['type']}\\n\"\n",
    "        event_text += f\"æè¿°ï¼š{event_data['description']}\\n\"\n",
    "\n",
    "        if event_data.get(\"triggers\"):\n",
    "            event_text += \"è§¸ç™¼æ¢ä»¶ï¼š\" + \"ã€\".join(event_data[\"triggers\"]) + \"\\n\"\n",
    "        if event_data.get(\"outcomes\"):\n",
    "            event_text += \"å¯èƒ½çµæœï¼š\" + \"ã€\".join(event_data[\"outcomes\"]) + \"\\n\"\n",
    "        if event_data.get(\"requirements\"):\n",
    "            reqs = \"éœ€æ±‚ï¼š\" + \"ã€\".join(\n",
    "                [f\"{k}:{v}\" for k, v in event_data[\"requirements\"].items()]\n",
    "            )\n",
    "            event_text += reqs\n",
    "\n",
    "        return [\n",
    "            {\n",
    "                \"text\": event_text.strip(),\n",
    "                \"meta\": {\n",
    "                    \"type\": \"event\",\n",
    "                    \"name\": event_data[\"name\"],\n",
    "                    \"category\": \"full_info\",\n",
    "                    \"source\": \"world_kb\",\n",
    "                },\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    def process_world_data(self, world_data: Dict) -> List[Dict]:\n",
    "        \"\"\"Process complete world data into chunks\"\"\"\n",
    "        all_chunks = []\n",
    "\n",
    "        # Process each category\n",
    "        for char in world_data.get(\"characters\", []):\n",
    "            all_chunks.extend(self.process_character(char))\n",
    "\n",
    "        for loc in world_data.get(\"locations\", []):\n",
    "            all_chunks.extend(self.process_location(loc))\n",
    "\n",
    "        for item in world_data.get(\"items\", []):\n",
    "            all_chunks.extend(self.process_item(item))\n",
    "\n",
    "        for event in world_data.get(\"events\", []):\n",
    "            all_chunks.extend(self.process_event(event))\n",
    "\n",
    "        return all_chunks\n",
    "\n",
    "\n",
    "# Test the processor\n",
    "processor = WorldKBProcessor()\n",
    "chunks = processor.process_world_data(sample_data)\n",
    "\n",
    "print(f\"âœ“ Processed world data into {len(chunks)} chunks\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1} ({chunk['meta']['type']} - {chunk['meta']['category']}):\")\n",
    "    print(chunk[\"text\"][:100] + \"...\" if len(chunk[\"text\"]) > 100 else chunk[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a661dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Build FAISS Index for World KB\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class WorldKBIndex:\n",
    "    \"\"\"FAISS index specifically for game world knowledge base\"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"BAAI/bge-m3\"):\n",
    "        print(f\"Loading embedding model: {model_name}\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.chunks = []\n",
    "\n",
    "    def build_index(self, chunks: List[Dict]):\n",
    "        \"\"\"Build FAISS index from world KB chunks\"\"\"\n",
    "        print(f\"Building index for {len(chunks)} chunks...\")\n",
    "\n",
    "        # Extract texts and compute embeddings\n",
    "        texts = [chunk[\"text\"] for chunk in chunks]\n",
    "        print(\"Computing embeddings...\")\n",
    "        embeddings = self.model.encode(\n",
    "            texts, normalize_embeddings=True, batch_size=16, show_progress_bar=True\n",
    "        ).astype(\"float32\")\n",
    "\n",
    "        # Create FAISS index\n",
    "        dimension = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(\n",
    "            dimension\n",
    "        )  # Inner product for normalized vectors\n",
    "        self.index.add(embeddings)\n",
    "\n",
    "        # Store chunks for retrieval\n",
    "        self.chunks = chunks\n",
    "\n",
    "        print(\n",
    "            f\"âœ“ Built index with {self.index.ntotal} vectors (dimension: {dimension})\"\n",
    "        )\n",
    "        return self.index\n",
    "\n",
    "    def search(\n",
    "        self, query: str, k: int = 5, filter_type: Optional[str] = None\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Search world KB with optional type filtering\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not built yet. Call build_index() first.\")\n",
    "\n",
    "        # Compute query embedding\n",
    "        query_embedding = self.model.encode([query], normalize_embeddings=True).astype(\n",
    "            \"float32\"\n",
    "        )\n",
    "\n",
    "        # Search index\n",
    "        scores, indices = self.index.search(\n",
    "            query_embedding, min(k * 3, len(self.chunks))\n",
    "        )\n",
    "\n",
    "        # Get results with metadata\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx == -1:  # Invalid index\n",
    "                continue\n",
    "\n",
    "            chunk = self.chunks[idx]\n",
    "\n",
    "            # Apply type filter if specified\n",
    "            if filter_type and chunk[\"meta\"][\"type\"] != filter_type:\n",
    "                continue\n",
    "\n",
    "            results.append(\n",
    "                {\"text\": chunk[\"text\"], \"meta\": chunk[\"meta\"], \"score\": float(score)}\n",
    "            )\n",
    "\n",
    "            if len(results) >= k:\n",
    "                break\n",
    "\n",
    "        return results\n",
    "\n",
    "    def save_index(self, index_path: str, chunks_path: str):\n",
    "        \"\"\"Save index and chunks to disk\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"No index to save\")\n",
    "\n",
    "        # Save FAISS index\n",
    "        faiss.write_index(self.index, index_path)\n",
    "\n",
    "        # Save chunks as JSONL\n",
    "        with open(chunks_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for chunk in self.chunks:\n",
    "                f.write(json.dumps(chunk, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        print(f\"âœ“ Saved index to {index_path}\")\n",
    "        print(f\"âœ“ Saved chunks to {chunks_path}\")\n",
    "\n",
    "    def load_index(self, index_path: str, chunks_path: str):\n",
    "        \"\"\"Load index and chunks from disk\"\"\"\n",
    "        # Load FAISS index\n",
    "        self.index = faiss.read_index(index_path)\n",
    "\n",
    "        # Load chunks\n",
    "        self.chunks = []\n",
    "        with open(chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                self.chunks.append(json.loads(line.strip()))\n",
    "\n",
    "        print(f\"âœ“ Loaded index with {self.index.ntotal} vectors\")\n",
    "        print(f\"âœ“ Loaded {len(self.chunks)} chunks\")\n",
    "\n",
    "\n",
    "# Build the world KB index\n",
    "kb_index = WorldKBIndex()\n",
    "kb_index.build_index(chunks)\n",
    "\n",
    "# Save to indices folder\n",
    "Path(\"indices\").mkdir(exist_ok=True)\n",
    "kb_index.save_index(\"indices/world_kb.faiss\", \"indices/world_kb_chunks.jsonl\")\n",
    "\n",
    "print(\"\\n=== World KB Index Ready ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff8be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: World KB Retrieval System\n",
    "class WorldKBRetriever:\n",
    "    \"\"\"High-level retrieval system for game world knowledge\"\"\"\n",
    "\n",
    "    def __init__(self, index_path: str, chunks_path: str):\n",
    "        self.kb_index = WorldKBIndex()\n",
    "        self.kb_index.load_index(index_path, chunks_path)\n",
    "\n",
    "    def get_character_info(self, character_name: str) -> Dict:\n",
    "        \"\"\"Get comprehensive character information\"\"\"\n",
    "        results = self.kb_index.search(\n",
    "            f\"è§’è‰² {character_name}\", k=10, filter_type=\"character\"\n",
    "        )\n",
    "\n",
    "        # Group by category\n",
    "        info = {\"basic_info\": [], \"background\": [], \"relationships\": []}\n",
    "\n",
    "        for result in results:\n",
    "            category = result[\"meta\"][\"category\"]\n",
    "            if category in info:\n",
    "                info[category].append(result)\n",
    "\n",
    "        return info\n",
    "\n",
    "    def get_location_info(self, location_name: str) -> Dict:\n",
    "        \"\"\"Get comprehensive location information\"\"\"\n",
    "        results = self.kb_index.search(\n",
    "            f\"åœ°é» {location_name}\", k=10, filter_type=\"location\"\n",
    "        )\n",
    "\n",
    "        info = {\"basic_info\": [], \"connections\": [], \"features\": []}\n",
    "\n",
    "        for result in results:\n",
    "            category = result[\"meta\"][\"category\"]\n",
    "            if category in info:\n",
    "                info[category].append(result)\n",
    "\n",
    "        return info\n",
    "\n",
    "    def search_items_by_type(self, item_type: str) -> List[Dict]:\n",
    "        \"\"\"Search items by type\"\"\"\n",
    "        return self.kb_index.search(f\"ç‰©å“ é¡å‹ {item_type}\", k=10, filter_type=\"item\")\n",
    "\n",
    "    def find_related_events(self, context: str) -> List[Dict]:\n",
    "        \"\"\"Find events related to given context\"\"\"\n",
    "        return self.kb_index.search(f\"äº‹ä»¶ {context}\", k=5, filter_type=\"event\")\n",
    "\n",
    "    def contextual_search(self, query: str, k: int = 5) -> List[Dict]:\n",
    "        \"\"\"General contextual search across all world knowledge\"\"\"\n",
    "        return self.kb_index.search(query, k=k)\n",
    "\n",
    "\n",
    "# Test the retrieval system\n",
    "retriever = WorldKBRetriever(\"indices/world_kb.faiss\", \"indices/world_kb_chunks.jsonl\")\n",
    "\n",
    "print(\"=== Testing World KB Retrieval ===\\n\")\n",
    "\n",
    "# Test character lookup\n",
    "char_info = retriever.get_character_info(\"è‰¾è‰äº\")\n",
    "print(\"Character Info for è‰¾è‰äº:\")\n",
    "for category, results in char_info.items():\n",
    "    if results:\n",
    "        print(f\"  {category}: {len(results)} results\")\n",
    "        print(f\"    {results[0]['text'][:80]}...\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Test location lookup\n",
    "loc_info = retriever.get_location_info(\"ç¿¡ç¿ æ£®æ—\")\n",
    "print(\"Location Info for ç¿¡ç¿ æ£®æ—:\")\n",
    "for category, results in loc_info.items():\n",
    "    if results:\n",
    "        print(f\"  {category}: {len(results)} results\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Test contextual search\n",
    "context_results = retriever.contextual_search(\"é­”æ³• æ³•å¸«\")\n",
    "print(\"Contextual search for 'é­”æ³• æ³•å¸«':\")\n",
    "for i, result in enumerate(context_results[:2]):\n",
    "    print(\n",
    "        f\"  {i+1}. [{result['meta']['type']}] {result['text'][:60]}... (score: {result['score']:.3f})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9efb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Game Event Generator with RAG\n",
    "class GameEventGenerator:\n",
    "    \"\"\"Generate dynamic game events using world KB knowledge\"\"\"\n",
    "\n",
    "    def __init__(self, retriever: WorldKBRetriever):\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def generate_location_event(self, location_name: str, player_context: Dict) -> Dict:\n",
    "        \"\"\"Generate an event for a specific location\"\"\"\n",
    "\n",
    "        # Get location info from KB\n",
    "        loc_info = self.retriever.get_location_info(location_name)\n",
    "\n",
    "        # Build context from KB\n",
    "        context_parts = []\n",
    "        for category, results in loc_info.items():\n",
    "            for result in results:\n",
    "                context_parts.append(result[\"text\"])\n",
    "\n",
    "        location_context = \"\\n\".join(context_parts[:3])  # Limit context size\n",
    "\n",
    "        # Find related events\n",
    "        related_events = self.retriever.find_related_events(location_name)\n",
    "        event_context = \"\"\n",
    "        if related_events:\n",
    "            event_context = related_events[0][\"text\"]\n",
    "\n",
    "        # Simple event generation logic (in real game, this would use LLM)\n",
    "        event_data = {\n",
    "            \"location\": location_name,\n",
    "            \"context_used\": {\n",
    "                \"location_info\": len(context_parts),\n",
    "                \"related_events\": len(related_events),\n",
    "            },\n",
    "            \"event_prompt\": f\"\"\"\n",
    "åŸºæ–¼ä»¥ä¸‹ä¸–ç•ŒçŸ¥è­˜ç”Ÿæˆäº‹ä»¶ï¼š\n",
    "\n",
    "åœ°é»ä¿¡æ¯ï¼š\n",
    "{location_context}\n",
    "\n",
    "ç›¸é—œäº‹ä»¶ï¼š\n",
    "{event_context}\n",
    "\n",
    "ç©å®¶ç‹€æ…‹ï¼š\n",
    "- ç­‰ç´šï¼š{player_context.get('level', 1)}\n",
    "- è·æ¥­ï¼š{player_context.get('class', 'å†’éšªè€…')}\n",
    "- ç•¶å‰ä½ç½®ï¼š{location_name}\n",
    "\n",
    "è«‹ç”Ÿæˆä¸€å€‹é©åˆæ­¤æƒ…å¢ƒçš„éŠæˆ²äº‹ä»¶ã€‚\n",
    "\"\"\".strip(),\n",
    "        }\n",
    "\n",
    "        return event_data\n",
    "\n",
    "    def generate_character_encounter(self, character_name: str) -> Dict:\n",
    "        \"\"\"Generate encounter with specific character\"\"\"\n",
    "\n",
    "        char_info = self.retriever.get_character_info(character_name)\n",
    "\n",
    "        # Collect character context\n",
    "        context_parts = []\n",
    "        for category, results in char_info.items():\n",
    "            for result in results:\n",
    "                context_parts.append(result[\"text\"])\n",
    "\n",
    "        character_context = \"\\n\".join(context_parts[:2])\n",
    "\n",
    "        encounter_data = {\n",
    "            \"character\": character_name,\n",
    "            \"context_used\": {\"character_info\": len(context_parts)},\n",
    "            \"encounter_prompt\": f\"\"\"\n",
    "åŸºæ–¼ä»¥ä¸‹è§’è‰²ä¿¡æ¯ç”Ÿæˆé­é‡äº‹ä»¶ï¼š\n",
    "\n",
    "è§’è‰²ä¿¡æ¯ï¼š\n",
    "{character_context}\n",
    "\n",
    "ç”Ÿæˆèˆ‡æ­¤è§’è‰²çš„äº’å‹•äº‹ä»¶ï¼ŒåŒ…æ‹¬å¯èƒ½çš„å°è©±é¸é …å’Œçµæœã€‚\n",
    "\"\"\".strip(),\n",
    "        }\n",
    "\n",
    "        return encounter_data\n",
    "\n",
    "\n",
    "# Test event generation\n",
    "event_gen = GameEventGenerator(retriever)\n",
    "\n",
    "print(\"=== Testing Event Generation ===\\n\")\n",
    "\n",
    "# Test location event\n",
    "player_context = {\"level\": 3, \"class\": \"æ³•å¸«\"}\n",
    "location_event = event_gen.generate_location_event(\"ç¿¡ç¿ æ£®æ—\", player_context)\n",
    "print(\"Location Event for ç¿¡ç¿ æ£®æ—:\")\n",
    "print(f\"Context used: {location_event['context_used']}\")\n",
    "print(f\"Prompt length: {len(location_event['event_prompt'])} characters\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Test character encounter\n",
    "char_encounter = event_gen.generate_character_encounter(\"è‰¾è‰äº\")\n",
    "print(\"Character Encounter for è‰¾è‰äº:\")\n",
    "print(f\"Context used: {char_encounter['context_used']}\")\n",
    "print(f\"Prompt length: {len(char_encounter['encounter_prompt'])} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529d208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Smoke Test - Complete World KB Pipeline\n",
    "def smoke_test_world_kb():\n",
    "    \"\"\"Comprehensive smoke test for world KB system\"\"\"\n",
    "\n",
    "    print(\"ğŸ§ª World KB Smoke Test\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Test 1: Data creation\n",
    "    print(\"\\n1. Testing sample data creation...\")\n",
    "    data = create_sample_world_data()\n",
    "    assert len(data[\"characters\"]) >= 2, \"Should have at least 2 characters\"\n",
    "    assert len(data[\"locations\"]) >= 2, \"Should have at least 2 locations\"\n",
    "    print(\"âœ“ Sample data created successfully\")\n",
    "\n",
    "    # Test 2: Document processing\n",
    "    print(\"\\n2. Testing document processing...\")\n",
    "    processor = WorldKBProcessor()\n",
    "    chunks = processor.process_world_data(data)\n",
    "    assert len(chunks) > 0, \"Should generate chunks\"\n",
    "    assert all(\n",
    "        \"text\" in chunk and \"meta\" in chunk for chunk in chunks\n",
    "    ), \"Chunks should have text and meta\"\n",
    "    print(f\"âœ“ Generated {len(chunks)} chunks\")\n",
    "\n",
    "    # Test 3: Index building\n",
    "    print(\"\\n3. Testing index building...\")\n",
    "    kb_index = WorldKBIndex()\n",
    "    index = kb_index.build_index(chunks)\n",
    "    assert index.ntotal == len(chunks), \"Index should contain all chunks\"\n",
    "    print(f\"âœ“ Built index with {index.ntotal} vectors\")\n",
    "\n",
    "    # Test 4: Search functionality\n",
    "    print(\"\\n4. Testing search functionality...\")\n",
    "    results = kb_index.search(\"è‰¾è‰äº\", k=3)\n",
    "    assert len(results) > 0, \"Should find results for character search\"\n",
    "    assert all(\"score\" in r for r in results), \"Results should have scores\"\n",
    "    print(f\"âœ“ Found {len(results)} results for character search\")\n",
    "\n",
    "    # Test 5: Retrieval system\n",
    "    print(\"\\n5. Testing retrieval system...\")\n",
    "    # Create temporary files for testing\n",
    "    kb_index.save_index(\"test_world.faiss\", \"test_world_chunks.jsonl\")\n",
    "    retriever = WorldKBRetriever(\"test_world.faiss\", \"test_world_chunks.jsonl\")\n",
    "\n",
    "    char_info = retriever.get_character_info(\"è‰¾è‰äº\")\n",
    "    assert any(\n",
    "        len(info) > 0 for info in char_info.values()\n",
    "    ), \"Should find character info\"\n",
    "    print(\"âœ“ Character retrieval working\")\n",
    "\n",
    "    # Test 6: Event generation\n",
    "    print(\"\\n6. Testing event generation...\")\n",
    "    event_gen = GameEventGenerator(retriever)\n",
    "    location_event = event_gen.generate_location_event(\"ç¿¡ç¿ æ£®æ—\", {\"level\": 1})\n",
    "    assert \"event_prompt\" in location_event, \"Should generate event prompt\"\n",
    "    assert (\n",
    "        len(location_event[\"event_prompt\"]) > 100\n",
    "    ), \"Event prompt should be substantial\"\n",
    "    print(\"âœ“ Event generation working\")\n",
    "\n",
    "    # Cleanup\n",
    "    import os\n",
    "\n",
    "    try:\n",
    "        os.remove(\"test_world.faiss\")\n",
    "        os.remove(\"test_world_chunks.jsonl\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(\"\\nğŸ‰ All tests passed! World KB system is ready.\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_world_kb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0964c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Integration Example - World Query System\n",
    "class WorldQuerySystem:\n",
    "    \"\"\"Complete query system for game world information\"\"\"\n",
    "\n",
    "    def __init__(self, retriever: WorldKBRetriever):\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def answer_world_question(self, question: str) -> Dict:\n",
    "        \"\"\"Answer questions about the game world using RAG\"\"\"\n",
    "\n",
    "        # Search for relevant information\n",
    "        results = self.retriever.contextual_search(question, k=5)\n",
    "\n",
    "        # Build context from search results\n",
    "        context_parts = []\n",
    "        sources = []\n",
    "\n",
    "        for i, result in enumerate(results):\n",
    "            context_parts.append(f\"[{i+1}] {result['text']}\")\n",
    "            sources.append(\n",
    "                {\n",
    "                    \"index\": i + 1,\n",
    "                    \"type\": result[\"meta\"][\"type\"],\n",
    "                    \"name\": result[\"meta\"][\"name\"],\n",
    "                    \"category\": result[\"meta\"][\"category\"],\n",
    "                    \"score\": result[\"score\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "        # Generate answer prompt (in real game, this would go to LLM)\n",
    "        answer_prompt = f\"\"\"\n",
    "å•é¡Œï¼š{question}\n",
    "\n",
    "ç›¸é—œä¸–ç•ŒçŸ¥è­˜ï¼š\n",
    "{context}\n",
    "\n",
    "è«‹æ ¹æ“šä¸Šè¿°ä¸–ç•ŒçŸ¥è­˜å›ç­”å•é¡Œï¼Œä¸¦åœ¨å›ç­”ä¸­æ¨™è¨»å¼•ç”¨ä¾†æº [1]ã€[2] ç­‰ã€‚\n",
    "å¦‚æœçŸ¥è­˜ä¸è¶³ä»¥å›ç­”å•é¡Œï¼Œè«‹èªªæ˜éœ€è¦æ›´å¤šå“ªæ–¹é¢çš„ä¿¡æ¯ã€‚\n",
    "\"\"\"\n",
    "\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"context\": context,\n",
    "            \"sources\": sources,\n",
    "            \"answer_prompt\": answer_prompt,\n",
    "            \"source_count\": len(sources),\n",
    "        }\n",
    "\n",
    "    def get_location_summary(self, location: str) -> str:\n",
    "        \"\"\"Get a comprehensive summary of a location\"\"\"\n",
    "        loc_info = self.retriever.get_location_info(location)\n",
    "\n",
    "        summary_parts = []\n",
    "        for category, results in loc_info.items():\n",
    "            if results:\n",
    "                summary_parts.append(f\"{category}: {results[0]['text']}\")\n",
    "\n",
    "        return \"\\n\".join(summary_parts)\n",
    "\n",
    "    def find_quest_hooks(self, player_interests: List[str]) -> List[Dict]:\n",
    "        \"\"\"Find potential quest hooks based on player interests\"\"\"\n",
    "        quest_hooks = []\n",
    "\n",
    "        for interest in player_interests:\n",
    "            results = self.retriever.contextual_search(f\"ä»»å‹™ {interest}\", k=3)\n",
    "            for result in results:\n",
    "                if result[\"meta\"][\"type\"] == \"event\":\n",
    "                    quest_hooks.append(\n",
    "                        {\n",
    "                            \"hook\": result[\"text\"],\n",
    "                            \"interest\": interest,\n",
    "                            \"score\": result[\"score\"],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        # Sort by score and remove duplicates\n",
    "        quest_hooks.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        seen = set()\n",
    "        unique_hooks = []\n",
    "        for hook in quest_hooks:\n",
    "            hook_text = hook[\"hook\"][:50]  # First 50 chars as identifier\n",
    "            if hook_text not in seen:\n",
    "                seen.add(hook_text)\n",
    "                unique_hooks.append(hook)\n",
    "\n",
    "        return unique_hooks[:5]  # Return top 5\n",
    "\n",
    "\n",
    "# Test the query system\n",
    "query_system = WorldQuerySystem(retriever)\n",
    "\n",
    "print(\"=== Testing World Query System ===\\n\")\n",
    "\n",
    "# Test world question answering\n",
    "question = \"è‰¾è‰äºæœ‰ä»€éº¼ç‰¹æ®Šèƒ½åŠ›ï¼Ÿ\"\n",
    "answer_data = query_system.answer_world_question(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Found {answer_data['source_count']} relevant sources\")\n",
    "print(\"Sources:\")\n",
    "for source in answer_data[\"sources\"][:2]:\n",
    "    print(\n",
    "        f\"  [{source['index']}] {source['type']} - {source['name']} ({source['score']:.3f})\"\n",
    "    )\n",
    "\n",
    "print()\n",
    "\n",
    "# Test location summary\n",
    "location = \"éŠ€æœˆåŸ\"\n",
    "summary = query_system.get_location_summary(location)\n",
    "print(f\"Location Summary for {location}:\")\n",
    "print(summary[:200] + \"...\" if len(summary) > 200 else summary)\n",
    "\n",
    "print()\n",
    "\n",
    "# Test quest hook finding\n",
    "player_interests = [\"é­”æ³•\", \"å†’éšª\"]\n",
    "quest_hooks = query_system.find_quest_hooks(player_interests)\n",
    "print(f\"Quest hooks for interests {player_interests}:\")\n",
    "for i, hook in enumerate(quest_hooks[:2]):\n",
    "    print(\n",
    "        f\"  {i+1}. {hook['hook'][:80]}... (interest: {hook['interest']}, score: {hook['score']:.3f})\"\n",
    "    )\n",
    "\n",
    "print(\"\\nâœ… World KB system fully operational!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Performance and Usage Tips\n",
    "print(\"=== World KB Performance Metrics ===\")\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "\n",
    "def measure_performance():\n",
    "    \"\"\"Measure key performance metrics\"\"\"\n",
    "\n",
    "    # Memory usage\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "    # Index size\n",
    "    try:\n",
    "        index_size_mb = os.path.getsize(\"indices/world_kb.faiss\") / 1024 / 1024\n",
    "        chunks_size_mb = os.path.getsize(\"indices/world_kb_chunks.jsonl\") / 1024 / 1024\n",
    "    except:\n",
    "        index_size_mb = chunks_size_mb = 0\n",
    "\n",
    "    # Search latency\n",
    "    start_time = time.time()\n",
    "    results = retriever.contextual_search(\"æ¸¬è©¦æŸ¥è©¢\", k=5)\n",
    "    search_latency_ms = (time.time() - start_time) * 1000\n",
    "\n",
    "    print(f\"Memory usage: {memory_mb:.1f} MB\")\n",
    "    print(f\"Index size: {index_size_mb:.2f} MB\")\n",
    "    print(f\"Chunks size: {chunks_size_mb:.2f} MB\")\n",
    "    print(f\"Search latency: {search_latency_ms:.1f} ms\")\n",
    "    print(f\"Chunks in index: {len(retriever.kb_index.chunks)}\")\n",
    "\n",
    "    return {\n",
    "        \"memory_mb\": memory_mb,\n",
    "        \"index_size_mb\": index_size_mb,\n",
    "        \"search_latency_ms\": search_latency_ms,\n",
    "        \"chunk_count\": len(retriever.kb_index.chunks),\n",
    "    }\n",
    "\n",
    "\n",
    "metrics = measure_performance()\n",
    "\n",
    "print(\"\\n=== Usage Tips ===\")\n",
    "print(\n",
    "    \"\"\"\n",
    "1. **æ“´å±•ä¸–ç•Œå…§å®¹**ï¼š\n",
    "   - åœ¨ data/world_kb/ æ·»åŠ æ›´å¤š JSON æª”æ¡ˆ\n",
    "   - ä½¿ç”¨ WorldKBProcessor è™•ç†æ–°å…§å®¹\n",
    "   - é‡å»ºç´¢å¼•ä»¥åŒ…å«æ–°å…§å®¹\n",
    "\n",
    "2. **å„ªåŒ–æª¢ç´¢æ•ˆæœ**ï¼š\n",
    "   - ä½¿ç”¨å…·é«”çš„æŸ¥è©¢è©å½™\n",
    "   - åˆ©ç”¨é¡å‹éæ¿¾ (filter_type) ç¸®å°æœå°‹ç¯„åœ\n",
    "   - èª¿æ•´ k å€¼å¹³è¡¡å¬å›ç‡å’Œç²¾ç¢ºåº¦\n",
    "\n",
    "3. **è¨˜æ†¶é«”å„ªåŒ–**ï¼š\n",
    "   - ç›®å‰è¨˜æ†¶é«”ä½¿ç”¨ï¼š{:.1f} MB\n",
    "   - å¯ä½¿ç”¨è¼ƒå°çš„åµŒå…¥æ¨¡å‹ï¼ˆå¦‚ bge-small-zhï¼‰\n",
    "   - è€ƒæ…®ä½¿ç”¨ FAISS IVF ç´¢å¼•è™•ç†å¤§å‹ä¸–ç•Œ\n",
    "\n",
    "4. **éŠæˆ²æ•´åˆ**ï¼š\n",
    "   - å°‡ retriever æ•´åˆåˆ°éŠæˆ²ä¸»è¿´åœˆ\n",
    "   - å¿«å–å¸¸ç”¨æŸ¥è©¢çµæœ\n",
    "   - ä½¿ç”¨ filter_type æé«˜æŸ¥è©¢æ•ˆç‡\n",
    "\n",
    "5. **å…§å®¹ç®¡ç†**ï¼š\n",
    "   - å®šæœŸå‚™ä»½ç´¢å¼•æª”æ¡ˆ\n",
    "   - ç‰ˆæœ¬æ§åˆ¶ä¸–ç•Œå…§å®¹è®Šæ›´\n",
    "   - ç›£æ§ç´¢å¼•å¤§å°å’ŒæŸ¥è©¢å»¶é²\n",
    "\"\"\".format(\n",
    "        metrics[\"memory_mb\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ® Ready to power your text adventure game!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b998d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: What We Built / Pitfalls / Next Steps\n",
    "print(\"=== What We Built ===\")\n",
    "print(\n",
    "    \"\"\"\n",
    "âœ… å®Œæ•´çš„éŠæˆ²ä¸–ç•ŒçŸ¥è­˜åº«ç³»çµ±ï¼š\n",
    "   â€¢ Pydantic schema å®šç¾©è§’è‰²ã€åœ°é»ã€ç‰©å“ã€äº‹ä»¶\n",
    "   â€¢ ä¸–ç•Œå…§å®¹è™•ç†å™¨ï¼Œå°‡çµæ§‹åŒ–æ•¸æ“šè½‰ç‚ºå¯æª¢ç´¢ç‰‡æ®µ\n",
    "   â€¢ FAISS å‘é‡ç´¢å¼•ï¼Œæ”¯æ´èªç¾©æœå°‹\n",
    "   â€¢ å°ˆé–€çš„æª¢ç´¢ç³»çµ±ï¼Œæ”¯æ´é¡å‹éæ¿¾å’Œåˆ†é¡æŸ¥è©¢\n",
    "   â€¢ äº‹ä»¶ç”Ÿæˆå™¨ï¼ŒåŸºæ–¼ä¸–ç•ŒçŸ¥è­˜ç”¢ç”Ÿå‹•æ…‹å…§å®¹\n",
    "   â€¢ å®Œæ•´çš„æŸ¥è©¢ç³»çµ±ï¼Œæ”¯æ´å•ç­”å’Œä»»å‹™ç·šç´¢ç™¼ç¾\n",
    "\n",
    "âœ… æ ¸å¿ƒåŠŸèƒ½ï¼š\n",
    "   â€¢ å¤šé¡å‹å…§å®¹ç´¢å¼•ï¼ˆè§’è‰²ã€åœ°é»ã€ç‰©å“ã€äº‹ä»¶ï¼‰\n",
    "   â€¢ èªç¾©æœå°‹å’Œç²¾ç¢ºæª¢ç´¢\n",
    "   â€¢ ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„äº‹ä»¶ç”Ÿæˆ\n",
    "   â€¢ å¼•ç”¨è¿½è¹¤å’Œä¾†æºæ¨™è¨»\n",
    "   â€¢ æ•ˆèƒ½ç›£æ§å’Œå„ªåŒ–å»ºè­°\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== Pitfalls é¿å‘æŒ‡å— ===\")\n",
    "print(\n",
    "    \"\"\"\n",
    "âš ï¸  å¸¸è¦‹å•é¡Œï¼š\n",
    "   â€¢ ç´¢å¼•æª”æ¡ˆå¯èƒ½å¾ˆå¤§ï¼Œç¢ºä¿æœ‰è¶³å¤ å„²å­˜ç©ºé–“\n",
    "   â€¢ åµŒå…¥è¨ˆç®—éœ€è¦æ™‚é–“ï¼Œå¤§å‹ä¸–ç•Œå»ºè­°åˆ†æ‰¹è™•ç†\n",
    "   â€¢ æŸ¥è©¢çµæœå“è³ªå–æ±ºæ–¼åŸå§‹å…§å®¹çš„çµæ§‹åŒ–ç¨‹åº¦\n",
    "   â€¢ ä¸­æ–‡åˆ†è©å¯èƒ½å½±éŸ¿æª¢ç´¢æ•ˆæœï¼Œè€ƒæ…®ä½¿ç”¨å°ˆæ¥­ä¸­æ–‡åµŒå…¥æ¨¡å‹\n",
    "\n",
    "âš ï¸  æ•ˆèƒ½æ³¨æ„äº‹é …ï¼š\n",
    "   â€¢ é¿å…é »ç¹é‡å»ºç´¢å¼•ï¼Œä½¿ç”¨å¢é‡æ›´æ–°\n",
    "   â€¢ å¿«å–å¸¸ç”¨æŸ¥è©¢çµæœ\n",
    "   â€¢ ç›£æ§è¨˜æ†¶é«”ä½¿ç”¨ï¼Œå¿…è¦æ™‚ä½¿ç”¨è¼ƒå°æ¨¡å‹\n",
    "   â€¢ å¤§å‹ä¸–ç•Œè€ƒæ…®ä½¿ç”¨ FAISS GPU ç‰ˆæœ¬\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== Next Steps å¾ŒçºŒç™¼å±• ===\")\n",
    "print(\n",
    "    \"\"\"\n",
    "ğŸš€ ç«‹å³å¯è¡Œï¼š\n",
    "   â€¢ æ•´åˆåˆ° nb41 ç‹€æ…‹æ©Ÿæ ¸å¿ƒ\n",
    "   â€¢ æ·»åŠ æ›´è±å¯Œçš„ä¸–ç•Œå…§å®¹\n",
    "   â€¢ å¯¦ç¾å¢é‡ç´¢å¼•æ›´æ–°æ©Ÿåˆ¶\n",
    "\n",
    "ğŸš€ é€²éšåŠŸèƒ½ï¼š\n",
    "   â€¢ å¤šèªè¨€ä¸–ç•Œå…§å®¹æ”¯æ´\n",
    "   â€¢ æ™‚é–“è»¸ç›¸é—œçš„å‹•æ…‹ä¸–ç•Œç‹€æ…‹\n",
    "   â€¢ ç©å®¶è¡Œç‚ºå°ä¸–ç•ŒçŸ¥è­˜çš„å½±éŸ¿\n",
    "   â€¢ æ›´æ™ºæ…§çš„äº‹ä»¶ç”Ÿæˆï¼ˆæ•´åˆ LLMï¼‰\n",
    "\n",
    "ğŸš€ ç³»çµ±æ•´åˆï¼š\n",
    "   â€¢ èˆ‡ nb42 äº‹ä»¶ç”Ÿæˆç³»çµ±æ·±åº¦æ•´åˆ\n",
    "   â€¢ æ”¯æ´ nb45 å­˜æª”ç³»çµ±çš„ä¸–ç•Œç‹€æ…‹\n",
    "   â€¢ ç‚º nb46 æ•˜äº‹é¢¨æ ¼æä¾›ä¸€è‡´æ€§æª¢æŸ¥\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== Reproducibility é‡ç¾æ­¥é©Ÿ ===\")\n",
    "print(\n",
    "    \"\"\"\n",
    "1. ç¢ºä¿ç’°å¢ƒè®Šæ•¸ AI_CACHE_ROOT å·²è¨­ç½®\n",
    "2. å®‰è£ä¾è³´ï¼šsentence-transformers, faiss-cpu, opencc, pydantic\n",
    "3. é‹è¡Œæ‰€æœ‰ cells æŒ‰é †åºåŸ·è¡Œ\n",
    "4. æª¢æŸ¥ indices/ è³‡æ–™å¤¾ä¸‹çš„ç´¢å¼•æª”æ¡ˆ\n",
    "5. é©—è­‰ smoke test å…¨éƒ¨é€šé\n",
    "\n",
    "æª”æ¡ˆè¼¸å‡ºï¼š\n",
    "â€¢ indices/world_kb.faiss (å‘é‡ç´¢å¼•)\n",
    "â€¢ indices/world_kb_chunks.jsonl (æ–‡æª”ç‰‡æ®µ)\n",
    "â€¢ data/world_kb/world_data.json (ç¯„ä¾‹ä¸–ç•Œæ•¸æ“š)\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"\\nğŸ¯ æœ¬ Notebook ç‚º Stage 5 çš„åŸºçŸ³ï¼Œç‚ºå¾ŒçºŒæ–‡å­—å†’éšªéŠæˆ²æä¾›äº†å¼·å¤§çš„ä¸–ç•ŒçŸ¥è­˜æª¢ç´¢èƒ½åŠ›ï¼\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
