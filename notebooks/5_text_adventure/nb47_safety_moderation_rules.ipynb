{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ada440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb47_safety_moderation_rules.ipynb\n",
    "# Stage 5: Text Adventure - Safety and Content Moderation\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (複製到每本 notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b83644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 2: Safety Word Lists and Age Rating System\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "class AgeRating(Enum):\n",
    "    \"\"\"Age rating categories for content filtering\"\"\"\n",
    "\n",
    "    GENERAL = \"G\"  # All ages\n",
    "    TEEN = \"T\"  # 13+\n",
    "    MATURE = \"M\"  # 17+\n",
    "    ADULT = \"A\"  # 18+\n",
    "\n",
    "\n",
    "class ContentType(Enum):\n",
    "    \"\"\"Types of potentially harmful content\"\"\"\n",
    "\n",
    "    VIOLENCE = \"violence\"\n",
    "    SEXUAL = \"sexual\"\n",
    "    PROFANITY = \"profanity\"\n",
    "    HATE_SPEECH = \"hate_speech\"\n",
    "    DRUG_REFERENCE = \"drug_reference\"\n",
    "    GAMBLING = \"gambling\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ContentFilter:\n",
    "    \"\"\"Configuration for content filtering\"\"\"\n",
    "\n",
    "    age_rating: AgeRating\n",
    "    blocked_types: List[ContentType]\n",
    "    severity_threshold: float = 0.5  # 0.0-1.0\n",
    "    replacement_style: str = \"euphemistic\"  # euphemistic, censored, alternative\n",
    "\n",
    "\n",
    "class SafetyModerator:\n",
    "    \"\"\"Content safety and moderation system for text adventure games\"\"\"\n",
    "\n",
    "    def __init__(self, filter_config: ContentFilter):\n",
    "        self.config = filter_config\n",
    "        self.word_lists = self._load_word_lists()\n",
    "        self.replacement_dict = self._load_replacements()\n",
    "\n",
    "    def _load_word_lists(self) -> Dict[ContentType, List[str]]:\n",
    "        \"\"\"Load categorized word lists for content detection\"\"\"\n",
    "        return {\n",
    "            ContentType.VIOLENCE: [\n",
    "                \"殺死\",\n",
    "                \"謀殺\",\n",
    "                \"暴力\",\n",
    "                \"攻擊\",\n",
    "                \"傷害\",\n",
    "                \"血腥\",\n",
    "                \"殘忍\",\n",
    "                \"屠殺\",\n",
    "                \"砍殺\",\n",
    "                \"刺殺\",\n",
    "                \"毆打\",\n",
    "                \"虐待\",\n",
    "                \"折磨\",\n",
    "            ],\n",
    "            ContentType.SEXUAL: [\n",
    "                \"性行為\",\n",
    "                \"色情\",\n",
    "                \"裸體\",\n",
    "                \"性器官\",\n",
    "                \"做愛\",\n",
    "                \"性交\",\n",
    "                \"誘惑\",\n",
    "                \"調情\",\n",
    "                \"挑逗\",\n",
    "                \"親密\",\n",
    "                \"床戲\",\n",
    "            ],\n",
    "            ContentType.PROFANITY: [\n",
    "                \"混蛋\",\n",
    "                \"王八蛋\",\n",
    "                \"狗屎\",\n",
    "                \"他媽的\",\n",
    "                \"幹你娘\",\n",
    "                \"操\",\n",
    "                \"臭婊子\",\n",
    "                \"賤人\",\n",
    "                \"白痴\",\n",
    "                \"智障\",\n",
    "                \"廢物\",\n",
    "            ],\n",
    "            ContentType.HATE_SPEECH: [\n",
    "                \"種族歧視\",\n",
    "                \"性別歧視\",\n",
    "                \"仇恨\",\n",
    "                \"歧視\",\n",
    "                \"偏見\",\n",
    "                \"排外\",\n",
    "                \"極端主義\",\n",
    "                \"恐怖主義\",\n",
    "            ],\n",
    "            ContentType.DRUG_REFERENCE: [\n",
    "                \"毒品\",\n",
    "                \"吸毒\",\n",
    "                \"大麻\",\n",
    "                \"海洛因\",\n",
    "                \"古柯鹼\",\n",
    "                \"搖頭丸\",\n",
    "                \"興奮劑\",\n",
    "                \"致幻劑\",\n",
    "                \"藥物濫用\",\n",
    "            ],\n",
    "            ContentType.GAMBLING: [\n",
    "                \"賭博\",\n",
    "                \"賭場\",\n",
    "                \"下注\",\n",
    "                \"彩票\",\n",
    "                \"老虎機\",\n",
    "                \"撲克牌賭博\",\n",
    "                \"賭債\",\n",
    "                \"賭癮\",\n",
    "                \"博弈\",\n",
    "            ],\n",
    "        }\n",
    "\n",
    "    def _load_replacements(self) -> Dict[str, Dict[str, str]]:\n",
    "        \"\"\"Load replacement text for different styles\"\"\"\n",
    "        return {\n",
    "            \"euphemistic\": {\n",
    "                \"殺死\": \"擊敗\",\n",
    "                \"暴力\": \"衝突\",\n",
    "                \"血腥\": \"激烈\",\n",
    "                \"性行為\": \"親密接觸\",\n",
    "                \"裸體\": \"衣著不整\",\n",
    "                \"混蛋\": \"壞人\",\n",
    "                \"他媽的\": \"該死的\",\n",
    "                \"毒品\": \"禁藥\",\n",
    "                \"賭博\": \"遊戲\",\n",
    "            },\n",
    "            \"censored\": {\n",
    "                \"殺死\": \"K**\",\n",
    "                \"暴力\": \"**力\",\n",
    "                \"血腥\": \"**腥\",\n",
    "                \"性行為\": \"***為\",\n",
    "                \"裸體\": \"**體\",\n",
    "                \"混蛋\": \"**蛋\",\n",
    "                \"他媽的\": \"***的\",\n",
    "                \"毒品\": \"**品\",\n",
    "                \"賭博\": \"**博\",\n",
    "            },\n",
    "            \"alternative\": {\n",
    "                \"殺死\": \"阻止\",\n",
    "                \"暴力\": \"對抗\",\n",
    "                \"血腥\": \"危險\",\n",
    "                \"性行為\": \"浪漫\",\n",
    "                \"裸體\": \"換裝\",\n",
    "                \"混蛋\": \"反派\",\n",
    "                \"他媽的\": \"可惡\",\n",
    "                \"毒品\": \"藥草\",\n",
    "                \"賭博\": \"競技\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def detect_content_issues(self, text: str) -> Dict[ContentType, float]:\n",
    "        \"\"\"Detect potentially problematic content and return severity scores\"\"\"\n",
    "        issues = {}\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        for content_type, words in self.word_lists.items():\n",
    "            if content_type not in self.config.blocked_types:\n",
    "                continue\n",
    "\n",
    "            matches = 0\n",
    "            total_words = len(text.split())\n",
    "\n",
    "            for word in words:\n",
    "                if word.lower() in text_lower:\n",
    "                    matches += 1\n",
    "\n",
    "            # Calculate severity based on word frequency\n",
    "            severity = min(matches / max(total_words, 1) * 10, 1.0)\n",
    "            if severity > 0:\n",
    "                issues[content_type] = severity\n",
    "\n",
    "        return issues\n",
    "\n",
    "    def moderate_content(self, text: str) -> Tuple[str, bool, Dict]:\n",
    "        \"\"\"Moderate content and return cleaned text, approval status, and report\"\"\"\n",
    "        issues = self.detect_content_issues(text)\n",
    "\n",
    "        # Check if content violates age rating\n",
    "        violations = {\n",
    "            k: v for k, v in issues.items() if v >= self.config.severity_threshold\n",
    "        }\n",
    "\n",
    "        if not violations:\n",
    "            return text, True, {\"issues\": issues, \"violations\": []}\n",
    "\n",
    "        # Apply content filtering\n",
    "        cleaned_text = self._apply_replacements(text)\n",
    "\n",
    "        report = {\n",
    "            \"issues\": issues,\n",
    "            \"violations\": list(violations.keys()),\n",
    "            \"age_rating\": self.config.age_rating.value,\n",
    "            \"action\": \"content_filtered\",\n",
    "        }\n",
    "\n",
    "        return cleaned_text, False, report\n",
    "\n",
    "    def _apply_replacements(self, text: str) -> str:\n",
    "        \"\"\"Apply content replacements based on style configuration\"\"\"\n",
    "        replacements = self.replacement_dict.get(self.config.replacement_style, {})\n",
    "\n",
    "        for original, replacement in replacements.items():\n",
    "            text = re.sub(re.escape(original), replacement, text, flags=re.IGNORECASE)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def generate_alternative_content(\n",
    "        self, original_text: str, context: str = \"\"\n",
    "    ) -> str:\n",
    "        \"\"\"Generate alternative content when original is blocked\"\"\"\n",
    "        if self.config.replacement_style == \"euphemistic\":\n",
    "            return f\"[故事轉向更溫和的發展] {context}中，情況得到了和平的解決。\"\n",
    "        elif self.config.replacement_style == \"censored\":\n",
    "            return f\"[內容已過濾] 由於內容不適合當前設定，故事將跳過這個部分。\"\n",
    "        else:  # alternative\n",
    "            return f\"[替代敘述] {context}中，角色們選擇了更合適的行動方案。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5270419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 3: Content Detector with Scoring\n",
    "class AdvancedContentDetector:\n",
    "    \"\"\"More sophisticated content detection using pattern matching and context\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.violence_patterns = [\n",
    "            r\"(殺|砍|刺|打|揍|毆|擊).{0,3}(死|傷|害|倒)\",\n",
    "            r\"(血|屍|骨|肉).{0,2}(流|飛|散|碎)\",\n",
    "            r\"(武器|刀|劍|槍|炸彈).{0,5}(攻擊|傷害|殺)\",\n",
    "        ]\n",
    "\n",
    "        self.sexual_patterns = [\n",
    "            r\"(脫|除|褪).{0,2}(衣|褲|裙)\",\n",
    "            r\"(撫摸|愛撫|親吻).{0,3}(身體|肌膚)\",\n",
    "            r\"(床|房間|私密).{0,5}(活動|行為)\",\n",
    "        ]\n",
    "\n",
    "        self.context_modifiers = {\n",
    "            \"遊戲\": 0.7,  # Gaming context reduces severity\n",
    "            \"虛構\": 0.8,  # Fictional context\n",
    "            \"幻想\": 0.8,  # Fantasy context\n",
    "            \"現實\": 1.2,  # Reality increases severity\n",
    "            \"教學\": 0.5,  # Educational context\n",
    "        }\n",
    "\n",
    "    def analyze_with_context(self, text: str, game_context: Dict) -> Dict:\n",
    "        \"\"\"Analyze content considering game context and narrative style\"\"\"\n",
    "        base_score = self._pattern_based_detection(text)\n",
    "\n",
    "        # Apply context modifiers\n",
    "        context_modifier = 1.0\n",
    "        for ctx_word, modifier in self.context_modifiers.items():\n",
    "            if ctx_word in text or ctx_word in game_context.get(\"genre\", \"\"):\n",
    "                context_modifier *= modifier\n",
    "\n",
    "        # Adjust scores based on narrative tone\n",
    "        narrative_tone = game_context.get(\"tone\", \"neutral\")\n",
    "        if narrative_tone == \"comedic\":\n",
    "            context_modifier *= 0.6\n",
    "        elif narrative_tone == \"dark\":\n",
    "            context_modifier *= 1.3\n",
    "        elif narrative_tone == \"educational\":\n",
    "            context_modifier *= 0.4\n",
    "\n",
    "        adjusted_scores = {k: v * context_modifier for k, v in base_score.items()}\n",
    "\n",
    "        return {\n",
    "            \"base_scores\": base_score,\n",
    "            \"context_modifier\": context_modifier,\n",
    "            \"final_scores\": adjusted_scores,\n",
    "            \"game_context\": game_context,\n",
    "        }\n",
    "\n",
    "    def _pattern_based_detection(self, text: str) -> Dict[ContentType, float]:\n",
    "        \"\"\"Use regex patterns for more nuanced detection\"\"\"\n",
    "        scores = {}\n",
    "\n",
    "        # Violence detection\n",
    "        violence_matches = sum(\n",
    "            1 for pattern in self.violence_patterns if re.search(pattern, text)\n",
    "        )\n",
    "        if violence_matches > 0:\n",
    "            scores[ContentType.VIOLENCE] = min(violence_matches * 0.3, 1.0)\n",
    "\n",
    "        # Sexual content detection\n",
    "        sexual_matches = sum(\n",
    "            1 for pattern in self.sexual_patterns if re.search(pattern, text)\n",
    "        )\n",
    "        if sexual_matches > 0:\n",
    "            scores[ContentType.SEXUAL] = min(sexual_matches * 0.4, 1.0)\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b10400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 4: RAG Integration with Safety Checks\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SafeGameNarrator:\n",
    "    \"\"\"RAG-powered game narrator with integrated safety moderation\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, moderator: SafetyModerator, embedding_model: str = \"BAAI/bge-m3\"\n",
    "    ):\n",
    "        self.moderator = moderator\n",
    "        self.embedder = SentenceTransformer(embedding_model)\n",
    "        self.safe_alternatives_db = self._build_safe_alternatives()\n",
    "\n",
    "    def _build_safe_alternatives(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Build database of safe alternative narrative elements\"\"\"\n",
    "        return {\n",
    "            \"conflict_resolution\": [\n",
    "                \"雙方達成了和解\",\n",
    "                \"通過談判解決了爭端\",\n",
    "                \"找到了共同點\",\n",
    "                \"智慧戰勝了暴力\",\n",
    "                \"和平的解決方案出現了\",\n",
    "            ],\n",
    "            \"character_interaction\": [\n",
    "                \"角色間建立了友誼\",\n",
    "                \"相互理解加深了\",\n",
    "                \"合作關係形成了\",\n",
    "                \"信任逐漸建立\",\n",
    "                \"共同目標團結了大家\",\n",
    "            ],\n",
    "            \"plot_advancement\": [\n",
    "                \"故事朝著積極的方向發展\",\n",
    "                \"新的機會出現了\",\n",
    "                \"希望的曙光照耀著前路\",\n",
    "                \"智慧指引著行動\",\n",
    "                \"勇氣克服了困難\",\n",
    "            ],\n",
    "            \"world_building\": [\n",
    "                \"世界充滿了奇妙的景色\",\n",
    "                \"神秘而美麗的地方等待探索\",\n",
    "                \"古老的智慧隱藏在各處\",\n",
    "                \"自然的力量帶來和諧\",\n",
    "                \"文明展現出進步的光輝\",\n",
    "            ],\n",
    "        }\n",
    "\n",
    "    def generate_safe_narrative(\n",
    "        self, prompt: str, context: Dict, max_attempts: int = 3\n",
    "    ) -> Dict:\n",
    "        \"\"\"Generate narrative content with safety checking and fallbacks\"\"\"\n",
    "        result = {\n",
    "            \"final_text\": \"\",\n",
    "            \"safety_report\": {},\n",
    "            \"generation_attempts\": 0,\n",
    "            \"safety_fallback_used\": False,\n",
    "        }\n",
    "\n",
    "        for attempt in range(max_attempts):\n",
    "            result[\"generation_attempts\"] = attempt + 1\n",
    "\n",
    "            # Generate content (simplified - would use LLM in practice)\n",
    "            generated_text = self._generate_text_snippet(prompt, context)\n",
    "\n",
    "            # Check safety\n",
    "            moderated_text, is_safe, safety_report = self.moderator.moderate_content(\n",
    "                generated_text\n",
    "            )\n",
    "            result[\"safety_report\"] = safety_report\n",
    "\n",
    "            if is_safe:\n",
    "                result[\"final_text\"] = moderated_text\n",
    "                return result\n",
    "\n",
    "            # If not safe, try to find alternative narrative\n",
    "            if attempt < max_attempts - 1:\n",
    "                prompt = self._create_safer_prompt(prompt, safety_report)\n",
    "            else:\n",
    "                # Final fallback to safe alternatives\n",
    "                result[\"final_text\"] = self._get_safe_fallback(context)\n",
    "                result[\"safety_fallback_used\"] = True\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _generate_text_snippet(self, prompt: str, context: Dict) -> str:\n",
    "        \"\"\"Simplified text generation - replace with actual LLM call\"\"\"\n",
    "        # This is a placeholder - in real implementation, would call LLM\n",
    "        templates = [\n",
    "            f\"在{context.get('location', '某處')}，{prompt}的情況發展為...\",\n",
    "            f\"角色面對{prompt}的挑戰時，決定採取行動...\",\n",
    "            f\"故事在{prompt}的轉折點上，展現出新的可能性...\",\n",
    "        ]\n",
    "        return np.random.choice(templates)\n",
    "\n",
    "    def _create_safer_prompt(self, original_prompt: str, safety_report: Dict) -> str:\n",
    "        \"\"\"Modify prompt to generate safer content\"\"\"\n",
    "        violations = safety_report.get(\"violations\", [])\n",
    "\n",
    "        safe_modifiers = {\n",
    "            ContentType.VIOLENCE: \"以和平的方式\",\n",
    "            ContentType.SEXUAL: \"以純潔的友誼\",\n",
    "            ContentType.PROFANITY: \"以禮貌的對話\",\n",
    "            ContentType.HATE_SPEECH: \"以相互尊重\",\n",
    "        }\n",
    "\n",
    "        for violation in violations:\n",
    "            if violation in safe_modifiers:\n",
    "                original_prompt = f\"{safe_modifiers[violation]}, {original_prompt}\"\n",
    "\n",
    "        return f\"請以適合全年齡的方式描述: {original_prompt}\"\n",
    "\n",
    "    def _get_safe_fallback(self, context: Dict) -> str:\n",
    "        \"\"\"Get safe fallback content when generation fails safety check\"\"\"\n",
    "        context_type = context.get(\"scene_type\", \"plot_advancement\")\n",
    "        alternatives = self.safe_alternatives_db.get(\n",
    "            context_type, self.safe_alternatives_db[\"plot_advancement\"]\n",
    "        )\n",
    "        return np.random.choice(alternatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c469cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 5: Game Integration Example\n",
    "class SafeGameEngine:\n",
    "    \"\"\"Text adventure game engine with integrated safety moderation\"\"\"\n",
    "\n",
    "    def __init__(self, age_rating: AgeRating = AgeRating.TEEN):\n",
    "        # Configure safety filter based on age rating\n",
    "        if age_rating == AgeRating.GENERAL:\n",
    "            blocked_types = list(ContentType)  # Block everything\n",
    "            threshold = 0.1\n",
    "        elif age_rating == AgeRating.TEEN:\n",
    "            blocked_types = [\n",
    "                ContentType.SEXUAL,\n",
    "                ContentType.PROFANITY,\n",
    "                ContentType.HATE_SPEECH,\n",
    "            ]\n",
    "            threshold = 0.3\n",
    "        elif age_rating == AgeRating.MATURE:\n",
    "            blocked_types = [ContentType.SEXUAL, ContentType.HATE_SPEECH]\n",
    "            threshold = 0.5\n",
    "        else:  # ADULT\n",
    "            blocked_types = [ContentType.HATE_SPEECH]\n",
    "            threshold = 0.7\n",
    "\n",
    "        filter_config = ContentFilter(\n",
    "            age_rating=age_rating,\n",
    "            blocked_types=blocked_types,\n",
    "            severity_threshold=threshold,\n",
    "            replacement_style=\"euphemistic\",\n",
    "        )\n",
    "\n",
    "        self.moderator = SafetyModerator(filter_config)\n",
    "        self.narrator = SafeGameNarrator(self.moderator)\n",
    "        self.content_log = []  # Log for monitoring\n",
    "\n",
    "    def process_player_action(self, action: str, game_state: Dict) -> Dict:\n",
    "        \"\"\"Process player action with safety checks\"\"\"\n",
    "        # First check if player input is appropriate\n",
    "        input_moderated, input_safe, input_report = self.moderator.moderate_content(\n",
    "            action\n",
    "        )\n",
    "\n",
    "        if not input_safe:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"message\": \"抱歉，該行動不被允許。請嘗試其他方式。\",\n",
    "                \"safety_report\": input_report,\n",
    "            }\n",
    "\n",
    "        # Generate narrative response\n",
    "        context = {\n",
    "            \"location\": game_state.get(\"current_location\", \"未知地點\"),\n",
    "            \"scene_type\": game_state.get(\"scene_type\", \"plot_advancement\"),\n",
    "            \"tone\": game_state.get(\"narrative_tone\", \"neutral\"),\n",
    "            \"genre\": \"文字冒險遊戲\",\n",
    "        }\n",
    "\n",
    "        narrative_result = self.narrator.generate_safe_narrative(\n",
    "            prompt=f\"玩家執行了動作: {input_moderated}\", context=context\n",
    "        )\n",
    "\n",
    "        # Log the interaction for monitoring\n",
    "        self.content_log.append(\n",
    "            {\n",
    "                \"timestamp\": str(np.datetime64(\"now\")),\n",
    "                \"player_input\": action,\n",
    "                \"input_safe\": input_safe,\n",
    "                \"narrative_safe\": not narrative_result[\"safety_fallback_used\"],\n",
    "                \"age_rating\": self.moderator.config.age_rating.value,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"narrative\": narrative_result[\"final_text\"],\n",
    "            \"safety_report\": narrative_result[\"safety_report\"],\n",
    "            \"fallback_used\": narrative_result[\"safety_fallback_used\"],\n",
    "        }\n",
    "\n",
    "    def get_safety_statistics(self) -> Dict:\n",
    "        \"\"\"Get safety moderation statistics\"\"\"\n",
    "        if not self.content_log:\n",
    "            return {\"total_interactions\": 0}\n",
    "\n",
    "        total = len(self.content_log)\n",
    "        safe_inputs = sum(1 for log in self.content_log if log[\"input_safe\"])\n",
    "        safe_narratives = sum(1 for log in self.content_log if log[\"narrative_safe\"])\n",
    "\n",
    "        return {\n",
    "            \"total_interactions\": total,\n",
    "            \"safe_input_rate\": safe_inputs / total,\n",
    "            \"safe_narrative_rate\": safe_narratives / total,\n",
    "            \"current_age_rating\": self.moderator.config.age_rating.value,\n",
    "            \"blocked_content_types\": [\n",
    "                ct.value for ct in self.moderator.config.blocked_types\n",
    "            ],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e95b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 6: Smoke Test\n",
    "print(\"=== 內容安全系統煙霧測試 ===\\n\")\n",
    "\n",
    "# Test different age ratings\n",
    "test_content = [\n",
    "    \"角色拔出劍來攻擊敵人\",  # Mild violence\n",
    "    \"他們互相親吻擁抱\",  # Mild sexual\n",
    "    \"這個混蛋真是可惡\",  # Profanity\n",
    "    \"探索神秘的森林\",  # Safe content\n",
    "]\n",
    "\n",
    "age_ratings = [AgeRating.GENERAL, AgeRating.TEEN, AgeRating.MATURE]\n",
    "\n",
    "for rating in age_ratings:\n",
    "    print(f\"Age Rating: {rating.value}\")\n",
    "    game = SafeGameEngine(rating)\n",
    "\n",
    "    for content in test_content:\n",
    "        result = game.process_player_action(\n",
    "            content,\n",
    "            {\"current_location\": \"測試環境\", \"scene_type\": \"conflict_resolution\"},\n",
    "        )\n",
    "\n",
    "        status = \"✅ 允許\" if result[\"success\"] else \"❌ 被阻擋\"\n",
    "        print(f\"  '{content}' -> {status}\")\n",
    "\n",
    "    stats = game.get_safety_statistics()\n",
    "    print(\n",
    "        f\"  安全率: 輸入 {stats['safe_input_rate']:.1%}, 敘述 {stats['safe_narrative_rate']:.1%}\\n\"\n",
    "    )\n",
    "\n",
    "# Test content detection accuracy\n",
    "print(\"=== 內容檢測準確性測試 ===\")\n",
    "detector = AdvancedContentDetector()\n",
    "\n",
    "test_cases = [\n",
    "    (\"遊戲中的角色進行了激烈的戰鬥\", {\"genre\": \"遊戲\"}),\n",
    "    (\"現實中發生了暴力事件\", {\"genre\": \"新聞\"}),\n",
    "    (\"教學演示如何安全地使用工具\", {\"genre\": \"教育\"}),\n",
    "]\n",
    "\n",
    "for text, context in test_cases:\n",
    "    analysis = detector.analyze_with_context(text, context)\n",
    "    print(f\"文本: '{text}'\")\n",
    "    print(f\"情境: {context}\")\n",
    "    print(f\"基礎分數: {analysis['base_scores']}\")\n",
    "    print(f\"情境調整: {analysis['context_modifier']:.2f}\")\n",
    "    print(f\"最終分數: {analysis['final_scores']}\\n\")\n",
    "\n",
    "print(\"煙霧測試完成 ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ad113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 年齡分級對應的安全設定\n",
    "SAFETY_CONFIGS = {\n",
    "    AgeRating.GENERAL: {\n",
    "        \"blocked_types\": list(ContentType),  # 全部封鎖\n",
    "        \"threshold\": 0.1,\n",
    "        \"replacement_style\": \"alternative\",\n",
    "    },\n",
    "    AgeRating.TEEN: {\n",
    "        \"blocked_types\": [\n",
    "            ContentType.SEXUAL,\n",
    "            ContentType.PROFANITY,\n",
    "            ContentType.HATE_SPEECH,\n",
    "        ],\n",
    "        \"threshold\": 0.3,\n",
    "        \"replacement_style\": \"euphemistic\",\n",
    "    },\n",
    "    AgeRating.MATURE: {\n",
    "        \"blocked_types\": [ContentType.SEXUAL, ContentType.HATE_SPEECH],\n",
    "        \"threshold\": 0.5,\n",
    "        \"replacement_style\": \"censored\",\n",
    "    },\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
