{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b41f80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb71 | WebSocket ‰∏≤ÊµÅËº∏Âá∫ÂØ¶‰Ωú\n",
    "# Goal: Real-time streaming LLM responses via WebSocket\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (Ë§áË£ΩÂà∞ÊØèÊú¨ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2777b864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Dependencies & Imports\n",
    "import json\n",
    "import asyncio\n",
    "import uuid\n",
    "from typing import Dict, Set, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "from fastapi import FastAPI, WebSocket, WebSocketDisconnect\n",
    "from fastapi.responses import HTMLResponse\n",
    "import uvicorn\n",
    "\n",
    "# For demo purposes - in production, import from shared_utils\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898e1516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: WebSocket Protocol Design\n",
    "\"\"\"\n",
    "WebSocket Message Protocol:\n",
    "\n",
    "Client ‚Üí Server:\n",
    "{\n",
    "  \"type\": \"generate\",\n",
    "  \"id\": \"unique-request-id\",\n",
    "  \"data\": {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"temperature\": 0.7\n",
    "  }\n",
    "}\n",
    "\n",
    "{\n",
    "  \"type\": \"cancel\",\n",
    "  \"id\": \"request-id-to-cancel\"\n",
    "}\n",
    "\n",
    "Server ‚Üí Client:\n",
    "{\n",
    "  \"type\": \"token\",\n",
    "  \"id\": \"request-id\",\n",
    "  \"data\": {\"token\": \"Hello\", \"is_final\": false}\n",
    "}\n",
    "\n",
    "{\n",
    "  \"type\": \"done\",\n",
    "  \"id\": \"request-id\",\n",
    "  \"data\": {\"total_tokens\": 127, \"duration_ms\": 5420}\n",
    "}\n",
    "\n",
    "{\n",
    "  \"type\": \"error\",\n",
    "  \"id\": \"request-id\",\n",
    "  \"data\": {\"error\": \"Generation failed\", \"code\": \"MODEL_ERROR\"}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class WSMessage:\n",
    "    @staticmethod\n",
    "    def token(request_id: str, token: str, is_final: bool = False):\n",
    "        return {\n",
    "            \"type\": \"token\",\n",
    "            \"id\": request_id,\n",
    "            \"data\": {\"token\": token, \"is_final\": is_final},\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def done(request_id: str, total_tokens: int, duration_ms: float):\n",
    "        return {\n",
    "            \"type\": \"done\",\n",
    "            \"id\": request_id,\n",
    "            \"data\": {\"total_tokens\": total_tokens, \"duration_ms\": duration_ms},\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def error(request_id: str, error: str, code: str = \"UNKNOWN\"):\n",
    "        return {\n",
    "            \"type\": \"error\",\n",
    "            \"id\": request_id,\n",
    "            \"data\": {\"error\": error, \"code\": code},\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c979f213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: LLM Adapter Integration (Streaming-enabled)\n",
    "class StreamingLLMAdapter:\n",
    "    def __init__(self, model_id: str = \"Qwen/Qwen2.5-7B-Instruct\"):\n",
    "        print(f\"Loading {model_id}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=\"auto\",\n",
    "            load_in_4bit=True,  # Low VRAM option\n",
    "        )\n",
    "\n",
    "        # Pad token for batch processing\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        print(f\"‚úì Model loaded on {self.model.device}\")\n",
    "\n",
    "    def format_messages(self, messages):\n",
    "        # Simple chat template - adapt to model's format\n",
    "        formatted = \"\"\n",
    "        for msg in messages:\n",
    "            role = msg.get(\"role\", \"user\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            if role == \"system\":\n",
    "                formatted += f\"System: {content}\\n\"\n",
    "            elif role == \"user\":\n",
    "                formatted += f\"User: {content}\\n\"\n",
    "            elif role == \"assistant\":\n",
    "                formatted += f\"Assistant: {content}\\n\"\n",
    "        formatted += \"Assistant: \"\n",
    "        return formatted\n",
    "\n",
    "    async def generate_stream(\n",
    "        self, messages, max_new_tokens=256, temperature=0.7, stop_event=None\n",
    "    ):\n",
    "        \"\"\"Generate tokens with async streaming\"\"\"\n",
    "        try:\n",
    "            prompt = self.format_messages(messages)\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "            # TextIteratorStreamer for token-by-token output\n",
    "            streamer = TextIteratorStreamer(\n",
    "                self.tokenizer, skip_prompt=True, skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            generation_kwargs = {\n",
    "                **inputs,\n",
    "                \"max_new_tokens\": max_new_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"do_sample\": True,\n",
    "                \"streamer\": streamer,\n",
    "                \"pad_token_id\": self.tokenizer.eos_token_id,\n",
    "            }\n",
    "\n",
    "            # Run generation in separate thread\n",
    "            def generate():\n",
    "                self.model.generate(**generation_kwargs)\n",
    "\n",
    "            thread = Thread(target=generate)\n",
    "            thread.start()\n",
    "\n",
    "            # Stream tokens\n",
    "            for token in streamer:\n",
    "                if stop_event and stop_event.is_set():\n",
    "                    print(\"Generation cancelled by stop_event\")\n",
    "                    break\n",
    "                yield token\n",
    "\n",
    "            thread.join()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Generation error: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de8d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: WebSocket Connection Manager\n",
    "class ConnectionManager:\n",
    "    def __init__(self):\n",
    "        self.active_connections: Dict[str, WebSocket] = {}\n",
    "        self.active_generations: Dict[str, asyncio.Event] = {}\n",
    "\n",
    "    async def connect(self, websocket: WebSocket, client_id: str):\n",
    "        await websocket.accept()\n",
    "        self.active_connections[client_id] = websocket\n",
    "        print(f\"Client {client_id} connected. Total: {len(self.active_connections)}\")\n",
    "\n",
    "    def disconnect(self, client_id: str):\n",
    "        # Cancel any active generations\n",
    "        if client_id in self.active_generations:\n",
    "            self.active_generations[client_id].set()\n",
    "            del self.active_generations[client_id]\n",
    "\n",
    "        if client_id in self.active_connections:\n",
    "            del self.active_connections[client_id]\n",
    "        print(f\"Client {client_id} disconnected. Total: {len(self.active_connections)}\")\n",
    "\n",
    "    async def send_message(self, client_id: str, message: dict):\n",
    "        if client_id in self.active_connections:\n",
    "            try:\n",
    "                await self.active_connections[client_id].send_text(json.dumps(message))\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to send to {client_id}: {e}\")\n",
    "                self.disconnect(client_id)\n",
    "\n",
    "    def cancel_generation(self, client_id: str, request_id: str):\n",
    "        if client_id in self.active_generations:\n",
    "            self.active_generations[client_id].set()\n",
    "            print(f\"Cancelled generation {request_id} for client {client_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae294a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: FastAPI App with WebSocket Endpoint\n",
    "app = FastAPI(title=\"Streaming LLM API\", version=\"1.0.0\")\n",
    "\n",
    "# Global instances\n",
    "llm_adapter = None  # Will be initialized when needed\n",
    "connection_manager = ConnectionManager()\n",
    "\n",
    "\n",
    "def get_llm_adapter():\n",
    "    global llm_adapter\n",
    "    if llm_adapter is None:\n",
    "        llm_adapter = StreamingLLMAdapter()\n",
    "    return llm_adapter\n",
    "\n",
    "\n",
    "@app.websocket(\"/ws/{client_id}\")\n",
    "async def websocket_endpoint(websocket: WebSocket, client_id: str):\n",
    "    await connection_manager.connect(websocket, client_id)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Receive message from client\n",
    "            data = await websocket.receive_text()\n",
    "            message = json.loads(data)\n",
    "\n",
    "            message_type = message.get(\"type\")\n",
    "            request_id = message.get(\"id\", str(uuid.uuid4()))\n",
    "\n",
    "            if message_type == \"generate\":\n",
    "                await handle_generate(client_id, request_id, message.get(\"data\", {}))\n",
    "\n",
    "            elif message_type == \"cancel\":\n",
    "                connection_manager.cancel_generation(client_id, request_id)\n",
    "                await connection_manager.send_message(\n",
    "                    client_id, {\"type\": \"cancelled\", \"id\": request_id}\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                await connection_manager.send_message(\n",
    "                    client_id,\n",
    "                    WSMessage.error(request_id, \"Unknown message type\", \"INVALID_TYPE\"),\n",
    "                )\n",
    "\n",
    "    except WebSocketDisconnect:\n",
    "        connection_manager.disconnect(client_id)\n",
    "    except Exception as e:\n",
    "        print(f\"WebSocket error for {client_id}: {e}\")\n",
    "        connection_manager.disconnect(client_id)\n",
    "\n",
    "\n",
    "async def handle_generate(client_id: str, request_id: str, data: dict):\n",
    "    \"\"\"Handle generation request with streaming\"\"\"\n",
    "    try:\n",
    "        # Validate input\n",
    "        messages = data.get(\"messages\", [])\n",
    "        if not messages:\n",
    "            await connection_manager.send_message(\n",
    "                client_id,\n",
    "                WSMessage.error(request_id, \"No messages provided\", \"MISSING_MESSAGES\"),\n",
    "            )\n",
    "            return\n",
    "\n",
    "        max_new_tokens = min(data.get(\"max_new_tokens\", 256), 512)  # Cap for safety\n",
    "        temperature = data.get(\"temperature\", 0.7)\n",
    "\n",
    "        # Create stop event for this generation\n",
    "        stop_event = asyncio.Event()\n",
    "        connection_manager.active_generations[client_id] = stop_event\n",
    "\n",
    "        # Get LLM adapter\n",
    "        adapter = get_llm_adapter()\n",
    "\n",
    "        # Start generation with timing\n",
    "        start_time = time.time()\n",
    "        token_count = 0\n",
    "\n",
    "        async for token in adapter.generate_stream(\n",
    "            messages,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            stop_event=stop_event,\n",
    "        ):\n",
    "            if stop_event.is_set():\n",
    "                break\n",
    "\n",
    "            # Send token to client\n",
    "            await connection_manager.send_message(\n",
    "                client_id, WSMessage.token(request_id, token)\n",
    "            )\n",
    "            token_count += 1\n",
    "\n",
    "        # Send completion message\n",
    "        duration_ms = (time.time() - start_time) * 1000\n",
    "        await connection_manager.send_message(\n",
    "            client_id, WSMessage.done(request_id, token_count, duration_ms)\n",
    "        )\n",
    "\n",
    "        # Cleanup\n",
    "        if client_id in connection_manager.active_generations:\n",
    "            del connection_manager.active_generations[client_id]\n",
    "\n",
    "    except Exception as e:\n",
    "        await connection_manager.send_message(\n",
    "            client_id, WSMessage.error(request_id, str(e), \"GENERATION_ERROR\")\n",
    "        )\n",
    "\n",
    "        # Cleanup on error\n",
    "        if client_id in connection_manager.active_generations:\n",
    "            del connection_manager.active_generations[client_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdf89f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Client-side HTML/JavaScript Demo\n",
    "@app.get(\"/\")\n",
    "async def get_demo_page():\n",
    "    html_content = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>WebSocket Streaming Demo</title>\n",
    "    <style>\n",
    "        body { font-family: Arial, sans-serif; margin: 20px; }\n",
    "        #output { border: 1px solid #ccc; padding: 10px; height: 300px; overflow-y: auto; white-space: pre-wrap; }\n",
    "        button { margin: 5px; padding: 10px; }\n",
    "        input, textarea { width: 100%; margin: 5px 0; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>WebSocket Streaming LLM Demo</h1>\n",
    "\n",
    "    <div>\n",
    "        <label>Message:</label>\n",
    "        <textarea id=\"messageInput\" rows=\"3\" placeholder=\"Type your message here...\">Ë´ã‰ªãÁ¥π‰ªÄÈ∫ºÊòØ RAGÔºü</textarea>\n",
    "    </div>\n",
    "\n",
    "    <div>\n",
    "        <label>Max Tokens:</label>\n",
    "        <input type=\"number\" id=\"maxTokens\" value=\"128\" min=\"1\" max=\"512\">\n",
    "\n",
    "        <label>Temperature:</label>\n",
    "        <input type=\"number\" id=\"temperature\" value=\"0.7\" min=\"0\" max=\"2\" step=\"0.1\">\n",
    "    </div>\n",
    "\n",
    "    <div>\n",
    "        <button onclick=\"connect()\">Connect</button>\n",
    "        <button onclick=\"sendMessage()\">Send Message</button>\n",
    "        <button onclick=\"cancelGeneration()\">Cancel</button>\n",
    "        <button onclick=\"disconnect()\">Disconnect</button>\n",
    "        <button onclick=\"clearOutput()\">Clear</button>\n",
    "    </div>\n",
    "\n",
    "    <div>\n",
    "        <label>Status: <span id=\"status\">Disconnected</span></label>\n",
    "    </div>\n",
    "\n",
    "    <div id=\"output\"></div>\n",
    "\n",
    "    <script>\n",
    "        let ws = null;\n",
    "        let clientId = 'client-' + Math.random().toString(36).substr(2, 9);\n",
    "        let currentRequestId = null;\n",
    "\n",
    "        function connect() {\n",
    "            if (ws) {\n",
    "                ws.close();\n",
    "            }\n",
    "\n",
    "            ws = new WebSocket(`ws://localhost:8000/ws/${clientId}`);\n",
    "\n",
    "            ws.onopen = function() {\n",
    "                document.getElementById('status').textContent = 'Connected';\n",
    "                appendOutput('[SYSTEM] Connected to WebSocket\\\\n');\n",
    "            };\n",
    "\n",
    "            ws.onmessage = function(event) {\n",
    "                const message = JSON.parse(event.data);\n",
    "                handleMessage(message);\n",
    "            };\n",
    "\n",
    "            ws.onclose = function() {\n",
    "                document.getElementById('status').textContent = 'Disconnected';\n",
    "                appendOutput('[SYSTEM] Disconnected\\\\n');\n",
    "            };\n",
    "\n",
    "            ws.onerror = function(error) {\n",
    "                appendOutput(`[ERROR] ${error}\\\\n`);\n",
    "            };\n",
    "        }\n",
    "\n",
    "        function handleMessage(message) {\n",
    "            const { type, id, data } = message;\n",
    "\n",
    "            switch(type) {\n",
    "                case 'token':\n",
    "                    appendOutput(data.token);\n",
    "                    break;\n",
    "\n",
    "                case 'done':\n",
    "                    appendOutput(`\\\\n[DONE] ${data.total_tokens} tokens in ${data.duration_ms.toFixed(1)}ms\\\\n\\\\n`);\n",
    "                    currentRequestId = null;\n",
    "                    break;\n",
    "\n",
    "                case 'error':\n",
    "                    appendOutput(`\\\\n[ERROR] ${data.error} (${data.code})\\\\n\\\\n`);\n",
    "                    currentRequestId = null;\n",
    "                    break;\n",
    "\n",
    "                case 'cancelled':\n",
    "                    appendOutput(`\\\\n[CANCELLED] Generation stopped\\\\n\\\\n`);\n",
    "                    currentRequestId = null;\n",
    "                    break;\n",
    "            }\n",
    "        }\n",
    "\n",
    "        function sendMessage() {\n",
    "            if (!ws || ws.readyState !== WebSocket.OPEN) {\n",
    "                alert('Please connect first');\n",
    "                return;\n",
    "            }\n",
    "\n",
    "            const message = document.getElementById('messageInput').value.trim();\n",
    "            if (!message) {\n",
    "                alert('Please enter a message');\n",
    "                return;\n",
    "            }\n",
    "\n",
    "            currentRequestId = 'req-' + Date.now();\n",
    "\n",
    "            const request = {\n",
    "                type: 'generate',\n",
    "                id: currentRequestId,\n",
    "                data: {\n",
    "                    messages: [{ role: 'user', content: message }],\n",
    "                    max_new_tokens: parseInt(document.getElementById('maxTokens').value),\n",
    "                    temperature: parseFloat(document.getElementById('temperature').value)\n",
    "                }\n",
    "            };\n",
    "\n",
    "            appendOutput(`[USER] ${message}\\\\n[ASSISTANT] `);\n",
    "            ws.send(JSON.stringify(request));\n",
    "        }\n",
    "\n",
    "        function cancelGeneration() {\n",
    "            if (currentRequestId && ws && ws.readyState === WebSocket.OPEN) {\n",
    "                ws.send(JSON.stringify({\n",
    "                    type: 'cancel',\n",
    "                    id: currentRequestId\n",
    "                }));\n",
    "            }\n",
    "        }\n",
    "\n",
    "        function disconnect() {\n",
    "            if (ws) {\n",
    "                ws.close();\n",
    "                ws = null;\n",
    "            }\n",
    "        }\n",
    "\n",
    "        function appendOutput(text) {\n",
    "            const output = document.getElementById('output');\n",
    "            output.textContent += text;\n",
    "            output.scrollTop = output.scrollHeight;\n",
    "        }\n",
    "\n",
    "        function clearOutput() {\n",
    "            document.getElementById('output').textContent = '';\n",
    "        }\n",
    "\n",
    "        // Auto-connect on page load\n",
    "        window.onload = function() {\n",
    "            // Don't auto-connect in demo\n",
    "            // connect();\n",
    "        };\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "    \"\"\"\n",
    "    return HTMLResponse(content=html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee549b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Health Check & Info Endpoints\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"active_connections\": len(connection_manager.active_connections),\n",
    "        \"active_generations\": len(connection_manager.active_generations),\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "    }\n",
    "\n",
    "\n",
    "@app.get(\"/info\")\n",
    "async def api_info():\n",
    "    return {\n",
    "        \"name\": \"Streaming LLM WebSocket API\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"endpoints\": {\"websocket\": \"/ws/{client_id}\", \"demo\": \"/\", \"health\": \"/health\"},\n",
    "        \"message_types\": [\"generate\", \"cancel\"],\n",
    "        \"response_types\": [\"token\", \"done\", \"error\", \"cancelled\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d35dd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Smoke Test Function\n",
    "def run_server(host=\"127.0.0.1\", port=8000):\n",
    "    \"\"\"Start the WebSocket server\"\"\"\n",
    "    print(f\"üöÄ Starting WebSocket streaming server...\")\n",
    "    print(f\"üì° WebSocket endpoint: ws://{host}:{port}/ws/{{client_id}}\")\n",
    "    print(f\"üåê Demo page: http://{host}:{port}/\")\n",
    "    print(f\"üíö Health check: http://{host}:{port}/health\")\n",
    "    print(\"\\nPress Ctrl+C to stop\")\n",
    "\n",
    "    uvicorn.run(\n",
    "        app,\n",
    "        host=host,\n",
    "        port=port,\n",
    "        reload=False,  # Disable in production\n",
    "        access_log=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5a01ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Smoke Test - Start Server\n",
    "if __name__ == \"__main__\":\n",
    "    # For notebook testing - comment out to avoid blocking\n",
    "    print(\"‚úÖ WebSocket streaming server ready!\")\n",
    "    print(\"Uncomment the line below to start the server:\")\n",
    "    print(\"# run_server()\")\n",
    "\n",
    "    # Uncomment to start server:\n",
    "    # run_server()\n",
    "\n",
    "\"\"\"\n",
    "=== Smoke Test Instructions ===\n",
    "\n",
    "1. Uncomment `run_server()` above and run this cell\n",
    "2. Open browser to http://localhost:8000/\n",
    "3. Click \"Connect\" button\n",
    "4. Type a message and click \"Send Message\"\n",
    "5. Watch real-time streaming tokens appear\n",
    "6. Test \"Cancel\" button during generation\n",
    "7. Check http://localhost:8000/health for metrics\n",
    "\n",
    "Expected output:\n",
    "- WebSocket connection established\n",
    "- Streaming tokens appear one by one\n",
    "- Final stats: token count + duration\n",
    "- Cancellation works mid-generation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc4a358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ê∏¨Ë©¶Áî®ÊúÄÂ∞èÂÆ¢Êà∂Á´Ø\n",
    "import asyncio\n",
    "import websockets\n",
    "import json\n",
    "\n",
    "\n",
    "async def test_websocket():\n",
    "    uri = \"ws://localhost:8000/ws/test-client\"\n",
    "    async with websockets.connect(uri) as websocket:\n",
    "        # Send generation request\n",
    "        request = {\n",
    "            \"type\": \"generate\",\n",
    "            \"id\": \"test-001\",\n",
    "            \"data\": {\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n",
    "                \"max_new_tokens\": 50,\n",
    "                \"temperature\": 0.7,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        await websocket.send(json.dumps(request))\n",
    "\n",
    "        # Receive streaming response\n",
    "        tokens = []\n",
    "        async for message in websocket:\n",
    "            data = json.loads(message)\n",
    "            print(f\"Received: {data}\")\n",
    "\n",
    "            if data[\"type\"] == \"token\":\n",
    "                tokens.append(data[\"data\"][\"token\"])\n",
    "            elif data[\"type\"] == \"done\":\n",
    "                print(f\"‚úÖ Generation complete: {''.join(tokens)}\")\n",
    "                break\n",
    "            elif data[\"type\"] == \"error\":\n",
    "                print(f\"‚ùå Error: {data['data']['error']}\")\n",
    "                break\n",
    "\n",
    "\n",
    "# asyncio.run(test_websocket())  # Uncomment to test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
