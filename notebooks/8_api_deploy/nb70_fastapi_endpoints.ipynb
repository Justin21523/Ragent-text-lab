{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44513988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb70_fastapi_endpoints.ipynb\n",
    "# FastAPI 端點實作：Chat/RAG/Agent/Game 服務化\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (複製到每本 notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dd73f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 2: 依賴導入與基礎設定\n",
    "from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List, Dict, Optional, Any\n",
    "import uvicorn\n",
    "import asyncio\n",
    "import time\n",
    "import logging\n",
    "from contextlib import asynccontextmanager\n",
    "import json\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Global state for models (will be initialized on startup)\n",
    "app_state = {\n",
    "    \"llm_adapter\": None,\n",
    "    \"rag_retriever\": None,\n",
    "    \"agent_orchestrator\": None,\n",
    "    \"game_engine\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8225f81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 3: Pydantic 請求/回應模型\n",
    "# Request/Response schemas\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str = Field(..., min_length=1, max_length=4096)\n",
    "    max_tokens: Optional[int] = Field(default=256, ge=1, le=2048)\n",
    "    temperature: Optional[float] = Field(default=0.7, ge=0.0, le=2.0)\n",
    "    conversation_id: Optional[str] = None\n",
    "\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    response: str\n",
    "    conversation_id: str\n",
    "    tokens_used: int\n",
    "    latency_ms: float\n",
    "\n",
    "\n",
    "class RAGRequest(BaseModel):\n",
    "    query: str = Field(..., min_length=1, max_length=1024)\n",
    "    domain: Optional[str] = Field(default=\"general\")\n",
    "    top_k: Optional[int] = Field(default=5, ge=1, le=20)\n",
    "    use_rerank: Optional[bool] = True\n",
    "\n",
    "\n",
    "class RAGResponse(BaseModel):\n",
    "    answer: str\n",
    "    sources: List[Dict[str, Any]]\n",
    "    query_embedding_time: float\n",
    "    retrieval_time: float\n",
    "    generation_time: float\n",
    "\n",
    "\n",
    "class AgentRequest(BaseModel):\n",
    "    task: str = Field(..., min_length=1, max_length=2048)\n",
    "    mode: str = Field(\n",
    "        default=\"research_write\",\n",
    "        pattern=\"^(research|plan|write|review|research_write)$\",\n",
    "    )\n",
    "    max_iterations: Optional[int] = Field(default=3, ge=1, le=10)\n",
    "\n",
    "\n",
    "class AgentResponse(BaseModel):\n",
    "    result: str\n",
    "    execution_log: List[Dict[str, Any]]\n",
    "    total_time: float\n",
    "    iterations_used: int\n",
    "\n",
    "\n",
    "class GameRequest(BaseModel):\n",
    "    action: str = Field(..., min_length=1, max_length=512)\n",
    "    session_id: Optional[str] = None\n",
    "    save_state: Optional[bool] = False\n",
    "\n",
    "\n",
    "class GameResponse(BaseModel):\n",
    "    narrative: str\n",
    "    choices: List[str]\n",
    "    game_state: Dict[str, Any]\n",
    "    session_id: str\n",
    "\n",
    "\n",
    "class HealthResponse(BaseModel):\n",
    "    status: str\n",
    "    version: str\n",
    "    models_loaded: Dict[str, bool]\n",
    "    uptime_seconds: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1429722",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 4: 模型初始化邏輯\n",
    "# Mock implementations - replace with actual shared_utils imports\n",
    "class MockLLMAdapter:\n",
    "    def __init__(self):\n",
    "        self.model_name = \"Mock-7B-Instruct\"\n",
    "\n",
    "    def generate(self, messages, max_tokens=256, temperature=0.7):\n",
    "        # Simulate processing time\n",
    "        time.sleep(0.1)\n",
    "        user_msg = messages[-1][\"content\"] if messages else \"\"\n",
    "        return f\"Mock response to: {user_msg[:50]}...\"\n",
    "\n",
    "    def count_tokens(self, text):\n",
    "        return len(text.split()) * 1.3  # rough estimate\n",
    "\n",
    "\n",
    "class MockRAGRetriever:\n",
    "    def search(self, query, top_k=5):\n",
    "        # Mock retrieval results\n",
    "        return [\n",
    "            {\n",
    "                \"text\": f\"Retrieved document {i+1} for query: {query[:30]}...\",\n",
    "                \"meta\": {\"source_id\": f\"doc_{i+1}\", \"score\": 0.9 - i * 0.1},\n",
    "                \"score\": 0.9 - i * 0.1,\n",
    "            }\n",
    "            for i in range(min(top_k, 3))\n",
    "        ]\n",
    "\n",
    "\n",
    "class MockAgentOrchestrator:\n",
    "    def execute_task(self, task, mode=\"research_write\", max_iterations=3):\n",
    "        # Mock agent execution\n",
    "        execution_log = [\n",
    "            {\"step\": 1, \"role\": \"researcher\", \"action\": f\"Research: {task[:30]}...\"},\n",
    "            {\"step\": 2, \"role\": \"writer\", \"action\": \"Generated response\"},\n",
    "        ]\n",
    "        return f\"Agent result for task: {task[:50]}...\", execution_log\n",
    "\n",
    "\n",
    "class MockGameEngine:\n",
    "    def __init__(self):\n",
    "        self.sessions = {}\n",
    "\n",
    "    def process_action(self, action, session_id=None):\n",
    "        if not session_id:\n",
    "            session_id = f\"game_{int(time.time())}\"\n",
    "\n",
    "        # Mock game state\n",
    "        state = self.sessions.get(\n",
    "            session_id, {\"hp\": 100, \"level\": 1, \"location\": \"forest\"}\n",
    "        )\n",
    "\n",
    "        narrative = f\"You decided to {action}. The forest grows darker...\"\n",
    "        choices = [\"Go north\", \"Rest here\", \"Check inventory\"]\n",
    "\n",
    "        self.sessions[session_id] = state\n",
    "        return narrative, choices, state, session_id\n",
    "\n",
    "\n",
    "# Initialize mock models (replace with real implementations)\n",
    "def initialize_models():\n",
    "    \"\"\"Initialize all models and components\"\"\"\n",
    "    logger.info(\"Initializing models...\")\n",
    "\n",
    "    app_state[\"llm_adapter\"] = MockLLMAdapter()\n",
    "    app_state[\"rag_retriever\"] = MockRAGRetriever()\n",
    "    app_state[\"agent_orchestrator\"] = MockAgentOrchestrator()\n",
    "    app_state[\"game_engine\"] = MockGameEngine()\n",
    "\n",
    "    logger.info(\"Models initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e110236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 5: FastAPI 應用設定\n",
    "# FastAPI app with lifespan management\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    # Startup\n",
    "    logger.info(\"Starting up FastAPI application...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    initialize_models()\n",
    "\n",
    "    app.state.start_time = start_time\n",
    "    logger.info(f\"Startup completed in {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    yield\n",
    "\n",
    "    # Shutdown\n",
    "    logger.info(\"Shutting down FastAPI application...\")\n",
    "\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"RAGent Text Lab API\",\n",
    "    description=\"Multi-modal text AI API with Chat/RAG/Agent/Game capabilities\",\n",
    "    version=\"1.0.0\",\n",
    "    lifespan=lifespan,\n",
    ")\n",
    "\n",
    "# Add CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # Configure appropriately for production\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Request logging middleware\n",
    "@app.middleware(\"http\")\n",
    "async def log_requests(request, call_next):\n",
    "    start_time = time.time()\n",
    "    response = await call_next(request)\n",
    "    process_time = time.time() - start_time\n",
    "\n",
    "    logger.info(\n",
    "        f\"{request.method} {request.url.path} - \"\n",
    "        f\"Status: {response.status_code} - \"\n",
    "        f\"Time: {process_time:.3f}s\"\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3d0efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 6: Health Check 端點\n",
    "@app.get(\"/health\", response_model=HealthResponse)\n",
    "async def health_check():\n",
    "    \"\"\"健康檢查端點\"\"\"\n",
    "    uptime = time.time() - app.state.start_time\n",
    "\n",
    "    models_loaded = {\n",
    "        \"llm_adapter\": app_state[\"llm_adapter\"] is not None,\n",
    "        \"rag_retriever\": app_state[\"rag_retriever\"] is not None,\n",
    "        \"agent_orchestrator\": app_state[\"agent_orchestrator\"] is not None,\n",
    "        \"game_engine\": app_state[\"game_engine\"] is not None,\n",
    "    }\n",
    "\n",
    "    return HealthResponse(\n",
    "        status=\"healthy\" if all(models_loaded.values()) else \"degraded\",\n",
    "        version=\"1.0.0\",\n",
    "        models_loaded=models_loaded,\n",
    "        uptime_seconds=uptime,\n",
    "    )\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    \"\"\"根端點\"\"\"\n",
    "    return {\"message\": \"RAGent Text Lab API\", \"docs\": \"/docs\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 7: Chat 端點實作\n",
    "@app.post(\"/chat\", response_model=ChatResponse)\n",
    "async def chat_endpoint(request: ChatRequest):\n",
    "    \"\"\"基本對話端點\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        if not app_state[\"llm_adapter\"]:\n",
    "            raise HTTPException(status_code=503, detail=\"LLM adapter not initialized\")\n",
    "\n",
    "        # Prepare messages\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": request.message},\n",
    "        ]\n",
    "\n",
    "        # Generate response\n",
    "        response_text = app_state[\"llm_adapter\"].generate(\n",
    "            messages=messages,\n",
    "            max_tokens=request.max_tokens,\n",
    "            temperature=request.temperature,\n",
    "        )\n",
    "\n",
    "        # Calculate metrics\n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        tokens_used = app_state[\"llm_adapter\"].count_tokens(response_text)\n",
    "        conversation_id = request.conversation_id or f\"chat_{int(time.time())}\"\n",
    "\n",
    "        return ChatResponse(\n",
    "            response=response_text,\n",
    "            conversation_id=conversation_id,\n",
    "            tokens_used=int(tokens_used),\n",
    "            latency_ms=latency_ms,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Chat endpoint error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"Chat processing failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe6233",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 8: RAG 端點實作\n",
    "@app.post(\"/rag\", response_model=RAGResponse)\n",
    "async def rag_endpoint(request: RAGRequest):\n",
    "    \"\"\"RAG 檢索增強回答端點\"\"\"\n",
    "    try:\n",
    "        if not app_state[\"rag_retriever\"] or not app_state[\"llm_adapter\"]:\n",
    "            raise HTTPException(\n",
    "                status_code=503, detail=\"RAG components not initialized\"\n",
    "            )\n",
    "\n",
    "        # Timing for different phases\n",
    "        embed_start = time.time()\n",
    "\n",
    "        # Retrieve relevant documents\n",
    "        retrieval_start = time.time()\n",
    "        retrieved_docs = app_state[\"rag_retriever\"].search(\n",
    "            query=request.query, top_k=request.top_k\n",
    "        )\n",
    "        retrieval_time = time.time() - retrieval_start\n",
    "\n",
    "        # Build context from retrieved documents\n",
    "        context = \"\\n\\n\".join(\n",
    "            [f\"[{i+1}] {doc['text']}\" for i, doc in enumerate(retrieved_docs)]\n",
    "        )\n",
    "\n",
    "        # Generate answer with RAG context\n",
    "        gen_start = time.time()\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Answer based on the provided context. Use citations like [1], [2].\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Question: {request.query}\\n\\nContext:\\n{context}\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        answer = app_state[\"llm_adapter\"].generate(\n",
    "            messages=messages, max_tokens=512, temperature=0.3\n",
    "        )\n",
    "        generation_time = time.time() - gen_start\n",
    "\n",
    "        # Prepare sources for response\n",
    "        sources = [\n",
    "            {\n",
    "                \"id\": i + 1,\n",
    "                \"text\": (\n",
    "                    doc[\"text\"][:200] + \"...\" if len(doc[\"text\"]) > 200 else doc[\"text\"]\n",
    "                ),\n",
    "                \"metadata\": doc[\"meta\"],\n",
    "                \"score\": doc[\"score\"],\n",
    "            }\n",
    "            for i, doc in enumerate(retrieved_docs)\n",
    "        ]\n",
    "\n",
    "        return RAGResponse(\n",
    "            answer=answer,\n",
    "            sources=sources,\n",
    "            query_embedding_time=retrieval_start - embed_start,\n",
    "            retrieval_time=retrieval_time,\n",
    "            generation_time=generation_time,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"RAG endpoint error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"RAG processing failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef5b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 9: Agent 端點實作\n",
    "@app.post(\"/agent\", response_model=AgentResponse)\n",
    "async def agent_endpoint(request: AgentRequest):\n",
    "    \"\"\"多代理協作端點\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        if not app_state[\"agent_orchestrator\"]:\n",
    "            raise HTTPException(\n",
    "                status_code=503, detail=\"Agent orchestrator not initialized\"\n",
    "            )\n",
    "\n",
    "        # Execute agent task\n",
    "        result, execution_log = app_state[\"agent_orchestrator\"].execute_task(\n",
    "            task=request.task, mode=request.mode, max_iterations=request.max_iterations\n",
    "        )\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "\n",
    "        return AgentResponse(\n",
    "            result=result,\n",
    "            execution_log=execution_log,\n",
    "            total_time=total_time,\n",
    "            iterations_used=len(execution_log),\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Agent endpoint error: {str(e)}\")\n",
    "        raise HTTPException(\n",
    "            status_code=500, detail=f\"Agent processing failed: {str(e)}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578a26dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 10: Game 端點實作\n",
    "@app.post(\"/game\", response_model=GameResponse)\n",
    "async def game_endpoint(request: GameRequest):\n",
    "    \"\"\"文字冒險遊戲端點\"\"\"\n",
    "    try:\n",
    "        if not app_state[\"game_engine\"]:\n",
    "            raise HTTPException(status_code=503, detail=\"Game engine not initialized\")\n",
    "\n",
    "        # Process game action\n",
    "        narrative, choices, game_state, session_id = app_state[\"game_engine\"].process_action(\n",
    "            action=request.action,\n",
    "            session_id=request.session_id\n",
    "        )\n",
    "\n",
    "        return GameResponse(\n",
    "            narrative=narrative,\n",
    "            choices=choices,\n",
    "            game_state=game_state,\n",
    "            session_id=session_id\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Game endpoint error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"Game processing failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a5aacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 11: 錯誤處理\n",
    "\n",
    "# Global exception handlers\n",
    "@app.exception_handler(HTTPException)\n",
    "async def http_exception_handler(request, exc):\n",
    "    return JSONResponse(\n",
    "        status_code=exc.status_code,\n",
    "        content={\n",
    "            \"error\": exc.detail,\n",
    "            \"path\": str(request.url.path),\n",
    "            \"method\": request.method,\n",
    "            \"timestamp\": time.time(),\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "@app.exception_handler(Exception)\n",
    "async def general_exception_handler(request, exc):\n",
    "    logger.error(f\"Unhandled exception: {str(exc)}\")\n",
    "    return JSONResponse(\n",
    "        status_code=500,\n",
    "        content={\n",
    "            \"error\": \"Internal server error\",\n",
    "            \"path\": str(request.url.path),\n",
    "            \"method\": request.method,\n",
    "            \"timestamp\": time.time(),\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15075f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 12: Smoke Test - 本地啟動\n",
    "# Smoke test - start server in background for testing\n",
    "import threading\n",
    "import requests\n",
    "import time\n",
    "\n",
    "\n",
    "def start_server():\n",
    "    \"\"\"Start FastAPI server in background thread\"\"\"\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8000, log_level=\"info\")\n",
    "\n",
    "\n",
    "# Start server in background\n",
    "server_thread = threading.Thread(target=start_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "# Wait for server startup\n",
    "time.sleep(3)\n",
    "\n",
    "print(\"🚀 FastAPI server started at http://127.0.0.1:8000\")\n",
    "print(\"📚 API Documentation: http://127.0.0.1:8000/docs\")\n",
    "print(\"🔍 Alternative docs: http://127.0.0.1:8000/redoc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910a4a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 13: API 測試\n",
    "# Test the endpoints\n",
    "base_url = \"http://127.0.0.1:8000\"\n",
    "\n",
    "try:\n",
    "    # Test health check\n",
    "    health_response = requests.get(f\"{base_url}/health\")\n",
    "    print(f\"Health Check: {health_response.status_code}\")\n",
    "    print(f\"Response: {health_response.json()}\\n\")\n",
    "\n",
    "    # Test chat endpoint\n",
    "    chat_payload = {\n",
    "        \"message\": \"Hello, how are you?\",\n",
    "        \"max_tokens\": 100,\n",
    "        \"temperature\": 0.7,\n",
    "    }\n",
    "    chat_response = requests.post(f\"{base_url}/chat\", json=chat_payload)\n",
    "    print(f\"Chat: {chat_response.status_code}\")\n",
    "    print(f\"Response: {chat_response.json()}\\n\")\n",
    "\n",
    "    # Test RAG endpoint\n",
    "    rag_payload = {\"query\": \"What is machine learning?\", \"top_k\": 3}\n",
    "    rag_response = requests.post(f\"{base_url}/rag\", json=rag_payload)\n",
    "    print(f\"RAG: {rag_response.status_code}\")\n",
    "    print(f\"Response: {rag_response.json()}\\n\")\n",
    "\n",
    "    # Test agent endpoint\n",
    "    agent_payload = {\n",
    "        \"task\": \"Research and write about renewable energy\",\n",
    "        \"mode\": \"research_write\",\n",
    "    }\n",
    "    agent_response = requests.post(f\"{base_url}/agent\", json=agent_payload)\n",
    "    print(f\"Agent: {agent_response.status_code}\")\n",
    "    print(f\"Response: {agent_response.json()}\\n\")\n",
    "\n",
    "    # Test game endpoint\n",
    "    game_payload = {\"action\": \"explore the forest\"}\n",
    "    game_response = requests.post(f\"{base_url}/game\", json=game_payload)\n",
    "    print(f\"Game: {game_response.status_code}\")\n",
    "    print(f\"Response: {game_response.json()}\\n\")\n",
    "\n",
    "    print(\"✅ All endpoints responding successfully!\")\n",
    "\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"❌ Server not responding. Make sure it's started.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Test failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e559d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 14: 生產環境配置建議\n",
    "# Production deployment considerations\n",
    "\n",
    "production_config = {\n",
    "    \"server\": {\n",
    "        \"host\": \"0.0.0.0\",\n",
    "        \"port\": 8000,\n",
    "        \"workers\": 4,  # For uvicorn with --workers\n",
    "        \"log_level\": \"warning\",\n",
    "    },\n",
    "    \"security\": {\n",
    "        \"cors_origins\": [\"https://yourdomain.com\"],\n",
    "        \"rate_limit\": \"100/minute\",\n",
    "        \"api_key_required\": True,\n",
    "    },\n",
    "    \"performance\": {\"max_request_size\": \"10MB\", \"timeout\": 300, \"keep_alive\": 75},\n",
    "    \"monitoring\": {\n",
    "        \"health_check_interval\": 30,\n",
    "        \"metrics_enabled\": True,\n",
    "        \"log_requests\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"🔧 Production Configuration:\")\n",
    "print(json.dumps(production_config, indent=2))\n",
    "\n",
    "print(\"\\n📋 Deployment Checklist:\")\n",
    "checklist = [\n",
    "    \"Set environment variables (MODEL_ID, API_KEYS)\",\n",
    "    \"Configure CORS origins properly\",\n",
    "    \"Set up rate limiting middleware\",\n",
    "    \"Add API authentication\",\n",
    "    \"Configure logging aggregation\",\n",
    "    \"Set up health monitoring\",\n",
    "    \"Use HTTPS in production\",\n",
    "    \"Set appropriate resource limits\",\n",
    "]\n",
    "\n",
    "for i, item in enumerate(checklist, 1):\n",
    "    print(f\"{i}. {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed6c331",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 15: 停止伺服器 (清理)\n",
    "# Note: In production, use proper process management\n",
    "print(\"📝 Note: Server is running in background thread.\")\n",
    "print(\"To properly stop in production, use:\")\n",
    "print(\"- uvicorn main:app --reload (development)\")\n",
    "print(\"- gunicorn main:app -w 4 -k uvicorn.workers.UvicornWorker (production)\")\n",
    "print(\"- Docker containers with proper signal handling\")\n",
    "print(\"- Process managers like supervisor or systemd\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
