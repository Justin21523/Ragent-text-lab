{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44513988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb70_fastapi_endpoints.ipynb\n",
    "# FastAPI Á´ØÈªûÂØ¶‰ΩúÔºöChat/RAG/Agent/Game ÊúçÂãôÂåñ\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (Ë§áË£ΩÂà∞ÊØèÊú¨ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dd73f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 2: ‰æùË≥¥Â∞éÂÖ•ËàáÂü∫Á§éË®≠ÂÆö\n",
    "from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List, Dict, Optional, Any\n",
    "import uvicorn\n",
    "import asyncio\n",
    "import time\n",
    "import logging\n",
    "from contextlib import asynccontextmanager\n",
    "import json\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Global state for models (will be initialized on startup)\n",
    "app_state = {\n",
    "    \"llm_adapter\": None,\n",
    "    \"rag_retriever\": None,\n",
    "    \"agent_orchestrator\": None,\n",
    "    \"game_engine\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8225f81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 3: Pydantic Ë´ãÊ±Ç/ÂõûÊáâÊ®°Âûã\n",
    "# Request/Response schemas\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str = Field(..., min_length=1, max_length=4096)\n",
    "    max_tokens: Optional[int] = Field(default=256, ge=1, le=2048)\n",
    "    temperature: Optional[float] = Field(default=0.7, ge=0.0, le=2.0)\n",
    "    conversation_id: Optional[str] = None\n",
    "\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    response: str\n",
    "    conversation_id: str\n",
    "    tokens_used: int\n",
    "    latency_ms: float\n",
    "\n",
    "\n",
    "class RAGRequest(BaseModel):\n",
    "    query: str = Field(..., min_length=1, max_length=1024)\n",
    "    domain: Optional[str] = Field(default=\"general\")\n",
    "    top_k: Optional[int] = Field(default=5, ge=1, le=20)\n",
    "    use_rerank: Optional[bool] = True\n",
    "\n",
    "\n",
    "class RAGResponse(BaseModel):\n",
    "    answer: str\n",
    "    sources: List[Dict[str, Any]]\n",
    "    query_embedding_time: float\n",
    "    retrieval_time: float\n",
    "    generation_time: float\n",
    "\n",
    "\n",
    "class AgentRequest(BaseModel):\n",
    "    task: str = Field(..., min_length=1, max_length=2048)\n",
    "    mode: str = Field(\n",
    "        default=\"research_write\",\n",
    "        pattern=\"^(research|plan|write|review|research_write)$\",\n",
    "    )\n",
    "    max_iterations: Optional[int] = Field(default=3, ge=1, le=10)\n",
    "\n",
    "\n",
    "class AgentResponse(BaseModel):\n",
    "    result: str\n",
    "    execution_log: List[Dict[str, Any]]\n",
    "    total_time: float\n",
    "    iterations_used: int\n",
    "\n",
    "\n",
    "class GameRequest(BaseModel):\n",
    "    action: str = Field(..., min_length=1, max_length=512)\n",
    "    session_id: Optional[str] = None\n",
    "    save_state: Optional[bool] = False\n",
    "\n",
    "\n",
    "class GameResponse(BaseModel):\n",
    "    narrative: str\n",
    "    choices: List[str]\n",
    "    game_state: Dict[str, Any]\n",
    "    session_id: str\n",
    "\n",
    "\n",
    "class HealthResponse(BaseModel):\n",
    "    status: str\n",
    "    version: str\n",
    "    models_loaded: Dict[str, bool]\n",
    "    uptime_seconds: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1429722",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 4: Ê®°ÂûãÂàùÂßãÂåñÈÇèËºØ\n",
    "# Mock implementations - replace with actual shared_utils imports\n",
    "class MockLLMAdapter:\n",
    "    def __init__(self):\n",
    "        self.model_name = \"Mock-7B-Instruct\"\n",
    "\n",
    "    def generate(self, messages, max_tokens=256, temperature=0.7):\n",
    "        # Simulate processing time\n",
    "        time.sleep(0.1)\n",
    "        user_msg = messages[-1][\"content\"] if messages else \"\"\n",
    "        return f\"Mock response to: {user_msg[:50]}...\"\n",
    "\n",
    "    def count_tokens(self, text):\n",
    "        return len(text.split()) * 1.3  # rough estimate\n",
    "\n",
    "\n",
    "class MockRAGRetriever:\n",
    "    def search(self, query, top_k=5):\n",
    "        # Mock retrieval results\n",
    "        return [\n",
    "            {\n",
    "                \"text\": f\"Retrieved document {i+1} for query: {query[:30]}...\",\n",
    "                \"meta\": {\"source_id\": f\"doc_{i+1}\", \"score\": 0.9 - i * 0.1},\n",
    "                \"score\": 0.9 - i * 0.1,\n",
    "            }\n",
    "            for i in range(min(top_k, 3))\n",
    "        ]\n",
    "\n",
    "\n",
    "class MockAgentOrchestrator:\n",
    "    def execute_task(self, task, mode=\"research_write\", max_iterations=3):\n",
    "        # Mock agent execution\n",
    "        execution_log = [\n",
    "            {\"step\": 1, \"role\": \"researcher\", \"action\": f\"Research: {task[:30]}...\"},\n",
    "            {\"step\": 2, \"role\": \"writer\", \"action\": \"Generated response\"},\n",
    "        ]\n",
    "        return f\"Agent result for task: {task[:50]}...\", execution_log\n",
    "\n",
    "\n",
    "class MockGameEngine:\n",
    "    def __init__(self):\n",
    "        self.sessions = {}\n",
    "\n",
    "    def process_action(self, action, session_id=None):\n",
    "        if not session_id:\n",
    "            session_id = f\"game_{int(time.time())}\"\n",
    "\n",
    "        # Mock game state\n",
    "        state = self.sessions.get(\n",
    "            session_id, {\"hp\": 100, \"level\": 1, \"location\": \"forest\"}\n",
    "        )\n",
    "\n",
    "        narrative = f\"You decided to {action}. The forest grows darker...\"\n",
    "        choices = [\"Go north\", \"Rest here\", \"Check inventory\"]\n",
    "\n",
    "        self.sessions[session_id] = state\n",
    "        return narrative, choices, state, session_id\n",
    "\n",
    "\n",
    "# Initialize mock models (replace with real implementations)\n",
    "def initialize_models():\n",
    "    \"\"\"Initialize all models and components\"\"\"\n",
    "    logger.info(\"Initializing models...\")\n",
    "\n",
    "    app_state[\"llm_adapter\"] = MockLLMAdapter()\n",
    "    app_state[\"rag_retriever\"] = MockRAGRetriever()\n",
    "    app_state[\"agent_orchestrator\"] = MockAgentOrchestrator()\n",
    "    app_state[\"game_engine\"] = MockGameEngine()\n",
    "\n",
    "    logger.info(\"Models initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e110236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 5: FastAPI ÊáâÁî®Ë®≠ÂÆö\n",
    "# FastAPI app with lifespan management\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    # Startup\n",
    "    logger.info(\"Starting up FastAPI application...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    initialize_models()\n",
    "\n",
    "    app.state.start_time = start_time\n",
    "    logger.info(f\"Startup completed in {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    yield\n",
    "\n",
    "    # Shutdown\n",
    "    logger.info(\"Shutting down FastAPI application...\")\n",
    "\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"RAGent Text Lab API\",\n",
    "    description=\"Multi-modal text AI API with Chat/RAG/Agent/Game capabilities\",\n",
    "    version=\"1.0.0\",\n",
    "    lifespan=lifespan,\n",
    ")\n",
    "\n",
    "# Add CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # Configure appropriately for production\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Request logging middleware\n",
    "@app.middleware(\"http\")\n",
    "async def log_requests(request, call_next):\n",
    "    start_time = time.time()\n",
    "    response = await call_next(request)\n",
    "    process_time = time.time() - start_time\n",
    "\n",
    "    logger.info(\n",
    "        f\"{request.method} {request.url.path} - \"\n",
    "        f\"Status: {response.status_code} - \"\n",
    "        f\"Time: {process_time:.3f}s\"\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3d0efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 6: Health Check Á´ØÈªû\n",
    "@app.get(\"/health\", response_model=HealthResponse)\n",
    "async def health_check():\n",
    "    \"\"\"ÂÅ•Â∫∑Ê™¢Êü•Á´ØÈªû\"\"\"\n",
    "    uptime = time.time() - app.state.start_time\n",
    "\n",
    "    models_loaded = {\n",
    "        \"llm_adapter\": app_state[\"llm_adapter\"] is not None,\n",
    "        \"rag_retriever\": app_state[\"rag_retriever\"] is not None,\n",
    "        \"agent_orchestrator\": app_state[\"agent_orchestrator\"] is not None,\n",
    "        \"game_engine\": app_state[\"game_engine\"] is not None,\n",
    "    }\n",
    "\n",
    "    return HealthResponse(\n",
    "        status=\"healthy\" if all(models_loaded.values()) else \"degraded\",\n",
    "        version=\"1.0.0\",\n",
    "        models_loaded=models_loaded,\n",
    "        uptime_seconds=uptime,\n",
    "    )\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    \"\"\"Ê†πÁ´ØÈªû\"\"\"\n",
    "    return {\"message\": \"RAGent Text Lab API\", \"docs\": \"/docs\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 7: Chat Á´ØÈªûÂØ¶‰Ωú\n",
    "@app.post(\"/chat\", response_model=ChatResponse)\n",
    "async def chat_endpoint(request: ChatRequest):\n",
    "    \"\"\"Âü∫Êú¨Â∞çË©±Á´ØÈªû\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        if not app_state[\"llm_adapter\"]:\n",
    "            raise HTTPException(status_code=503, detail=\"LLM adapter not initialized\")\n",
    "\n",
    "        # Prepare messages\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": request.message},\n",
    "        ]\n",
    "\n",
    "        # Generate response\n",
    "        response_text = app_state[\"llm_adapter\"].generate(\n",
    "            messages=messages,\n",
    "            max_tokens=request.max_tokens,\n",
    "            temperature=request.temperature,\n",
    "        )\n",
    "\n",
    "        # Calculate metrics\n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        tokens_used = app_state[\"llm_adapter\"].count_tokens(response_text)\n",
    "        conversation_id = request.conversation_id or f\"chat_{int(time.time())}\"\n",
    "\n",
    "        return ChatResponse(\n",
    "            response=response_text,\n",
    "            conversation_id=conversation_id,\n",
    "            tokens_used=int(tokens_used),\n",
    "            latency_ms=latency_ms,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Chat endpoint error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"Chat processing failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe6233",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 8: RAG Á´ØÈªûÂØ¶‰Ωú\n",
    "@app.post(\"/rag\", response_model=RAGResponse)\n",
    "async def rag_endpoint(request: RAGRequest):\n",
    "    \"\"\"RAG Ê™¢Á¥¢Â¢ûÂº∑ÂõûÁ≠îÁ´ØÈªû\"\"\"\n",
    "    try:\n",
    "        if not app_state[\"rag_retriever\"] or not app_state[\"llm_adapter\"]:\n",
    "            raise HTTPException(\n",
    "                status_code=503, detail=\"RAG components not initialized\"\n",
    "            )\n",
    "\n",
    "        # Timing for different phases\n",
    "        embed_start = time.time()\n",
    "\n",
    "        # Retrieve relevant documents\n",
    "        retrieval_start = time.time()\n",
    "        retrieved_docs = app_state[\"rag_retriever\"].search(\n",
    "            query=request.query, top_k=request.top_k\n",
    "        )\n",
    "        retrieval_time = time.time() - retrieval_start\n",
    "\n",
    "        # Build context from retrieved documents\n",
    "        context = \"\\n\\n\".join(\n",
    "            [f\"[{i+1}] {doc['text']}\" for i, doc in enumerate(retrieved_docs)]\n",
    "        )\n",
    "\n",
    "        # Generate answer with RAG context\n",
    "        gen_start = time.time()\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Answer based on the provided context. Use citations like [1], [2].\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Question: {request.query}\\n\\nContext:\\n{context}\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        answer = app_state[\"llm_adapter\"].generate(\n",
    "            messages=messages, max_tokens=512, temperature=0.3\n",
    "        )\n",
    "        generation_time = time.time() - gen_start\n",
    "\n",
    "        # Prepare sources for response\n",
    "        sources = [\n",
    "            {\n",
    "                \"id\": i + 1,\n",
    "                \"text\": (\n",
    "                    doc[\"text\"][:200] + \"...\" if len(doc[\"text\"]) > 200 else doc[\"text\"]\n",
    "                ),\n",
    "                \"metadata\": doc[\"meta\"],\n",
    "                \"score\": doc[\"score\"],\n",
    "            }\n",
    "            for i, doc in enumerate(retrieved_docs)\n",
    "        ]\n",
    "\n",
    "        return RAGResponse(\n",
    "            answer=answer,\n",
    "            sources=sources,\n",
    "            query_embedding_time=retrieval_start - embed_start,\n",
    "            retrieval_time=retrieval_time,\n",
    "            generation_time=generation_time,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"RAG endpoint error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"RAG processing failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef5b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 9: Agent Á´ØÈªûÂØ¶‰Ωú\n",
    "@app.post(\"/agent\", response_model=AgentResponse)\n",
    "async def agent_endpoint(request: AgentRequest):\n",
    "    \"\"\"Â§ö‰ª£ÁêÜÂçî‰ΩúÁ´ØÈªû\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        if not app_state[\"agent_orchestrator\"]:\n",
    "            raise HTTPException(\n",
    "                status_code=503, detail=\"Agent orchestrator not initialized\"\n",
    "            )\n",
    "\n",
    "        # Execute agent task\n",
    "        result, execution_log = app_state[\"agent_orchestrator\"].execute_task(\n",
    "            task=request.task, mode=request.mode, max_iterations=request.max_iterations\n",
    "        )\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "\n",
    "        return AgentResponse(\n",
    "            result=result,\n",
    "            execution_log=execution_log,\n",
    "            total_time=total_time,\n",
    "            iterations_used=len(execution_log),\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Agent endpoint error: {str(e)}\")\n",
    "        raise HTTPException(\n",
    "            status_code=500, detail=f\"Agent processing failed: {str(e)}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578a26dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 10: Game Á´ØÈªûÂØ¶‰Ωú\n",
    "@app.post(\"/game\", response_model=GameResponse)\n",
    "async def game_endpoint(request: GameRequest):\n",
    "    \"\"\"ÊñáÂ≠óÂÜíÈö™ÈÅäÊà≤Á´ØÈªû\"\"\"\n",
    "    try:\n",
    "        if not app_state[\"game_engine\"]:\n",
    "            raise HTTPException(status_code=503, detail=\"Game engine not initialized\")\n",
    "\n",
    "        # Process game action\n",
    "        narrative, choices, game_state, session_id = app_state[\"game_engine\"].process_action(\n",
    "            action=request.action,\n",
    "            session_id=request.session_id\n",
    "        )\n",
    "\n",
    "        return GameResponse(\n",
    "            narrative=narrative,\n",
    "            choices=choices,\n",
    "            game_state=game_state,\n",
    "            session_id=session_id\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Game endpoint error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"Game processing failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a5aacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 11: ÈåØË™§ËôïÁêÜ\n",
    "\n",
    "# Global exception handlers\n",
    "@app.exception_handler(HTTPException)\n",
    "async def http_exception_handler(request, exc):\n",
    "    return JSONResponse(\n",
    "        status_code=exc.status_code,\n",
    "        content={\n",
    "            \"error\": exc.detail,\n",
    "            \"path\": str(request.url.path),\n",
    "            \"method\": request.method,\n",
    "            \"timestamp\": time.time(),\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "@app.exception_handler(Exception)\n",
    "async def general_exception_handler(request, exc):\n",
    "    logger.error(f\"Unhandled exception: {str(exc)}\")\n",
    "    return JSONResponse(\n",
    "        status_code=500,\n",
    "        content={\n",
    "            \"error\": \"Internal server error\",\n",
    "            \"path\": str(request.url.path),\n",
    "            \"method\": request.method,\n",
    "            \"timestamp\": time.time(),\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15075f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 12: Smoke Test - Êú¨Âú∞ÂïüÂãï\n",
    "# Smoke test - start server in background for testing\n",
    "import threading\n",
    "import requests\n",
    "import time\n",
    "\n",
    "\n",
    "def start_server():\n",
    "    \"\"\"Start FastAPI server in background thread\"\"\"\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8000, log_level=\"info\")\n",
    "\n",
    "\n",
    "# Start server in background\n",
    "server_thread = threading.Thread(target=start_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "# Wait for server startup\n",
    "time.sleep(3)\n",
    "\n",
    "print(\"üöÄ FastAPI server started at http://127.0.0.1:8000\")\n",
    "print(\"üìö API Documentation: http://127.0.0.1:8000/docs\")\n",
    "print(\"üîç Alternative docs: http://127.0.0.1:8000/redoc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910a4a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 13: API Ê∏¨Ë©¶\n",
    "# Test the endpoints\n",
    "base_url = \"http://127.0.0.1:8000\"\n",
    "\n",
    "try:\n",
    "    # Test health check\n",
    "    health_response = requests.get(f\"{base_url}/health\")\n",
    "    print(f\"Health Check: {health_response.status_code}\")\n",
    "    print(f\"Response: {health_response.json()}\\n\")\n",
    "\n",
    "    # Test chat endpoint\n",
    "    chat_payload = {\n",
    "        \"message\": \"Hello, how are you?\",\n",
    "        \"max_tokens\": 100,\n",
    "        \"temperature\": 0.7,\n",
    "    }\n",
    "    chat_response = requests.post(f\"{base_url}/chat\", json=chat_payload)\n",
    "    print(f\"Chat: {chat_response.status_code}\")\n",
    "    print(f\"Response: {chat_response.json()}\\n\")\n",
    "\n",
    "    # Test RAG endpoint\n",
    "    rag_payload = {\"query\": \"What is machine learning?\", \"top_k\": 3}\n",
    "    rag_response = requests.post(f\"{base_url}/rag\", json=rag_payload)\n",
    "    print(f\"RAG: {rag_response.status_code}\")\n",
    "    print(f\"Response: {rag_response.json()}\\n\")\n",
    "\n",
    "    # Test agent endpoint\n",
    "    agent_payload = {\n",
    "        \"task\": \"Research and write about renewable energy\",\n",
    "        \"mode\": \"research_write\",\n",
    "    }\n",
    "    agent_response = requests.post(f\"{base_url}/agent\", json=agent_payload)\n",
    "    print(f\"Agent: {agent_response.status_code}\")\n",
    "    print(f\"Response: {agent_response.json()}\\n\")\n",
    "\n",
    "    # Test game endpoint\n",
    "    game_payload = {\"action\": \"explore the forest\"}\n",
    "    game_response = requests.post(f\"{base_url}/game\", json=game_payload)\n",
    "    print(f\"Game: {game_response.status_code}\")\n",
    "    print(f\"Response: {game_response.json()}\\n\")\n",
    "\n",
    "    print(\"‚úÖ All endpoints responding successfully!\")\n",
    "\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"‚ùå Server not responding. Make sure it's started.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Test failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e559d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 14: ÁîüÁî¢Áí∞Â¢ÉÈÖçÁΩÆÂª∫Ë≠∞\n",
    "# Production deployment considerations\n",
    "\n",
    "production_config = {\n",
    "    \"server\": {\n",
    "        \"host\": \"0.0.0.0\",\n",
    "        \"port\": 8000,\n",
    "        \"workers\": 4,  # For uvicorn with --workers\n",
    "        \"log_level\": \"warning\",\n",
    "    },\n",
    "    \"security\": {\n",
    "        \"cors_origins\": [\"https://yourdomain.com\"],\n",
    "        \"rate_limit\": \"100/minute\",\n",
    "        \"api_key_required\": True,\n",
    "    },\n",
    "    \"performance\": {\"max_request_size\": \"10MB\", \"timeout\": 300, \"keep_alive\": 75},\n",
    "    \"monitoring\": {\n",
    "        \"health_check_interval\": 30,\n",
    "        \"metrics_enabled\": True,\n",
    "        \"log_requests\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"üîß Production Configuration:\")\n",
    "print(json.dumps(production_config, indent=2))\n",
    "\n",
    "print(\"\\nüìã Deployment Checklist:\")\n",
    "checklist = [\n",
    "    \"Set environment variables (MODEL_ID, API_KEYS)\",\n",
    "    \"Configure CORS origins properly\",\n",
    "    \"Set up rate limiting middleware\",\n",
    "    \"Add API authentication\",\n",
    "    \"Configure logging aggregation\",\n",
    "    \"Set up health monitoring\",\n",
    "    \"Use HTTPS in production\",\n",
    "    \"Set appropriate resource limits\",\n",
    "]\n",
    "\n",
    "for i, item in enumerate(checklist, 1):\n",
    "    print(f\"{i}. {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed6c331",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 15: ÂÅúÊ≠¢‰º∫ÊúçÂô® (Ê∏ÖÁêÜ)\n",
    "# Note: In production, use proper process management\n",
    "print(\"üìù Note: Server is running in background thread.\")\n",
    "print(\"To properly stop in production, use:\")\n",
    "print(\"- uvicorn main:app --reload (development)\")\n",
    "print(\"- gunicorn main:app -w 4 -k uvicorn.workers.UvicornWorker (production)\")\n",
    "print(\"- Docker containers with proper signal handling\")\n",
    "print(\"- Process managers like supervisor or systemd\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
