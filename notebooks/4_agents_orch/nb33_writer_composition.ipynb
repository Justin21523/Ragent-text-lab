{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62674fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writer Composition Agent Implementation\n",
    "# Stage 4 - Multi-Agent Orchestrator\n",
    "# File: notebooks/4_agents_orch/nb33_writer_composition.ipynb\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (複製到每本 notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89837849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Dependencies and Imports\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "# LLM Adapter (from previous notebooks)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LLMAdapter:\n",
    "    def __init__(self, model_id: str, device_map=\"auto\", **kwargs):\n",
    "        self.model_id = model_id\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=device_map,\n",
    "            torch_dtype=torch.float16,\n",
    "            load_in_4bit=True,  # Low VRAM\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def generate(\n",
    "        self, messages: List[Dict], max_new_tokens=512, temperature=0.7, **kwargs\n",
    "    ):\n",
    "        # Simple chat template\n",
    "        prompt = \"\"\n",
    "        for msg in messages:\n",
    "            role = msg.get(\"role\", \"user\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            if role == \"system\":\n",
    "                prompt += f\"System: {content}\\n\"\n",
    "            elif role == \"user\":\n",
    "                prompt += f\"User: {content}\\n\"\n",
    "            elif role == \"assistant\":\n",
    "                prompt += f\"Assistant: {content}\\n\"\n",
    "        prompt += \"Assistant: \"\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            prompt, return_tensors=\"pt\", truncation=True, max_length=3072\n",
    "        )\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        response = self.tokenizer.decode(\n",
    "            outputs[0][inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n",
    "        )\n",
    "        return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e781543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Writer Agent Core Class\n",
    "@dataclass\n",
    "class WritingSection:\n",
    "    \"\"\"Represents a section to be written\"\"\"\n",
    "\n",
    "    title: str\n",
    "    outline_points: List[str]\n",
    "    target_length: int = 300  # Target word count\n",
    "    context: str = \"\"  # Background context\n",
    "    citations: List[str] = field(default_factory=list)\n",
    "    content: str = \"\"  # Generated content\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class WritingProject:\n",
    "    \"\"\"Complete writing project structure\"\"\"\n",
    "\n",
    "    title: str\n",
    "    sections: List[WritingSection]\n",
    "    research_context: str = \"\"\n",
    "    style_guide: Dict = field(default_factory=dict)\n",
    "    references: List[str] = field(default_factory=list)\n",
    "\n",
    "\n",
    "class WriterAgent:\n",
    "    \"\"\"Writer agent for content composition\"\"\"\n",
    "\n",
    "    def __init__(self, llm_adapter: LLMAdapter, style_config: Dict = None):\n",
    "        self.llm = llm_adapter\n",
    "        self.style_config = style_config or {}\n",
    "        self.writing_history = []\n",
    "\n",
    "    def load_style_dictionary(self, style_path: str) -> Dict:\n",
    "        \"\"\"Load style dictionary from YAML\"\"\"\n",
    "        try:\n",
    "            with open(style_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return yaml.safe_load(f)\n",
    "        except FileNotFoundError:\n",
    "            return self._get_default_style()\n",
    "\n",
    "    def _get_default_style(self) -> Dict:\n",
    "        \"\"\"Default Chinese writing style\"\"\"\n",
    "        return {\n",
    "            \"tone\": \"formal-neutral\",\n",
    "            \"format\": {\n",
    "                \"bullets\": True,\n",
    "                \"numbered_steps\": True,\n",
    "                \"citations\": \"brackets\",\n",
    "            },\n",
    "            \"glossary\": [\n",
    "                {\"src\": \"RAG\", \"tgt\": \"檢索增強生成\"},\n",
    "                {\"src\": \"LLM\", \"tgt\": \"大型語言模型\"},\n",
    "                {\"src\": \"Agent\", \"tgt\": \"智能代理\"},\n",
    "            ],\n",
    "            \"avoid_phrases\": [\"以下是\", \"作為一個AI\"],\n",
    "            \"style_rules\": [\n",
    "                \"使用繁體中文撰寫\",\n",
    "                \"保持段落簡潔，每段不超過150字\",\n",
    "                \"適當使用列點和編號\",\n",
    "                \"引用格式使用 [1], [2] 方式\",\n",
    "            ],\n",
    "        }\n",
    "\n",
    "    def _build_style_prompt(self, style_dict: Dict) -> str:\n",
    "        \"\"\"Convert style dictionary to prompt instructions\"\"\"\n",
    "        tone = style_dict.get(\"tone\", \"neutral\")\n",
    "        rules = \"\\n\".join(style_dict.get(\"style_rules\", []))\n",
    "\n",
    "        glossary = \"\"\n",
    "        for term in style_dict.get(\"glossary\", []):\n",
    "            glossary += f\"- {term['src']} → {term['tgt']}\\n\"\n",
    "\n",
    "        avoid = \", \".join(style_dict.get(\"avoid_phrases\", []))\n",
    "\n",
    "        return f\"\"\"\n",
    "寫作風格指南：\n",
    "語調：{tone}\n",
    "專業術語對照：\n",
    "{glossary}\n",
    "避免用詞：{avoid}\n",
    "寫作規則：\n",
    "{rules}\n",
    "\"\"\"\n",
    "\n",
    "    def write_section(\n",
    "        self,\n",
    "        section: WritingSection,\n",
    "        project_context: str = \"\",\n",
    "        previous_sections: List[str] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Write a single section based on outline\"\"\"\n",
    "        previous_sections = previous_sections or []\n",
    "        style_prompt = self._build_style_prompt(self.style_config)\n",
    "\n",
    "        # Build context from previous sections for coherence\n",
    "        context_summary = \"\"\n",
    "        if previous_sections:\n",
    "            context_summary = (\n",
    "                f\"\\n前文摘要：\\n{' '.join(previous_sections[-2:])}\"  # Last 2 sections\n",
    "            )\n",
    "\n",
    "        system_prompt = f\"\"\"你是專業的中文寫作助手。請根據大綱要點撰寫內容。\n",
    "\n",
    "{style_prompt}\n",
    "\n",
    "要求：\n",
    "1. 內容必須基於提供的大綱要點\n",
    "2. 保持與前文的邏輯連貫性\n",
    "3. 目標長度約 {section.target_length} 字\n",
    "4. 如有引用資料，使用 [1], [2] 格式標註\n",
    "5. 直接輸出內容，不要包含「以下是」等引導語\n",
    "\"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"\n",
    "專案背景：{project_context}\n",
    "\n",
    "章節標題：{section.title}\n",
    "\n",
    "大綱要點：\n",
    "{chr(10).join([f\"- {point}\" for point in section.outline_points])}\n",
    "\n",
    "{f\"參考資料：{section.context}\" if section.context else \"\"}\n",
    "\n",
    "{context_summary}\n",
    "\n",
    "請撰寫此章節的內容：\n",
    "\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "\n",
    "        content = self.llm.generate(\n",
    "            messages,\n",
    "            max_new_tokens=min(800, section.target_length * 2),  # Control length\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "        section.content = content\n",
    "        return content\n",
    "\n",
    "    def write_project(self, project: WritingProject) -> str:\n",
    "        \"\"\"Write complete project with all sections\"\"\"\n",
    "        full_content = f\"# {project.title}\\n\\n\"\n",
    "        written_sections = []\n",
    "\n",
    "        for i, section in enumerate(project.sections):\n",
    "            print(f\"Writing section {i+1}/{len(project.sections)}: {section.title}\")\n",
    "\n",
    "            content = self.write_section(\n",
    "                section, project.research_context, written_sections\n",
    "            )\n",
    "\n",
    "            # Format section\n",
    "            section_text = f\"## {section.title}\\n\\n{content}\\n\\n\"\n",
    "            full_content += section_text\n",
    "            written_sections.append(content)\n",
    "\n",
    "            # Brief pause between sections\n",
    "            time.sleep(1)\n",
    "\n",
    "        # Add references if any\n",
    "        if project.references:\n",
    "            full_content += \"## 參考資料\\n\\n\"\n",
    "            for i, ref in enumerate(project.references, 1):\n",
    "                full_content += f\"[{i}] {ref}\\n\"\n",
    "\n",
    "        return full_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e09d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Style Dictionary Integration\n",
    "def create_style_config():\n",
    "    \"\"\"Create sample style configuration\"\"\"\n",
    "    return {\n",
    "        \"tone\": \"professional-friendly\",\n",
    "        \"format\": {\n",
    "            \"bullets\": True,\n",
    "            \"numbered_steps\": True,\n",
    "            \"citations\": \"brackets\",\n",
    "            \"max_paragraph_length\": 150,\n",
    "        },\n",
    "        \"glossary\": [\n",
    "            {\"src\": \"RAG\", \"tgt\": \"檢索增強生成\"},\n",
    "            {\"src\": \"Retrieval\", \"tgt\": \"檢索\"},\n",
    "            {\"src\": \"Embedding\", \"tgt\": \"嵌入向量\"},\n",
    "            {\"src\": \"Chunk\", \"tgt\": \"文本片段\"},\n",
    "            {\"src\": \"LLM\", \"tgt\": \"大型語言模型\"},\n",
    "            {\"src\": \"Agent\", \"tgt\": \"智能代理\"},\n",
    "            {\"src\": \"Orchestrator\", \"tgt\": \"協調器\"},\n",
    "        ],\n",
    "        \"avoid_phrases\": [\"以下是\", \"作為一個AI助手\", \"讓我來為您\", \"總的來說\"],\n",
    "        \"style_rules\": [\n",
    "            \"使用繁體中文撰寫正文\",\n",
    "            \"技術術語保持中英文對照一致性\",\n",
    "            \"每段落不超過150字，保持簡潔\",\n",
    "            \"使用條列式重點整理\",\n",
    "            \"引用格式統一使用 [數字] 方式\",\n",
    "            \"保持客觀專業語調，避免過度主觀表達\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "def save_style_config(config: Dict, path: str):\n",
    "    \"\"\"Save style configuration to YAML\"\"\"\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.dump(config, f, allow_unicode=True, default_flow_style=False)\n",
    "\n",
    "\n",
    "# Create and save sample style\n",
    "style_config = create_style_config()\n",
    "style_path = \"configs/styles/zh_general.yaml\"\n",
    "save_style_config(style_config, style_path)\n",
    "print(f\"Style config saved to: {style_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ec140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Sectional Writing Strategy\n",
    "class SectionalWriter:\n",
    "    \"\"\"Handles sectional writing with smart splitting\"\"\"\n",
    "\n",
    "    def __init__(self, writer_agent: WriterAgent, max_section_length=400):\n",
    "        self.writer = writer_agent\n",
    "        self.max_section_length = max_section_length\n",
    "\n",
    "    def split_long_outline(\n",
    "        self, title: str, outline_points: List[str], target_total_length: int = 1000\n",
    "    ) -> List[WritingSection]:\n",
    "        \"\"\"Split long outline into manageable sections\"\"\"\n",
    "        sections = []\n",
    "\n",
    "        if len(outline_points) <= 3 or target_total_length <= self.max_section_length:\n",
    "            # Single section\n",
    "            sections.append(\n",
    "                WritingSection(\n",
    "                    title=title,\n",
    "                    outline_points=outline_points,\n",
    "                    target_length=min(target_total_length, self.max_section_length),\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            # Split into multiple sections\n",
    "            points_per_section = max(2, len(outline_points) // 3)\n",
    "            length_per_section = target_total_length // (\n",
    "                (len(outline_points) + points_per_section - 1) // points_per_section\n",
    "            )\n",
    "\n",
    "            for i in range(0, len(outline_points), points_per_section):\n",
    "                section_points = outline_points[i : i + points_per_section]\n",
    "                section_num = i // points_per_section + 1\n",
    "\n",
    "                section_title = (\n",
    "                    f\"{title} - 第{section_num}部分\"\n",
    "                    if len(outline_points) > 3\n",
    "                    else title\n",
    "                )\n",
    "\n",
    "                sections.append(\n",
    "                    WritingSection(\n",
    "                        title=section_title,\n",
    "                        outline_points=section_points,\n",
    "                        target_length=min(length_per_section, self.max_section_length),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return sections\n",
    "\n",
    "    def estimate_writing_time(self, sections: List[WritingSection]) -> int:\n",
    "        \"\"\"Estimate total writing time in seconds\"\"\"\n",
    "        total_length = sum(s.target_length for s in sections)\n",
    "        # Rough estimate: 1 second per 2 characters (Chinese)\n",
    "        return max(30, total_length // 2 + len(sections) * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ea9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Content Cohesion and Formatting\n",
    "class ContentFormatter:\n",
    "    \"\"\"Handles content formatting and cohesion\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def ensure_cohesion(sections: List[str]) -> List[str]:\n",
    "        \"\"\"Ensure smooth transitions between sections\"\"\"\n",
    "        if len(sections) <= 1:\n",
    "            return sections\n",
    "\n",
    "        cohesive_sections = []\n",
    "        for i, section in enumerate(sections):\n",
    "            if i > 0:\n",
    "                # Add transition if needed\n",
    "                prev_ends_with = sections[i - 1].strip()[-10:]\n",
    "                curr_starts_with = section.strip()[:10:]\n",
    "\n",
    "                # Simple heuristic for adding transitions\n",
    "                if not any(\n",
    "                    word in curr_starts_with\n",
    "                    for word in [\"此外\", \"另外\", \"接下來\", \"然而\", \"因此\"]\n",
    "                ):\n",
    "                    # Check if we need a transition\n",
    "                    section = (\n",
    "                        f\"此外，{section}\"\n",
    "                        if not section.startswith((\"在\", \"對於\", \"關於\"))\n",
    "                        else section\n",
    "                    )\n",
    "\n",
    "            cohesive_sections.append(section)\n",
    "\n",
    "        return cohesive_sections\n",
    "\n",
    "    @staticmethod\n",
    "    def format_citations(content: str, references: List[str]) -> str:\n",
    "        \"\"\"Format citations consistently\"\"\"\n",
    "        # Ensure citation numbers are in order\n",
    "        import re\n",
    "\n",
    "        # Find all citation patterns\n",
    "        citations = re.findall(r\"\\[(\\d+)\\]\", content)\n",
    "        if not citations:\n",
    "            return content\n",
    "\n",
    "        # Renumber citations sequentially\n",
    "        citation_map = {}\n",
    "        for i, cite in enumerate(sorted(set(citations), key=int), 1):\n",
    "            citation_map[cite] = str(i)\n",
    "\n",
    "        # Replace citations in content\n",
    "        formatted_content = content\n",
    "        for old_num, new_num in citation_map.items():\n",
    "            formatted_content = formatted_content.replace(\n",
    "                f\"[{old_num}]\", f\"[{new_num}]\"\n",
    "            )\n",
    "\n",
    "        return formatted_content\n",
    "\n",
    "    @staticmethod\n",
    "    def add_word_count(content: str) -> str:\n",
    "        \"\"\"Add word count information\"\"\"\n",
    "        # Chinese character count (rough approximation)\n",
    "        chinese_chars = len([c for c in content if \"\\u4e00\" <= c <= \"\\u9fff\"])\n",
    "        total_chars = len(content.replace(\" \", \"\").replace(\"\\n\", \"\"))\n",
    "\n",
    "        return f\"{content}\\n\\n---\\n*字數統計：約 {chinese_chars} 中文字，總計 {total_chars} 字符*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dc2086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Blackboard Integration\n",
    "class Blackboard(dict):\n",
    "    \"\"\"Simple blackboard for inter-agent communication\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.history = []\n",
    "\n",
    "    def update_state(self, key: str, value, agent: str = \"unknown\"):\n",
    "        \"\"\"Update state with tracking\"\"\"\n",
    "        self[key] = value\n",
    "        self.history.append(\n",
    "            {\"timestamp\": time.time(), \"agent\": agent, \"key\": key, \"action\": \"update\"}\n",
    "        )\n",
    "\n",
    "    def get_state(self, key: str, default=None):\n",
    "        \"\"\"Get state value\"\"\"\n",
    "        return self.get(key, default)\n",
    "\n",
    "\n",
    "def integrate_with_blackboard(\n",
    "    blackboard: Blackboard, writer: WriterAgent\n",
    ") -> WritingProject:\n",
    "    \"\"\"Create writing project from blackboard state\"\"\"\n",
    "\n",
    "    # Get research context\n",
    "    research_data = blackboard.get_state(\"research_summary\", \"\")\n",
    "\n",
    "    # Get outline from planner\n",
    "    outline_data = blackboard.get_state(\"outline\", {})\n",
    "    if not outline_data:\n",
    "        raise ValueError(\"No outline found in blackboard. Run planner first.\")\n",
    "\n",
    "    # Get style preferences\n",
    "    style_prefs = blackboard.get_state(\"style_preferences\", {})\n",
    "\n",
    "    # Create writing project\n",
    "    project = WritingProject(\n",
    "        title=outline_data.get(\"title\", \"未命名專案\"),\n",
    "        research_context=research_data,\n",
    "        references=blackboard.get_state(\"references\", []),\n",
    "    )\n",
    "\n",
    "    # Convert outline to sections\n",
    "    sections_data = outline_data.get(\"sections\", [])\n",
    "    for section_data in sections_data:\n",
    "        section = WritingSection(\n",
    "            title=section_data.get(\"title\", \"未命名章節\"),\n",
    "            outline_points=section_data.get(\"points\", []),\n",
    "            target_length=section_data.get(\"target_length\", 300),\n",
    "            context=research_data,  # Share research context\n",
    "        )\n",
    "        project.sections.append(section)\n",
    "\n",
    "    return project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea586b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Complete Writing Workflow Demo\n",
    "def demo_complete_writing_workflow():\n",
    "    \"\"\"Demonstrate complete writing workflow\"\"\"\n",
    "\n",
    "    # Initialize LLM (using smaller model for demo)\n",
    "    print(\"Initializing LLM...\")\n",
    "    llm = LLMAdapter(\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "\n",
    "    # Load style configuration\n",
    "    style_config = create_style_config()\n",
    "\n",
    "    # Initialize writer agent\n",
    "    writer = WriterAgent(llm, style_config)\n",
    "\n",
    "    # Create blackboard with sample data\n",
    "    blackboard = Blackboard()\n",
    "\n",
    "    # Simulate previous agent outputs\n",
    "    blackboard.update_state(\n",
    "        \"research_summary\",\n",
    "        \"RAG（檢索增強生成）是結合資訊檢索與語言生成的技術，能讓大型語言模型基於外部知識庫提供更準確的回答。\"\n",
    "        \"主要組成包括文檔分割、向量嵌入、相似度檢索和答案生成等步驟。\",\n",
    "        \"researcher\",\n",
    "    )\n",
    "\n",
    "    blackboard.update_state(\n",
    "        \"outline\",\n",
    "        {\n",
    "            \"title\": \"RAG 系統實作指南\",\n",
    "            \"sections\": [\n",
    "                {\n",
    "                    \"title\": \"RAG 系統概述\",\n",
    "                    \"points\": [\n",
    "                        \"RAG 的基本概念與架構\",\n",
    "                        \"與傳統搜尋引擎的差異\",\n",
    "                        \"主要應用場景\",\n",
    "                    ],\n",
    "                    \"target_length\": 300,\n",
    "                },\n",
    "                {\n",
    "                    \"title\": \"技術實作要點\",\n",
    "                    \"points\": [\n",
    "                        \"文檔預處理與分割策略\",\n",
    "                        \"嵌入模型選擇與優化\",\n",
    "                        \"檢索與重排機制\",\n",
    "                    ],\n",
    "                    \"target_length\": 400,\n",
    "                },\n",
    "                {\n",
    "                    \"title\": \"評估與優化\",\n",
    "                    \"points\": [\n",
    "                        \"檢索品質評估指標\",\n",
    "                        \"回答品質評估方法\",\n",
    "                        \"系統效能優化建議\",\n",
    "                    ],\n",
    "                    \"target_length\": 300,\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        \"planner\",\n",
    "    )\n",
    "\n",
    "    blackboard.update_state(\n",
    "        \"references\",\n",
    "        [\n",
    "            \"Lewis et al. (2020). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\",\n",
    "            \"BGE: General Embedding Model (BAAI, 2023)\",\n",
    "            \"FAISS: A Library for Efficient Similarity Search (Facebook AI, 2017)\",\n",
    "        ],\n",
    "        \"researcher\",\n",
    "    )\n",
    "\n",
    "    # Create writing project from blackboard\n",
    "    print(\"\\nCreating writing project from blackboard...\")\n",
    "    project = integrate_with_blackboard(blackboard, writer)\n",
    "\n",
    "    # Execute writing\n",
    "    print(f\"\\nStarting writing project: {project.title}\")\n",
    "    print(f\"Sections to write: {len(project.sections)}\")\n",
    "\n",
    "    # Write the complete project\n",
    "    final_content = writer.write_project(project)\n",
    "\n",
    "    # Format and finalize\n",
    "    formatter = ContentFormatter()\n",
    "    final_content = formatter.format_citations(final_content, project.references)\n",
    "    final_content = formatter.add_word_count(final_content)\n",
    "\n",
    "    # Update blackboard with results\n",
    "    blackboard.update_state(\"final_content\", final_content, \"writer\")\n",
    "    blackboard.update_state(\"writing_completed\", True, \"writer\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"WRITING COMPLETED\")\n",
    "    print(\"=\" * 50)\n",
    "    print(final_content[:500] + \"...\" if len(final_content) > 500 else final_content)\n",
    "\n",
    "    return final_content, blackboard\n",
    "\n",
    "\n",
    "# Run the demo\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        content, bb = demo_complete_writing_workflow()\n",
    "        print(f\"\\nBlackboard final state keys: {list(bb.keys())}\")\n",
    "        print(f\"Total characters generated: {len(content)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Demo failed: {e}\")\n",
    "        print(\"This is expected in minimal environment. Check dependencies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9e94eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Smoke Test\n",
    "def smoke_test_writer():\n",
    "    \"\"\"Minimal smoke test for writer functionality\"\"\"\n",
    "    print(\"Running Writer Agent Smoke Test...\")\n",
    "\n",
    "    try:\n",
    "        # Test style configuration\n",
    "        style = create_style_config()\n",
    "        assert \"tone\" in style\n",
    "        assert \"glossary\" in style\n",
    "        print(\"✓ Style configuration OK\")\n",
    "\n",
    "        # Test section creation\n",
    "        section = WritingSection(\n",
    "            title=\"測試章節\", outline_points=[\"要點一\", \"要點二\"], target_length=200\n",
    "        )\n",
    "        assert section.title == \"測試章節\"\n",
    "        assert len(section.outline_points) == 2\n",
    "        print(\"✓ WritingSection creation OK\")\n",
    "\n",
    "        # Test blackboard\n",
    "        bb = Blackboard()\n",
    "        bb.update_state(\"test_key\", \"test_value\", \"test_agent\")\n",
    "        assert bb.get_state(\"test_key\") == \"test_value\"\n",
    "        assert len(bb.history) == 1\n",
    "        print(\"✓ Blackboard integration OK\")\n",
    "\n",
    "        # Test content formatting\n",
    "        formatter = ContentFormatter()\n",
    "        test_content = \"這是測試內容 [2] 和引用 [1]。\"\n",
    "        formatted = formatter.format_citations(test_content, [\"ref1\", \"ref2\"])\n",
    "        assert \"[1]\" in formatted\n",
    "        print(\"✓ Content formatting OK\")\n",
    "\n",
    "        print(\"\\n🎉 All smoke tests passed!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Smoke test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_writer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a713bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Summary and Next Steps\n",
    "print(\n",
    "    \"\"\"\n",
    "## 📝 Writer Agent 實作完成\n",
    "\n",
    "### 已完成功能：\n",
    "1. ✅ Writer 代理核心類別 - 支援分段寫作與風格控制\n",
    "2. ✅ Style Dictionary 整合 - 語氣、術語、格式統一\n",
    "3. ✅ 分段寫作策略 - 避免過長內容，提升品質\n",
    "4. ✅ 內容銜接機制 - 段落間邏輯連貫\n",
    "5. ✅ 黑板系統整合 - 讀取研究資料與大綱\n",
    "6. ✅ 格式化與引用 - 統一引用格式，字數統計\n",
    "\n",
    "### 核心概念：\n",
    "- **分段寫作**：將長文檔分割為可管理的段落，逐一生成\n",
    "- **風格一致性**：透過 Style Dictionary 確保語氣與術語統一\n",
    "- **上下文連貫**：使用前文摘要維持邏輯連貫性\n",
    "- **引用標準化**：統一 [1], [2] 格式，自動重新編號\n",
    "\n",
    "### 常見陷阱：\n",
    "⚠️ **長度控制**：設定合理的 target_length，避免生成過長內容\n",
    "⚠️ **風格漂移**：確保 Style Dictionary 規則清晰且一致\n",
    "⚠️ **記憶體使用**：分段寫作時注意累積的上下文長度\n",
    "⚠️ **引用對齊**：確保引用編號與參考資料清單一致\n",
    "\n",
    "### 下一步行動：\n",
    "1. 實作 **nb34_reviewer_groundedness.ipynb** - 事實核查與引用驗證\n",
    "2. 整合 **失敗重試機制** - 處理生成品質不佳的情況\n",
    "3. 加入 **版本控制** - 支援多次修改與版本比較\n",
    "4. 優化 **效能監控** - 追蹤寫作速度與品質指標\n",
    "\n",
    "### Git 工作流：\n",
    "```bash\n",
    "git checkout -b feature/nb33-writer-composition\n",
    "git add notebooks/4_agents_orch/nb33_writer_composition.ipynb\n",
    "git add configs/styles/zh_general.yaml\n",
    "git commit -m \"feat(agent): writer composition with style dictionary\"\n",
    "git commit -m \"feat(agent): sectional writing and blackboard integration\"\n",
    "```\n",
    "\n",
    "🚀 Writer Agent 已準備好與其他代理協作！\n",
    "\"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
