{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1305f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (è¤‡è£½åˆ°æ¯æœ¬ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f5f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: GoT æ ¸å¿ƒæ¦‚å¿µèˆ‡ DAG çµæ§‹\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Set, Any, Optional, Union, Callable\n",
    "from enum import Enum\n",
    "import asyncio\n",
    "import json\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "\n",
    "class NodeType(Enum):\n",
    "    TASK = \"task\"  # åŸ·è¡Œå…·é«”ä»»å‹™\n",
    "    CONDITION = \"condition\"  # æ¢ä»¶åˆ¤æ–·åˆ†æ”¯\n",
    "    AGGREGATION = \"agg\"  # çµæœèšåˆ\n",
    "    PARALLEL = \"parallel\"  # ä¸¦è¡ŒåŸ·è¡Œçµ„\n",
    "\n",
    "\n",
    "class NodeStatus(Enum):\n",
    "    PENDING = \"pending\"\n",
    "    RUNNING = \"running\"\n",
    "    COMPLETED = \"completed\"\n",
    "    FAILED = \"failed\"\n",
    "    SKIPPED = \"skipped\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DAGNode:\n",
    "    \"\"\"DAG ç¯€é»å®šç¾©\"\"\"\n",
    "\n",
    "    node_id: str\n",
    "    node_type: NodeType\n",
    "    task_description: str\n",
    "    dependencies: Set[str] = field(default_factory=set)  # å‰ç½®ç¯€é»\n",
    "    agent_role: Optional[str] = None  # åŸ·è¡Œè§’è‰² (researcher/planner/writer/reviewer)\n",
    "    condition_func: Optional[Callable] = None  # æ¢ä»¶ç¯€é»çš„åˆ¤æ–·å‡½æ•¸\n",
    "    aggregation_func: Optional[Callable] = None  # èšåˆç¯€é»çš„åˆä½µå‡½æ•¸\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    # åŸ·è¡Œç‹€æ…‹\n",
    "    status: NodeStatus = NodeStatus.PENDING\n",
    "    result: Any = None\n",
    "    error: Optional[str] = None\n",
    "    start_time: Optional[float] = None\n",
    "    end_time: Optional[float] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DAGExecutionContext:\n",
    "    \"\"\"DAG åŸ·è¡Œä¸Šä¸‹æ–‡\"\"\"\n",
    "\n",
    "    nodes: Dict[str, DAGNode] = field(default_factory=dict)\n",
    "    edges: Dict[str, Set[str]] = field(default_factory=lambda: defaultdict(set))\n",
    "    results: Dict[str, Any] = field(default_factory=dict)\n",
    "    blackboard: Dict[str, Any] = field(default_factory=dict)\n",
    "    execution_log: List[Dict] = field(default_factory=list)\n",
    "\n",
    "\n",
    "print(\"âœ“ GoT æ ¸å¿ƒçµæ§‹å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9ab447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: DAG ç¯€é»é¡å‹è¨­è¨ˆ\n",
    "class DAGBuilder:\n",
    "    \"\"\"DAG å»ºæ§‹å™¨\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.context = DAGExecutionContext()\n",
    "\n",
    "    def add_task_node(\n",
    "        self,\n",
    "        node_id: str,\n",
    "        description: str,\n",
    "        agent_role: str,\n",
    "        dependencies: List[str] = None,\n",
    "        **metadata,\n",
    "    ) -> \"DAGBuilder\":\n",
    "        \"\"\"æ–°å¢ä»»å‹™ç¯€é»\"\"\"\n",
    "        node = DAGNode(\n",
    "            node_id=node_id,\n",
    "            node_type=NodeType.TASK,\n",
    "            task_description=description,\n",
    "            agent_role=agent_role,\n",
    "            dependencies=set(dependencies or []),\n",
    "            metadata=metadata,\n",
    "        )\n",
    "        self.context.nodes[node_id] = node\n",
    "\n",
    "        # å»ºç«‹é‚Šé—œä¿‚\n",
    "        for dep in dependencies or []:\n",
    "            self.context.edges[dep].add(node_id)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def add_condition_node(\n",
    "        self,\n",
    "        node_id: str,\n",
    "        description: str,\n",
    "        condition_func: Callable,\n",
    "        dependencies: List[str] = None,\n",
    "        **metadata,\n",
    "    ) -> \"DAGBuilder\":\n",
    "        \"\"\"æ–°å¢æ¢ä»¶ç¯€é»\"\"\"\n",
    "        node = DAGNode(\n",
    "            node_id=node_id,\n",
    "            node_type=NodeType.CONDITION,\n",
    "            task_description=description,\n",
    "            condition_func=condition_func,\n",
    "            dependencies=set(dependencies or []),\n",
    "            metadata=metadata,\n",
    "        )\n",
    "        self.context.nodes[node_id] = node\n",
    "\n",
    "        for dep in dependencies or []:\n",
    "            self.context.edges[dep].add(node_id)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def add_aggregation_node(\n",
    "        self,\n",
    "        node_id: str,\n",
    "        description: str,\n",
    "        aggregation_func: Callable,\n",
    "        dependencies: List[str],\n",
    "        **metadata,\n",
    "    ) -> \"DAGBuilder\":\n",
    "        \"\"\"æ–°å¢èšåˆç¯€é»\"\"\"\n",
    "        node = DAGNode(\n",
    "            node_id=node_id,\n",
    "            node_type=NodeType.AGGREGATION,\n",
    "            task_description=description,\n",
    "            aggregation_func=aggregation_func,\n",
    "            dependencies=set(dependencies),\n",
    "            metadata=metadata,\n",
    "        )\n",
    "        self.context.nodes[node_id] = node\n",
    "\n",
    "        for dep in dependencies:\n",
    "            self.context.edges[dep].add(node_id)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def build(self) -> DAGExecutionContext:\n",
    "        \"\"\"å»ºæ§‹ä¸¦é©—è­‰ DAG\"\"\"\n",
    "        self._validate_dag()\n",
    "        return self.context\n",
    "\n",
    "    def _validate_dag(self):\n",
    "        \"\"\"é©—è­‰ DAG æ˜¯å¦æœ‰æ•ˆï¼ˆç„¡å¾ªç’°ã€ä¾è³´å­˜åœ¨ï¼‰\"\"\"\n",
    "        # æª¢æŸ¥å¾ªç’°ä¾è³´\n",
    "        if self._has_cycle():\n",
    "            raise ValueError(\"DAG contains cycles\")\n",
    "\n",
    "        # æª¢æŸ¥ä¾è³´ç¯€é»å­˜åœ¨\n",
    "        for node_id, node in self.context.nodes.items():\n",
    "            for dep in node.dependencies:\n",
    "                if dep not in self.context.nodes:\n",
    "                    raise ValueError(\n",
    "                        f\"Node {node_id} depends on non-existent node {dep}\"\n",
    "                    )\n",
    "\n",
    "    def _has_cycle(self) -> bool:\n",
    "        \"\"\"æª¢æŸ¥æ˜¯å¦æœ‰å¾ªç’°ä¾è³´ï¼ˆDFSï¼‰\"\"\"\n",
    "        visited = set()\n",
    "        rec_stack = set()\n",
    "\n",
    "        def dfs(node_id):\n",
    "            if node_id in rec_stack:\n",
    "                return True\n",
    "            if node_id in visited:\n",
    "                return False\n",
    "\n",
    "            visited.add(node_id)\n",
    "            rec_stack.add(node_id)\n",
    "\n",
    "            for neighbor in self.context.edges.get(node_id, []):\n",
    "                if dfs(neighbor):\n",
    "                    return True\n",
    "\n",
    "            rec_stack.remove(node_id)\n",
    "            return False\n",
    "\n",
    "        for node_id in self.context.nodes:\n",
    "            if node_id not in visited:\n",
    "                if dfs(node_id):\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "\n",
    "# ç¯„ä¾‹ï¼šå»ºæ§‹ç°¡å–®ç ”ç©¶ DAG\n",
    "def create_research_dag():\n",
    "    \"\"\"å»ºç«‹ç ”ç©¶å ±å‘Š DAG ç¯„ä¾‹\"\"\"\n",
    "\n",
    "    # æ¢ä»¶å‡½æ•¸ï¼šåˆ¤æ–·æ˜¯å¦éœ€è¦æ·±åº¦ç ”ç©¶\n",
    "    def needs_deep_research(context):\n",
    "        initial_result = context.results.get(\"initial_research\", \"\")\n",
    "        return len(initial_result) < 500  # å¦‚æœåˆæ­¥ç ”ç©¶å¤ªçŸ­ï¼Œéœ€è¦æ·±åº¦ç ”ç©¶\n",
    "\n",
    "    # èšåˆå‡½æ•¸ï¼šåˆä½µå¤šå€‹ç ”ç©¶çµæœ\n",
    "    def aggregate_research(context):\n",
    "        results = []\n",
    "        for key in [\"initial_research\", \"deep_research\", \"expert_interview\"]:\n",
    "            if key in context.results:\n",
    "                results.append(context.results[key])\n",
    "        return \"\\n\\n\".join(results)\n",
    "\n",
    "    dag = (\n",
    "        DAGBuilder()\n",
    "        .add_task_node(\"initial_research\", \"é€²è¡Œåˆæ­¥ä¸»é¡Œç ”ç©¶\", \"researcher\")\n",
    "        .add_condition_node(\n",
    "            \"check_depth\",\n",
    "            \"æª¢æŸ¥æ˜¯å¦éœ€è¦æ·±åº¦ç ”ç©¶\",\n",
    "            needs_deep_research,\n",
    "            [\"initial_research\"],\n",
    "        )\n",
    "        .add_task_node(\n",
    "            \"deep_research\",\n",
    "            \"æ·±åº¦å°ˆé¡Œç ”ç©¶\",\n",
    "            \"researcher\",\n",
    "            [\"check_depth\"],\n",
    "            condition_value=True,\n",
    "        )\n",
    "        .add_task_node(\n",
    "            \"expert_interview\", \"å°ˆå®¶è¨ªè«‡æ‘˜è¦\", \"researcher\", [\"initial_research\"]\n",
    "        )\n",
    "        .add_aggregation_node(\n",
    "            \"research_summary\",\n",
    "            \"ç ”ç©¶çµæœèšåˆ\",\n",
    "            aggregate_research,\n",
    "            [\"initial_research\", \"deep_research\", \"expert_interview\"],\n",
    "        )\n",
    "        .add_task_node(\"outline_plan\", \"å»ºç«‹å ±å‘Šå¤§ç¶±\", \"planner\", [\"research_summary\"])\n",
    "        .add_task_node(\"draft_writing\", \"æ’°å¯«åˆç¨¿\", \"writer\", [\"outline_plan\"])\n",
    "        .add_task_node(\"review_check\", \"å…§å®¹å¯©æŸ¥\", \"reviewer\", [\"draft_writing\"])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    return dag\n",
    "\n",
    "\n",
    "# æ¸¬è©¦ DAG å»ºæ§‹\n",
    "test_dag = create_research_dag()\n",
    "print(f\"âœ“ DAG å»ºæ§‹å®Œæˆï¼Œå…± {len(test_dag.nodes)} å€‹ç¯€é»\")\n",
    "print(f\"  ç¯€é»ï¼š{list(test_dag.nodes.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1625f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: DAG åŸ·è¡Œå¼•æ“å¯¦ä½œ\n",
    "import time\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "class DAGExecutor:\n",
    "    \"\"\"DAG åŸ·è¡Œå¼•æ“\"\"\"\n",
    "\n",
    "    def __init__(self, agent_registry: Dict[str, Any] = None):\n",
    "        self.agent_registry = agent_registry or {}\n",
    "        self.executor = ThreadPoolExecutor(max_workers=4)\n",
    "\n",
    "    async def execute_dag(self, context: DAGExecutionContext) -> Dict[str, Any]:\n",
    "        \"\"\"åŸ·è¡Œå®Œæ•´ DAG\"\"\"\n",
    "        print(\"ğŸš€ é–‹å§‹åŸ·è¡Œ DAG...\")\n",
    "\n",
    "        # è¨ˆç®—æ‹“æ’²æ’åº\n",
    "        execution_order = self._topological_sort(context)\n",
    "        print(f\"ğŸ“‹ åŸ·è¡Œé †åºï¼š{execution_order}\")\n",
    "\n",
    "        # æŒ‰å±¤ç´šä¸¦è¡ŒåŸ·è¡Œ\n",
    "        execution_levels = self._group_by_level(context, execution_order)\n",
    "\n",
    "        for level_idx, level_nodes in enumerate(execution_levels):\n",
    "            print(f\"\\nğŸ”„ åŸ·è¡Œç¬¬ {level_idx + 1} å±¤ï¼š{level_nodes}\")\n",
    "\n",
    "            # ä¸¦è¡ŒåŸ·è¡ŒåŒä¸€å±¤çš„ç¯€é»\n",
    "            tasks = []\n",
    "            for node_id in level_nodes:\n",
    "                if self._can_execute_node(context, node_id):\n",
    "                    task = self._execute_node_async(context, node_id)\n",
    "                    tasks.append(task)\n",
    "\n",
    "            if tasks:\n",
    "                await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "        print(f\"\\nâœ… DAG åŸ·è¡Œå®Œæˆ\")\n",
    "        return context.results\n",
    "\n",
    "    def _topological_sort(self, context: DAGExecutionContext) -> List[str]:\n",
    "        \"\"\"æ‹“æ’²æ’åºè¨ˆç®—åŸ·è¡Œé †åº\"\"\"\n",
    "        in_degree = defaultdict(int)\n",
    "\n",
    "        # è¨ˆç®—å…¥åº¦\n",
    "        for node_id in context.nodes:\n",
    "            in_degree[node_id] = len(context.nodes[node_id].dependencies)\n",
    "\n",
    "        # Kahn's algorithm\n",
    "        queue = deque([node_id for node_id, degree in in_degree.items() if degree == 0])\n",
    "        result = []\n",
    "\n",
    "        while queue:\n",
    "            node_id = queue.popleft()\n",
    "            result.append(node_id)\n",
    "\n",
    "            # æ›´æ–°å¾Œç¹¼ç¯€é»çš„å…¥åº¦\n",
    "            for neighbor in context.edges.get(node_id, []):\n",
    "                in_degree[neighbor] -= 1\n",
    "                if in_degree[neighbor] == 0:\n",
    "                    queue.append(neighbor)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _group_by_level(\n",
    "        self, context: DAGExecutionContext, execution_order: List[str]\n",
    "    ) -> List[List[str]]:\n",
    "        \"\"\"æŒ‰ä¾è³´å±¤ç´šåˆ†çµ„ï¼ŒåŒå±¤å¯ä¸¦è¡ŒåŸ·è¡Œ\"\"\"\n",
    "        levels = []\n",
    "        processed = set()\n",
    "\n",
    "        while len(processed) < len(execution_order):\n",
    "            current_level = []\n",
    "\n",
    "            for node_id in execution_order:\n",
    "                if node_id in processed:\n",
    "                    continue\n",
    "\n",
    "                # æª¢æŸ¥æ‰€æœ‰ä¾è³´æ˜¯å¦å·²è™•ç†\n",
    "                node = context.nodes[node_id]\n",
    "                if all(dep in processed for dep in node.dependencies):\n",
    "                    current_level.append(node_id)\n",
    "\n",
    "            for node_id in current_level:\n",
    "                processed.add(node_id)\n",
    "\n",
    "            if current_level:\n",
    "                levels.append(current_level)\n",
    "            else:\n",
    "                break  # é¿å…ç„¡é™å¾ªç’°\n",
    "\n",
    "        return levels\n",
    "\n",
    "    def _can_execute_node(self, context: DAGExecutionContext, node_id: str) -> bool:\n",
    "        \"\"\"æª¢æŸ¥ç¯€é»æ˜¯å¦å¯åŸ·è¡Œ\"\"\"\n",
    "        node = context.nodes[node_id]\n",
    "\n",
    "        # æª¢æŸ¥ä¾è³´æ˜¯å¦éƒ½å®Œæˆ\n",
    "        for dep_id in node.dependencies:\n",
    "            dep_node = context.nodes[dep_id]\n",
    "            if dep_node.status != NodeStatus.COMPLETED:\n",
    "                return False\n",
    "\n",
    "            # æ¢ä»¶ç¯€é»ç‰¹æ®Šè™•ç†\n",
    "            if dep_node.node_type == NodeType.CONDITION:\n",
    "                condition_result = dep_node.result\n",
    "                node_condition = node.metadata.get(\"condition_value\")\n",
    "                if node_condition is not None and condition_result != node_condition:\n",
    "                    node.status = NodeStatus.SKIPPED\n",
    "                    print(f\"â­ï¸  ç¯€é» {node_id} è¢«æ¢ä»¶è·³é\")\n",
    "                    return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    async def _execute_node_async(self, context: DAGExecutionContext, node_id: str):\n",
    "        \"\"\"ç•°æ­¥åŸ·è¡Œå–®å€‹ç¯€é»\"\"\"\n",
    "        node = context.nodes[node_id]\n",
    "        node.status = NodeStatus.RUNNING\n",
    "        node.start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            print(f\"â–¶ï¸  åŸ·è¡Œç¯€é»: {node_id} ({node.task_description})\")\n",
    "\n",
    "            if node.node_type == NodeType.TASK:\n",
    "                result = await self._execute_task_node(context, node)\n",
    "            elif node.node_type == NodeType.CONDITION:\n",
    "                result = await self._execute_condition_node(context, node)\n",
    "            elif node.node_type == NodeType.AGGREGATION:\n",
    "                result = await self._execute_aggregation_node(context, node)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown node type: {node.node_type}\")\n",
    "\n",
    "            node.result = result\n",
    "            node.status = NodeStatus.COMPLETED\n",
    "            context.results[node_id] = result\n",
    "\n",
    "            node.end_time = time.time()\n",
    "            elapsed = node.end_time - node.start_time\n",
    "            print(f\"âœ… ç¯€é» {node_id} å®Œæˆ ({elapsed:.2f}s)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            node.status = NodeStatus.FAILED\n",
    "            node.error = str(e)\n",
    "            node.end_time = time.time()\n",
    "            print(f\"âŒ ç¯€é» {node_id} å¤±æ•—: {e}\")\n",
    "            raise\n",
    "\n",
    "    async def _execute_task_node(\n",
    "        self, context: DAGExecutionContext, node: DAGNode\n",
    "    ) -> str:\n",
    "        \"\"\"åŸ·è¡Œä»»å‹™ç¯€é»\"\"\"\n",
    "        # æ¨¡æ“¬ Agent åŸ·è¡Œ\n",
    "        if node.agent_role and node.agent_role in self.agent_registry:\n",
    "            agent = self.agent_registry[node.agent_role]\n",
    "            # é€™è£¡æ‡‰è©²èª¿ç”¨çœŸå¯¦çš„ agent\n",
    "            await asyncio.sleep(0.5)  # æ¨¡æ“¬åŸ·è¡Œæ™‚é–“\n",
    "            return f\"[{node.agent_role}] å®Œæˆä»»å‹™: {node.task_description}\"\n",
    "        else:\n",
    "            # æ¨¡æ“¬åŸ·è¡Œ\n",
    "            await asyncio.sleep(0.3)\n",
    "            return f\"Mock result for: {node.task_description}\"\n",
    "\n",
    "    async def _execute_condition_node(\n",
    "        self, context: DAGExecutionContext, node: DAGNode\n",
    "    ) -> bool:\n",
    "        \"\"\"åŸ·è¡Œæ¢ä»¶ç¯€é»\"\"\"\n",
    "        if node.condition_func:\n",
    "            return node.condition_func(context)\n",
    "        return True\n",
    "\n",
    "    async def _execute_aggregation_node(\n",
    "        self, context: DAGExecutionContext, node: DAGNode\n",
    "    ) -> str:\n",
    "        \"\"\"åŸ·è¡Œèšåˆç¯€é»\"\"\"\n",
    "        if node.aggregation_func:\n",
    "            return node.aggregation_func(context)\n",
    "        return \"Aggregated result\"\n",
    "\n",
    "\n",
    "print(\"âœ“ DAG åŸ·è¡Œå¼•æ“å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd6e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: èˆ‡ Agents æ•´åˆ\n",
    "# ç°¡åŒ–çš„ Agent ç³»çµ±æ•´åˆ\n",
    "class MockAgent:\n",
    "    def __init__(self, role: str):\n",
    "        self.role = role\n",
    "\n",
    "    async def execute(self, task: str, context: Dict = None) -> str:\n",
    "        \"\"\"æ¨¡æ“¬ Agent åŸ·è¡Œä»»å‹™\"\"\"\n",
    "        await asyncio.sleep(0.2)  # æ¨¡æ“¬æ€è€ƒæ™‚é–“\n",
    "\n",
    "        if self.role == \"researcher\":\n",
    "            return f\"ğŸ“Š ç ”ç©¶å ±å‘Šï¼š{task} - ç™¼ç¾äº† 3 å€‹é—œéµé»å’Œ 5 ç¯‡ç›¸é—œæ–‡ç»\"\n",
    "        elif self.role == \"planner\":\n",
    "            return f\"ğŸ“‹ è¦åŠƒå¤§ç¶±ï¼š{task} - å»ºç«‹äº† 4 å€‹ç« ç¯€çš„è©³ç´°æ¶æ§‹\"\n",
    "        elif self.role == \"writer\":\n",
    "            return f\"âœï¸  å¯«ä½œå…§å®¹ï¼š{task} - å®Œæˆäº† 800 å­—çš„å°ˆæ¥­å…§å®¹\"\n",
    "        elif self.role == \"reviewer\":\n",
    "            return f\"ğŸ” å¯©æŸ¥çµæœï¼š{task} - æª¢æŸ¥äº† 5 å€‹è¦é»ï¼Œå»ºè­° 2 è™•ä¿®æ”¹\"\n",
    "        else:\n",
    "            return f\"[{self.role}] å®Œæˆï¼š{task}\"\n",
    "\n",
    "\n",
    "# å»ºç«‹ Agent è¨»å†Šè¡¨\n",
    "agent_registry = {\n",
    "    \"researcher\": MockAgent(\"researcher\"),\n",
    "    \"planner\": MockAgent(\"planner\"),\n",
    "    \"writer\": MockAgent(\"writer\"),\n",
    "    \"reviewer\": MockAgent(\"reviewer\"),\n",
    "}\n",
    "\n",
    "\n",
    "# å¢å¼·ç‰ˆ DAG åŸ·è¡Œå™¨ï¼ˆæ•´åˆçœŸå¯¦ Agentï¼‰\n",
    "class AgentDAGExecutor(DAGExecutor):\n",
    "    \"\"\"æ•´åˆ Agent çš„ DAG åŸ·è¡Œå™¨\"\"\"\n",
    "\n",
    "    async def _execute_task_node(\n",
    "        self, context: DAGExecutionContext, node: DAGNode\n",
    "    ) -> str:\n",
    "        \"\"\"åŸ·è¡Œä»»å‹™ç¯€é»ï¼ˆèª¿ç”¨çœŸå¯¦ Agentï¼‰\"\"\"\n",
    "        if node.agent_role and node.agent_role in self.agent_registry:\n",
    "            agent = self.agent_registry[node.agent_role]\n",
    "\n",
    "            # æº–å‚™ä¸Šä¸‹æ–‡è³‡è¨Š\n",
    "            task_context = {\n",
    "                \"dependencies\": {\n",
    "                    dep_id: context.results.get(dep_id) for dep_id in node.dependencies\n",
    "                },\n",
    "                \"blackboard\": context.blackboard,\n",
    "                \"node_metadata\": node.metadata,\n",
    "            }\n",
    "\n",
    "            # åŸ·è¡Œ Agent ä»»å‹™\n",
    "            result = await agent.execute(node.task_description, task_context)\n",
    "            return result\n",
    "        else:\n",
    "            # å›é€€åˆ°åŸºç¤å¯¦ä½œ\n",
    "            return await super()._execute_task_node(context, node)\n",
    "\n",
    "\n",
    "print(\"âœ“ Agent æ•´åˆå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64547a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: å¯¦éš›æ¡ˆä¾‹ï¼šç ”ç©¶å ±å‘Š DAG\n",
    "# å»ºç«‹å®Œæ•´çš„ç ”ç©¶å ±å‘Š DAG æ¡ˆä¾‹\n",
    "def create_comprehensive_research_dag():\n",
    "    \"\"\"å»ºç«‹ç¶œåˆç ”ç©¶å ±å‘Š DAG\"\"\"\n",
    "\n",
    "    def needs_expert_review(context):\n",
    "        \"\"\"åˆ¤æ–·æ˜¯å¦éœ€è¦å°ˆå®¶å¯©æŸ¥\"\"\"\n",
    "        research_length = len(context.results.get(\"initial_research\", \"\"))\n",
    "        return research_length > 1000  # é•·ç ”ç©¶éœ€è¦å°ˆå®¶å¯©æŸ¥\n",
    "\n",
    "    def needs_data_analysis(context):\n",
    "        \"\"\"åˆ¤æ–·æ˜¯å¦éœ€è¦æ•¸æ“šåˆ†æ\"\"\"\n",
    "        research = context.results.get(\"initial_research\", \"\")\n",
    "        return \"æ•¸æ“š\" in research or \"statistics\" in research.lower()\n",
    "\n",
    "    def combine_research_sources(context):\n",
    "        \"\"\"åˆä½µå¤šå€‹ç ”ç©¶ä¾†æº\"\"\"\n",
    "        sources = []\n",
    "        for key in [\"initial_research\", \"expert_research\", \"data_analysis\"]:\n",
    "            if key in context.results and context.results[key]:\n",
    "                sources.append(f\"=== {key.upper()} ===\\n{context.results[key]}\")\n",
    "        return \"\\n\\n\".join(sources)\n",
    "\n",
    "    def final_quality_check(context):\n",
    "        \"\"\"æœ€çµ‚å“è³ªæª¢æŸ¥èšåˆ\"\"\"\n",
    "        review_result = context.results.get(\"content_review\", \"\")\n",
    "        expert_result = context.results.get(\"expert_review\", \"\")\n",
    "\n",
    "        if expert_result:\n",
    "            return f\"ç¶œåˆå¯©æŸ¥ï¼š\\nå…§å®¹å¯©æŸ¥ï¼š{review_result}\\nå°ˆå®¶å¯©æŸ¥ï¼š{expert_result}\"\n",
    "        else:\n",
    "            return f\"å…§å®¹å¯©æŸ¥ï¼š{review_result}\"\n",
    "\n",
    "    dag = (\n",
    "        DAGBuilder()\n",
    "        # ç¬¬1å±¤ï¼šåˆå§‹ç ”ç©¶\n",
    "        .add_task_node(\"initial_research\", \"åŸ·è¡Œåˆæ­¥ä¸»é¡Œç ”ç©¶æ”¶é›†\", \"researcher\")\n",
    "        # ç¬¬2å±¤ï¼šæ¢ä»¶åˆ†æ”¯\n",
    "        .add_condition_node(\n",
    "            \"check_expert_need\",\n",
    "            \"æª¢æŸ¥æ˜¯å¦éœ€è¦å°ˆå®¶å¯©æŸ¥\",\n",
    "            needs_expert_review,\n",
    "            [\"initial_research\"],\n",
    "        )\n",
    "        .add_condition_node(\n",
    "            \"check_data_need\",\n",
    "            \"æª¢æŸ¥æ˜¯å¦éœ€è¦æ•¸æ“šåˆ†æ\",\n",
    "            needs_data_analysis,\n",
    "            [\"initial_research\"],\n",
    "        )\n",
    "        # ç¬¬3å±¤ï¼šæ¢ä»¶åŸ·è¡Œ\n",
    "        .add_task_node(\n",
    "            \"expert_research\",\n",
    "            \"å°ˆå®¶è«®è©¢èˆ‡æ·±åº¦ç ”ç©¶\",\n",
    "            \"researcher\",\n",
    "            [\"check_expert_need\"],\n",
    "            condition_value=True,\n",
    "        )\n",
    "        .add_task_node(\n",
    "            \"data_analysis\",\n",
    "            \"ç›¸é—œæ•¸æ“šåˆ†æ\",\n",
    "            \"researcher\",\n",
    "            [\"check_data_need\"],\n",
    "            condition_value=True,\n",
    "        )\n",
    "        # ç¬¬4å±¤ï¼šç ”ç©¶èšåˆ\n",
    "        .add_aggregation_node(\n",
    "            \"research_synthesis\",\n",
    "            \"ç ”ç©¶çµæœç¶œåˆ\",\n",
    "            combine_research_sources,\n",
    "            [\"initial_research\", \"expert_research\", \"data_analysis\"],\n",
    "        )\n",
    "        # ç¬¬5å±¤ï¼šè¦åŠƒèˆ‡å¯«ä½œä¸¦è¡Œ\n",
    "        .add_task_node(\n",
    "            \"outline_creation\", \"å»ºç«‹å ±å‘Šæ¶æ§‹å¤§ç¶±\", \"planner\", [\"research_synthesis\"]\n",
    "        )\n",
    "        .add_task_node(\n",
    "            \"style_guide\", \"ç¢ºå®šå¯«ä½œé¢¨æ ¼æŒ‡å—\", \"planner\", [\"research_synthesis\"]\n",
    "        )\n",
    "        # ç¬¬6å±¤ï¼šä¸¦è¡Œå¯«ä½œ\n",
    "        .add_task_node(\n",
    "            \"intro_section\",\n",
    "            \"æ’°å¯«å¼•è¨€ç« ç¯€\",\n",
    "            \"writer\",\n",
    "            [\"outline_creation\", \"style_guide\"],\n",
    "        )\n",
    "        .add_task_node(\n",
    "            \"main_content\",\n",
    "            \"æ’°å¯«ä¸»è¦å…§å®¹\",\n",
    "            \"writer\",\n",
    "            [\"outline_creation\", \"style_guide\"],\n",
    "        )\n",
    "        .add_task_node(\n",
    "            \"conclusion\", \"æ’°å¯«çµè«–å»ºè­°\", \"writer\", [\"outline_creation\", \"style_guide\"]\n",
    "        )\n",
    "        # ç¬¬7å±¤ï¼šå…§å®¹æ•´åˆ\n",
    "        .add_aggregation_node(\n",
    "            \"draft_assembly\",\n",
    "            \"çµ„è£å®Œæ•´åˆç¨¿\",\n",
    "            lambda ctx: \"\\n\\n\".join(\n",
    "                [\n",
    "                    ctx.results.get(\"intro_section\", \"\"),\n",
    "                    ctx.results.get(\"main_content\", \"\"),\n",
    "                    ctx.results.get(\"conclusion\", \"\"),\n",
    "                ]\n",
    "            ),\n",
    "            [\"intro_section\", \"main_content\", \"conclusion\"],\n",
    "        )\n",
    "        # ç¬¬8å±¤ï¼šå¯©æŸ¥\n",
    "        .add_task_node(\"content_review\", \"å…§å®¹å“è³ªå¯©æŸ¥\", \"reviewer\", [\"draft_assembly\"])\n",
    "        .add_task_node(\n",
    "            \"expert_review\",\n",
    "            \"å°ˆå®¶æœ€çµ‚å¯©æŸ¥\",\n",
    "            \"reviewer\",\n",
    "            [\"draft_assembly\", \"check_expert_need\"],\n",
    "            condition_value=True,\n",
    "        )\n",
    "        # ç¬¬9å±¤ï¼šæœ€çµ‚æ•´åˆ\n",
    "        .add_aggregation_node(\n",
    "            \"final_report\",\n",
    "            \"æœ€çµ‚å ±å‘Šç”Ÿæˆ\",\n",
    "            final_quality_check,\n",
    "            [\"content_review\", \"expert_review\"],\n",
    "        )\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    return dag\n",
    "\n",
    "\n",
    "# å»ºç«‹ä¸¦åŸ·è¡Œç¶œåˆ DAG\n",
    "comprehensive_dag = create_comprehensive_research_dag()\n",
    "print(f\"ğŸ—ï¸  å»ºç«‹ç¶œåˆç ”ç©¶ DAGï¼š{len(comprehensive_dag.nodes)} å€‹ç¯€é»\")\n",
    "\n",
    "# é¡¯ç¤º DAG çµæ§‹\n",
    "print(\"\\nğŸ“Š DAG ç¯€é»çµæ§‹ï¼š\")\n",
    "for node_id, node in comprehensive_dag.nodes.items():\n",
    "    deps = f\"ä¾è³´: {list(node.dependencies)}\" if node.dependencies else \"ç„¡ä¾è³´\"\n",
    "    print(f\"  {node_id} ({node.node_type.value}) - {deps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aff0e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: å¯è¦–åŒ–èˆ‡ç›£æ§\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from datetime import datetime\n",
    "\n",
    "def visualize_dag(context: DAGExecutionContext, save_path: str = None):\n",
    "    \"\"\"å¯è¦–åŒ– DAG çµæ§‹èˆ‡åŸ·è¡Œç‹€æ…‹\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # æ·»åŠ ç¯€é»\n",
    "    for node_id, node in context.nodes.items():\n",
    "        G.add_node(node_id,\n",
    "                  node_type=node.node_type.value,\n",
    "                  status=node.status.value,\n",
    "                  agent=node.agent_role or \"system\")\n",
    "\n",
    "    # æ·»åŠ é‚Š\n",
    "    for node_id, node in context.nodes.items():\n",
    "        for dep in node.dependencies:\n",
    "            G.add_edge(dep, node_id)\n",
    "\n",
    "    # è¨­å®šåœ–å½¢\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    pos = nx.spring_layout(G, k=3, iterations=50)\n",
    "\n",
    "    # ç¯€é»é¡è‰²ä¾ç‹€æ…‹\n",
    "    color_map = {\n",
    "        \"pending\": \"lightgray\",\n",
    "        \"running\": \"yellow\",\n",
    "        \"completed\": \"lightgreen\",\n",
    "        \"failed\": \"red\",\n",
    "        \"skipped\": \"orange\"\n",
    "    }\n",
    "\n",
    "    node_colors = [color_map.get(context.nodes[node].status.value, \"gray\")\n",
    "                  for node in G.nodes()]\n",
    "\n",
    "    # ç¹ªè£½\n",
    "    nx.draw(G, pos,\n",
    "           node_color=node_colors,\n",
    "           node_size=3000,\n",
    "           font_size=8,\n",
    "           font_weight=\"bold\",\n",
    "           arrows=True,\n",
    "           arrowsize=20,\n",
    "           edge_color=\"gray\",\n",
    "           with_labels=True)\n",
    "\n",
    "    # æ·»åŠ åœ–ä¾‹\n",
    "    legend_elements = [plt.Rectangle((0,0),1,1, color=color, label=status.title())\n",
    "                      for status, color in color_map.items()]\n",
    "    plt.legend(handles=legend_elements, loc=\"upper right\")\n",
    "\n",
    "    plt.title(\"Graph of Thought (GoT) DAG åŸ·è¡Œç‹€æ…‹\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "def generate_execution_report(context: DAGExecutionContext) -> str:\n",
    "    \"\"\"ç”ŸæˆåŸ·è¡Œå ±å‘Š\"\"\"\n",
    "    report = []\n",
    "    report.append(\"# GoT DAG åŸ·è¡Œå ±å‘Š\")\n",
    "    report.append(f\"ç”Ÿæˆæ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report.append(\"\")\n",
    "\n",
    "    # çµ±è¨ˆè³‡è¨Š\n",
    "    total_nodes = len(context.nodes)\n",
    "    completed = sum(1 for n in context.nodes.values() if n.status == NodeStatus.COMPLETED)\n",
    "    failed = sum(1 for n in context.nodes.values() if n.status == NodeStatus.FAILED)\n",
    "    skipped = sum(1 for n in context.nodes.values() if n.status == NodeStatus.SKIPPED)\n",
    "\n",
    "    report.append(\"## åŸ·è¡Œçµ±è¨ˆ\")\n",
    "    report.append(f\"- ç¸½ç¯€é»æ•¸ï¼š{total_nodes}\")\n",
    "    report.append(f\"- æˆåŠŸå®Œæˆï¼š{completed}\")\n",
    "    report.append(f\"- åŸ·è¡Œå¤±æ•—ï¼š{failed}\")\n",
    "    report.append(f\"- æ¢ä»¶è·³éï¼š{skipped}\")\n",
    "    report.append(\"\")\n",
    "\n",
    "    # åŸ·è¡Œæ™‚é–“åˆ†æ\n",
    "    execution_times = []\n",
    "    for node in context.nodes.values():\n",
    "        if node.start_time and node.end_time:\n",
    "            execution_times.append((node.node_id, node.end_time - node.start_time))\n",
    "\n",
    "    if execution_times:\n",
    "        execution_times.sort(key=lambda x: x[1], reverse=True)\n",
    "        report.append(\"## åŸ·è¡Œæ™‚é–“åˆ†æï¼ˆå‰5åï¼‰\")\n",
    "        for node_id, duration in execution_times[:5]:\n",
    "            report.append(f\"- {node_id}: {duration:.2f}ç§’\")\n",
    "        report.append(\"\")\n",
    "\n",
    "    # ç¯€é»è©³æƒ…\n",
    "    report.append(\"## ç¯€é»åŸ·è¡Œè©³æƒ…\")\n",
    "    for node_id, node in context.nodes.items():\n",
    "        status_icon = {\"completed\": \"âœ…\", \"failed\": \"âŒ\", \"skipped\": \"â­ï¸\"}.get(node.status.value, \"â¸ï¸\")\n",
    "        report.append(f\"### {status_icon} {node_id}\")\n",
    "        report.append(f\"- é¡å‹ï¼š{node.node_type.value}\")\n",
    "        report.append(f\"- ç‹€æ…‹ï¼š{node.status.value}\")\n",
    "        if node.agent_role:\n",
    "           report.append(f\"- åŸ·è¡Œè§’è‰²ï¼š{node.agent_role}\")\n",
    "        if node.error:\n",
    "           report.append(f\"- éŒ¯èª¤ï¼š{node.error}\")\n",
    "        if hasattr(node, 'result') and node.result:\n",
    "           result_preview = str(node.result)[:100] + \"...\" if len(str(node.result)) > 100 else str(node.result)\n",
    "           report.append(f\"- çµæœé è¦½ï¼š{result_preview}\")\n",
    "        report.append(\"\")\n",
    "\n",
    "    return \"\\n\".join(report)\n",
    "\n",
    "print(\"âœ“ å¯è¦–åŒ–èˆ‡ç›£æ§æ¨¡çµ„å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9688dbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: å¤±æ•—è™•ç†èˆ‡é‡è©¦\n",
    "class DAGFailureHandler:\n",
    "    \"\"\"DAG å¤±æ•—è™•ç†èˆ‡é‡è©¦æ©Ÿåˆ¶\"\"\"\n",
    "\n",
    "    def __init__(self, executor: DAGExecutor):\n",
    "        self.executor = executor\n",
    "\n",
    "    async def retry_failed_nodes(\n",
    "        self, context: DAGExecutionContext, max_retries: int = 3\n",
    "    ) -> bool:\n",
    "        \"\"\"é‡è©¦å¤±æ•—çš„ç¯€é»\"\"\"\n",
    "        failed_nodes = [\n",
    "            node_id\n",
    "            for node_id, node in context.nodes.items()\n",
    "            if node.status == NodeStatus.FAILED\n",
    "        ]\n",
    "\n",
    "        if not failed_nodes:\n",
    "            print(\"âœ… æ²’æœ‰å¤±æ•—ç¯€é»éœ€è¦é‡è©¦\")\n",
    "            return True\n",
    "\n",
    "        print(f\"ğŸ”„ ç™¼ç¾ {len(failed_nodes)} å€‹å¤±æ•—ç¯€é»ï¼Œé–‹å§‹é‡è©¦...\")\n",
    "\n",
    "        for retry_count in range(max_retries):\n",
    "            print(f\"\\nğŸ”„ é‡è©¦ç¬¬ {retry_count + 1} æ¬¡\")\n",
    "\n",
    "            # é‡ç½®å¤±æ•—ç¯€é»ç‹€æ…‹\n",
    "            for node_id in failed_nodes:\n",
    "                node = context.nodes[node_id]\n",
    "                node.status = NodeStatus.PENDING\n",
    "                node.error = None\n",
    "                node.result = None\n",
    "\n",
    "            # é‡æ–°è¨ˆç®—éœ€è¦åŸ·è¡Œçš„ç¯€é»ï¼ˆåŒ…å«ä¾è³´ï¼‰\n",
    "            nodes_to_rerun = self._get_affected_nodes(context, failed_nodes)\n",
    "            print(f\"ğŸ“‹ éœ€è¦é‡æ–°åŸ·è¡Œçš„ç¯€é»ï¼š{nodes_to_rerun}\")\n",
    "\n",
    "            # åŸ·è¡Œå­ DAG\n",
    "            try:\n",
    "                await self._execute_partial_dag(context, nodes_to_rerun)\n",
    "\n",
    "                # æª¢æŸ¥æ˜¯å¦é‚„æœ‰å¤±æ•—ç¯€é»\n",
    "                current_failed = [\n",
    "                    node_id\n",
    "                    for node_id, node in context.nodes.items()\n",
    "                    if node.status == NodeStatus.FAILED\n",
    "                ]\n",
    "\n",
    "                if not current_failed:\n",
    "                    print(f\"âœ… é‡è©¦æˆåŠŸï¼æ‰€æœ‰ç¯€é»åŸ·è¡Œå®Œæˆ\")\n",
    "                    return True\n",
    "                else:\n",
    "                    failed_nodes = current_failed\n",
    "                    print(f\"âš ï¸  ä»æœ‰ {len(failed_nodes)} å€‹ç¯€é»å¤±æ•—\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ é‡è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\")\n",
    "\n",
    "        print(f\"âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œä»æœ‰å¤±æ•—ç¯€é»ï¼š{failed_nodes}\")\n",
    "        return False\n",
    "\n",
    "    def _get_affected_nodes(\n",
    "        self, context: DAGExecutionContext, failed_nodes: List[str]\n",
    "    ) -> List[str]:\n",
    "        \"\"\"ç²å–å—å¤±æ•—ç¯€é»å½±éŸ¿çš„æ‰€æœ‰ç¯€é»\"\"\"\n",
    "        affected = set(failed_nodes)\n",
    "\n",
    "        # æ‰¾åˆ°æ‰€æœ‰ä¾è³´å¤±æ•—ç¯€é»çš„å¾ŒçºŒç¯€é»\n",
    "        def find_dependents(node_id):\n",
    "            dependents = []\n",
    "            for candidate_id, candidate_node in context.nodes.items():\n",
    "                if node_id in candidate_node.dependencies:\n",
    "                    dependents.append(candidate_id)\n",
    "                    dependents.extend(find_dependents(candidate_id))\n",
    "            return dependents\n",
    "\n",
    "        for failed_node in failed_nodes:\n",
    "            affected.update(find_dependents(failed_node))\n",
    "\n",
    "        return list(affected)\n",
    "\n",
    "    async def _execute_partial_dag(\n",
    "        self, context: DAGExecutionContext, nodes_to_execute: List[str]\n",
    "    ):\n",
    "        \"\"\"åŸ·è¡Œéƒ¨åˆ† DAG ç¯€é»\"\"\"\n",
    "        # é‡ç½®æŒ‡å®šç¯€é»ç‹€æ…‹\n",
    "        for node_id in nodes_to_execute:\n",
    "            if node_id in context.nodes:\n",
    "                node = context.nodes[node_id]\n",
    "                if node.status in [NodeStatus.FAILED, NodeStatus.COMPLETED]:\n",
    "                    node.status = NodeStatus.PENDING\n",
    "                    node.error = None\n",
    "                    if node_id in context.results:\n",
    "                        del context.results[node_id]\n",
    "\n",
    "        # å»ºç«‹å­ DAG ä¸Šä¸‹æ–‡\n",
    "        sub_context = DAGExecutionContext()\n",
    "        sub_context.nodes = {\n",
    "            nid: context.nodes[nid] for nid in nodes_to_execute if nid in context.nodes\n",
    "        }\n",
    "        sub_context.edges = {\n",
    "            nid: context.edges[nid] for nid in context.edges if nid in nodes_to_execute\n",
    "        }\n",
    "        sub_context.results = context.results.copy()\n",
    "        sub_context.blackboard = context.blackboard.copy()\n",
    "\n",
    "        # åŸ·è¡Œå­ DAG\n",
    "        results = await self.executor.execute_dag(sub_context)\n",
    "\n",
    "        # æ›´æ–°åŸå§‹ä¸Šä¸‹æ–‡\n",
    "        context.results.update(results)\n",
    "        for node_id in nodes_to_execute:\n",
    "            if node_id in sub_context.nodes:\n",
    "                context.nodes[node_id] = sub_context.nodes[node_id]\n",
    "\n",
    "    def create_checkpoint(self, context: DAGExecutionContext) -> Dict:\n",
    "        \"\"\"å»ºç«‹æª¢æŸ¥é»\"\"\"\n",
    "        checkpoint = {\n",
    "            \"timestamp\": time.time(),\n",
    "            \"results\": context.results.copy(),\n",
    "            \"blackboard\": context.blackboard.copy(),\n",
    "            \"node_states\": {},\n",
    "        }\n",
    "\n",
    "        for node_id, node in context.nodes.items():\n",
    "            checkpoint[\"node_states\"][node_id] = {\n",
    "                \"status\": node.status.value,\n",
    "                \"result\": node.result,\n",
    "                \"error\": node.error,\n",
    "            }\n",
    "\n",
    "        return checkpoint\n",
    "\n",
    "    def restore_checkpoint(self, context: DAGExecutionContext, checkpoint: Dict):\n",
    "        \"\"\"å¾æª¢æŸ¥é»æ¢å¾©\"\"\"\n",
    "        context.results = checkpoint[\"results\"].copy()\n",
    "        context.blackboard = checkpoint[\"blackboard\"].copy()\n",
    "\n",
    "        for node_id, state in checkpoint[\"node_states\"].items():\n",
    "            if node_id in context.nodes:\n",
    "                node = context.nodes[node_id]\n",
    "                node.status = NodeStatus(state[\"status\"])\n",
    "                node.result = state[\"result\"]\n",
    "                node.error = state[\"error\"]\n",
    "\n",
    "\n",
    "print(\"âœ“ å¤±æ•—è™•ç†èˆ‡é‡è©¦æ©Ÿåˆ¶å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba35cc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Smoke Test - å®Œæ•´ DAG åŸ·è¡Œé©—è­‰\n",
    "async def run_got_smoke_test():\n",
    "    \"\"\"GoT DAG å®Œæ•´åŠŸèƒ½ç…™éœ§æ¸¬è©¦\"\"\"\n",
    "    print(\"ğŸ§ª é–‹å§‹ GoT DAG Smoke Test\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # 1. å»ºç«‹æ¸¬è©¦ DAG\n",
    "    print(\"\\n1ï¸âƒ£ å»ºç«‹æ¸¬è©¦ DAG...\")\n",
    "    test_dag = create_comprehensive_research_dag()\n",
    "\n",
    "    # 2. å»ºç«‹åŸ·è¡Œå™¨\n",
    "    print(\"\\n2ï¸âƒ£ åˆå§‹åŒ–åŸ·è¡Œå™¨...\")\n",
    "    executor = AgentDAGExecutor(agent_registry)\n",
    "    failure_handler = DAGFailureHandler(executor)\n",
    "\n",
    "    # 3. å»ºç«‹æª¢æŸ¥é»\n",
    "    print(\"\\n3ï¸âƒ£ å»ºç«‹åˆå§‹æª¢æŸ¥é»...\")\n",
    "    checkpoint = failure_handler.create_checkpoint(test_dag)\n",
    "\n",
    "    # 4. åŸ·è¡Œ DAG\n",
    "    print(\"\\n4ï¸âƒ£ åŸ·è¡Œå®Œæ•´ DAG...\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        results = await executor.execute_dag(test_dag)\n",
    "        execution_time = time.time() - start_time\n",
    "\n",
    "        print(f\"\\nâœ… DAG åŸ·è¡Œå®Œæˆï¼ç¸½è€—æ™‚ï¼š{execution_time:.2f}ç§’\")\n",
    "        print(f\"ğŸ“Š ç”¢ç”Ÿçµæœæ•¸ï¼š{len(results)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ DAG åŸ·è¡Œå¤±æ•—ï¼š{e}\")\n",
    "        print(\"\\nğŸ”„ å˜—è©¦å¤±æ•—è™•ç†...\")\n",
    "\n",
    "        # æ¢å¾©æª¢æŸ¥é»ä¸¦é‡è©¦\n",
    "        failure_handler.restore_checkpoint(test_dag, checkpoint)\n",
    "        retry_success = await failure_handler.retry_failed_nodes(test_dag)\n",
    "\n",
    "        if retry_success:\n",
    "            print(\"âœ… é‡è©¦æˆåŠŸï¼\")\n",
    "        else:\n",
    "            print(\"âŒ é‡è©¦å¤±æ•—\")\n",
    "\n",
    "    # 5. ç”Ÿæˆå ±å‘Š\n",
    "    print(\"\\n5ï¸âƒ£ ç”ŸæˆåŸ·è¡Œå ±å‘Š...\")\n",
    "    report = generate_execution_report(test_dag)\n",
    "\n",
    "    # å„²å­˜å ±å‘Š\n",
    "    import os\n",
    "\n",
    "    os.makedirs(\"outs/got_reports\", exist_ok=True)\n",
    "    report_path = f\"outs/got_reports/dag_execution_{int(time.time())}.md\"\n",
    "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report)\n",
    "    print(f\"ğŸ“„ å ±å‘Šå·²å„²å­˜ï¼š{report_path}\")\n",
    "\n",
    "    # 6. å¯è¦–åŒ– DAGï¼ˆå¦‚æœæœ‰ matplotlibï¼‰\n",
    "    print(\"\\n6ï¸âƒ£ ç”Ÿæˆ DAG å¯è¦–åŒ–...\")\n",
    "    try:\n",
    "        viz_path = f\"outs/got_reports/dag_visualization_{int(time.time())}.png\"\n",
    "        visualize_dag(test_dag, viz_path)\n",
    "        print(f\"ğŸ“Š å¯è¦–åŒ–å·²å„²å­˜ï¼š{viz_path}\")\n",
    "    except ImportError:\n",
    "        print(\"âš ï¸  matplotlib æœªå®‰è£ï¼Œè·³éå¯è¦–åŒ–\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  å¯è¦–åŒ–å¤±æ•—ï¼š{e}\")\n",
    "\n",
    "    # 7. é©—è­‰é—œéµçµæœ\n",
    "    print(\"\\n7ï¸âƒ£ é©—è­‰æ¸¬è©¦çµæœ...\")\n",
    "\n",
    "    # æª¢æŸ¥æ˜¯å¦æœ‰æœ€çµ‚çµæœ\n",
    "    if \"final_report\" in results:\n",
    "        print(\"âœ… æœ€çµ‚å ±å‘Šç”ŸæˆæˆåŠŸ\")\n",
    "    else:\n",
    "        print(\"âŒ æœ€çµ‚å ±å‘Šç”Ÿæˆå¤±æ•—\")\n",
    "\n",
    "    # æª¢æŸ¥åŸ·è¡Œè·¯å¾‘\n",
    "    completed_nodes = [\n",
    "        nid\n",
    "        for nid, node in test_dag.nodes.items()\n",
    "        if node.status == NodeStatus.COMPLETED\n",
    "    ]\n",
    "    print(f\"âœ… å®Œæˆç¯€é»æ•¸ï¼š{len(completed_nodes)}/{len(test_dag.nodes)}\")\n",
    "\n",
    "    # æª¢æŸ¥æ¢ä»¶åˆ†æ”¯\n",
    "    condition_nodes = [\n",
    "        nid\n",
    "        for nid, node in test_dag.nodes.items()\n",
    "        if node.node_type == NodeType.CONDITION\n",
    "    ]\n",
    "    print(f\"ğŸ”€ æ¢ä»¶ç¯€é»è™•ç†ï¼š{len(condition_nodes)} å€‹\")\n",
    "\n",
    "    # æª¢æŸ¥èšåˆç¯€é»\n",
    "    agg_nodes = [\n",
    "        nid\n",
    "        for nid, node in test_dag.nodes.items()\n",
    "        if node.node_type == NodeType.AGGREGATION\n",
    "    ]\n",
    "    print(f\"ğŸ”— èšåˆç¯€é»è™•ç†ï¼š{len(agg_nodes)} å€‹\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ğŸ‰ GoT DAG Smoke Test å®Œæˆï¼\")\n",
    "\n",
    "    return {\n",
    "        \"success\": len(\n",
    "            [n for n in test_dag.nodes.values() if n.status == NodeStatus.COMPLETED]\n",
    "        )\n",
    "        > 0,\n",
    "        \"total_nodes\": len(test_dag.nodes),\n",
    "        \"completed_nodes\": len(completed_nodes),\n",
    "        \"execution_time\": execution_time if \"execution_time\" in locals() else 0,\n",
    "        \"final_results\": len(results),\n",
    "    }\n",
    "\n",
    "\n",
    "# åŸ·è¡Œç…™éœ§æ¸¬è©¦\n",
    "smoke_result = await run_got_smoke_test()\n",
    "print(f\"\\nğŸ“‹ æ¸¬è©¦æ‘˜è¦ï¼š{smoke_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8e2550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoke Test çµæœé©—è­‰\n",
    "# æœ€å°é©—è­‰ï¼šç¢ºä¿ DAG è‡³å°‘æœ‰4å€‹ç¯€é»åŸ·è¡Œå®Œæˆ\n",
    "assert (\n",
    "    smoke_result[\"completed_nodes\"] >= 4\n",
    "), f\"æœŸæœ›è‡³å°‘4å€‹ç¯€é»å®Œæˆï¼Œå¯¦éš›ï¼š{smoke_result['completed_nodes']}\"\n",
    "assert smoke_result[\"success\"], \"DAG åŸ·è¡Œæ‡‰è©²æˆåŠŸ\"\n",
    "assert smoke_result[\"final_results\"] > 0, \"æ‡‰è©²ç”¢ç”Ÿæœ€çµ‚çµæœ\"\n",
    "\n",
    "print(\"âœ… æ‰€æœ‰ç…™éœ§æ¸¬è©¦é©—è­‰é€šéï¼\")\n",
    "print(\"\\nğŸ¯ GoT æ ¸å¿ƒåŠŸèƒ½ï¼š\")\n",
    "print(\"  âœ“ DAG ç¯€é»å®šç¾©èˆ‡å»ºæ§‹\")\n",
    "print(\"  âœ“ æ‹“æ’²æ’åºèˆ‡ä¸¦è¡ŒåŸ·è¡Œ\")\n",
    "print(\"  âœ“ æ¢ä»¶åˆ†æ”¯èˆ‡èšåˆç¯€é»\")\n",
    "print(\"  âœ“ Agent è§’è‰²æ•´åˆ\")\n",
    "print(\"  âœ“ å¤±æ•—è™•ç†èˆ‡é‡è©¦æ©Ÿåˆ¶\")\n",
    "print(\"  âœ“ åŸ·è¡Œç›£æ§èˆ‡å¯è¦–åŒ–\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
