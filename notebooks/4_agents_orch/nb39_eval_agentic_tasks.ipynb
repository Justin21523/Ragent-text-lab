{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af6acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb39_eval_agentic_tasks.ipynb\n",
    "# å¤šä»£ç†ä»»å‹™è©•ä¼°åŸºæº–\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (è¤‡è£½åˆ°æ¯æœ¬ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bc8a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import and Setup\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Mock imports - replace with actual modules from previous notebooks\n",
    "from shared_utils.agents.orchestrator import Orchestrator\n",
    "from shared_utils.agents.roles import Researcher, Planner, Writer, Reviewer\n",
    "from shared_utils.agents.blackboard import Blackboard\n",
    "from shared_utils.rag.retriever import Retriever\n",
    "from shared_utils.metrics.text_eval import calculate_rouge, calculate_consistency\n",
    "from shared_utils.adapters.llm_adapter import LLMAdapter\n",
    "\n",
    "# Create output directory\n",
    "outs_dir = Path(\"outs/eval_agentic\")\n",
    "outs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"è©•ä¼°ç’°å¢ƒåˆå§‹åŒ–å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c575510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Evaluation Task Definitions\n",
    "@dataclass\n",
    "class EvalTask:\n",
    "    \"\"\"Standard evaluation task definition\"\"\"\n",
    "\n",
    "    id: str\n",
    "    name: str\n",
    "    query: str\n",
    "    domain: str\n",
    "    complexity: str  # simple|medium|complex\n",
    "    expected_sections: List[str]\n",
    "    max_time_seconds: int = 300\n",
    "    min_citations: int = 2\n",
    "\n",
    "\n",
    "# Define standard evaluation tasks\n",
    "EVAL_TASKS = [\n",
    "    EvalTask(\n",
    "        id=\"task_001\",\n",
    "        name=\"RAGæŠ€è¡“ç°¡ä»‹\",\n",
    "        query=\"è«‹è§£é‡‹ä»€éº¼æ˜¯RAGï¼ˆæª¢ç´¢å¢å¼·ç”Ÿæˆï¼‰ï¼Œä¸¦èªªæ˜å…¶æ ¸å¿ƒå„ªå‹¢å’Œæ‡‰ç”¨å ´æ™¯\",\n",
    "        domain=\"tech\",\n",
    "        complexity=\"simple\",\n",
    "        expected_sections=[\"å®šç¾©\", \"æ ¸å¿ƒçµ„ä»¶\", \"å„ªå‹¢\", \"æ‡‰ç”¨\"],\n",
    "        max_time_seconds=180,\n",
    "        min_citations=3,\n",
    "    ),\n",
    "    EvalTask(\n",
    "        id=\"task_002\",\n",
    "        name=\"å¤šæ¨¡æ…‹AIç™¼å±•åˆ†æ\",\n",
    "        query=\"åˆ†æç•¶å‰å¤šæ¨¡æ…‹AIçš„ç™¼å±•è¶¨å‹¢ï¼Œæ¯”è¼ƒä¸»è¦æŠ€è¡“è·¯ç·šï¼Œä¸¦é æ¸¬æœªä¾†ç™¼å±•æ–¹å‘\",\n",
    "        domain=\"tech\",\n",
    "        complexity=\"medium\",\n",
    "        expected_sections=[\"ç¾ç‹€åˆ†æ\", \"æŠ€è¡“æ¯”è¼ƒ\", \"ç™¼å±•è¶¨å‹¢\", \"æœªä¾†é æ¸¬\"],\n",
    "        max_time_seconds=240,\n",
    "        min_citations=5,\n",
    "    ),\n",
    "    EvalTask(\n",
    "        id=\"task_003\",\n",
    "        name=\"æ•™è‚²AIå€«ç†è€ƒé‡\",\n",
    "        query=\"æ·±å…¥æ¢è¨AIåœ¨æ•™è‚²é ˜åŸŸæ‡‰ç”¨çš„å€«ç†è­°é¡Œï¼ŒåŒ…æ‹¬éš±ç§ä¿è­·ã€å…¬å¹³æ€§ã€é€æ˜åº¦ç­‰ï¼Œä¸¦æå‡ºå…·é«”å»ºè­°\",\n",
    "        domain=\"edu\",\n",
    "        complexity=\"complex\",\n",
    "        expected_sections=[\"èƒŒæ™¯\", \"ä¸»è¦å€«ç†è­°é¡Œ\", \"æ¡ˆä¾‹åˆ†æ\", \"è§£æ±ºæ–¹æ¡ˆ\", \"å¯¦æ–½å»ºè­°\"],\n",
    "        max_time_seconds=300,\n",
    "        min_citations=7,\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"å®šç¾©äº† {len(EVAL_TASKS)} å€‹è©•ä¼°ä»»å‹™\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d0e98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Metrics Calculation Framework\n",
    "@dataclass\n",
    "class AgenticsMetrics:\n",
    "    \"\"\"Agentic task evaluation metrics\"\"\"\n",
    "\n",
    "    task_id: str\n",
    "    completion_rate: float  # 0-1, task completion percentage\n",
    "    citation_accuracy: float  # 0-1, citation quality score\n",
    "    content_consistency: float  # 0-1, internal consistency\n",
    "    section_coverage: float  # 0-1, expected sections covered\n",
    "    execution_time: float  # seconds\n",
    "    total_tokens: int\n",
    "    error_count: int\n",
    "    retry_count: int\n",
    "\n",
    "    def overall_score(self) -> float:\n",
    "        \"\"\"Calculate weighted overall score\"\"\"\n",
    "        weights = {\n",
    "            \"completion\": 0.25,\n",
    "            \"citation\": 0.20,\n",
    "            \"consistency\": 0.20,\n",
    "            \"coverage\": 0.20,\n",
    "            \"efficiency\": 0.15,  # based on time/token efficiency\n",
    "        }\n",
    "\n",
    "        efficiency = min(\n",
    "            1.0, 180.0 / max(self.execution_time, 30)\n",
    "        )  # normalize to 3min baseline\n",
    "\n",
    "        return (\n",
    "            weights[\"completion\"] * self.completion_rate\n",
    "            + weights[\"citation\"] * self.citation_accuracy\n",
    "            + weights[\"consistency\"] * self.content_consistency\n",
    "            + weights[\"coverage\"] * self.section_coverage\n",
    "            + weights[\"efficiency\"] * efficiency\n",
    "        )\n",
    "\n",
    "\n",
    "def calculate_citation_accuracy(content: str, citations: List[str]) -> float:\n",
    "    \"\"\"Calculate citation accuracy based on content-citation alignment\"\"\"\n",
    "    if not citations:\n",
    "        return 0.0\n",
    "\n",
    "    # Simple heuristic: check if citations are properly formatted and referenced\n",
    "    citation_refs = []\n",
    "    for i, cite in enumerate(citations, 1):\n",
    "        if f\"[{i}]\" in content:\n",
    "            citation_refs.append(1)\n",
    "        else:\n",
    "            citation_refs.append(0)\n",
    "\n",
    "    return sum(citation_refs) / len(citations) if citations else 0.0\n",
    "\n",
    "\n",
    "def calculate_section_coverage(content: str, expected_sections: List[str]) -> float:\n",
    "    \"\"\"Calculate how well content covers expected sections\"\"\"\n",
    "    content_lower = content.lower()\n",
    "    covered = 0\n",
    "\n",
    "    for section in expected_sections:\n",
    "        # Simple keyword matching - could be enhanced with semantic similarity\n",
    "        if any(keyword in content_lower for keyword in section.lower().split()):\n",
    "            covered += 1\n",
    "\n",
    "    return covered / len(expected_sections) if expected_sections else 1.0\n",
    "\n",
    "\n",
    "def evaluate_agentic_task(\n",
    "    orchestrator: Orchestrator, task: EvalTask\n",
    ") -> AgenticsMetrics:\n",
    "    \"\"\"Evaluate single agentic task execution\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # Execute task through orchestrator\n",
    "        result = orchestrator.execute_task(\n",
    "            query=task.query, max_time=task.max_time_seconds, domain=task.domain\n",
    "        )\n",
    "\n",
    "        execution_time = time.time() - start_time\n",
    "\n",
    "        # Extract results\n",
    "        final_content = result.get(\"final_output\", \"\")\n",
    "        citations = result.get(\"citations\", [])\n",
    "        total_tokens = result.get(\"total_tokens\", 0)\n",
    "        error_count = result.get(\"error_count\", 0)\n",
    "        retry_count = result.get(\"retry_count\", 0)\n",
    "\n",
    "        # Calculate metrics\n",
    "        completion_rate = (\n",
    "            1.0 if len(final_content) > 200 else len(final_content) / 200.0\n",
    "        )\n",
    "        citation_accuracy = calculate_citation_accuracy(final_content, citations)\n",
    "        content_consistency = calculate_consistency(final_content)\n",
    "        section_coverage = calculate_section_coverage(\n",
    "            final_content, task.expected_sections\n",
    "        )\n",
    "\n",
    "        return AgenticsMetrics(\n",
    "            task_id=task.id,\n",
    "            completion_rate=min(1.0, completion_rate),\n",
    "            citation_accuracy=citation_accuracy,\n",
    "            content_consistency=content_consistency,\n",
    "            section_coverage=section_coverage,\n",
    "            execution_time=execution_time,\n",
    "            total_tokens=total_tokens,\n",
    "            error_count=error_count,\n",
    "            retry_count=retry_count,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        execution_time = time.time() - start_time\n",
    "        print(f\"Task {task.id} failed: {e}\")\n",
    "\n",
    "        return AgenticsMetrics(\n",
    "            task_id=task.id,\n",
    "            completion_rate=0.0,\n",
    "            citation_accuracy=0.0,\n",
    "            content_consistency=0.0,\n",
    "            section_coverage=0.0,\n",
    "            execution_time=execution_time,\n",
    "            total_tokens=0,\n",
    "            error_count=1,\n",
    "            retry_count=0,\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"è©•ä¼°æŒ‡æ¨™æ¡†æ¶å»ºç«‹å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a641d441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Execute Evaluation and Collect Results\n",
    "def run_agentic_evaluation(tasks: List[EvalTask]) -> List[AgenticsMetrics]:\n",
    "    \"\"\"Run evaluation on all tasks and collect metrics\"\"\"\n",
    "\n",
    "    # Initialize orchestrator (mock setup - adjust based on actual implementation)\n",
    "    llm_adapter = LLMAdapter(\n",
    "        model_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        backend=\"transformers\",\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=\"auto\",\n",
    "    )\n",
    "\n",
    "    # Mock retriever setup\n",
    "    retriever = None  # Would be initialized with actual RAG components\n",
    "\n",
    "    orchestrator = Orchestrator(\n",
    "        llm_adapter=llm_adapter,\n",
    "        retriever=retriever,\n",
    "        config={\"max_iterations\": 5, \"timeout_seconds\": 300, \"retry_attempts\": 3},\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "\n",
    "    print(\"é–‹å§‹åŸ·è¡Œä»£ç†ä»»å‹™è©•ä¼°...\")\n",
    "    for task in tasks:\n",
    "        print(f\"\\nåŸ·è¡Œä»»å‹™: {task.name} ({task.complexity})\")\n",
    "\n",
    "        metrics = evaluate_agentic_task(orchestrator, task)\n",
    "        results.append(metrics)\n",
    "\n",
    "        print(f\"å®Œæˆåº¦: {metrics.completion_rate:.2f}\")\n",
    "        print(f\"å¼•ç”¨æº–ç¢ºæ€§: {metrics.citation_accuracy:.2f}\")\n",
    "        print(f\"å…§å®¹ä¸€è‡´æ€§: {metrics.content_consistency:.2f}\")\n",
    "        print(f\"ç« ç¯€è¦†è“‹åº¦: {metrics.section_coverage:.2f}\")\n",
    "        print(f\"ç¸½åˆ†: {metrics.overall_score():.2f}\")\n",
    "        print(f\"åŸ·è¡Œæ™‚é–“: {metrics.execution_time:.1f}s\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run evaluation (commented out for demo - would run actual evaluation)\n",
    "# eval_results = run_agentic_evaluation(EVAL_TASKS)\n",
    "\n",
    "# Demo results for illustration\n",
    "demo_results = [\n",
    "    AgenticsMetrics(\n",
    "        task_id=\"task_001\",\n",
    "        completion_rate=0.85,\n",
    "        citation_accuracy=0.75,\n",
    "        content_consistency=0.80,\n",
    "        section_coverage=0.90,\n",
    "        execution_time=156.3,\n",
    "        total_tokens=1247,\n",
    "        error_count=0,\n",
    "        retry_count=1,\n",
    "    ),\n",
    "    AgenticsMetrics(\n",
    "        task_id=\"task_002\",\n",
    "        completion_rate=0.78,\n",
    "        citation_accuracy=0.82,\n",
    "        content_consistency=0.75,\n",
    "        section_coverage=0.85,\n",
    "        execution_time=198.7,\n",
    "        total_tokens=1856,\n",
    "        error_count=1,\n",
    "        retry_count=2,\n",
    "    ),\n",
    "    AgenticsMetrics(\n",
    "        task_id=\"task_003\",\n",
    "        completion_rate=0.72,\n",
    "        citation_accuracy=0.68,\n",
    "        content_consistency=0.71,\n",
    "        section_coverage=0.80,\n",
    "        execution_time=245.2,\n",
    "        total_tokens=2134,\n",
    "        error_count=0,\n",
    "        retry_count=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "eval_results = demo_results\n",
    "print(f\"\\nè©•ä¼°å®Œæˆï¼Œå…±æ”¶é›† {len(eval_results)} å€‹çµæœ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e9fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Report Generation and Visualization\n",
    "def generate_evaluation_report(\n",
    "    results: List[AgenticsMetrics], tasks: List[EvalTask]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Generate comprehensive evaluation report\"\"\"\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_data = []\n",
    "    for result, task in zip(results, tasks):\n",
    "        row = asdict(result)\n",
    "        row.update(\n",
    "            {\n",
    "                \"task_name\": task.name,\n",
    "                \"complexity\": task.complexity,\n",
    "                \"domain\": task.domain,\n",
    "                \"overall_score\": result.overall_score(),\n",
    "                \"tokens_per_second\": result.total_tokens\n",
    "                / max(result.execution_time, 1),\n",
    "                \"success_rate\": 1.0 if result.error_count == 0 else 0.0,\n",
    "            }\n",
    "        )\n",
    "        df_data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(df_data)\n",
    "\n",
    "    # Save detailed results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = outs_dir / f\"agentic_eval_results_{timestamp}.csv\"\n",
    "    df.to_csv(results_file, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # Generate summary statistics\n",
    "    summary_stats = {\n",
    "        \"avg_overall_score\": df[\"overall_score\"].mean(),\n",
    "        \"avg_completion_rate\": df[\"completion_rate\"].mean(),\n",
    "        \"avg_citation_accuracy\": df[\"citation_accuracy\"].mean(),\n",
    "        \"avg_content_consistency\": df[\"content_consistency\"].mean(),\n",
    "        \"avg_section_coverage\": df[\"section_coverage\"].mean(),\n",
    "        \"avg_execution_time\": df[\"execution_time\"].mean(),\n",
    "        \"success_rate\": df[\"success_rate\"].mean(),\n",
    "        \"total_tokens\": df[\"total_tokens\"].sum(),\n",
    "        \"avg_tokens_per_second\": df[\"tokens_per_second\"].mean(),\n",
    "    }\n",
    "\n",
    "    # Save summary\n",
    "    summary_file = outs_dir / f\"agentic_eval_summary_{timestamp}.json\"\n",
    "    with open(summary_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary_stats, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"è©•ä¼°å ±è¡¨å·²ä¿å­˜: {results_file}\")\n",
    "    print(f\"æ‘˜è¦çµ±è¨ˆå·²ä¿å­˜: {summary_file}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_evaluation_charts(df: pd.DataFrame):\n",
    "    \"\"\"Create visualization charts for evaluation results\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle(\"å¤šä»£ç†ä»»å‹™è©•ä¼°çµæœ\", fontsize=16)\n",
    "\n",
    "    # Overall scores by complexity\n",
    "    axes[0, 0].bar(\n",
    "        df[\"complexity\"],\n",
    "        df[\"overall_score\"],\n",
    "        color=[\"lightblue\", \"lightgreen\", \"lightcoral\"],\n",
    "    )\n",
    "    axes[0, 0].set_title(\"æ•´é«”åˆ†æ•¸ vs ä»»å‹™è¤‡é›œåº¦\")\n",
    "    axes[0, 0].set_ylabel(\"æ•´é«”åˆ†æ•¸\")\n",
    "    axes[0, 0].set_ylim(0, 1)\n",
    "\n",
    "    # Metric breakdown\n",
    "    metrics = [\n",
    "        \"completion_rate\",\n",
    "        \"citation_accuracy\",\n",
    "        \"content_consistency\",\n",
    "        \"section_coverage\",\n",
    "    ]\n",
    "    metric_names = [\"å®Œæˆåº¦\", \"å¼•ç”¨æº–ç¢ºæ€§\", \"å…§å®¹ä¸€è‡´æ€§\", \"ç« ç¯€è¦†è“‹åº¦\"]\n",
    "\n",
    "    x = np.arange(len(df))\n",
    "    width = 0.2\n",
    "    for i, (metric, name) in enumerate(zip(metrics, metric_names)):\n",
    "        axes[0, 1].bar(x + i * width, df[metric], width, label=name, alpha=0.8)\n",
    "\n",
    "    axes[0, 1].set_title(\"å„é …æŒ‡æ¨™è¡¨ç¾\")\n",
    "    axes[0, 1].set_ylabel(\"åˆ†æ•¸\")\n",
    "    axes[0, 1].set_xlabel(\"ä»»å‹™\")\n",
    "    axes[0, 1].set_xticks(x + width * 1.5)\n",
    "    axes[0, 1].set_xticklabels([f\"T{i+1}\" for i in range(len(df))])\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].set_ylim(0, 1)\n",
    "\n",
    "    # Execution time vs complexity\n",
    "    complexity_order = [\"simple\", \"medium\", \"complex\"]\n",
    "    complexity_colors = {\"simple\": \"green\", \"medium\": \"orange\", \"complex\": \"red\"}\n",
    "\n",
    "    for complexity in complexity_order:\n",
    "        mask = df[\"complexity\"] == complexity\n",
    "        if mask.any():\n",
    "            axes[1, 0].scatter(\n",
    "                df[mask][\"execution_time\"],\n",
    "                df[mask][\"overall_score\"],\n",
    "                c=complexity_colors[complexity],\n",
    "                label=complexity,\n",
    "                s=100,\n",
    "                alpha=0.7,\n",
    "            )\n",
    "\n",
    "    axes[1, 0].set_xlabel(\"åŸ·è¡Œæ™‚é–“ (ç§’)\")\n",
    "    axes[1, 0].set_ylabel(\"æ•´é«”åˆ†æ•¸\")\n",
    "    axes[1, 0].set_title(\"åŸ·è¡Œæ™‚é–“ vs æ•´é«”åˆ†æ•¸\")\n",
    "    axes[1, 0].legend()\n",
    "\n",
    "    # Tokens/second performance\n",
    "    axes[1, 1].bar(range(len(df)), df[\"tokens_per_second\"], color=\"skyblue\")\n",
    "    axes[1, 1].set_title(\"Token ç”Ÿæˆæ•ˆç‡\")\n",
    "    axes[1, 1].set_ylabel(\"Tokens/ç§’\")\n",
    "    axes[1, 1].set_xlabel(\"ä»»å‹™\")\n",
    "    axes[1, 1].set_xticks(range(len(df)))\n",
    "    axes[1, 1].set_xticklabels([f\"T{i+1}\" for i in range(len(df))])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save chart\n",
    "    chart_file = (\n",
    "        outs_dir / f\"agentic_eval_charts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "    )\n",
    "    plt.savefig(chart_file, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"è©•ä¼°åœ–è¡¨å·²ä¿å­˜: {chart_file}\")\n",
    "\n",
    "\n",
    "# Generate report and charts\n",
    "df_results = generate_evaluation_report(eval_results, EVAL_TASKS)\n",
    "create_evaluation_charts(df_results)\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n=== è©•ä¼°æ‘˜è¦ ===\")\n",
    "print(f\"å¹³å‡æ•´é«”åˆ†æ•¸: {df_results['overall_score'].mean():.3f}\")\n",
    "print(f\"å¹³å‡å®Œæˆåº¦: {df_results['completion_rate'].mean():.3f}\")\n",
    "print(f\"å¹³å‡å¼•ç”¨æº–ç¢ºæ€§: {df_results['citation_accuracy'].mean():.3f}\")\n",
    "print(f\"å¹³å‡å…§å®¹ä¸€è‡´æ€§: {df_results['content_consistency'].mean():.3f}\")\n",
    "print(f\"å¹³å‡ç« ç¯€è¦†è“‹åº¦: {df_results['section_coverage'].mean():.3f}\")\n",
    "print(f\"å¹³å‡åŸ·è¡Œæ™‚é–“: {df_results['execution_time'].mean():.1f} ç§’\")\n",
    "print(f\"æˆåŠŸç‡: {df_results['success_rate'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c22878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Smoke Test\n",
    "def smoke_test_agentic_eval():\n",
    "    \"\"\"Quick smoke test for agentic evaluation system\"\"\"\n",
    "\n",
    "    print(\"=== ä»£ç†ä»»å‹™è©•ä¼°ç³»çµ±ç…™éœ§æ¸¬è©¦ ===\")\n",
    "\n",
    "    # Test 1: Task definition\n",
    "    test_task = EvalTask(\n",
    "        id=\"smoke_001\",\n",
    "        name=\"æ¸¬è©¦ä»»å‹™\",\n",
    "        query=\"ç°¡å–®æ¸¬è©¦æŸ¥è©¢\",\n",
    "        domain=\"general\",\n",
    "        complexity=\"simple\",\n",
    "        expected_sections=[\"ä»‹ç´¹\", \"çµè«–\"],\n",
    "        max_time_seconds=60,\n",
    "    )\n",
    "\n",
    "    assert test_task.id == \"smoke_001\"\n",
    "    assert test_task.complexity == \"simple\"\n",
    "    print(\"âœ“ ä»»å‹™å®šç¾©æ¸¬è©¦é€šé\")\n",
    "\n",
    "    # Test 2: Metrics calculation\n",
    "    test_metrics = AgenticsMetrics(\n",
    "        task_id=\"smoke_001\",\n",
    "        completion_rate=0.8,\n",
    "        citation_accuracy=0.7,\n",
    "        content_consistency=0.75,\n",
    "        section_coverage=0.85,\n",
    "        execution_time=45.0,\n",
    "        total_tokens=500,\n",
    "        error_count=0,\n",
    "        retry_count=0,\n",
    "    )\n",
    "\n",
    "    overall_score = test_metrics.overall_score()\n",
    "    assert 0 <= overall_score <= 1\n",
    "    assert overall_score > 0.5  # Should be reasonable score\n",
    "    print(f\"âœ“ æŒ‡æ¨™è¨ˆç®—æ¸¬è©¦é€šé (ç¸½åˆ†: {overall_score:.3f})\")\n",
    "\n",
    "    # Test 3: Citation accuracy calculation\n",
    "    test_content = \"é€™æ˜¯æ¸¬è©¦å…§å®¹ [1]ï¼ŒåŒ…å«å¼•ç”¨ [2]ã€‚\"\n",
    "    test_citations = [\"ä¾†æº1\", \"ä¾†æº2\"]\n",
    "    citation_acc = calculate_citation_accuracy(test_content, test_citations)\n",
    "    assert citation_acc == 1.0  # Both citations referenced\n",
    "    print(f\"âœ“ å¼•ç”¨æº–ç¢ºæ€§è¨ˆç®—æ¸¬è©¦é€šé (æº–ç¢ºæ€§: {citation_acc:.3f})\")\n",
    "\n",
    "    # Test 4: Section coverage calculation\n",
    "    test_content = \"é€™æ˜¯ä»‹ç´¹éƒ¨åˆ†çš„å…§å®¹ã€‚æœ€å¾Œæˆ‘å€‘å¾—å‡ºçµè«–ã€‚\"\n",
    "    test_sections = [\"ä»‹ç´¹\", \"çµè«–\"]\n",
    "    coverage = calculate_section_coverage(test_content, test_sections)\n",
    "    assert coverage == 1.0  # Both sections covered\n",
    "    print(f\"âœ“ ç« ç¯€è¦†è“‹åº¦è¨ˆç®—æ¸¬è©¦é€šé (è¦†è“‹åº¦: {coverage:.3f})\")\n",
    "\n",
    "    # Test 5: Report generation\n",
    "    test_results = [test_metrics]\n",
    "    test_tasks = [test_task]\n",
    "    df_test = generate_evaluation_report(test_results, test_tasks)\n",
    "    assert len(df_test) == 1\n",
    "    assert \"overall_score\" in df_test.columns\n",
    "    print(\"âœ“ å ±è¡¨ç”Ÿæˆæ¸¬è©¦é€šé\")\n",
    "\n",
    "    print(\"\\nğŸ‰ æ‰€æœ‰ç…™éœ§æ¸¬è©¦é€šéï¼ä»£ç†ä»»å‹™è©•ä¼°ç³»çµ±é‹ä½œæ­£å¸¸ã€‚\")\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_result = smoke_test_agentic_eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
