{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af6acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb39_eval_agentic_tasks.ipynb\n",
    "# 多代理任務評估基準\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (複製到每本 notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bc8a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import and Setup\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Mock imports - replace with actual modules from previous notebooks\n",
    "from shared_utils.agents.orchestrator import Orchestrator\n",
    "from shared_utils.agents.roles import Researcher, Planner, Writer, Reviewer\n",
    "from shared_utils.agents.blackboard import Blackboard\n",
    "from shared_utils.rag.retriever import Retriever\n",
    "from shared_utils.metrics.text_eval import calculate_rouge, calculate_consistency\n",
    "from shared_utils.adapters.llm_adapter import LLMAdapter\n",
    "\n",
    "# Create output directory\n",
    "outs_dir = Path(\"outs/eval_agentic\")\n",
    "outs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"評估環境初始化完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c575510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Evaluation Task Definitions\n",
    "@dataclass\n",
    "class EvalTask:\n",
    "    \"\"\"Standard evaluation task definition\"\"\"\n",
    "\n",
    "    id: str\n",
    "    name: str\n",
    "    query: str\n",
    "    domain: str\n",
    "    complexity: str  # simple|medium|complex\n",
    "    expected_sections: List[str]\n",
    "    max_time_seconds: int = 300\n",
    "    min_citations: int = 2\n",
    "\n",
    "\n",
    "# Define standard evaluation tasks\n",
    "EVAL_TASKS = [\n",
    "    EvalTask(\n",
    "        id=\"task_001\",\n",
    "        name=\"RAG技術簡介\",\n",
    "        query=\"請解釋什麼是RAG（檢索增強生成），並說明其核心優勢和應用場景\",\n",
    "        domain=\"tech\",\n",
    "        complexity=\"simple\",\n",
    "        expected_sections=[\"定義\", \"核心組件\", \"優勢\", \"應用\"],\n",
    "        max_time_seconds=180,\n",
    "        min_citations=3,\n",
    "    ),\n",
    "    EvalTask(\n",
    "        id=\"task_002\",\n",
    "        name=\"多模態AI發展分析\",\n",
    "        query=\"分析當前多模態AI的發展趨勢，比較主要技術路線，並預測未來發展方向\",\n",
    "        domain=\"tech\",\n",
    "        complexity=\"medium\",\n",
    "        expected_sections=[\"現狀分析\", \"技術比較\", \"發展趨勢\", \"未來預測\"],\n",
    "        max_time_seconds=240,\n",
    "        min_citations=5,\n",
    "    ),\n",
    "    EvalTask(\n",
    "        id=\"task_003\",\n",
    "        name=\"教育AI倫理考量\",\n",
    "        query=\"深入探討AI在教育領域應用的倫理議題，包括隱私保護、公平性、透明度等，並提出具體建議\",\n",
    "        domain=\"edu\",\n",
    "        complexity=\"complex\",\n",
    "        expected_sections=[\"背景\", \"主要倫理議題\", \"案例分析\", \"解決方案\", \"實施建議\"],\n",
    "        max_time_seconds=300,\n",
    "        min_citations=7,\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"定義了 {len(EVAL_TASKS)} 個評估任務\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d0e98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Metrics Calculation Framework\n",
    "@dataclass\n",
    "class AgenticsMetrics:\n",
    "    \"\"\"Agentic task evaluation metrics\"\"\"\n",
    "\n",
    "    task_id: str\n",
    "    completion_rate: float  # 0-1, task completion percentage\n",
    "    citation_accuracy: float  # 0-1, citation quality score\n",
    "    content_consistency: float  # 0-1, internal consistency\n",
    "    section_coverage: float  # 0-1, expected sections covered\n",
    "    execution_time: float  # seconds\n",
    "    total_tokens: int\n",
    "    error_count: int\n",
    "    retry_count: int\n",
    "\n",
    "    def overall_score(self) -> float:\n",
    "        \"\"\"Calculate weighted overall score\"\"\"\n",
    "        weights = {\n",
    "            \"completion\": 0.25,\n",
    "            \"citation\": 0.20,\n",
    "            \"consistency\": 0.20,\n",
    "            \"coverage\": 0.20,\n",
    "            \"efficiency\": 0.15,  # based on time/token efficiency\n",
    "        }\n",
    "\n",
    "        efficiency = min(\n",
    "            1.0, 180.0 / max(self.execution_time, 30)\n",
    "        )  # normalize to 3min baseline\n",
    "\n",
    "        return (\n",
    "            weights[\"completion\"] * self.completion_rate\n",
    "            + weights[\"citation\"] * self.citation_accuracy\n",
    "            + weights[\"consistency\"] * self.content_consistency\n",
    "            + weights[\"coverage\"] * self.section_coverage\n",
    "            + weights[\"efficiency\"] * efficiency\n",
    "        )\n",
    "\n",
    "\n",
    "def calculate_citation_accuracy(content: str, citations: List[str]) -> float:\n",
    "    \"\"\"Calculate citation accuracy based on content-citation alignment\"\"\"\n",
    "    if not citations:\n",
    "        return 0.0\n",
    "\n",
    "    # Simple heuristic: check if citations are properly formatted and referenced\n",
    "    citation_refs = []\n",
    "    for i, cite in enumerate(citations, 1):\n",
    "        if f\"[{i}]\" in content:\n",
    "            citation_refs.append(1)\n",
    "        else:\n",
    "            citation_refs.append(0)\n",
    "\n",
    "    return sum(citation_refs) / len(citations) if citations else 0.0\n",
    "\n",
    "\n",
    "def calculate_section_coverage(content: str, expected_sections: List[str]) -> float:\n",
    "    \"\"\"Calculate how well content covers expected sections\"\"\"\n",
    "    content_lower = content.lower()\n",
    "    covered = 0\n",
    "\n",
    "    for section in expected_sections:\n",
    "        # Simple keyword matching - could be enhanced with semantic similarity\n",
    "        if any(keyword in content_lower for keyword in section.lower().split()):\n",
    "            covered += 1\n",
    "\n",
    "    return covered / len(expected_sections) if expected_sections else 1.0\n",
    "\n",
    "\n",
    "def evaluate_agentic_task(\n",
    "    orchestrator: Orchestrator, task: EvalTask\n",
    ") -> AgenticsMetrics:\n",
    "    \"\"\"Evaluate single agentic task execution\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # Execute task through orchestrator\n",
    "        result = orchestrator.execute_task(\n",
    "            query=task.query, max_time=task.max_time_seconds, domain=task.domain\n",
    "        )\n",
    "\n",
    "        execution_time = time.time() - start_time\n",
    "\n",
    "        # Extract results\n",
    "        final_content = result.get(\"final_output\", \"\")\n",
    "        citations = result.get(\"citations\", [])\n",
    "        total_tokens = result.get(\"total_tokens\", 0)\n",
    "        error_count = result.get(\"error_count\", 0)\n",
    "        retry_count = result.get(\"retry_count\", 0)\n",
    "\n",
    "        # Calculate metrics\n",
    "        completion_rate = (\n",
    "            1.0 if len(final_content) > 200 else len(final_content) / 200.0\n",
    "        )\n",
    "        citation_accuracy = calculate_citation_accuracy(final_content, citations)\n",
    "        content_consistency = calculate_consistency(final_content)\n",
    "        section_coverage = calculate_section_coverage(\n",
    "            final_content, task.expected_sections\n",
    "        )\n",
    "\n",
    "        return AgenticsMetrics(\n",
    "            task_id=task.id,\n",
    "            completion_rate=min(1.0, completion_rate),\n",
    "            citation_accuracy=citation_accuracy,\n",
    "            content_consistency=content_consistency,\n",
    "            section_coverage=section_coverage,\n",
    "            execution_time=execution_time,\n",
    "            total_tokens=total_tokens,\n",
    "            error_count=error_count,\n",
    "            retry_count=retry_count,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        execution_time = time.time() - start_time\n",
    "        print(f\"Task {task.id} failed: {e}\")\n",
    "\n",
    "        return AgenticsMetrics(\n",
    "            task_id=task.id,\n",
    "            completion_rate=0.0,\n",
    "            citation_accuracy=0.0,\n",
    "            content_consistency=0.0,\n",
    "            section_coverage=0.0,\n",
    "            execution_time=execution_time,\n",
    "            total_tokens=0,\n",
    "            error_count=1,\n",
    "            retry_count=0,\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"評估指標框架建立完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a641d441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Execute Evaluation and Collect Results\n",
    "def run_agentic_evaluation(tasks: List[EvalTask]) -> List[AgenticsMetrics]:\n",
    "    \"\"\"Run evaluation on all tasks and collect metrics\"\"\"\n",
    "\n",
    "    # Initialize orchestrator (mock setup - adjust based on actual implementation)\n",
    "    llm_adapter = LLMAdapter(\n",
    "        model_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        backend=\"transformers\",\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=\"auto\",\n",
    "    )\n",
    "\n",
    "    # Mock retriever setup\n",
    "    retriever = None  # Would be initialized with actual RAG components\n",
    "\n",
    "    orchestrator = Orchestrator(\n",
    "        llm_adapter=llm_adapter,\n",
    "        retriever=retriever,\n",
    "        config={\"max_iterations\": 5, \"timeout_seconds\": 300, \"retry_attempts\": 3},\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "\n",
    "    print(\"開始執行代理任務評估...\")\n",
    "    for task in tasks:\n",
    "        print(f\"\\n執行任務: {task.name} ({task.complexity})\")\n",
    "\n",
    "        metrics = evaluate_agentic_task(orchestrator, task)\n",
    "        results.append(metrics)\n",
    "\n",
    "        print(f\"完成度: {metrics.completion_rate:.2f}\")\n",
    "        print(f\"引用準確性: {metrics.citation_accuracy:.2f}\")\n",
    "        print(f\"內容一致性: {metrics.content_consistency:.2f}\")\n",
    "        print(f\"章節覆蓋度: {metrics.section_coverage:.2f}\")\n",
    "        print(f\"總分: {metrics.overall_score():.2f}\")\n",
    "        print(f\"執行時間: {metrics.execution_time:.1f}s\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run evaluation (commented out for demo - would run actual evaluation)\n",
    "# eval_results = run_agentic_evaluation(EVAL_TASKS)\n",
    "\n",
    "# Demo results for illustration\n",
    "demo_results = [\n",
    "    AgenticsMetrics(\n",
    "        task_id=\"task_001\",\n",
    "        completion_rate=0.85,\n",
    "        citation_accuracy=0.75,\n",
    "        content_consistency=0.80,\n",
    "        section_coverage=0.90,\n",
    "        execution_time=156.3,\n",
    "        total_tokens=1247,\n",
    "        error_count=0,\n",
    "        retry_count=1,\n",
    "    ),\n",
    "    AgenticsMetrics(\n",
    "        task_id=\"task_002\",\n",
    "        completion_rate=0.78,\n",
    "        citation_accuracy=0.82,\n",
    "        content_consistency=0.75,\n",
    "        section_coverage=0.85,\n",
    "        execution_time=198.7,\n",
    "        total_tokens=1856,\n",
    "        error_count=1,\n",
    "        retry_count=2,\n",
    "    ),\n",
    "    AgenticsMetrics(\n",
    "        task_id=\"task_003\",\n",
    "        completion_rate=0.72,\n",
    "        citation_accuracy=0.68,\n",
    "        content_consistency=0.71,\n",
    "        section_coverage=0.80,\n",
    "        execution_time=245.2,\n",
    "        total_tokens=2134,\n",
    "        error_count=0,\n",
    "        retry_count=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "eval_results = demo_results\n",
    "print(f\"\\n評估完成，共收集 {len(eval_results)} 個結果\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e9fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Report Generation and Visualization\n",
    "def generate_evaluation_report(\n",
    "    results: List[AgenticsMetrics], tasks: List[EvalTask]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Generate comprehensive evaluation report\"\"\"\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_data = []\n",
    "    for result, task in zip(results, tasks):\n",
    "        row = asdict(result)\n",
    "        row.update(\n",
    "            {\n",
    "                \"task_name\": task.name,\n",
    "                \"complexity\": task.complexity,\n",
    "                \"domain\": task.domain,\n",
    "                \"overall_score\": result.overall_score(),\n",
    "                \"tokens_per_second\": result.total_tokens\n",
    "                / max(result.execution_time, 1),\n",
    "                \"success_rate\": 1.0 if result.error_count == 0 else 0.0,\n",
    "            }\n",
    "        )\n",
    "        df_data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(df_data)\n",
    "\n",
    "    # Save detailed results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = outs_dir / f\"agentic_eval_results_{timestamp}.csv\"\n",
    "    df.to_csv(results_file, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # Generate summary statistics\n",
    "    summary_stats = {\n",
    "        \"avg_overall_score\": df[\"overall_score\"].mean(),\n",
    "        \"avg_completion_rate\": df[\"completion_rate\"].mean(),\n",
    "        \"avg_citation_accuracy\": df[\"citation_accuracy\"].mean(),\n",
    "        \"avg_content_consistency\": df[\"content_consistency\"].mean(),\n",
    "        \"avg_section_coverage\": df[\"section_coverage\"].mean(),\n",
    "        \"avg_execution_time\": df[\"execution_time\"].mean(),\n",
    "        \"success_rate\": df[\"success_rate\"].mean(),\n",
    "        \"total_tokens\": df[\"total_tokens\"].sum(),\n",
    "        \"avg_tokens_per_second\": df[\"tokens_per_second\"].mean(),\n",
    "    }\n",
    "\n",
    "    # Save summary\n",
    "    summary_file = outs_dir / f\"agentic_eval_summary_{timestamp}.json\"\n",
    "    with open(summary_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary_stats, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"評估報表已保存: {results_file}\")\n",
    "    print(f\"摘要統計已保存: {summary_file}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_evaluation_charts(df: pd.DataFrame):\n",
    "    \"\"\"Create visualization charts for evaluation results\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle(\"多代理任務評估結果\", fontsize=16)\n",
    "\n",
    "    # Overall scores by complexity\n",
    "    axes[0, 0].bar(\n",
    "        df[\"complexity\"],\n",
    "        df[\"overall_score\"],\n",
    "        color=[\"lightblue\", \"lightgreen\", \"lightcoral\"],\n",
    "    )\n",
    "    axes[0, 0].set_title(\"整體分數 vs 任務複雜度\")\n",
    "    axes[0, 0].set_ylabel(\"整體分數\")\n",
    "    axes[0, 0].set_ylim(0, 1)\n",
    "\n",
    "    # Metric breakdown\n",
    "    metrics = [\n",
    "        \"completion_rate\",\n",
    "        \"citation_accuracy\",\n",
    "        \"content_consistency\",\n",
    "        \"section_coverage\",\n",
    "    ]\n",
    "    metric_names = [\"完成度\", \"引用準確性\", \"內容一致性\", \"章節覆蓋度\"]\n",
    "\n",
    "    x = np.arange(len(df))\n",
    "    width = 0.2\n",
    "    for i, (metric, name) in enumerate(zip(metrics, metric_names)):\n",
    "        axes[0, 1].bar(x + i * width, df[metric], width, label=name, alpha=0.8)\n",
    "\n",
    "    axes[0, 1].set_title(\"各項指標表現\")\n",
    "    axes[0, 1].set_ylabel(\"分數\")\n",
    "    axes[0, 1].set_xlabel(\"任務\")\n",
    "    axes[0, 1].set_xticks(x + width * 1.5)\n",
    "    axes[0, 1].set_xticklabels([f\"T{i+1}\" for i in range(len(df))])\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].set_ylim(0, 1)\n",
    "\n",
    "    # Execution time vs complexity\n",
    "    complexity_order = [\"simple\", \"medium\", \"complex\"]\n",
    "    complexity_colors = {\"simple\": \"green\", \"medium\": \"orange\", \"complex\": \"red\"}\n",
    "\n",
    "    for complexity in complexity_order:\n",
    "        mask = df[\"complexity\"] == complexity\n",
    "        if mask.any():\n",
    "            axes[1, 0].scatter(\n",
    "                df[mask][\"execution_time\"],\n",
    "                df[mask][\"overall_score\"],\n",
    "                c=complexity_colors[complexity],\n",
    "                label=complexity,\n",
    "                s=100,\n",
    "                alpha=0.7,\n",
    "            )\n",
    "\n",
    "    axes[1, 0].set_xlabel(\"執行時間 (秒)\")\n",
    "    axes[1, 0].set_ylabel(\"整體分數\")\n",
    "    axes[1, 0].set_title(\"執行時間 vs 整體分數\")\n",
    "    axes[1, 0].legend()\n",
    "\n",
    "    # Tokens/second performance\n",
    "    axes[1, 1].bar(range(len(df)), df[\"tokens_per_second\"], color=\"skyblue\")\n",
    "    axes[1, 1].set_title(\"Token 生成效率\")\n",
    "    axes[1, 1].set_ylabel(\"Tokens/秒\")\n",
    "    axes[1, 1].set_xlabel(\"任務\")\n",
    "    axes[1, 1].set_xticks(range(len(df)))\n",
    "    axes[1, 1].set_xticklabels([f\"T{i+1}\" for i in range(len(df))])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save chart\n",
    "    chart_file = (\n",
    "        outs_dir / f\"agentic_eval_charts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "    )\n",
    "    plt.savefig(chart_file, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"評估圖表已保存: {chart_file}\")\n",
    "\n",
    "\n",
    "# Generate report and charts\n",
    "df_results = generate_evaluation_report(eval_results, EVAL_TASKS)\n",
    "create_evaluation_charts(df_results)\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n=== 評估摘要 ===\")\n",
    "print(f\"平均整體分數: {df_results['overall_score'].mean():.3f}\")\n",
    "print(f\"平均完成度: {df_results['completion_rate'].mean():.3f}\")\n",
    "print(f\"平均引用準確性: {df_results['citation_accuracy'].mean():.3f}\")\n",
    "print(f\"平均內容一致性: {df_results['content_consistency'].mean():.3f}\")\n",
    "print(f\"平均章節覆蓋度: {df_results['section_coverage'].mean():.3f}\")\n",
    "print(f\"平均執行時間: {df_results['execution_time'].mean():.1f} 秒\")\n",
    "print(f\"成功率: {df_results['success_rate'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c22878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Smoke Test\n",
    "def smoke_test_agentic_eval():\n",
    "    \"\"\"Quick smoke test for agentic evaluation system\"\"\"\n",
    "\n",
    "    print(\"=== 代理任務評估系統煙霧測試 ===\")\n",
    "\n",
    "    # Test 1: Task definition\n",
    "    test_task = EvalTask(\n",
    "        id=\"smoke_001\",\n",
    "        name=\"測試任務\",\n",
    "        query=\"簡單測試查詢\",\n",
    "        domain=\"general\",\n",
    "        complexity=\"simple\",\n",
    "        expected_sections=[\"介紹\", \"結論\"],\n",
    "        max_time_seconds=60,\n",
    "    )\n",
    "\n",
    "    assert test_task.id == \"smoke_001\"\n",
    "    assert test_task.complexity == \"simple\"\n",
    "    print(\"✓ 任務定義測試通過\")\n",
    "\n",
    "    # Test 2: Metrics calculation\n",
    "    test_metrics = AgenticsMetrics(\n",
    "        task_id=\"smoke_001\",\n",
    "        completion_rate=0.8,\n",
    "        citation_accuracy=0.7,\n",
    "        content_consistency=0.75,\n",
    "        section_coverage=0.85,\n",
    "        execution_time=45.0,\n",
    "        total_tokens=500,\n",
    "        error_count=0,\n",
    "        retry_count=0,\n",
    "    )\n",
    "\n",
    "    overall_score = test_metrics.overall_score()\n",
    "    assert 0 <= overall_score <= 1\n",
    "    assert overall_score > 0.5  # Should be reasonable score\n",
    "    print(f\"✓ 指標計算測試通過 (總分: {overall_score:.3f})\")\n",
    "\n",
    "    # Test 3: Citation accuracy calculation\n",
    "    test_content = \"這是測試內容 [1]，包含引用 [2]。\"\n",
    "    test_citations = [\"來源1\", \"來源2\"]\n",
    "    citation_acc = calculate_citation_accuracy(test_content, test_citations)\n",
    "    assert citation_acc == 1.0  # Both citations referenced\n",
    "    print(f\"✓ 引用準確性計算測試通過 (準確性: {citation_acc:.3f})\")\n",
    "\n",
    "    # Test 4: Section coverage calculation\n",
    "    test_content = \"這是介紹部分的內容。最後我們得出結論。\"\n",
    "    test_sections = [\"介紹\", \"結論\"]\n",
    "    coverage = calculate_section_coverage(test_content, test_sections)\n",
    "    assert coverage == 1.0  # Both sections covered\n",
    "    print(f\"✓ 章節覆蓋度計算測試通過 (覆蓋度: {coverage:.3f})\")\n",
    "\n",
    "    # Test 5: Report generation\n",
    "    test_results = [test_metrics]\n",
    "    test_tasks = [test_task]\n",
    "    df_test = generate_evaluation_report(test_results, test_tasks)\n",
    "    assert len(df_test) == 1\n",
    "    assert \"overall_score\" in df_test.columns\n",
    "    print(\"✓ 報表生成測試通過\")\n",
    "\n",
    "    print(\"\\n🎉 所有煙霧測試通過！代理任務評估系統運作正常。\")\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_result = smoke_test_agentic_eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
