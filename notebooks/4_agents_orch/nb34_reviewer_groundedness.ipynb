{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9053048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (è¤‡è£½åˆ°æ¯æœ¬ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877187f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Dependencies & Imports\n",
    "import re, json, time\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from rapidfuzz import fuzz\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cfe5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Groundedness Detection Core\n",
    "class GroundednessDetector:\n",
    "    \"\"\"\n",
    "    Detects whether generated content is grounded in provided sources\n",
    "    using keyword overlap + semantic similarity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_model: str = \"BAAI/bge-m3\", threshold: float = 0.7):\n",
    "        self.embed_model = SentenceTransformer(embed_model)\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def extract_claims(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract factual claims from text (simple sentence splitting)\"\"\"\n",
    "        # Remove citations like [1], [2] first\n",
    "        clean_text = re.sub(r\"\\[\\d+\\]\", \"\", text)\n",
    "        # Split by Chinese punctuation\n",
    "        sentences = re.split(r\"[ã€‚ï¼ï¼Ÿï¼›]\", clean_text)\n",
    "        return [s.strip() for s in sentences if len(s.strip()) > 10]\n",
    "\n",
    "    def keyword_overlap_score(self, claim: str, source: str) -> float:\n",
    "        \"\"\"Calculate keyword overlap between claim and source\"\"\"\n",
    "        # Simple word-level Jaccard similarity\n",
    "        claim_words = set(re.findall(r\"\\w+\", claim.lower()))\n",
    "        source_words = set(re.findall(r\"\\w+\", source.lower()))\n",
    "\n",
    "        if not claim_words or not source_words:\n",
    "            return 0.0\n",
    "\n",
    "        intersection = len(claim_words & source_words)\n",
    "        union = len(claim_words | source_words)\n",
    "        return intersection / union if union > 0 else 0.0\n",
    "\n",
    "    def semantic_similarity_score(self, claim: str, source: str) -> float:\n",
    "        \"\"\"Calculate semantic similarity using embeddings\"\"\"\n",
    "        try:\n",
    "            embeddings = self.embed_model.encode(\n",
    "                [claim, source], normalize_embeddings=True\n",
    "            )\n",
    "            similarity = float(np.dot(embeddings[0], embeddings[1]))\n",
    "            return max(0.0, similarity)  # Ensure non-negative\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Embedding error: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def check_groundedness(self, claim: str, sources: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Check if a claim is grounded in the provided sources\n",
    "        Returns: {grounded: bool, best_source_idx: int, scores: dict}\n",
    "        \"\"\"\n",
    "        if not sources:\n",
    "            return {\"grounded\": False, \"best_source_idx\": -1, \"scores\": {}}\n",
    "\n",
    "        best_score = 0.0\n",
    "        best_idx = -1\n",
    "        all_scores = []\n",
    "\n",
    "        for i, source in enumerate(sources):\n",
    "            keyword_score = self.keyword_overlap_score(claim, source)\n",
    "            semantic_score = self.semantic_similarity_score(claim, source)\n",
    "\n",
    "            # Weighted combination (favor semantic similarity for Chinese)\n",
    "            combined_score = 0.3 * keyword_score + 0.7 * semantic_score\n",
    "            all_scores.append(\n",
    "                {\n",
    "                    \"source_idx\": i,\n",
    "                    \"keyword\": keyword_score,\n",
    "                    \"semantic\": semantic_score,\n",
    "                    \"combined\": combined_score,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if combined_score > best_score:\n",
    "                best_score = combined_score\n",
    "                best_idx = i\n",
    "\n",
    "        return {\n",
    "            \"grounded\": best_score >= self.threshold,\n",
    "            \"best_source_idx\": best_idx,\n",
    "            \"best_score\": best_score,\n",
    "            \"scores\": all_scores,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da95c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Citation Verification\n",
    "class CitationVerifier:\n",
    "    \"\"\"Verify that citations in text match the provided sources\"\"\"\n",
    "\n",
    "    def extract_citations(self, text: str) -> List[int]:\n",
    "        \"\"\"Extract citation numbers like [1], [2] from text\"\"\"\n",
    "        citations = re.findall(r\"\\[(\\d+)\\]\", text)\n",
    "        return [int(c) for c in citations]\n",
    "\n",
    "    def verify_citations(self, text: str, sources: List[Dict]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Verify that all citations in text are valid\n",
    "        sources: List of {text: str, meta: dict}\n",
    "        \"\"\"\n",
    "        citations = self.extract_citations(text)\n",
    "        max_source_idx = len(sources) - 1\n",
    "\n",
    "        valid_citations = [c for c in citations if 0 <= c - 1 <= max_source_idx]\n",
    "        invalid_citations = [\n",
    "            c for c in citations if c - 1 < 0 or c - 1 > max_source_idx\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"total_citations\": len(citations),\n",
    "            \"valid_citations\": len(valid_citations),\n",
    "            \"invalid_citations\": invalid_citations,\n",
    "            \"citation_rate\": len(valid_citations) / max(1, len(citations)),\n",
    "            \"has_citations\": len(citations) > 0,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e965bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Reviewer Role Implementation\n",
    "@dataclass\n",
    "class ReviewResult:\n",
    "    \"\"\"Result of content review\"\"\"\n",
    "\n",
    "    is_grounded: bool\n",
    "    groundedness_score: float\n",
    "    citation_issues: List[str]\n",
    "    recommendations: List[str]\n",
    "    detailed_analysis: Dict[str, Any]\n",
    "\n",
    "\n",
    "class ReviewerAgent:\n",
    "    \"\"\"\n",
    "    Reviewer role: Check groundedness and citation accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_adapter=None,\n",
    "        groundedness_threshold: float = 0.7,\n",
    "        citation_threshold: float = 0.8,\n",
    "    ):\n",
    "        self.llm_adapter = llm_adapter\n",
    "        self.detector = GroundednessDetector(threshold=groundedness_threshold)\n",
    "        self.verifier = CitationVerifier()\n",
    "        self.citation_threshold = citation_threshold\n",
    "\n",
    "    def analyze_content(self, content: str, sources: List[Dict]) -> ReviewResult:\n",
    "        \"\"\"\n",
    "        Comprehensive content analysis\n",
    "        sources: List of {text: str, meta: dict}\n",
    "        \"\"\"\n",
    "        # Extract claims from content\n",
    "        claims = self.detector.extract_claims(content)\n",
    "        logger.info(f\"Extracted {len(claims)} claims for analysis\")\n",
    "\n",
    "        # Check groundedness for each claim\n",
    "        source_texts = [s[\"text\"] for s in sources]\n",
    "        grounded_claims = 0\n",
    "        total_groundedness_score = 0.0\n",
    "        ungrounded_claims = []\n",
    "\n",
    "        for claim in claims:\n",
    "            result = self.detector.check_groundedness(claim, source_texts)\n",
    "            if result[\"grounded\"]:\n",
    "                grounded_claims += 1\n",
    "            else:\n",
    "                ungrounded_claims.append(claim)\n",
    "            total_groundedness_score += result[\"best_score\"]\n",
    "\n",
    "        avg_groundedness = total_groundedness_score / max(1, len(claims))\n",
    "\n",
    "        # Verify citations\n",
    "        citation_result = self.verifier.verify_citations(content, sources)\n",
    "\n",
    "        # Generate issues and recommendations\n",
    "        issues = []\n",
    "        recommendations = []\n",
    "\n",
    "        if avg_groundedness < self.detector.threshold:\n",
    "            issues.append(f\"æ•´é«”æœ‰æ“šå¯ä¾ç¨‹åº¦åä½ ({avg_groundedness:.2f})\")\n",
    "            recommendations.append(\"è«‹å¢åŠ æ›´å¤šå…·é«”äº‹å¯¦æ”¯æ’æˆ–å¼•ç”¨ä¾†æº\")\n",
    "\n",
    "        if ungrounded_claims:\n",
    "            issues.append(f\"ç™¼ç¾ {len(ungrounded_claims)} æ¢ç¼ºä¹ä¾æ“šçš„è²æ˜\")\n",
    "            recommendations.append(\n",
    "                \"è«‹ç‚ºä»¥ä¸‹è²æ˜æä¾›ä¾†æºæ”¯æ’ï¼š\" + \"; \".join(ungrounded_claims[:2])\n",
    "            )\n",
    "\n",
    "        if citation_result[\"citation_rate\"] < self.citation_threshold:\n",
    "            issues.append(f\"å¼•ç”¨æ¯”ä¾‹åä½ ({citation_result['citation_rate']:.2f})\")\n",
    "            recommendations.append(\"è«‹å¢åŠ å¼•ç”¨æ¨™è¨» [1], [2] ç­‰\")\n",
    "\n",
    "        if citation_result[\"invalid_citations\"]:\n",
    "            issues.append(f\"ç™¼ç¾ç„¡æ•ˆå¼•ç”¨: {citation_result['invalid_citations']}\")\n",
    "            recommendations.append(\"è«‹æª¢æŸ¥å¼•ç”¨ç·¨è™Ÿæ˜¯å¦æ­£ç¢º\")\n",
    "\n",
    "        is_grounded = (\n",
    "            avg_groundedness >= self.detector.threshold\n",
    "            and citation_result[\"citation_rate\"] >= self.citation_threshold\n",
    "            and not citation_result[\"invalid_citations\"]\n",
    "        )\n",
    "\n",
    "        return ReviewResult(\n",
    "            is_grounded=is_grounded,\n",
    "            groundedness_score=avg_groundedness,\n",
    "            citation_issues=issues,\n",
    "            recommendations=recommendations,\n",
    "            detailed_analysis={\n",
    "                \"claims_total\": len(claims),\n",
    "                \"claims_grounded\": grounded_claims,\n",
    "                \"ungrounded_claims\": ungrounded_claims[:3],  # Show first 3\n",
    "                \"citation_stats\": citation_result,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def generate_review_feedback(self, result: ReviewResult) -> str:\n",
    "        \"\"\"Generate natural language feedback based on review result\"\"\"\n",
    "        if result.is_grounded:\n",
    "            feedback = \"âœ… **å¯©æ ¸é€šé**: å…§å®¹å…·æœ‰è‰¯å¥½çš„äº‹å¯¦ä¾æ“šä¸”å¼•ç”¨è¦ç¯„ã€‚\\n\\n\"\n",
    "        else:\n",
    "            feedback = \"âš ï¸  **éœ€è¦ä¿®æ”¹**: ç™¼ç¾ä»¥ä¸‹å•é¡Œéœ€è¦è™•ç†ï¼š\\n\\n\"\n",
    "\n",
    "        if result.citation_issues:\n",
    "            feedback += \"**ç™¼ç¾å•é¡Œ:**\\n\"\n",
    "            for issue in result.citation_issues:\n",
    "                feedback += f\"- {issue}\\n\"\n",
    "            feedback += \"\\n\"\n",
    "\n",
    "        if result.recommendations:\n",
    "            feedback += \"**ä¿®æ”¹å»ºè­°:**\\n\"\n",
    "            for rec in result.recommendations:\n",
    "                feedback += f\"- {rec}\\n\"\n",
    "            feedback += \"\\n\"\n",
    "\n",
    "        feedback += f\"**æœ‰æ“šå¯ä¾å¾—åˆ†:** {result.groundedness_score:.2f}/1.0\\n\"\n",
    "        feedback += f\"**å·²æª¢æ ¸è²æ˜:** {result.detailed_analysis['claims_total']} æ¢\"\n",
    "\n",
    "        return feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1024142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Integration with Blackboard\n",
    "class ReviewerBlackboardIntegration:\n",
    "    \"\"\"Integration helper for Reviewer with shared blackboard\"\"\"\n",
    "\n",
    "    def __init__(self, reviewer: ReviewerAgent):\n",
    "        self.reviewer = reviewer\n",
    "\n",
    "    def review_writer_output(self, blackboard: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Review writer output using researcher sources\n",
    "        Updates blackboard with review results\n",
    "        \"\"\"\n",
    "        # Get writer output and researcher sources\n",
    "        writer_output = blackboard.get(\"writer_output\", \"\")\n",
    "        researcher_sources = blackboard.get(\"researcher_sources\", [])\n",
    "\n",
    "        if not writer_output:\n",
    "            logger.warning(\"No writer output to review\")\n",
    "            return blackboard\n",
    "\n",
    "        if not researcher_sources:\n",
    "            logger.warning(\"No sources available for grounding check\")\n",
    "\n",
    "        # Perform review\n",
    "        start_time = time.time()\n",
    "        review_result = self.reviewer.analyze_content(writer_output, researcher_sources)\n",
    "        review_time = time.time() - start_time\n",
    "\n",
    "        # Generate feedback\n",
    "        feedback = self.reviewer.generate_review_feedback(review_result)\n",
    "\n",
    "        # Update blackboard\n",
    "        blackboard.update(\n",
    "            {\n",
    "                \"review_result\": review_result,\n",
    "                \"review_feedback\": feedback,\n",
    "                \"review_passed\": review_result.is_grounded,\n",
    "                \"review_time\": review_time,\n",
    "                \"review_timestamp\": time.time(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        logger.info(\n",
    "            f\"Review completed in {review_time:.2f}s - \"\n",
    "            f\"{'PASSED' if review_result.is_grounded else 'FAILED'}\"\n",
    "        )\n",
    "\n",
    "        return blackboard\n",
    "\n",
    "    def suggest_revisions(self, blackboard: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"Generate specific revision suggestions\"\"\"\n",
    "        review_result = blackboard.get(\"review_result\")\n",
    "        if not review_result:\n",
    "            return []\n",
    "\n",
    "        suggestions = []\n",
    "\n",
    "        # Add specific suggestions based on analysis\n",
    "        ungrounded_claims = review_result.detailed_analysis.get(\"ungrounded_claims\", [])\n",
    "        for claim in ungrounded_claims[:2]:  # Show top 2\n",
    "            suggestions.append(f\"ç‚ºæ­¤è²æ˜æ·»åŠ ä¾†æºå¼•ç”¨: '{claim[:50]}...'\")\n",
    "\n",
    "        citation_stats = review_result.detailed_analysis.get(\"citation_stats\", {})\n",
    "        if citation_stats.get(\"total_citations\", 0) == 0:\n",
    "            suggestions.append(\"å…§å®¹å®Œå…¨ç¼ºä¹å¼•ç”¨ï¼Œè«‹æ·»åŠ  [1], [2] ç­‰å¼•ç”¨æ¨™è¨»\")\n",
    "\n",
    "        return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836c8412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: End-to-End Testing\n",
    "def test_reviewer_workflow():\n",
    "    \"\"\"Test complete reviewer workflow with mock data\"\"\"\n",
    "\n",
    "    # Mock researcher sources\n",
    "    mock_sources = [\n",
    "        {\n",
    "            \"text\": \"å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯åŸºæ–¼ Transformer æ¶æ§‹çš„æ·±åº¦å­¸ç¿’æ¨¡å‹ï¼Œèƒ½å¤ ç†è§£å’Œç”Ÿæˆäººé¡èªè¨€ã€‚\",\n",
    "            \"meta\": {\"source_id\": \"wiki_llm\", \"title\": \"å¤§å‹èªè¨€æ¨¡å‹\"},\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"RAGï¼ˆæª¢ç´¢å¢å¼·ç”Ÿæˆï¼‰æŠ€è¡“çµåˆäº†è³‡è¨Šæª¢ç´¢å’Œæ–‡æœ¬ç”Ÿæˆï¼Œå¯ä»¥æé«˜æ¨¡å‹å›ç­”çš„æº–ç¢ºæ€§ã€‚\",\n",
    "            \"meta\": {\"source_id\": \"rag_paper\", \"title\": \"RAG æŠ€è¡“è«–æ–‡\"},\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"BERT æ¨¡å‹åœ¨ 2018 å¹´ç”± Google æå‡ºï¼Œä½¿ç”¨é›™å‘ç·¨ç¢¼å™¨è¡¨ç¤ºã€‚\",\n",
    "            \"meta\": {\"source_id\": \"bert_paper\", \"title\": \"BERT åŸå§‹è«–æ–‡\"},\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Mock writer outputs (good and bad examples)\n",
    "    good_output = \"\"\"å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯ç¾ä»£ AI çš„é‡è¦ç™¼å±• [1]ã€‚é€™äº›æ¨¡å‹åŸºæ–¼ Transformer æ¶æ§‹ï¼Œ\n",
    "    å…·æœ‰å¼·å¤§çš„èªè¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚RAG æŠ€è¡“é€²ä¸€æ­¥æå‡äº†æ¨¡å‹çš„æº–ç¢ºæ€§ [2]ï¼Œé€šéçµåˆæª¢ç´¢å’Œç”Ÿæˆæ©Ÿåˆ¶ï¼Œ\n",
    "    è®“æ¨¡å‹èƒ½å¤ å­˜å–å¤–éƒ¨çŸ¥è­˜åº«ã€‚\"\"\"\n",
    "\n",
    "    bad_output = \"\"\"äººå·¥æ™ºæ…§å°‡åœ¨ 2025 å¹´å®Œå…¨å–ä»£äººé¡å·¥ä½œã€‚æ‰€æœ‰çš„ç¨‹å¼è¨­è¨ˆå¸«éƒ½æœƒå¤±æ¥­ã€‚\n",
    "    é‡å­è¨ˆç®—æ©Ÿå·²ç¶“èƒ½å¤ ç ´è§£æ‰€æœ‰åŠ å¯†ç®—æ³•ã€‚é€™äº›éƒ½æ˜¯ç¢ºå®šç„¡ç–‘çš„äº‹å¯¦ã€‚\"\"\"\n",
    "\n",
    "    # Initialize reviewer\n",
    "    reviewer = ReviewerAgent(groundedness_threshold=0.6, citation_threshold=0.3)\n",
    "\n",
    "    print(\"=== æ¸¬è©¦ 1: é«˜å“è³ªå…§å®¹ ===\")\n",
    "    result1 = reviewer.analyze_content(good_output, mock_sources)\n",
    "    feedback1 = reviewer.generate_review_feedback(result1)\n",
    "    print(feedback1)\n",
    "    print(f\"é€šéå¯©æ ¸: {result1.is_grounded}\")\n",
    "\n",
    "    print(\"\\n=== æ¸¬è©¦ 2: ä½å“è³ªå…§å®¹ ===\")\n",
    "    result2 = reviewer.analyze_content(bad_output, mock_sources)\n",
    "    feedback2 = reviewer.generate_review_feedback(result2)\n",
    "    print(feedback2)\n",
    "    print(f\"é€šéå¯©æ ¸: {result2.is_grounded}\")\n",
    "\n",
    "    # Test blackboard integration\n",
    "    print(\"\\n=== æ¸¬è©¦ 3: é»‘æ¿æ•´åˆ ===\")\n",
    "    blackboard = {\"writer_output\": good_output, \"researcher_sources\": mock_sources}\n",
    "\n",
    "    integration = ReviewerBlackboardIntegration(reviewer)\n",
    "    updated_blackboard = integration.review_writer_output(blackboard)\n",
    "\n",
    "    print(f\"å¯©æ ¸çµæœå·²æ›´æ–°åˆ°é»‘æ¿\")\n",
    "    print(f\"é€šé: {updated_blackboard['review_passed']}\")\n",
    "    print(f\"è€—æ™‚: {updated_blackboard['review_time']:.2f}s\")\n",
    "\n",
    "    return result1, result2, updated_blackboard\n",
    "\n",
    "\n",
    "# Run the test\n",
    "test_results = test_reviewer_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f679a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Smoke Test\n",
    "def smoke_test():\n",
    "    \"\"\"Quick smoke test for core functionality\"\"\"\n",
    "    print(\"ğŸ”¥ Reviewer Groundedness Smoke Test\")\n",
    "\n",
    "    # Test groundedness detector\n",
    "    detector = GroundednessDetector(threshold=0.5)\n",
    "    test_claim = \"å¤§å‹èªè¨€æ¨¡å‹ä½¿ç”¨ Transformer æ¶æ§‹\"\n",
    "    test_source = \"LLM åŸºæ–¼ Transformer æ¶æ§‹æ§‹å»ºï¼Œå…·æœ‰å¼·å¤§çš„èªè¨€èƒ½åŠ›\"\n",
    "\n",
    "    result = detector.check_groundedness(test_claim, [test_source])\n",
    "    print(\n",
    "        f\"âœ“ Groundedness detection: {result['grounded']} (score: {result['best_score']:.2f})\"\n",
    "    )\n",
    "\n",
    "    # Test citation verifier\n",
    "    verifier = CitationVerifier()\n",
    "    test_text = \"é€™æ˜¯ä¸€å€‹æ¸¬è©¦å¥å­ [1] å’Œå¦ä¸€å€‹å¼•ç”¨ [2]\"\n",
    "    citations = verifier.extract_citations(test_text)\n",
    "    print(f\"âœ“ Citation extraction: {citations}\")\n",
    "\n",
    "    # Test reviewer agent\n",
    "    reviewer = ReviewerAgent()\n",
    "    mock_sources = [{\"text\": test_source, \"meta\": {}}]\n",
    "    review_result = reviewer.analyze_content(test_claim, mock_sources)\n",
    "    print(f\"âœ“ Review analysis: grounded={review_result.is_grounded}\")\n",
    "\n",
    "    print(\"ğŸ¯ Smoke test completed successfully!\")\n",
    "\n",
    "\n",
    "smoke_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
