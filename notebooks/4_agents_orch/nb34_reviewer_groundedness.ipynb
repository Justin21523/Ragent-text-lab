{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9053048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (複製到每本 notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877187f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Dependencies & Imports\n",
    "import re, json, time\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from rapidfuzz import fuzz\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cfe5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Groundedness Detection Core\n",
    "class GroundednessDetector:\n",
    "    \"\"\"\n",
    "    Detects whether generated content is grounded in provided sources\n",
    "    using keyword overlap + semantic similarity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_model: str = \"BAAI/bge-m3\", threshold: float = 0.7):\n",
    "        self.embed_model = SentenceTransformer(embed_model)\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def extract_claims(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract factual claims from text (simple sentence splitting)\"\"\"\n",
    "        # Remove citations like [1], [2] first\n",
    "        clean_text = re.sub(r\"\\[\\d+\\]\", \"\", text)\n",
    "        # Split by Chinese punctuation\n",
    "        sentences = re.split(r\"[。！？；]\", clean_text)\n",
    "        return [s.strip() for s in sentences if len(s.strip()) > 10]\n",
    "\n",
    "    def keyword_overlap_score(self, claim: str, source: str) -> float:\n",
    "        \"\"\"Calculate keyword overlap between claim and source\"\"\"\n",
    "        # Simple word-level Jaccard similarity\n",
    "        claim_words = set(re.findall(r\"\\w+\", claim.lower()))\n",
    "        source_words = set(re.findall(r\"\\w+\", source.lower()))\n",
    "\n",
    "        if not claim_words or not source_words:\n",
    "            return 0.0\n",
    "\n",
    "        intersection = len(claim_words & source_words)\n",
    "        union = len(claim_words | source_words)\n",
    "        return intersection / union if union > 0 else 0.0\n",
    "\n",
    "    def semantic_similarity_score(self, claim: str, source: str) -> float:\n",
    "        \"\"\"Calculate semantic similarity using embeddings\"\"\"\n",
    "        try:\n",
    "            embeddings = self.embed_model.encode(\n",
    "                [claim, source], normalize_embeddings=True\n",
    "            )\n",
    "            similarity = float(np.dot(embeddings[0], embeddings[1]))\n",
    "            return max(0.0, similarity)  # Ensure non-negative\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Embedding error: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def check_groundedness(self, claim: str, sources: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Check if a claim is grounded in the provided sources\n",
    "        Returns: {grounded: bool, best_source_idx: int, scores: dict}\n",
    "        \"\"\"\n",
    "        if not sources:\n",
    "            return {\"grounded\": False, \"best_source_idx\": -1, \"scores\": {}}\n",
    "\n",
    "        best_score = 0.0\n",
    "        best_idx = -1\n",
    "        all_scores = []\n",
    "\n",
    "        for i, source in enumerate(sources):\n",
    "            keyword_score = self.keyword_overlap_score(claim, source)\n",
    "            semantic_score = self.semantic_similarity_score(claim, source)\n",
    "\n",
    "            # Weighted combination (favor semantic similarity for Chinese)\n",
    "            combined_score = 0.3 * keyword_score + 0.7 * semantic_score\n",
    "            all_scores.append(\n",
    "                {\n",
    "                    \"source_idx\": i,\n",
    "                    \"keyword\": keyword_score,\n",
    "                    \"semantic\": semantic_score,\n",
    "                    \"combined\": combined_score,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if combined_score > best_score:\n",
    "                best_score = combined_score\n",
    "                best_idx = i\n",
    "\n",
    "        return {\n",
    "            \"grounded\": best_score >= self.threshold,\n",
    "            \"best_source_idx\": best_idx,\n",
    "            \"best_score\": best_score,\n",
    "            \"scores\": all_scores,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da95c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Citation Verification\n",
    "class CitationVerifier:\n",
    "    \"\"\"Verify that citations in text match the provided sources\"\"\"\n",
    "\n",
    "    def extract_citations(self, text: str) -> List[int]:\n",
    "        \"\"\"Extract citation numbers like [1], [2] from text\"\"\"\n",
    "        citations = re.findall(r\"\\[(\\d+)\\]\", text)\n",
    "        return [int(c) for c in citations]\n",
    "\n",
    "    def verify_citations(self, text: str, sources: List[Dict]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Verify that all citations in text are valid\n",
    "        sources: List of {text: str, meta: dict}\n",
    "        \"\"\"\n",
    "        citations = self.extract_citations(text)\n",
    "        max_source_idx = len(sources) - 1\n",
    "\n",
    "        valid_citations = [c for c in citations if 0 <= c - 1 <= max_source_idx]\n",
    "        invalid_citations = [\n",
    "            c for c in citations if c - 1 < 0 or c - 1 > max_source_idx\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"total_citations\": len(citations),\n",
    "            \"valid_citations\": len(valid_citations),\n",
    "            \"invalid_citations\": invalid_citations,\n",
    "            \"citation_rate\": len(valid_citations) / max(1, len(citations)),\n",
    "            \"has_citations\": len(citations) > 0,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e965bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Reviewer Role Implementation\n",
    "@dataclass\n",
    "class ReviewResult:\n",
    "    \"\"\"Result of content review\"\"\"\n",
    "\n",
    "    is_grounded: bool\n",
    "    groundedness_score: float\n",
    "    citation_issues: List[str]\n",
    "    recommendations: List[str]\n",
    "    detailed_analysis: Dict[str, Any]\n",
    "\n",
    "\n",
    "class ReviewerAgent:\n",
    "    \"\"\"\n",
    "    Reviewer role: Check groundedness and citation accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_adapter=None,\n",
    "        groundedness_threshold: float = 0.7,\n",
    "        citation_threshold: float = 0.8,\n",
    "    ):\n",
    "        self.llm_adapter = llm_adapter\n",
    "        self.detector = GroundednessDetector(threshold=groundedness_threshold)\n",
    "        self.verifier = CitationVerifier()\n",
    "        self.citation_threshold = citation_threshold\n",
    "\n",
    "    def analyze_content(self, content: str, sources: List[Dict]) -> ReviewResult:\n",
    "        \"\"\"\n",
    "        Comprehensive content analysis\n",
    "        sources: List of {text: str, meta: dict}\n",
    "        \"\"\"\n",
    "        # Extract claims from content\n",
    "        claims = self.detector.extract_claims(content)\n",
    "        logger.info(f\"Extracted {len(claims)} claims for analysis\")\n",
    "\n",
    "        # Check groundedness for each claim\n",
    "        source_texts = [s[\"text\"] for s in sources]\n",
    "        grounded_claims = 0\n",
    "        total_groundedness_score = 0.0\n",
    "        ungrounded_claims = []\n",
    "\n",
    "        for claim in claims:\n",
    "            result = self.detector.check_groundedness(claim, source_texts)\n",
    "            if result[\"grounded\"]:\n",
    "                grounded_claims += 1\n",
    "            else:\n",
    "                ungrounded_claims.append(claim)\n",
    "            total_groundedness_score += result[\"best_score\"]\n",
    "\n",
    "        avg_groundedness = total_groundedness_score / max(1, len(claims))\n",
    "\n",
    "        # Verify citations\n",
    "        citation_result = self.verifier.verify_citations(content, sources)\n",
    "\n",
    "        # Generate issues and recommendations\n",
    "        issues = []\n",
    "        recommendations = []\n",
    "\n",
    "        if avg_groundedness < self.detector.threshold:\n",
    "            issues.append(f\"整體有據可依程度偏低 ({avg_groundedness:.2f})\")\n",
    "            recommendations.append(\"請增加更多具體事實支撐或引用來源\")\n",
    "\n",
    "        if ungrounded_claims:\n",
    "            issues.append(f\"發現 {len(ungrounded_claims)} 條缺乏依據的聲明\")\n",
    "            recommendations.append(\n",
    "                \"請為以下聲明提供來源支撐：\" + \"; \".join(ungrounded_claims[:2])\n",
    "            )\n",
    "\n",
    "        if citation_result[\"citation_rate\"] < self.citation_threshold:\n",
    "            issues.append(f\"引用比例偏低 ({citation_result['citation_rate']:.2f})\")\n",
    "            recommendations.append(\"請增加引用標註 [1], [2] 等\")\n",
    "\n",
    "        if citation_result[\"invalid_citations\"]:\n",
    "            issues.append(f\"發現無效引用: {citation_result['invalid_citations']}\")\n",
    "            recommendations.append(\"請檢查引用編號是否正確\")\n",
    "\n",
    "        is_grounded = (\n",
    "            avg_groundedness >= self.detector.threshold\n",
    "            and citation_result[\"citation_rate\"] >= self.citation_threshold\n",
    "            and not citation_result[\"invalid_citations\"]\n",
    "        )\n",
    "\n",
    "        return ReviewResult(\n",
    "            is_grounded=is_grounded,\n",
    "            groundedness_score=avg_groundedness,\n",
    "            citation_issues=issues,\n",
    "            recommendations=recommendations,\n",
    "            detailed_analysis={\n",
    "                \"claims_total\": len(claims),\n",
    "                \"claims_grounded\": grounded_claims,\n",
    "                \"ungrounded_claims\": ungrounded_claims[:3],  # Show first 3\n",
    "                \"citation_stats\": citation_result,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def generate_review_feedback(self, result: ReviewResult) -> str:\n",
    "        \"\"\"Generate natural language feedback based on review result\"\"\"\n",
    "        if result.is_grounded:\n",
    "            feedback = \"✅ **審核通過**: 內容具有良好的事實依據且引用規範。\\n\\n\"\n",
    "        else:\n",
    "            feedback = \"⚠️  **需要修改**: 發現以下問題需要處理：\\n\\n\"\n",
    "\n",
    "        if result.citation_issues:\n",
    "            feedback += \"**發現問題:**\\n\"\n",
    "            for issue in result.citation_issues:\n",
    "                feedback += f\"- {issue}\\n\"\n",
    "            feedback += \"\\n\"\n",
    "\n",
    "        if result.recommendations:\n",
    "            feedback += \"**修改建議:**\\n\"\n",
    "            for rec in result.recommendations:\n",
    "                feedback += f\"- {rec}\\n\"\n",
    "            feedback += \"\\n\"\n",
    "\n",
    "        feedback += f\"**有據可依得分:** {result.groundedness_score:.2f}/1.0\\n\"\n",
    "        feedback += f\"**已檢核聲明:** {result.detailed_analysis['claims_total']} 條\"\n",
    "\n",
    "        return feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1024142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Integration with Blackboard\n",
    "class ReviewerBlackboardIntegration:\n",
    "    \"\"\"Integration helper for Reviewer with shared blackboard\"\"\"\n",
    "\n",
    "    def __init__(self, reviewer: ReviewerAgent):\n",
    "        self.reviewer = reviewer\n",
    "\n",
    "    def review_writer_output(self, blackboard: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Review writer output using researcher sources\n",
    "        Updates blackboard with review results\n",
    "        \"\"\"\n",
    "        # Get writer output and researcher sources\n",
    "        writer_output = blackboard.get(\"writer_output\", \"\")\n",
    "        researcher_sources = blackboard.get(\"researcher_sources\", [])\n",
    "\n",
    "        if not writer_output:\n",
    "            logger.warning(\"No writer output to review\")\n",
    "            return blackboard\n",
    "\n",
    "        if not researcher_sources:\n",
    "            logger.warning(\"No sources available for grounding check\")\n",
    "\n",
    "        # Perform review\n",
    "        start_time = time.time()\n",
    "        review_result = self.reviewer.analyze_content(writer_output, researcher_sources)\n",
    "        review_time = time.time() - start_time\n",
    "\n",
    "        # Generate feedback\n",
    "        feedback = self.reviewer.generate_review_feedback(review_result)\n",
    "\n",
    "        # Update blackboard\n",
    "        blackboard.update(\n",
    "            {\n",
    "                \"review_result\": review_result,\n",
    "                \"review_feedback\": feedback,\n",
    "                \"review_passed\": review_result.is_grounded,\n",
    "                \"review_time\": review_time,\n",
    "                \"review_timestamp\": time.time(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        logger.info(\n",
    "            f\"Review completed in {review_time:.2f}s - \"\n",
    "            f\"{'PASSED' if review_result.is_grounded else 'FAILED'}\"\n",
    "        )\n",
    "\n",
    "        return blackboard\n",
    "\n",
    "    def suggest_revisions(self, blackboard: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"Generate specific revision suggestions\"\"\"\n",
    "        review_result = blackboard.get(\"review_result\")\n",
    "        if not review_result:\n",
    "            return []\n",
    "\n",
    "        suggestions = []\n",
    "\n",
    "        # Add specific suggestions based on analysis\n",
    "        ungrounded_claims = review_result.detailed_analysis.get(\"ungrounded_claims\", [])\n",
    "        for claim in ungrounded_claims[:2]:  # Show top 2\n",
    "            suggestions.append(f\"為此聲明添加來源引用: '{claim[:50]}...'\")\n",
    "\n",
    "        citation_stats = review_result.detailed_analysis.get(\"citation_stats\", {})\n",
    "        if citation_stats.get(\"total_citations\", 0) == 0:\n",
    "            suggestions.append(\"內容完全缺乏引用，請添加 [1], [2] 等引用標註\")\n",
    "\n",
    "        return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836c8412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: End-to-End Testing\n",
    "def test_reviewer_workflow():\n",
    "    \"\"\"Test complete reviewer workflow with mock data\"\"\"\n",
    "\n",
    "    # Mock researcher sources\n",
    "    mock_sources = [\n",
    "        {\n",
    "            \"text\": \"大型語言模型（LLM）是基於 Transformer 架構的深度學習模型，能夠理解和生成人類語言。\",\n",
    "            \"meta\": {\"source_id\": \"wiki_llm\", \"title\": \"大型語言模型\"},\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"RAG（檢索增強生成）技術結合了資訊檢索和文本生成，可以提高模型回答的準確性。\",\n",
    "            \"meta\": {\"source_id\": \"rag_paper\", \"title\": \"RAG 技術論文\"},\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"BERT 模型在 2018 年由 Google 提出，使用雙向編碼器表示。\",\n",
    "            \"meta\": {\"source_id\": \"bert_paper\", \"title\": \"BERT 原始論文\"},\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Mock writer outputs (good and bad examples)\n",
    "    good_output = \"\"\"大型語言模型（LLM）是現代 AI 的重要發展 [1]。這些模型基於 Transformer 架構，\n",
    "    具有強大的語言理解和生成能力。RAG 技術進一步提升了模型的準確性 [2]，通過結合檢索和生成機制，\n",
    "    讓模型能夠存取外部知識庫。\"\"\"\n",
    "\n",
    "    bad_output = \"\"\"人工智慧將在 2025 年完全取代人類工作。所有的程式設計師都會失業。\n",
    "    量子計算機已經能夠破解所有加密算法。這些都是確定無疑的事實。\"\"\"\n",
    "\n",
    "    # Initialize reviewer\n",
    "    reviewer = ReviewerAgent(groundedness_threshold=0.6, citation_threshold=0.3)\n",
    "\n",
    "    print(\"=== 測試 1: 高品質內容 ===\")\n",
    "    result1 = reviewer.analyze_content(good_output, mock_sources)\n",
    "    feedback1 = reviewer.generate_review_feedback(result1)\n",
    "    print(feedback1)\n",
    "    print(f\"通過審核: {result1.is_grounded}\")\n",
    "\n",
    "    print(\"\\n=== 測試 2: 低品質內容 ===\")\n",
    "    result2 = reviewer.analyze_content(bad_output, mock_sources)\n",
    "    feedback2 = reviewer.generate_review_feedback(result2)\n",
    "    print(feedback2)\n",
    "    print(f\"通過審核: {result2.is_grounded}\")\n",
    "\n",
    "    # Test blackboard integration\n",
    "    print(\"\\n=== 測試 3: 黑板整合 ===\")\n",
    "    blackboard = {\"writer_output\": good_output, \"researcher_sources\": mock_sources}\n",
    "\n",
    "    integration = ReviewerBlackboardIntegration(reviewer)\n",
    "    updated_blackboard = integration.review_writer_output(blackboard)\n",
    "\n",
    "    print(f\"審核結果已更新到黑板\")\n",
    "    print(f\"通過: {updated_blackboard['review_passed']}\")\n",
    "    print(f\"耗時: {updated_blackboard['review_time']:.2f}s\")\n",
    "\n",
    "    return result1, result2, updated_blackboard\n",
    "\n",
    "\n",
    "# Run the test\n",
    "test_results = test_reviewer_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f679a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Smoke Test\n",
    "def smoke_test():\n",
    "    \"\"\"Quick smoke test for core functionality\"\"\"\n",
    "    print(\"🔥 Reviewer Groundedness Smoke Test\")\n",
    "\n",
    "    # Test groundedness detector\n",
    "    detector = GroundednessDetector(threshold=0.5)\n",
    "    test_claim = \"大型語言模型使用 Transformer 架構\"\n",
    "    test_source = \"LLM 基於 Transformer 架構構建，具有強大的語言能力\"\n",
    "\n",
    "    result = detector.check_groundedness(test_claim, [test_source])\n",
    "    print(\n",
    "        f\"✓ Groundedness detection: {result['grounded']} (score: {result['best_score']:.2f})\"\n",
    "    )\n",
    "\n",
    "    # Test citation verifier\n",
    "    verifier = CitationVerifier()\n",
    "    test_text = \"這是一個測試句子 [1] 和另一個引用 [2]\"\n",
    "    citations = verifier.extract_citations(test_text)\n",
    "    print(f\"✓ Citation extraction: {citations}\")\n",
    "\n",
    "    # Test reviewer agent\n",
    "    reviewer = ReviewerAgent()\n",
    "    mock_sources = [{\"text\": test_source, \"meta\": {}}]\n",
    "    review_result = reviewer.analyze_content(test_claim, mock_sources)\n",
    "    print(f\"✓ Review analysis: grounded={review_result.is_grounded}\")\n",
    "\n",
    "    print(\"🎯 Smoke test completed successfully!\")\n",
    "\n",
    "\n",
    "smoke_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
