{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66682ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cache] ../ai_warehouse/cache | GPU: True\n"
     ]
    }
   ],
   "source": [
    "# nb31_researcher_with_rag.ipynb\n",
    "# Stage 4: Multi-Agent Orchestrator - Researcher with RAG Integration\n",
    "\"\"\"\n",
    "Goals:\n",
    "- Integrate Stage 2 RAG components into Researcher agent role\n",
    "- Implement knowledge retrieval with citation tracking\n",
    "- Create research synthesis with source attribution\n",
    "- Enable multi-source evidence gathering and summarization\n",
    "- Build foundation for 4-role orchestrator collaboration\n",
    "\n",
    "Prerequisites:\n",
    "- Completed nb10-nb19 (RAG basics)\n",
    "- Completed nb30 (orchestrator skeleton)\n",
    "- Basic understanding of agent roles and blackboard patterns\n",
    "\"\"\"\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (è¤‡è£½åˆ°æ¯æœ¬ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6fe234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 2: Dependencies and Imports\n",
    "# ================================\n",
    "\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "\n",
    "# Core ML components\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# RAG components\n",
    "import faiss\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Utility\n",
    "import re\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"âœ“ Dependencies loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f554c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 3: RAG Components Integration\n",
    "# ================================\n",
    "\n",
    "\n",
    "class RAGRetriever:\n",
    "    \"\"\"Lightweight RAG retriever for Researcher agent\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model: str = \"BAAI/bge-m3\",\n",
    "        index_path: str = \"indices/general.faiss\",\n",
    "        chunks_path: str = \"indices/chunks.jsonl\",\n",
    "        device: str = \"auto\",\n",
    "    ):\n",
    "\n",
    "        self.embedding_model = embedding_model\n",
    "        self.index_path = Path(index_path)\n",
    "        self.chunks_path = Path(chunks_path)\n",
    "\n",
    "        # Load embedding model\n",
    "        print(f\"Loading embedding model: {embedding_model}\")\n",
    "        self.embedder = SentenceTransformer(embedding_model, device=device)\n",
    "\n",
    "        # Load or create index\n",
    "        self.index = None\n",
    "        self.chunks = []\n",
    "        self._load_index()\n",
    "\n",
    "    def _load_index(self):\n",
    "        \"\"\"Load FAISS index and chunks metadata\"\"\"\n",
    "        try:\n",
    "            if self.index_path.exists() and self.chunks_path.exists():\n",
    "                # Load existing index\n",
    "                self.index = faiss.read_index(str(self.index_path))\n",
    "\n",
    "                # Load chunks metadata\n",
    "                with open(self.chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    self.chunks = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "                print(\n",
    "                    f\"âœ“ Loaded index with {self.index.ntotal} vectors and {len(self.chunks)} chunks\"\n",
    "                )\n",
    "            else:\n",
    "                print(\"âš  No existing index found - will create sample data\")\n",
    "                self._create_sample_index()\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading index: {e}\")\n",
    "            self._create_sample_index()\n",
    "\n",
    "    def _create_sample_index(self):\n",
    "        \"\"\"Create sample index for demonstration\"\"\"\n",
    "        # Sample Chinese documents for RAG demonstration\n",
    "        sample_docs = [\n",
    "            {\n",
    "                \"text\": \"RAGï¼ˆæª¢ç´¢å¢žå¼·ç”Ÿæˆï¼‰æ˜¯ä¸€ç¨®çµåˆè³‡è¨Šæª¢ç´¢èˆ‡ç”Ÿæˆå¼AIçš„æŠ€è¡“æž¶æ§‹ã€‚å®ƒèƒ½å¤ è®“èªžè¨€æ¨¡åž‹åœ¨ç”Ÿæˆå›žç­”æ™‚ï¼Œå…ˆå¾žå¤–éƒ¨çŸ¥è­˜åº«ä¸­æª¢ç´¢ç›¸é—œè³‡è¨Šï¼Œå†åŸºæ–¼é€™äº›è³‡è¨Šç”Ÿæˆæ›´æº–ç¢ºã€æ›´æœ‰æ ¹æ“šçš„å›žç­”ã€‚\",\n",
    "                \"meta\": {\n",
    "                    \"source_id\": \"rag_intro\",\n",
    "                    \"title\": \"RAGæŠ€è¡“ä»‹ç´¹\",\n",
    "                    \"section\": \"åŸºæœ¬æ¦‚å¿µ\",\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"å¤šä»£ç†ç³»çµ±ï¼ˆMulti-Agent Systemï¼‰æ˜¯ç”±å¤šå€‹è‡ªä¸»æ™ºèƒ½é«”çµ„æˆçš„ç³»çµ±ï¼Œæ¯å€‹æ™ºèƒ½é«”éƒ½æœ‰è‡ªå·±çš„ç›®æ¨™å’Œè¡Œç‚ºæ¨¡å¼ã€‚åœ¨AIæ‡‰ç”¨ä¸­ï¼Œä¸åŒè§’è‰²çš„ä»£ç†å¯ä»¥å”ä½œå®Œæˆè¤‡é›œä»»å‹™ï¼Œå¦‚ç ”ç©¶ã€è¦åŠƒã€å¯«ä½œå’Œå¯©æ ¸ã€‚\",\n",
    "                \"meta\": {\n",
    "                    \"source_id\": \"mas_intro\",\n",
    "                    \"title\": \"å¤šä»£ç†ç³»çµ±\",\n",
    "                    \"section\": \"ç³»çµ±æž¶æ§‹\",\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"å‘é‡æª¢ç´¢ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦ä¾†æ‰¾åˆ°èªžç¾©ç›¸é—œçš„æ–‡æª”ç‰‡æ®µã€‚å¸¸ç”¨çš„æ–¹æ³•åŒ…æ‹¬é¤˜å¼¦ç›¸ä¼¼åº¦ã€æ­æ°è·é›¢å’Œå…§ç©ã€‚FAISSæ˜¯Facebooké–‹ç™¼çš„é«˜æ•ˆå‘é‡æª¢ç´¢åº«ï¼Œæ”¯æŒå¤§è¦æ¨¡å‘é‡ç´¢å¼•å’Œå¿«é€Ÿç›¸ä¼¼åº¦æœç´¢ã€‚\",\n",
    "                \"meta\": {\n",
    "                    \"source_id\": \"vector_search\",\n",
    "                    \"title\": \"å‘é‡æª¢ç´¢æŠ€è¡“\",\n",
    "                    \"section\": \"æª¢ç´¢ç®—æ³•\",\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"ä¸­æ–‡æ–‡æœ¬è™•ç†é¢è‡¨ç¨ç‰¹æŒ‘æˆ°ï¼ŒåŒ…æ‹¬åˆ†è©žã€ç¹ç°¡è½‰æ›ã€èªžç¾©åˆ†æ®µç­‰ã€‚BGE-M3æ˜¯é‡å°ä¸­æ–‡å„ªåŒ–çš„å¤šèªžè¨€åµŒå…¥æ¨¡åž‹ï¼Œåœ¨ä¸­æ–‡èªžç¾©ç†è§£ä»»å‹™ä¸Šè¡¨ç¾å„ªç§€ã€‚\",\n",
    "                \"meta\": {\n",
    "                    \"source_id\": \"zh_nlp\",\n",
    "                    \"title\": \"ä¸­æ–‡NLPè™•ç†\",\n",
    "                    \"section\": \"æŠ€è¡“æŒ‘æˆ°\",\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"å¼•ç”¨æ¨™è¨»ï¼ˆCitationï¼‰åœ¨RAGç³»çµ±ä¸­è‡³é—œé‡è¦ï¼Œå®ƒç¢ºä¿ç”Ÿæˆçš„å…§å®¹æœ‰æ“šå¯ä¾ã€‚å¸¸è¦‹æ ¼å¼åŒ…æ‹¬æ‹¬è™Ÿæ¨™è¨»[1][2]å’Œè…³è¨»å½¢å¼ï¼ŒåŒæ™‚éœ€è¦æä¾›å®Œæ•´çš„ä¾†æºè³‡è¨Šä»¥ä¾¿é©—è­‰ã€‚\",\n",
    "                \"meta\": {\n",
    "                    \"source_id\": \"citation\",\n",
    "                    \"title\": \"å¼•ç”¨æ¨™è¨»ç³»çµ±\",\n",
    "                    \"section\": \"å¯¦ç¾æ–¹æ³•\",\n",
    "                },\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        # Create chunks\n",
    "        self.chunks = []\n",
    "        texts = []\n",
    "        for i, doc in enumerate(sample_docs):\n",
    "            chunk = {\"id\": i, \"text\": doc[\"text\"], \"meta\": doc[\"meta\"]}\n",
    "            self.chunks.append(chunk)\n",
    "            texts.append(doc[\"text\"])\n",
    "\n",
    "        # Create embeddings\n",
    "        print(\"Creating sample embeddings...\")\n",
    "        embeddings = self.embedder.encode(texts, normalize_embeddings=True)\n",
    "        embeddings = embeddings.astype(np.float32)\n",
    "\n",
    "        # Create FAISS index\n",
    "        dim = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(dim)  # Inner product for normalized vectors\n",
    "        self.index.add(embeddings)\n",
    "\n",
    "        # Save index and chunks\n",
    "        self.index_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        faiss.write_index(self.index, str(self.index_path))\n",
    "\n",
    "        with open(self.chunks_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for chunk in self.chunks:\n",
    "                f.write(json.dumps(chunk, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        print(f\"âœ“ Created sample index with {len(self.chunks)} chunks\")\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve relevant chunks for query\"\"\"\n",
    "        if not self.index or not self.chunks:\n",
    "            return []\n",
    "\n",
    "        # Encode query\n",
    "        query_embedding = self.embedder.encode([query], normalize_embeddings=True)\n",
    "        query_embedding = query_embedding.astype(np.float32)\n",
    "\n",
    "        # Search\n",
    "        scores, indices = self.index.search(\n",
    "            query_embedding, min(top_k, len(self.chunks))\n",
    "        )\n",
    "\n",
    "        # Format results\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx >= 0 and idx < len(self.chunks):  # Valid index\n",
    "                chunk = self.chunks[idx].copy()\n",
    "                chunk[\"score\"] = float(score)\n",
    "                results.append(chunk)\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# Test RAG retriever\n",
    "print(\"Setting up RAG retriever...\")\n",
    "rag_retriever = RAGRetriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d2eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 4: Researcher Agent Class\n",
    "# ================================\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ResearchTask:\n",
    "    \"\"\"Research task specification\"\"\"\n",
    "\n",
    "    query: str\n",
    "    domain: str = \"general\"\n",
    "    max_sources: int = 5\n",
    "    depth: str = \"moderate\"  # surface, moderate, deep\n",
    "    citation_style: str = \"brackets\"  # brackets, footnotes\n",
    "    language: str = \"zh\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ResearchResult:\n",
    "    \"\"\"Research result with citations\"\"\"\n",
    "\n",
    "    summary: str\n",
    "    key_findings: List[str]\n",
    "    sources: List[Dict[str, Any]]\n",
    "    confidence: float\n",
    "    timestamp: str\n",
    "    task: ResearchTask\n",
    "\n",
    "\n",
    "class ResearcherAgent:\n",
    "    \"\"\"Researcher agent with RAG integration\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_model: str = \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        rag_retriever: RAGRetriever = None,\n",
    "        max_new_tokens: int = 1024,\n",
    "        temperature: float = 0.3,\n",
    "    ):\n",
    "\n",
    "        self.model_name = llm_model\n",
    "        self.rag_retriever = rag_retriever\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Load LLM\n",
    "        print(f\"Loading LLM: {llm_model}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(llm_model, use_fast=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            llm_model,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        print(f\"âœ“ Researcher agent initialized\")\n",
    "\n",
    "    def _build_research_prompt(\n",
    "        self, task: ResearchTask, retrieved_chunks: List[Dict]\n",
    "    ) -> str:\n",
    "        \"\"\"Build research prompt with retrieved context\"\"\"\n",
    "\n",
    "        # Build context from retrieved chunks\n",
    "        context_parts = []\n",
    "        for i, chunk in enumerate(retrieved_chunks, 1):\n",
    "            source_info = chunk[\"meta\"]\n",
    "            source_desc = f\"{source_info.get('title', 'Unknown')} - {source_info.get('section', 'N/A')}\"\n",
    "            context_parts.append(f\"[ä¾†æº{i}] {source_desc}\\n{chunk['text']}\")\n",
    "\n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "        # Build citations list\n",
    "        citations = []\n",
    "        for i, chunk in enumerate(retrieved_chunks, 1):\n",
    "            meta = chunk[\"meta\"]\n",
    "            citation = (\n",
    "                f\"[{i}] {meta.get('source_id', 'N/A')} - {meta.get('title', 'Unknown')}\"\n",
    "            )\n",
    "            citations.append(citation)\n",
    "\n",
    "        citations_text = \"\\n\".join(citations)\n",
    "\n",
    "        # Research prompt template\n",
    "        system_prompt = \"\"\"ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ç ”ç©¶åŠ©ç†ï¼Œæ“…é•·å¾žå¤šå€‹ä¾†æºæ•´åˆè³‡è¨Šä¸¦ç”¢ç”Ÿæœ‰æ“šå¯ä¾çš„ç ”ç©¶æ‘˜è¦ã€‚\n",
    "\n",
    "ä»»å‹™è¦æ±‚ï¼š\n",
    "1. åŸºæ–¼æä¾›çš„ä¾†æºè³‡æ–™å›žç­”ç ”ç©¶å•é¡Œ\n",
    "2. åœ¨å›žç­”ä¸­ä½¿ç”¨ [1], [2] ç­‰æ¨™è¨»å¼•ç”¨ä¾†æº\n",
    "3. æä¾›é—œéµç™¼ç¾åˆ—è¡¨\n",
    "4. ç¢ºä¿å…§å®¹æº–ç¢ºä¸”æœ‰æ ¹æ“š\n",
    "5. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›žç­”\n",
    "\n",
    "å›žç­”æ ¼å¼ï¼š\n",
    "## ç ”ç©¶æ‘˜è¦\n",
    "[åŸºæ–¼ä¾†æºè³‡æ–™çš„ç¶œåˆåˆ†æž]\n",
    "\n",
    "## é—œéµç™¼ç¾\n",
    "- [ç™¼ç¾1] [å¼•ç”¨]\n",
    "- [ç™¼ç¾2] [å¼•ç”¨]\n",
    "- [ç™¼ç¾3] [å¼•ç”¨]\n",
    "\n",
    "ä¸è¦é€éœ²å…§éƒ¨æ€è€ƒéŽç¨‹ï¼Œç›´æŽ¥æä¾›ç ”ç©¶çµæžœã€‚\"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"ç ”ç©¶å•é¡Œï¼š{task.query}\n",
    "\n",
    "åƒè€ƒè³‡æ–™ï¼š\n",
    "{context}\n",
    "\n",
    "è«‹åŸºæ–¼ä»¥ä¸Šè³‡æ–™é€²è¡Œç ”ç©¶åˆ†æžï¼Œä¸¦åœ¨å›žç­”ä¸­é©ç•¶å¼•ç”¨ä¾†æºã€‚\n",
    "\n",
    "ä¾†æºåˆ—è¡¨ï¼š\n",
    "{citations_text}\"\"\"\n",
    "\n",
    "        return system_prompt, user_prompt\n",
    "\n",
    "    def _generate_response(self, system_prompt: str, user_prompt: str) -> str:\n",
    "        \"\"\"Generate LLM response\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "\n",
    "        # Apply chat template\n",
    "        formatted_prompt = self.tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            formatted_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=3072,  # Leave room for generation\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                temperature=self.temperature,\n",
    "                do_sample=self.temperature > 0,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        # Decode response\n",
    "        response = self.tokenizer.decode(\n",
    "            outputs[0][inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n",
    "        ).strip()\n",
    "\n",
    "        return response\n",
    "\n",
    "    def _parse_research_response(self, response: str) -> Tuple[str, List[str]]:\n",
    "        \"\"\"Parse research response to extract summary and key findings\"\"\"\n",
    "        lines = response.split(\"\\n\")\n",
    "\n",
    "        summary_lines = []\n",
    "        findings = []\n",
    "        current_section = \"summary\"\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            if \"## é—œéµç™¼ç¾\" in line or \"é—œéµç™¼ç¾\" in line:\n",
    "                current_section = \"findings\"\n",
    "                continue\n",
    "            elif \"##\" in line and current_section == \"findings\":\n",
    "                break\n",
    "\n",
    "            if current_section == \"summary\":\n",
    "                if not line.startswith(\"#\"):\n",
    "                    summary_lines.append(line)\n",
    "            elif current_section == \"findings\":\n",
    "                if line.startswith(\"-\") or line.startswith(\"â€¢\"):\n",
    "                    findings.append(line.lstrip(\"- â€¢\").strip())\n",
    "\n",
    "        summary = \" \".join(summary_lines).strip()\n",
    "\n",
    "        return summary, findings\n",
    "\n",
    "    def research(self, task: ResearchTask) -> ResearchResult:\n",
    "        \"\"\"Conduct research on given task\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        logger.info(f\"Starting research on: {task.query}\")\n",
    "\n",
    "        # Step 1: Retrieve relevant information\n",
    "        if self.rag_retriever:\n",
    "            retrieved_chunks = self.rag_retriever.retrieve(\n",
    "                task.query, top_k=task.max_sources\n",
    "            )\n",
    "            logger.info(f\"Retrieved {len(retrieved_chunks)} relevant chunks\")\n",
    "        else:\n",
    "            retrieved_chunks = []\n",
    "            logger.warning(\"No RAG retriever available\")\n",
    "\n",
    "        # Step 2: Generate research response\n",
    "        if retrieved_chunks:\n",
    "            system_prompt, user_prompt = self._build_research_prompt(\n",
    "                task, retrieved_chunks\n",
    "            )\n",
    "            response = self._generate_response(system_prompt, user_prompt)\n",
    "        else:\n",
    "            # Fallback: direct response without RAG\n",
    "            response = self._generate_fallback_response(task)\n",
    "\n",
    "        # Step 3: Parse response\n",
    "        summary, key_findings = self._parse_research_response(response)\n",
    "\n",
    "        # Step 4: Build result\n",
    "        sources = []\n",
    "        for chunk in retrieved_chunks:\n",
    "            source = {\n",
    "                \"id\": chunk.get(\"id\"),\n",
    "                \"text\": (\n",
    "                    chunk[\"text\"][:200] + \"...\"\n",
    "                    if len(chunk[\"text\"]) > 200\n",
    "                    else chunk[\"text\"]\n",
    "                ),\n",
    "                \"meta\": chunk[\"meta\"],\n",
    "                \"score\": chunk.get(\"score\", 0.0),\n",
    "            }\n",
    "            sources.append(source)\n",
    "\n",
    "        # Calculate confidence based on retrieval scores and content length\n",
    "        if sources:\n",
    "            avg_score = np.mean([s[\"score\"] for s in sources])\n",
    "            confidence = min(0.95, max(0.3, avg_score))\n",
    "        else:\n",
    "            confidence = 0.2  # Low confidence without sources\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.info(\n",
    "            f\"Research completed in {elapsed_time:.2f}s with confidence {confidence:.2f}\"\n",
    "        )\n",
    "\n",
    "        result = ResearchResult(\n",
    "            summary=summary,\n",
    "            key_findings=key_findings,\n",
    "            sources=sources,\n",
    "            confidence=confidence,\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            task=task,\n",
    "        )\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _generate_fallback_response(self, task: ResearchTask) -> str:\n",
    "        \"\"\"Generate response without RAG (fallback)\"\"\"\n",
    "        system_prompt = \"\"\"ä½ æ˜¯ä¸€ä½ç ”ç©¶åŠ©ç†ã€‚åŸºæ–¼ä½ çš„çŸ¥è­˜å›žç­”å•é¡Œï¼Œä¸¦æ˜Žç¢ºèªªæ˜Žé€™æ˜¯åŸºæ–¼ä¸€èˆ¬çŸ¥è­˜è€Œéžç‰¹å®šè³‡æ–™ä¾†æºçš„å›žç­”ã€‚\"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"ç ”ç©¶å•é¡Œï¼š{task.query}\n",
    "\n",
    "è«‹æä¾›ä¸€å€‹çµæ§‹åŒ–çš„å›žç­”ï¼ŒåŒ…å«ï¼š\n",
    "## ç ”ç©¶æ‘˜è¦\n",
    "## é—œéµç™¼ç¾\n",
    "\n",
    "æ³¨æ„ï¼šæ­¤å›žç­”åŸºæ–¼ä¸€èˆ¬çŸ¥è­˜ï¼Œæœªä½¿ç”¨ç‰¹å®šè³‡æ–™ä¾†æºã€‚\"\"\"\n",
    "\n",
    "        return self._generate_response(system_prompt, user_prompt)\n",
    "\n",
    "\n",
    "# Initialize researcher agent\n",
    "print(\"Initializing Researcher Agent...\")\n",
    "researcher = ResearcherAgent(rag_retriever=rag_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69de625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 5: Smoke Test - Basic Research\n",
    "# ================================\n",
    "\n",
    "\n",
    "def test_researcher_basic():\n",
    "    \"\"\"Smoke test for researcher agent\"\"\"\n",
    "    print(\"=== Smoke Test: Basic Research ===\")\n",
    "\n",
    "    # Define research task\n",
    "    task = ResearchTask(\n",
    "        query=\"ä»€éº¼æ˜¯RAGæŠ€è¡“ï¼Ÿå®ƒæœ‰å“ªäº›ä¸»è¦å„ªå‹¢ï¼Ÿ\",\n",
    "        domain=\"ai\",\n",
    "        max_sources=3,\n",
    "        depth=\"moderate\",\n",
    "    )\n",
    "\n",
    "    print(f\"Research Task: {task.query}\")\n",
    "    print(\"Running research...\")\n",
    "\n",
    "    # Conduct research\n",
    "    result = researcher.research(task)\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nðŸ“Š Research Results:\")\n",
    "    print(f\"Confidence: {result.confidence:.2f}\")\n",
    "    print(f\"Sources used: {len(result.sources)}\")\n",
    "\n",
    "    print(f\"\\nðŸ“ Summary:\")\n",
    "    print(result.summary)\n",
    "\n",
    "    print(f\"\\nðŸ” Key Findings:\")\n",
    "    for i, finding in enumerate(result.key_findings, 1):\n",
    "        print(f\"{i}. {finding}\")\n",
    "\n",
    "    print(f\"\\nðŸ“š Sources:\")\n",
    "    for i, source in enumerate(result.sources, 1):\n",
    "        meta = source[\"meta\"]\n",
    "        print(f\"[{i}] {meta.get('title', 'Unknown')} (Score: {source['score']:.3f})\")\n",
    "        print(f\"    {source['text']}\")\n",
    "\n",
    "    # Validation\n",
    "    assert result.summary, \"Summary should not be empty\"\n",
    "    assert result.key_findings, \"Should have key findings\"\n",
    "    assert result.confidence > 0, \"Confidence should be positive\"\n",
    "\n",
    "    print(\"\\nâœ… Smoke test passed!\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_result = test_researcher_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60163169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 6: Advanced Research Features\n",
    "# ================================\n",
    "\n",
    "\n",
    "class AdvancedResearcher(ResearcherAgent):\n",
    "    \"\"\"Enhanced researcher with advanced features\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.research_history = []\n",
    "\n",
    "    def multi_query_research(\n",
    "        self, main_query: str, sub_queries: List[str]\n",
    "    ) -> ResearchResult:\n",
    "        \"\"\"Research with multiple related queries\"\"\"\n",
    "        print(f\"Conducting multi-query research on: {main_query}\")\n",
    "\n",
    "        all_chunks = []\n",
    "\n",
    "        # Retrieve for main query\n",
    "        main_chunks = self.rag_retriever.retrieve(main_query, top_k=3)\n",
    "        all_chunks.extend(main_chunks)\n",
    "\n",
    "        # Retrieve for sub-queries\n",
    "        for sub_query in sub_queries:\n",
    "            sub_chunks = self.rag_retriever.retrieve(sub_query, top_k=2)\n",
    "            all_chunks.extend(sub_chunks)\n",
    "\n",
    "        # Deduplicate by chunk ID\n",
    "        seen_ids = set()\n",
    "        unique_chunks = []\n",
    "        for chunk in all_chunks:\n",
    "            chunk_id = chunk.get(\"id\")\n",
    "            if chunk_id not in seen_ids:\n",
    "                unique_chunks.append(chunk)\n",
    "                seen_ids.add(chunk_id)\n",
    "\n",
    "        # Sort by score and take top results\n",
    "        unique_chunks.sort(key=lambda x: x.get(\"score\", 0), reverse=True)\n",
    "        top_chunks = unique_chunks[:5]\n",
    "\n",
    "        # Create comprehensive task\n",
    "        task = ResearchTask(query=main_query, max_sources=len(top_chunks), depth=\"deep\")\n",
    "\n",
    "        # Generate response with all context\n",
    "        system_prompt, user_prompt = self._build_research_prompt(task, top_chunks)\n",
    "\n",
    "        # Enhanced prompt for multi-query research\n",
    "        enhanced_prompt = (\n",
    "            user_prompt\n",
    "            + f\"\"\"\n",
    "\n",
    "ç›¸é—œå­å•é¡Œï¼š\n",
    "{chr(10).join(f\"- {sq}\" for sq in sub_queries)}\n",
    "\n",
    "è«‹æä¾›ä¸€å€‹ç¶œåˆæ€§çš„ç ”ç©¶åˆ†æžï¼Œæ¶µè“‹ä¸»è¦å•é¡Œå’Œç›¸é—œå­å•é¡Œã€‚\"\"\"\n",
    "        )\n",
    "\n",
    "        response = self._generate_response(system_prompt, enhanced_prompt)\n",
    "        summary, key_findings = self._parse_research_response(response)\n",
    "\n",
    "        # Build enhanced result\n",
    "        sources = []\n",
    "        for chunk in top_chunks:\n",
    "            source = {\n",
    "                \"id\": chunk.get(\"id\"),\n",
    "                \"text\": (\n",
    "                    chunk[\"text\"][:300] + \"...\"\n",
    "                    if len(chunk[\"text\"]) > 300\n",
    "                    else chunk[\"text\"]\n",
    "                ),\n",
    "                \"meta\": chunk[\"meta\"],\n",
    "                \"score\": chunk.get(\"score\", 0.0),\n",
    "            }\n",
    "            sources.append(source)\n",
    "\n",
    "        result = ResearchResult(\n",
    "            summary=summary,\n",
    "            key_findings=key_findings,\n",
    "            sources=sources,\n",
    "            confidence=min(0.9, np.mean([s[\"score\"] for s in sources]) + 0.1),\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            task=task,\n",
    "        )\n",
    "\n",
    "        self.research_history.append(result)\n",
    "        return result\n",
    "\n",
    "    def get_research_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary of research history\"\"\"\n",
    "        if not self.research_history:\n",
    "            return {\"total_research\": 0, \"avg_confidence\": 0, \"top_topics\": []}\n",
    "\n",
    "        total = len(self.research_history)\n",
    "        avg_confidence = np.mean([r.confidence for r in self.research_history])\n",
    "\n",
    "        # Extract topics from queries\n",
    "        topics = [r.task.query for r in self.research_history]\n",
    "\n",
    "        return {\n",
    "            \"total_research\": total,\n",
    "            \"avg_confidence\": avg_confidence,\n",
    "            \"recent_topics\": topics[-5:],  # Last 5 topics\n",
    "            \"best_confidence\": max(r.confidence for r in self.research_history),\n",
    "        }\n",
    "\n",
    "\n",
    "# Test advanced researcher\n",
    "advanced_researcher = AdvancedResearcher(rag_retriever=rag_retriever)\n",
    "\n",
    "\n",
    "def test_multi_query_research():\n",
    "    \"\"\"Test multi-query research capability\"\"\"\n",
    "    print(\"\\n=== Testing Multi-Query Research ===\")\n",
    "\n",
    "    main_query = \"å¤šä»£ç†ç³»çµ±åœ¨AIä¸­çš„æ‡‰ç”¨\"\n",
    "    sub_queries = [\"ä»£ç†å”ä½œçš„åŸºæœ¬åŽŸç†\", \"å¤šä»£ç†ç³»çµ±çš„å„ªå‹¢\", \"å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹\"]\n",
    "\n",
    "    result = advanced_researcher.multi_query_research(main_query, sub_queries)\n",
    "\n",
    "    print(f\"Main Query: {main_query}\")\n",
    "    print(f\"Sub-queries: {sub_queries}\")\n",
    "    print(f\"\\nResult Confidence: {result.confidence:.2f}\")\n",
    "    print(f\"Sources: {len(result.sources)}\")\n",
    "    print(f\"\\nSummary:\\n{result.summary}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "multi_result = test_multi_query_research()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6687c9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 7: Integration with Blackboard\n",
    "# ================================\n",
    "\n",
    "\n",
    "class Blackboard:\n",
    "    \"\"\"Simple blackboard for agent communication\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "        self.history = []\n",
    "\n",
    "    def write(self, key: str, value: Any, agent: str = \"unknown\"):\n",
    "        \"\"\"Write data to blackboard\"\"\"\n",
    "        self.data[key] = value\n",
    "        self.history.append(\n",
    "            {\n",
    "                \"action\": \"write\",\n",
    "                \"key\": key,\n",
    "                \"agent\": agent,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def read(self, key: str) -> Any:\n",
    "        \"\"\"Read data from blackboard\"\"\"\n",
    "        return self.data.get(key)\n",
    "\n",
    "    def get_all(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get all data\"\"\"\n",
    "        return self.data.copy()\n",
    "\n",
    "\n",
    "def integrate_researcher_with_blackboard():\n",
    "    \"\"\"Demonstrate researcher integration with blackboard\"\"\"\n",
    "    print(\"\\n=== Researcher + Blackboard Integration ===\")\n",
    "\n",
    "    # Create blackboard\n",
    "    blackboard = Blackboard()\n",
    "\n",
    "    # Simulate research workflow\n",
    "    blackboard.write(\n",
    "        \"research_request\",\n",
    "        {\"topic\": \"å‘é‡æª¢ç´¢æŠ€è¡“çš„ç™¼å±•\", \"urgency\": \"medium\", \"deadline\": \"2024-12-31\"},\n",
    "        agent=\"planner\",\n",
    "    )\n",
    "\n",
    "    # Researcher reads request\n",
    "    request = blackboard.read(\"research_request\")\n",
    "    print(f\"Research request: {request}\")\n",
    "\n",
    "    # Conduct research\n",
    "    task = ResearchTask(query=request[\"topic\"], depth=\"moderate\", max_sources=4)\n",
    "\n",
    "    result = researcher.research(task)\n",
    "\n",
    "    # Write results to blackboard\n",
    "    research_output = {\n",
    "        \"summary\": result.summary,\n",
    "        \"key_findings\": result.key_findings,\n",
    "        \"source_count\": len(result.sources),\n",
    "        \"confidence\": result.confidence,\n",
    "        \"completed_at\": result.timestamp,\n",
    "    }\n",
    "\n",
    "    blackboard.write(\"research_results\", research_output, agent=\"researcher\")\n",
    "\n",
    "    print(f\"\\nResearch completed and written to blackboard:\")\n",
    "    print(f\"Confidence: {result.confidence:.2f}\")\n",
    "    print(f\"Key findings: {len(result.key_findings)}\")\n",
    "\n",
    "    # Other agents can now read the results\n",
    "    planner_view = blackboard.read(\"research_results\")\n",
    "    print(f\"\\nPlanner can access: {list(planner_view.keys())}\")\n",
    "\n",
    "    return blackboard, result\n",
    "\n",
    "\n",
    "bb, bb_result = integrate_researcher_with_blackboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67816bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 8: Performance Metrics and Monitoring\n",
    "# ================================\n",
    "\n",
    "\n",
    "class ResearchMetrics:\n",
    "    \"\"\"Track researcher performance metrics\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            \"total_queries\": 0,\n",
    "            \"avg_response_time\": 0,\n",
    "            \"avg_confidence\": 0,\n",
    "            \"source_utilization\": 0,\n",
    "            \"citation_accuracy\": 0,\n",
    "        }\n",
    "        self.query_times = []\n",
    "        self.confidences = []\n",
    "        self.source_counts = []\n",
    "\n",
    "    def record_research(self, result: ResearchResult, response_time: float):\n",
    "        \"\"\"Record metrics for a research session\"\"\"\n",
    "        self.metrics[\"total_queries\"] += 1\n",
    "        self.query_times.append(response_time)\n",
    "        self.confidences.append(result.confidence)\n",
    "        self.source_counts.append(len(result.sources))\n",
    "\n",
    "        # Update averages\n",
    "        self.metrics[\"avg_response_time\"] = np.mean(self.query_times)\n",
    "        self.metrics[\"avg_confidence\"] = np.mean(self.confidences)\n",
    "        self.metrics[\"source_utilization\"] = np.mean(self.source_counts)\n",
    "\n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get metrics summary\"\"\"\n",
    "        if self.metrics[\"total_queries\"] == 0:\n",
    "            return {\"status\": \"no_data\"}\n",
    "\n",
    "        return {\n",
    "            \"total_queries\": self.metrics[\"total_queries\"],\n",
    "            \"avg_response_time\": f\"{self.metrics['avg_response_time']:.2f}s\",\n",
    "            \"avg_confidence\": f\"{self.metrics['avg_confidence']:.2f}\",\n",
    "            \"avg_sources_per_query\": f\"{self.metrics['source_utilization']:.1f}\",\n",
    "            \"performance_score\": min(\n",
    "                1.0,\n",
    "                self.metrics[\"avg_confidence\"]\n",
    "                * (1 / max(1, self.metrics[\"avg_response_time\"] / 5)),\n",
    "            ),\n",
    "        }\n",
    "\n",
    "\n",
    "# Test metrics tracking\n",
    "metrics = ResearchMetrics()\n",
    "\n",
    "\n",
    "def benchmark_researcher():\n",
    "    \"\"\"Benchmark researcher performance\"\"\"\n",
    "    print(\"\\n=== Researcher Benchmark ===\")\n",
    "\n",
    "    test_queries = [\n",
    "        \"RAGæŠ€è¡“çš„æ ¸å¿ƒçµ„ä»¶æœ‰å“ªäº›ï¼Ÿ\",\n",
    "        \"å‘é‡æª¢ç´¢å¦‚ä½•æå‡æœç´¢æº–ç¢ºæ€§ï¼Ÿ\",\n",
    "        \"ä¸­æ–‡NLPè™•ç†é¢è‡¨ä»€éº¼æŒ‘æˆ°ï¼Ÿ\",\n",
    "        \"å¼•ç”¨æ¨™è¨»åœ¨å­¸è¡“å¯«ä½œä¸­çš„é‡è¦æ€§\",\n",
    "        \"å¤šä»£ç†ç³»çµ±çš„å”ä½œæ©Ÿåˆ¶\",\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for query in test_queries:\n",
    "        print(f\"\\nTesting: {query}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        task = ResearchTask(query=query, max_sources=3)\n",
    "        result = researcher.research(task)\n",
    "        end_time = time.time()\n",
    "\n",
    "        response_time = end_time - start_time\n",
    "        metrics.record_research(result, response_time)\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"query\": query,\n",
    "                \"confidence\": result.confidence,\n",
    "                \"sources\": len(result.sources),\n",
    "                \"time\": response_time,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"  Confidence: {result.confidence:.2f}, Sources: {len(result.sources)}, Time: {response_time:.2f}s\"\n",
    "        )\n",
    "\n",
    "    # Display summary\n",
    "    summary = metrics.get_summary()\n",
    "    print(f\"\\nðŸ“Š Benchmark Summary:\")\n",
    "    for key, value in summary.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    return results, summary\n",
    "\n",
    "\n",
    "benchmark_results, benchmark_summary = benchmark_researcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d506a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 9: Error Handling and Robustness\n",
    "# ================================\n",
    "\n",
    "\n",
    "class RobustResearcher(ResearcherAgent):\n",
    "    \"\"\"Researcher with enhanced error handling\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.error_count = 0\n",
    "        self.max_retries = 3\n",
    "\n",
    "    def safe_research(self, task: ResearchTask) -> Optional[ResearchResult]:\n",
    "        \"\"\"Research with error handling and retries\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                print(f\"Research attempt {attempt + 1}/{self.max_retries}\")\n",
    "                return self.research(task)\n",
    "\n",
    "            except torch.cuda.OutOfMemoryError:\n",
    "                print(\"âš  CUDA OOM - clearing cache and retrying...\")\n",
    "                torch.cuda.empty_cache()\n",
    "                self.error_count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš  Research error: {str(e)[:100]}\")\n",
    "                self.error_count += 1\n",
    "\n",
    "                if attempt == self.max_retries - 1:\n",
    "                    # Return fallback result\n",
    "                    return self._create_fallback_result(task, str(e))\n",
    "\n",
    "                time.sleep(1)  # Brief pause before retry\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _create_fallback_result(\n",
    "        self, task: ResearchTask, error_msg: str\n",
    "    ) -> ResearchResult:\n",
    "        \"\"\"Create fallback result when research fails\"\"\"\n",
    "        return ResearchResult(\n",
    "            summary=f\"ç„¡æ³•å®Œæˆå°ã€Œ{task.query}ã€çš„ç ”ç©¶åˆ†æžã€‚éŒ¯èª¤ï¼š{error_msg[:50]}...\",\n",
    "            key_findings=[\"ç ”ç©¶éŽç¨‹ä¸­é‡åˆ°æŠ€è¡“å•é¡Œ\", \"å»ºè­°ç¨å¾Œé‡è©¦æˆ–èª¿æ•´æŸ¥è©¢\"],\n",
    "            sources=[],\n",
    "            confidence=0.1,\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            task=task,\n",
    "        )\n",
    "\n",
    "    def get_error_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get error statistics\"\"\"\n",
    "        return {\n",
    "            \"total_errors\": self.error_count,\n",
    "            \"error_rate\": self.error_count / max(1, metrics.metrics[\"total_queries\"]),\n",
    "            \"status\": \"healthy\" if self.error_count < 3 else \"needs_attention\",\n",
    "        }\n",
    "\n",
    "\n",
    "# Test robust researcher\n",
    "robust_researcher = RobustResearcher(rag_retriever=rag_retriever)\n",
    "\n",
    "\n",
    "def test_error_handling():\n",
    "    \"\"\"Test error handling capabilities\"\"\"\n",
    "    print(\"\\n=== Testing Error Handling ===\")\n",
    "\n",
    "    # Test with normal query\n",
    "    normal_task = ResearchTask(query=\"ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ\")\n",
    "    result = robust_researcher.safe_research(normal_task)\n",
    "\n",
    "    print(f\"Normal query result: {result.confidence:.2f}\")\n",
    "\n",
    "    # Test with potentially problematic query (very long)\n",
    "    long_query = \"å¾ˆé•·çš„æŸ¥è©¢ \" * 100  # Artificially long query\n",
    "    long_task = ResearchTask(query=long_query)\n",
    "\n",
    "    try:\n",
    "        result = robust_researcher.safe_research(long_task)\n",
    "        print(f\"Long query handled: {result.confidence:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Long query failed: {e}\")\n",
    "\n",
    "    # Get error stats\n",
    "    error_stats = robust_researcher.get_error_stats()\n",
    "    print(f\"Error stats: {error_stats}\")\n",
    "\n",
    "    return error_stats\n",
    "\n",
    "\n",
    "error_stats = test_error_handling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3ce76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 10: Export and Integration Utils\n",
    "# ================================\n",
    "\n",
    "\n",
    "def save_research_result(result: ResearchResult, output_dir: str = \"outs\"):\n",
    "    \"\"\"Save research result to file\"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "\n",
    "    # Create filename from timestamp and query\n",
    "    safe_query = re.sub(r\"[^\\w\\s-]\", \"\", result.task.query)[:50]\n",
    "    safe_query = re.sub(r\"[-\\s]+\", \"-\", safe_query)\n",
    "    filename = f\"research_{safe_query}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "    filepath = output_path / filename\n",
    "\n",
    "    # Convert to serializable format\n",
    "    data = {\n",
    "        \"task\": asdict(result.task),\n",
    "        \"result\": {\n",
    "            \"summary\": result.summary,\n",
    "            \"key_findings\": result.key_findings,\n",
    "            \"sources\": result.sources,\n",
    "            \"confidence\": result.confidence,\n",
    "            \"timestamp\": result.timestamp,\n",
    "        },\n",
    "        \"metadata\": {\n",
    "            \"model\": researcher.model_name,\n",
    "            \"embedding_model\": rag_retriever.embedding_model,\n",
    "            \"total_sources\": len(result.sources),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Research result saved to: {filepath}\")\n",
    "    return str(filepath)\n",
    "\n",
    "\n",
    "def load_research_result(filepath: str) -> ResearchResult:\n",
    "    \"\"\"Load research result from file\"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    task = ResearchTask(**data[\"task\"])\n",
    "    result_data = data[\"result\"]\n",
    "\n",
    "    result = ResearchResult(\n",
    "        summary=result_data[\"summary\"],\n",
    "        key_findings=result_data[\"key_findings\"],\n",
    "        sources=result_data[\"sources\"],\n",
    "        confidence=result_data[\"confidence\"],\n",
    "        timestamp=result_data[\"timestamp\"],\n",
    "        task=task,\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_research_report(results: List[ResearchResult]) -> str:\n",
    "    \"\"\"Create formatted research report from multiple results\"\"\"\n",
    "    report_lines = [\n",
    "        \"# ç ”ç©¶å ±å‘Š\",\n",
    "        f\"ç”Ÿæˆæ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "        f\"ç¸½ç ”ç©¶æ•¸é‡ï¼š{len(results)}\",\n",
    "        \"\",\n",
    "    ]\n",
    "\n",
    "    for i, result in enumerate(results, 1):\n",
    "        report_lines.extend(\n",
    "            [\n",
    "                f\"## ç ”ç©¶ {i}: {result.task.query}\",\n",
    "                f\"**å¯ä¿¡åº¦ï¼š** {result.confidence:.2f}\",\n",
    "                f\"**ä¾†æºæ•¸é‡ï¼š** {len(result.sources)}\",\n",
    "                \"\",\n",
    "                \"### æ‘˜è¦\",\n",
    "                result.summary,\n",
    "                \"\",\n",
    "                \"### é—œéµç™¼ç¾\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for j, finding in enumerate(result.key_findings, 1):\n",
    "            report_lines.append(f\"{j}. {finding}\")\n",
    "\n",
    "        report_lines.extend(\n",
    "            [\n",
    "                \"\",\n",
    "                \"### åƒè€ƒä¾†æº\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for j, source in enumerate(result.sources, 1):\n",
    "            meta = source[\"meta\"]\n",
    "            title = meta.get(\"title\", \"Unknown\")\n",
    "            score = source.get(\"score\", 0)\n",
    "            report_lines.append(f\"{j}. {title} (ç›¸é—œåº¦: {score:.3f})\")\n",
    "\n",
    "        report_lines.append(\"\\n---\\n\")\n",
    "\n",
    "    return \"\\n\".join(report_lines)\n",
    "\n",
    "\n",
    "# Test export functionality\n",
    "def test_export_functionality():\n",
    "    \"\"\"Test research result export and reporting\"\"\"\n",
    "    print(\"\\n=== Testing Export Functionality ===\")\n",
    "\n",
    "    # Save a research result\n",
    "    filepath = save_research_result(smoke_result)\n",
    "\n",
    "    # Load it back\n",
    "    loaded_result = load_research_result(filepath)\n",
    "    print(f\"Loaded result confidence: {loaded_result.confidence:.2f}\")\n",
    "\n",
    "    # Create a report with multiple results\n",
    "    sample_results = (\n",
    "        [smoke_result, multi_result] if \"multi_result\" in locals() else [smoke_result]\n",
    "    )\n",
    "    report = create_research_report(sample_results)\n",
    "\n",
    "    # Save report\n",
    "    report_path = (\n",
    "        Path(\"outs\") / f\"research_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "    )\n",
    "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "    print(f\"Report saved to: {report_path}\")\n",
    "    print(f\"Report length: {len(report)} characters\")\n",
    "\n",
    "    return report_path\n",
    "\n",
    "\n",
    "report_path = test_export_functionality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccad3701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 11: Final Integration Test\n",
    "# ================================\n",
    "\n",
    "\n",
    "def comprehensive_integration_test():\n",
    "    \"\"\"Comprehensive test of all researcher features\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ðŸ§ª COMPREHENSIVE RESEARCHER INTEGRATION TEST\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    test_scenarios = [\n",
    "        {\n",
    "            \"name\": \"Basic RAG Research\",\n",
    "            \"query\": \"è§£é‡‹å‘é‡æª¢ç´¢çš„å·¥ä½œåŽŸç†\",\n",
    "            \"expected_sources\": 2,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Multi-domain Query\",\n",
    "            \"query\": \"RAGæŠ€è¡“åœ¨å¤šä»£ç†ç³»çµ±ä¸­çš„æ‡‰ç”¨\",\n",
    "            \"expected_sources\": 3,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Technical Deep Dive\",\n",
    "            \"query\": \"FAISSç´¢å¼•çš„å„ªåŒ–ç­–ç•¥\",\n",
    "            \"expected_sources\": 2,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    all_results = []\n",
    "    total_start = time.time()\n",
    "\n",
    "    for i, scenario in enumerate(test_scenarios, 1):\n",
    "        print(f\"\\nðŸ“‹ Test {i}: {scenario['name']}\")\n",
    "        print(f\"Query: {scenario['query']}\")\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Create task\n",
    "        task = ResearchTask(\n",
    "            query=scenario[\"query\"],\n",
    "            max_sources=scenario[\"expected_sources\"],\n",
    "            depth=\"moderate\",\n",
    "        )\n",
    "\n",
    "        # Execute research\n",
    "        result = robust_researcher.safe_research(task)\n",
    "\n",
    "        if result:\n",
    "            elapsed = time.time() - start\n",
    "            all_results.append(result)\n",
    "\n",
    "            print(f\"âœ… Completed in {elapsed:.2f}s\")\n",
    "            print(f\"   Confidence: {result.confidence:.2f}\")\n",
    "            print(f\"   Sources: {len(result.sources)}\")\n",
    "            print(f\"   Findings: {len(result.key_findings)}\")\n",
    "\n",
    "            # Validate result quality\n",
    "            if result.confidence > 0.3 and len(result.sources) > 0:\n",
    "                print(\"   âœ… Quality check passed\")\n",
    "            else:\n",
    "                print(\"   âš  Quality check warning\")\n",
    "        else:\n",
    "            print(\"   âŒ Research failed\")\n",
    "\n",
    "    total_time = time.time() - total_start\n",
    "\n",
    "    # Generate summary report\n",
    "    if all_results:\n",
    "        print(f\"\\nðŸ“Š INTEGRATION TEST SUMMARY\")\n",
    "        print(f\"Total time: {total_time:.2f}s\")\n",
    "        print(\n",
    "            f\"Success rate: {len(all_results)}/{len(test_scenarios)} ({len(all_results)/len(test_scenarios)*100:.1f}%)\"\n",
    "        )\n",
    "        print(f\"Average confidence: {np.mean([r.confidence for r in all_results]):.2f}\")\n",
    "        print(\n",
    "            f\"Average sources per query: {np.mean([len(r.sources) for r in all_results]):.1f}\"\n",
    "        )\n",
    "\n",
    "        # Save comprehensive report\n",
    "        final_report = create_research_report(all_results)\n",
    "        final_report_path = (\n",
    "            Path(\"outs\")\n",
    "            / f\"integration_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "        )\n",
    "        with open(final_report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(final_report)\n",
    "\n",
    "        print(f\"Final report saved: {final_report_path}\")\n",
    "\n",
    "        return all_results, final_report_path\n",
    "    else:\n",
    "        print(\"âŒ All tests failed\")\n",
    "        return [], None\n",
    "\n",
    "\n",
    "integration_results, final_report_path = comprehensive_integration_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e52daa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 12: Summary and Next Steps\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸ“‹ NB31 SUMMARY: RESEARCHER WITH RAG INTEGRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\n",
    "    \"\"\"\n",
    "## ðŸŽ¯ Goals Completed:\n",
    "âœ… Integrated Stage 2 RAG components into Researcher agent role\n",
    "âœ… Implemented knowledge retrieval with citation tracking\n",
    "âœ… Created research synthesis with source attribution\n",
    "âœ… Enabled multi-source evidence gathering and summarization\n",
    "âœ… Built foundation for 4-role orchestrator collaboration\n",
    "\n",
    "## ðŸ”§ Core Components Built:\n",
    "â€¢ RAGRetriever: å‘é‡æª¢ç´¢èˆ‡FAISSç´¢å¼•æ•´åˆ\n",
    "â€¢ ResearcherAgent: å…·å‚™RAGèƒ½åŠ›çš„ç ”ç©¶åŠ©ç†\n",
    "â€¢ AdvancedResearcher: å¤šæŸ¥è©¢èˆ‡æ­·å²è¿½è¹¤\n",
    "â€¢ RobustResearcher: éŒ¯èª¤è™•ç†èˆ‡é‡è©¦æ©Ÿåˆ¶\n",
    "â€¢ Blackboard Integration: èˆ‡å¤šä»£ç†é»‘æ¿ç³»çµ±æ•´åˆ\n",
    "â€¢ Research Metrics: æ•ˆèƒ½è¿½è¹¤èˆ‡åŸºæº–æ¸¬è©¦\n",
    "\n",
    "## ðŸ— Key Parameters (Low-VRAM Optimized):\n",
    "â€¢ Embedding Model: BAAI/bge-m3 (multilingual, efficient)\n",
    "â€¢ LLM: Qwen2.5-7B-Instruct (device_map=\"auto\", fp16)\n",
    "â€¢ Max Context: 3072 tokens (ç•™ç©ºé–“çµ¦ç”Ÿæˆ)\n",
    "â€¢ Top-K Retrieval: 3-5 chunks (å¹³è¡¡å“è³ªèˆ‡é€Ÿåº¦)\n",
    "â€¢ Temperature: 0.3 (ç ”ç©¶ä»»å‹™éœ€è¦ä¸€è‡´æ€§)\n",
    "â€¢ Batch Processing: æ”¯æ´æ‰¹æ¬¡åµŒå…¥ä»¥æå‡æ•ˆçŽ‡\n",
    "\n",
    "## ðŸ§ª Smoke Test Results:\"\"\"\n",
    ")\n",
    "\n",
    "if \"smoke_result\" in locals():\n",
    "    print(f\"â€¢ Basic Research: âœ… (Confidence: {smoke_result.confidence:.2f})\")\n",
    "if \"multi_result\" in locals():\n",
    "    print(f\"â€¢ Multi-Query: âœ… (Confidence: {multi_result.confidence:.2f})\")\n",
    "if \"error_stats\" in locals():\n",
    "    print(f\"â€¢ Error Handling: âœ… (Status: {error_stats.get('status', 'unknown')})\")\n",
    "if \"benchmark_summary\" in locals():\n",
    "    print(f\"â€¢ Performance: âœ… (Avg: {benchmark_summary.get('avg_confidence', 'N/A')})\")\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "## âš¡ Performance Metrics:\"\"\"\n",
    ")\n",
    "if \"metrics\" in locals():\n",
    "    summary = metrics.get_summary()\n",
    "    if summary.get(\"status\") != \"no_data\":\n",
    "        for key, value in summary.items():\n",
    "            print(f\"â€¢ {key}: {value}\")\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "## ðŸŽ® When to Use This:\n",
    "â€¢ éœ€è¦åŸºæ–¼ç‰¹å®šçŸ¥è­˜åº«çš„ç ”ç©¶åˆ†æž\n",
    "â€¢ å¤šä»£ç†å”ä½œä¸­çš„è³‡è¨Šæ”¶é›†è§’è‰²\n",
    "â€¢ å­¸è¡“å¯«ä½œçš„æ–‡ç»èª¿ç ”éšŽæ®µ\n",
    "â€¢ æŠ€è¡“æ–‡æª”çš„èƒŒæ™¯è³‡æ–™æ•´ç†\n",
    "â€¢ éœ€è¦å¼•ç”¨ä¾†æºçš„å…§å®¹ç”Ÿæˆ\n",
    "\n",
    "## âš  Pitfalls & Solutions:\n",
    "â€¢ ç´¢å¼•ç‚ºç©º â†’ æª¢æŸ¥data/ç›®éŒ„èˆ‡chunks.jsonl\n",
    "â€¢ CUDA OOM â†’ é™ä½Žbatch_sizeæˆ–ä½¿ç”¨CPU\n",
    "â€¢ å¼•ç”¨æ ¼å¼éŒ¯äº‚ â†’ ç¢ºä¿chunksåŒ…å«æ­£ç¢ºmetaè³‡æ–™\n",
    "â€¢ æª¢ç´¢ä¸ç›¸é—œ â†’ èª¿æ•´embeddingæ¨¡åž‹æˆ–æŸ¥è©¢æ”¹å¯«\n",
    "â€¢ ç”ŸæˆéŽé•· â†’ é™åˆ¶max_new_tokensèˆ‡contexté•·åº¦\n",
    "\n",
    "## ðŸš€ Next Steps (nb32):\n",
    "â€¢ Planner Agent: åŸºæ–¼ç ”ç©¶çµæžœç”Ÿæˆå¤§ç¶±\n",
    "â€¢ Writer-Researcherå”ä½œ: å…±äº«é»‘æ¿é€šä¿¡\n",
    "â€¢ çµæ§‹åŒ–è¼¸å‡º: JSON schemaé©—è­‰\n",
    "â€¢ å¤šè¼ªå°è©±: è¿½å•èˆ‡æ·±å…¥ç ”ç©¶\n",
    "â€¢ é ˜åŸŸå°ˆç²¾: æŠ€è¡“/æ³•å¾‹/æ•™è‚²ç­‰å°ˆæ¥­æ¨¡å¼\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸŽ‰ NB31 COMPLETED - Ready for nb32_planner_outline.ipynb\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
