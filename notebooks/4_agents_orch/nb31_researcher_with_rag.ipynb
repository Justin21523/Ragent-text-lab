{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66682ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cache] ../ai_warehouse/cache | GPU: True\n"
     ]
    }
   ],
   "source": [
    "# nb31_researcher_with_rag.ipynb\n",
    "# Stage 4: Multi-Agent Orchestrator - Researcher with RAG Integration\n",
    "\"\"\"\n",
    "Goals:\n",
    "- Integrate Stage 2 RAG components into Researcher agent role\n",
    "- Implement knowledge retrieval with citation tracking\n",
    "- Create research synthesis with source attribution\n",
    "- Enable multi-source evidence gathering and summarization\n",
    "- Build foundation for 4-role orchestrator collaboration\n",
    "\n",
    "Prerequisites:\n",
    "- Completed nb10-nb19 (RAG basics)\n",
    "- Completed nb30 (orchestrator skeleton)\n",
    "- Basic understanding of agent roles and blackboard patterns\n",
    "\"\"\"\n",
    "\n",
    "# Cell1:  Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (複製到每本 notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6fe234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 2: Dependencies and Imports\n",
    "# ================================\n",
    "\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "\n",
    "# Core ML components\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# RAG components\n",
    "import faiss\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Utility\n",
    "import re\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✓ Dependencies loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f554c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 3: RAG Components Integration\n",
    "# ================================\n",
    "\n",
    "\n",
    "class RAGRetriever:\n",
    "    \"\"\"Lightweight RAG retriever for Researcher agent\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model: str = \"BAAI/bge-m3\",\n",
    "        index_path: str = \"indices/general.faiss\",\n",
    "        chunks_path: str = \"indices/chunks.jsonl\",\n",
    "        device: str = \"auto\",\n",
    "    ):\n",
    "\n",
    "        self.embedding_model = embedding_model\n",
    "        self.index_path = Path(index_path)\n",
    "        self.chunks_path = Path(chunks_path)\n",
    "\n",
    "        # Load embedding model\n",
    "        print(f\"Loading embedding model: {embedding_model}\")\n",
    "        self.embedder = SentenceTransformer(embedding_model, device=device)\n",
    "\n",
    "        # Load or create index\n",
    "        self.index = None\n",
    "        self.chunks = []\n",
    "        self._load_index()\n",
    "\n",
    "    def _load_index(self):\n",
    "        \"\"\"Load FAISS index and chunks metadata\"\"\"\n",
    "        try:\n",
    "            if self.index_path.exists() and self.chunks_path.exists():\n",
    "                # Load existing index\n",
    "                self.index = faiss.read_index(str(self.index_path))\n",
    "\n",
    "                # Load chunks metadata\n",
    "                with open(self.chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    self.chunks = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "                print(\n",
    "                    f\"✓ Loaded index with {self.index.ntotal} vectors and {len(self.chunks)} chunks\"\n",
    "                )\n",
    "            else:\n",
    "                print(\"⚠ No existing index found - will create sample data\")\n",
    "                self._create_sample_index()\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading index: {e}\")\n",
    "            self._create_sample_index()\n",
    "\n",
    "    def _create_sample_index(self):\n",
    "        \"\"\"Create sample index for demonstration\"\"\"\n",
    "        # Sample Chinese documents for RAG demonstration\n",
    "        sample_docs = [\n",
    "            {\n",
    "                \"text\": \"RAG（檢索增強生成）是一種結合資訊檢索與生成式AI的技術架構。它能夠讓語言模型在生成回答時，先從外部知識庫中檢索相關資訊，再基於這些資訊生成更準確、更有根據的回答。\",\n",
    "                \"meta\": {\n",
    "                    \"source_id\": \"rag_intro\",\n",
    "                    \"title\": \"RAG技術介紹\",\n",
    "                    \"section\": \"基本概念\",\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"多代理系統（Multi-Agent System）是由多個自主智能體組成的系統，每個智能體都有自己的目標和行為模式。在AI應用中，不同角色的代理可以協作完成複雜任務，如研究、規劃、寫作和審核。\",\n",
    "                \"meta\": {\n",
    "                    \"source_id\": \"mas_intro\",\n",
    "                    \"title\": \"多代理系統\",\n",
    "                    \"section\": \"系統架構\",\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"向量檢索使用向量相似度來找到語義相關的文檔片段。常用的方法包括餘弦相似度、歐氏距離和內積。FAISS是Facebook開發的高效向量檢索庫，支持大規模向量索引和快速相似度搜索。\",\n",
    "                \"meta\": {\n",
    "                    \"source_id\": \"vector_search\",\n",
    "                    \"title\": \"向量檢索技術\",\n",
    "                    \"section\": \"檢索算法\",\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"中文文本處理面臨獨特挑戰，包括分詞、繁簡轉換、語義分段等。BGE-M3是針對中文優化的多語言嵌入模型，在中文語義理解任務上表現優秀。\",\n",
    "                \"meta\": {\n",
    "                    \"source_id\": \"zh_nlp\",\n",
    "                    \"title\": \"中文NLP處理\",\n",
    "                    \"section\": \"技術挑戰\",\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"引用標註（Citation）在RAG系統中至關重要，它確保生成的內容有據可依。常見格式包括括號標註[1][2]和腳註形式，同時需要提供完整的來源資訊以便驗證。\",\n",
    "                \"meta\": {\n",
    "                    \"source_id\": \"citation\",\n",
    "                    \"title\": \"引用標註系統\",\n",
    "                    \"section\": \"實現方法\",\n",
    "                },\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        # Create chunks\n",
    "        self.chunks = []\n",
    "        texts = []\n",
    "        for i, doc in enumerate(sample_docs):\n",
    "            chunk = {\"id\": i, \"text\": doc[\"text\"], \"meta\": doc[\"meta\"]}\n",
    "            self.chunks.append(chunk)\n",
    "            texts.append(doc[\"text\"])\n",
    "\n",
    "        # Create embeddings\n",
    "        print(\"Creating sample embeddings...\")\n",
    "        embeddings = self.embedder.encode(texts, normalize_embeddings=True)\n",
    "        embeddings = embeddings.astype(np.float32)\n",
    "\n",
    "        # Create FAISS index\n",
    "        dim = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(dim)  # Inner product for normalized vectors\n",
    "        self.index.add(embeddings)\n",
    "\n",
    "        # Save index and chunks\n",
    "        self.index_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        faiss.write_index(self.index, str(self.index_path))\n",
    "\n",
    "        with open(self.chunks_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for chunk in self.chunks:\n",
    "                f.write(json.dumps(chunk, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        print(f\"✓ Created sample index with {len(self.chunks)} chunks\")\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve relevant chunks for query\"\"\"\n",
    "        if not self.index or not self.chunks:\n",
    "            return []\n",
    "\n",
    "        # Encode query\n",
    "        query_embedding = self.embedder.encode([query], normalize_embeddings=True)\n",
    "        query_embedding = query_embedding.astype(np.float32)\n",
    "\n",
    "        # Search\n",
    "        scores, indices = self.index.search(\n",
    "            query_embedding, min(top_k, len(self.chunks))\n",
    "        )\n",
    "\n",
    "        # Format results\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx >= 0 and idx < len(self.chunks):  # Valid index\n",
    "                chunk = self.chunks[idx].copy()\n",
    "                chunk[\"score\"] = float(score)\n",
    "                results.append(chunk)\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# Test RAG retriever\n",
    "print(\"Setting up RAG retriever...\")\n",
    "rag_retriever = RAGRetriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d2eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 4: Researcher Agent Class\n",
    "# ================================\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ResearchTask:\n",
    "    \"\"\"Research task specification\"\"\"\n",
    "\n",
    "    query: str\n",
    "    domain: str = \"general\"\n",
    "    max_sources: int = 5\n",
    "    depth: str = \"moderate\"  # surface, moderate, deep\n",
    "    citation_style: str = \"brackets\"  # brackets, footnotes\n",
    "    language: str = \"zh\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ResearchResult:\n",
    "    \"\"\"Research result with citations\"\"\"\n",
    "\n",
    "    summary: str\n",
    "    key_findings: List[str]\n",
    "    sources: List[Dict[str, Any]]\n",
    "    confidence: float\n",
    "    timestamp: str\n",
    "    task: ResearchTask\n",
    "\n",
    "\n",
    "class ResearcherAgent:\n",
    "    \"\"\"Researcher agent with RAG integration\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_model: str = \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        rag_retriever: RAGRetriever = None,\n",
    "        max_new_tokens: int = 1024,\n",
    "        temperature: float = 0.3,\n",
    "    ):\n",
    "\n",
    "        self.model_name = llm_model\n",
    "        self.rag_retriever = rag_retriever\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Load LLM\n",
    "        print(f\"Loading LLM: {llm_model}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(llm_model, use_fast=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            llm_model,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        print(f\"✓ Researcher agent initialized\")\n",
    "\n",
    "    def _build_research_prompt(\n",
    "        self, task: ResearchTask, retrieved_chunks: List[Dict]\n",
    "    ) -> str:\n",
    "        \"\"\"Build research prompt with retrieved context\"\"\"\n",
    "\n",
    "        # Build context from retrieved chunks\n",
    "        context_parts = []\n",
    "        for i, chunk in enumerate(retrieved_chunks, 1):\n",
    "            source_info = chunk[\"meta\"]\n",
    "            source_desc = f\"{source_info.get('title', 'Unknown')} - {source_info.get('section', 'N/A')}\"\n",
    "            context_parts.append(f\"[來源{i}] {source_desc}\\n{chunk['text']}\")\n",
    "\n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "        # Build citations list\n",
    "        citations = []\n",
    "        for i, chunk in enumerate(retrieved_chunks, 1):\n",
    "            meta = chunk[\"meta\"]\n",
    "            citation = (\n",
    "                f\"[{i}] {meta.get('source_id', 'N/A')} - {meta.get('title', 'Unknown')}\"\n",
    "            )\n",
    "            citations.append(citation)\n",
    "\n",
    "        citations_text = \"\\n\".join(citations)\n",
    "\n",
    "        # Research prompt template\n",
    "        system_prompt = \"\"\"你是一位專業的研究助理，擅長從多個來源整合資訊並產生有據可依的研究摘要。\n",
    "\n",
    "任務要求：\n",
    "1. 基於提供的來源資料回答研究問題\n",
    "2. 在回答中使用 [1], [2] 等標註引用來源\n",
    "3. 提供關鍵發現列表\n",
    "4. 確保內容準確且有根據\n",
    "5. 使用繁體中文回答\n",
    "\n",
    "回答格式：\n",
    "## 研究摘要\n",
    "[基於來源資料的綜合分析]\n",
    "\n",
    "## 關鍵發現\n",
    "- [發現1] [引用]\n",
    "- [發現2] [引用]\n",
    "- [發現3] [引用]\n",
    "\n",
    "不要透露內部思考過程，直接提供研究結果。\"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"研究問題：{task.query}\n",
    "\n",
    "參考資料：\n",
    "{context}\n",
    "\n",
    "請基於以上資料進行研究分析，並在回答中適當引用來源。\n",
    "\n",
    "來源列表：\n",
    "{citations_text}\"\"\"\n",
    "\n",
    "        return system_prompt, user_prompt\n",
    "\n",
    "    def _generate_response(self, system_prompt: str, user_prompt: str) -> str:\n",
    "        \"\"\"Generate LLM response\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "\n",
    "        # Apply chat template\n",
    "        formatted_prompt = self.tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            formatted_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=3072,  # Leave room for generation\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                temperature=self.temperature,\n",
    "                do_sample=self.temperature > 0,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        # Decode response\n",
    "        response = self.tokenizer.decode(\n",
    "            outputs[0][inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n",
    "        ).strip()\n",
    "\n",
    "        return response\n",
    "\n",
    "    def _parse_research_response(self, response: str) -> Tuple[str, List[str]]:\n",
    "        \"\"\"Parse research response to extract summary and key findings\"\"\"\n",
    "        lines = response.split(\"\\n\")\n",
    "\n",
    "        summary_lines = []\n",
    "        findings = []\n",
    "        current_section = \"summary\"\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            if \"## 關鍵發現\" in line or \"關鍵發現\" in line:\n",
    "                current_section = \"findings\"\n",
    "                continue\n",
    "            elif \"##\" in line and current_section == \"findings\":\n",
    "                break\n",
    "\n",
    "            if current_section == \"summary\":\n",
    "                if not line.startswith(\"#\"):\n",
    "                    summary_lines.append(line)\n",
    "            elif current_section == \"findings\":\n",
    "                if line.startswith(\"-\") or line.startswith(\"•\"):\n",
    "                    findings.append(line.lstrip(\"- •\").strip())\n",
    "\n",
    "        summary = \" \".join(summary_lines).strip()\n",
    "\n",
    "        return summary, findings\n",
    "\n",
    "    def research(self, task: ResearchTask) -> ResearchResult:\n",
    "        \"\"\"Conduct research on given task\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        logger.info(f\"Starting research on: {task.query}\")\n",
    "\n",
    "        # Step 1: Retrieve relevant information\n",
    "        if self.rag_retriever:\n",
    "            retrieved_chunks = self.rag_retriever.retrieve(\n",
    "                task.query, top_k=task.max_sources\n",
    "            )\n",
    "            logger.info(f\"Retrieved {len(retrieved_chunks)} relevant chunks\")\n",
    "        else:\n",
    "            retrieved_chunks = []\n",
    "            logger.warning(\"No RAG retriever available\")\n",
    "\n",
    "        # Step 2: Generate research response\n",
    "        if retrieved_chunks:\n",
    "            system_prompt, user_prompt = self._build_research_prompt(\n",
    "                task, retrieved_chunks\n",
    "            )\n",
    "            response = self._generate_response(system_prompt, user_prompt)\n",
    "        else:\n",
    "            # Fallback: direct response without RAG\n",
    "            response = self._generate_fallback_response(task)\n",
    "\n",
    "        # Step 3: Parse response\n",
    "        summary, key_findings = self._parse_research_response(response)\n",
    "\n",
    "        # Step 4: Build result\n",
    "        sources = []\n",
    "        for chunk in retrieved_chunks:\n",
    "            source = {\n",
    "                \"id\": chunk.get(\"id\"),\n",
    "                \"text\": (\n",
    "                    chunk[\"text\"][:200] + \"...\"\n",
    "                    if len(chunk[\"text\"]) > 200\n",
    "                    else chunk[\"text\"]\n",
    "                ),\n",
    "                \"meta\": chunk[\"meta\"],\n",
    "                \"score\": chunk.get(\"score\", 0.0),\n",
    "            }\n",
    "            sources.append(source)\n",
    "\n",
    "        # Calculate confidence based on retrieval scores and content length\n",
    "        if sources:\n",
    "            avg_score = np.mean([s[\"score\"] for s in sources])\n",
    "            confidence = min(0.95, max(0.3, avg_score))\n",
    "        else:\n",
    "            confidence = 0.2  # Low confidence without sources\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.info(\n",
    "            f\"Research completed in {elapsed_time:.2f}s with confidence {confidence:.2f}\"\n",
    "        )\n",
    "\n",
    "        result = ResearchResult(\n",
    "            summary=summary,\n",
    "            key_findings=key_findings,\n",
    "            sources=sources,\n",
    "            confidence=confidence,\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            task=task,\n",
    "        )\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _generate_fallback_response(self, task: ResearchTask) -> str:\n",
    "        \"\"\"Generate response without RAG (fallback)\"\"\"\n",
    "        system_prompt = \"\"\"你是一位研究助理。基於你的知識回答問題，並明確說明這是基於一般知識而非特定資料來源的回答。\"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"研究問題：{task.query}\n",
    "\n",
    "請提供一個結構化的回答，包含：\n",
    "## 研究摘要\n",
    "## 關鍵發現\n",
    "\n",
    "注意：此回答基於一般知識，未使用特定資料來源。\"\"\"\n",
    "\n",
    "        return self._generate_response(system_prompt, user_prompt)\n",
    "\n",
    "\n",
    "# Initialize researcher agent\n",
    "print(\"Initializing Researcher Agent...\")\n",
    "researcher = ResearcherAgent(rag_retriever=rag_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69de625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 5: Smoke Test - Basic Research\n",
    "# ================================\n",
    "\n",
    "\n",
    "def test_researcher_basic():\n",
    "    \"\"\"Smoke test for researcher agent\"\"\"\n",
    "    print(\"=== Smoke Test: Basic Research ===\")\n",
    "\n",
    "    # Define research task\n",
    "    task = ResearchTask(\n",
    "        query=\"什麼是RAG技術？它有哪些主要優勢？\",\n",
    "        domain=\"ai\",\n",
    "        max_sources=3,\n",
    "        depth=\"moderate\",\n",
    "    )\n",
    "\n",
    "    print(f\"Research Task: {task.query}\")\n",
    "    print(\"Running research...\")\n",
    "\n",
    "    # Conduct research\n",
    "    result = researcher.research(task)\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\n📊 Research Results:\")\n",
    "    print(f\"Confidence: {result.confidence:.2f}\")\n",
    "    print(f\"Sources used: {len(result.sources)}\")\n",
    "\n",
    "    print(f\"\\n📝 Summary:\")\n",
    "    print(result.summary)\n",
    "\n",
    "    print(f\"\\n🔍 Key Findings:\")\n",
    "    for i, finding in enumerate(result.key_findings, 1):\n",
    "        print(f\"{i}. {finding}\")\n",
    "\n",
    "    print(f\"\\n📚 Sources:\")\n",
    "    for i, source in enumerate(result.sources, 1):\n",
    "        meta = source[\"meta\"]\n",
    "        print(f\"[{i}] {meta.get('title', 'Unknown')} (Score: {source['score']:.3f})\")\n",
    "        print(f\"    {source['text']}\")\n",
    "\n",
    "    # Validation\n",
    "    assert result.summary, \"Summary should not be empty\"\n",
    "    assert result.key_findings, \"Should have key findings\"\n",
    "    assert result.confidence > 0, \"Confidence should be positive\"\n",
    "\n",
    "    print(\"\\n✅ Smoke test passed!\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_result = test_researcher_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60163169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 6: Advanced Research Features\n",
    "# ================================\n",
    "\n",
    "\n",
    "class AdvancedResearcher(ResearcherAgent):\n",
    "    \"\"\"Enhanced researcher with advanced features\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.research_history = []\n",
    "\n",
    "    def multi_query_research(\n",
    "        self, main_query: str, sub_queries: List[str]\n",
    "    ) -> ResearchResult:\n",
    "        \"\"\"Research with multiple related queries\"\"\"\n",
    "        print(f\"Conducting multi-query research on: {main_query}\")\n",
    "\n",
    "        all_chunks = []\n",
    "\n",
    "        # Retrieve for main query\n",
    "        main_chunks = self.rag_retriever.retrieve(main_query, top_k=3)\n",
    "        all_chunks.extend(main_chunks)\n",
    "\n",
    "        # Retrieve for sub-queries\n",
    "        for sub_query in sub_queries:\n",
    "            sub_chunks = self.rag_retriever.retrieve(sub_query, top_k=2)\n",
    "            all_chunks.extend(sub_chunks)\n",
    "\n",
    "        # Deduplicate by chunk ID\n",
    "        seen_ids = set()\n",
    "        unique_chunks = []\n",
    "        for chunk in all_chunks:\n",
    "            chunk_id = chunk.get(\"id\")\n",
    "            if chunk_id not in seen_ids:\n",
    "                unique_chunks.append(chunk)\n",
    "                seen_ids.add(chunk_id)\n",
    "\n",
    "        # Sort by score and take top results\n",
    "        unique_chunks.sort(key=lambda x: x.get(\"score\", 0), reverse=True)\n",
    "        top_chunks = unique_chunks[:5]\n",
    "\n",
    "        # Create comprehensive task\n",
    "        task = ResearchTask(query=main_query, max_sources=len(top_chunks), depth=\"deep\")\n",
    "\n",
    "        # Generate response with all context\n",
    "        system_prompt, user_prompt = self._build_research_prompt(task, top_chunks)\n",
    "\n",
    "        # Enhanced prompt for multi-query research\n",
    "        enhanced_prompt = (\n",
    "            user_prompt\n",
    "            + f\"\"\"\n",
    "\n",
    "相關子問題：\n",
    "{chr(10).join(f\"- {sq}\" for sq in sub_queries)}\n",
    "\n",
    "請提供一個綜合性的研究分析，涵蓋主要問題和相關子問題。\"\"\"\n",
    "        )\n",
    "\n",
    "        response = self._generate_response(system_prompt, enhanced_prompt)\n",
    "        summary, key_findings = self._parse_research_response(response)\n",
    "\n",
    "        # Build enhanced result\n",
    "        sources = []\n",
    "        for chunk in top_chunks:\n",
    "            source = {\n",
    "                \"id\": chunk.get(\"id\"),\n",
    "                \"text\": (\n",
    "                    chunk[\"text\"][:300] + \"...\"\n",
    "                    if len(chunk[\"text\"]) > 300\n",
    "                    else chunk[\"text\"]\n",
    "                ),\n",
    "                \"meta\": chunk[\"meta\"],\n",
    "                \"score\": chunk.get(\"score\", 0.0),\n",
    "            }\n",
    "            sources.append(source)\n",
    "\n",
    "        result = ResearchResult(\n",
    "            summary=summary,\n",
    "            key_findings=key_findings,\n",
    "            sources=sources,\n",
    "            confidence=min(0.9, np.mean([s[\"score\"] for s in sources]) + 0.1),\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            task=task,\n",
    "        )\n",
    "\n",
    "        self.research_history.append(result)\n",
    "        return result\n",
    "\n",
    "    def get_research_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary of research history\"\"\"\n",
    "        if not self.research_history:\n",
    "            return {\"total_research\": 0, \"avg_confidence\": 0, \"top_topics\": []}\n",
    "\n",
    "        total = len(self.research_history)\n",
    "        avg_confidence = np.mean([r.confidence for r in self.research_history])\n",
    "\n",
    "        # Extract topics from queries\n",
    "        topics = [r.task.query for r in self.research_history]\n",
    "\n",
    "        return {\n",
    "            \"total_research\": total,\n",
    "            \"avg_confidence\": avg_confidence,\n",
    "            \"recent_topics\": topics[-5:],  # Last 5 topics\n",
    "            \"best_confidence\": max(r.confidence for r in self.research_history),\n",
    "        }\n",
    "\n",
    "\n",
    "# Test advanced researcher\n",
    "advanced_researcher = AdvancedResearcher(rag_retriever=rag_retriever)\n",
    "\n",
    "\n",
    "def test_multi_query_research():\n",
    "    \"\"\"Test multi-query research capability\"\"\"\n",
    "    print(\"\\n=== Testing Multi-Query Research ===\")\n",
    "\n",
    "    main_query = \"多代理系統在AI中的應用\"\n",
    "    sub_queries = [\"代理協作的基本原理\", \"多代理系統的優勢\", \"實際應用案例\"]\n",
    "\n",
    "    result = advanced_researcher.multi_query_research(main_query, sub_queries)\n",
    "\n",
    "    print(f\"Main Query: {main_query}\")\n",
    "    print(f\"Sub-queries: {sub_queries}\")\n",
    "    print(f\"\\nResult Confidence: {result.confidence:.2f}\")\n",
    "    print(f\"Sources: {len(result.sources)}\")\n",
    "    print(f\"\\nSummary:\\n{result.summary}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "multi_result = test_multi_query_research()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6687c9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 7: Integration with Blackboard\n",
    "# ================================\n",
    "\n",
    "\n",
    "class Blackboard:\n",
    "    \"\"\"Simple blackboard for agent communication\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "        self.history = []\n",
    "\n",
    "    def write(self, key: str, value: Any, agent: str = \"unknown\"):\n",
    "        \"\"\"Write data to blackboard\"\"\"\n",
    "        self.data[key] = value\n",
    "        self.history.append(\n",
    "            {\n",
    "                \"action\": \"write\",\n",
    "                \"key\": key,\n",
    "                \"agent\": agent,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def read(self, key: str) -> Any:\n",
    "        \"\"\"Read data from blackboard\"\"\"\n",
    "        return self.data.get(key)\n",
    "\n",
    "    def get_all(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get all data\"\"\"\n",
    "        return self.data.copy()\n",
    "\n",
    "\n",
    "def integrate_researcher_with_blackboard():\n",
    "    \"\"\"Demonstrate researcher integration with blackboard\"\"\"\n",
    "    print(\"\\n=== Researcher + Blackboard Integration ===\")\n",
    "\n",
    "    # Create blackboard\n",
    "    blackboard = Blackboard()\n",
    "\n",
    "    # Simulate research workflow\n",
    "    blackboard.write(\n",
    "        \"research_request\",\n",
    "        {\"topic\": \"向量檢索技術的發展\", \"urgency\": \"medium\", \"deadline\": \"2024-12-31\"},\n",
    "        agent=\"planner\",\n",
    "    )\n",
    "\n",
    "    # Researcher reads request\n",
    "    request = blackboard.read(\"research_request\")\n",
    "    print(f\"Research request: {request}\")\n",
    "\n",
    "    # Conduct research\n",
    "    task = ResearchTask(query=request[\"topic\"], depth=\"moderate\", max_sources=4)\n",
    "\n",
    "    result = researcher.research(task)\n",
    "\n",
    "    # Write results to blackboard\n",
    "    research_output = {\n",
    "        \"summary\": result.summary,\n",
    "        \"key_findings\": result.key_findings,\n",
    "        \"source_count\": len(result.sources),\n",
    "        \"confidence\": result.confidence,\n",
    "        \"completed_at\": result.timestamp,\n",
    "    }\n",
    "\n",
    "    blackboard.write(\"research_results\", research_output, agent=\"researcher\")\n",
    "\n",
    "    print(f\"\\nResearch completed and written to blackboard:\")\n",
    "    print(f\"Confidence: {result.confidence:.2f}\")\n",
    "    print(f\"Key findings: {len(result.key_findings)}\")\n",
    "\n",
    "    # Other agents can now read the results\n",
    "    planner_view = blackboard.read(\"research_results\")\n",
    "    print(f\"\\nPlanner can access: {list(planner_view.keys())}\")\n",
    "\n",
    "    return blackboard, result\n",
    "\n",
    "\n",
    "bb, bb_result = integrate_researcher_with_blackboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67816bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 8: Performance Metrics and Monitoring\n",
    "# ================================\n",
    "\n",
    "\n",
    "class ResearchMetrics:\n",
    "    \"\"\"Track researcher performance metrics\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            \"total_queries\": 0,\n",
    "            \"avg_response_time\": 0,\n",
    "            \"avg_confidence\": 0,\n",
    "            \"source_utilization\": 0,\n",
    "            \"citation_accuracy\": 0,\n",
    "        }\n",
    "        self.query_times = []\n",
    "        self.confidences = []\n",
    "        self.source_counts = []\n",
    "\n",
    "    def record_research(self, result: ResearchResult, response_time: float):\n",
    "        \"\"\"Record metrics for a research session\"\"\"\n",
    "        self.metrics[\"total_queries\"] += 1\n",
    "        self.query_times.append(response_time)\n",
    "        self.confidences.append(result.confidence)\n",
    "        self.source_counts.append(len(result.sources))\n",
    "\n",
    "        # Update averages\n",
    "        self.metrics[\"avg_response_time\"] = np.mean(self.query_times)\n",
    "        self.metrics[\"avg_confidence\"] = np.mean(self.confidences)\n",
    "        self.metrics[\"source_utilization\"] = np.mean(self.source_counts)\n",
    "\n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get metrics summary\"\"\"\n",
    "        if self.metrics[\"total_queries\"] == 0:\n",
    "            return {\"status\": \"no_data\"}\n",
    "\n",
    "        return {\n",
    "            \"total_queries\": self.metrics[\"total_queries\"],\n",
    "            \"avg_response_time\": f\"{self.metrics['avg_response_time']:.2f}s\",\n",
    "            \"avg_confidence\": f\"{self.metrics['avg_confidence']:.2f}\",\n",
    "            \"avg_sources_per_query\": f\"{self.metrics['source_utilization']:.1f}\",\n",
    "            \"performance_score\": min(\n",
    "                1.0,\n",
    "                self.metrics[\"avg_confidence\"]\n",
    "                * (1 / max(1, self.metrics[\"avg_response_time\"] / 5)),\n",
    "            ),\n",
    "        }\n",
    "\n",
    "\n",
    "# Test metrics tracking\n",
    "metrics = ResearchMetrics()\n",
    "\n",
    "\n",
    "def benchmark_researcher():\n",
    "    \"\"\"Benchmark researcher performance\"\"\"\n",
    "    print(\"\\n=== Researcher Benchmark ===\")\n",
    "\n",
    "    test_queries = [\n",
    "        \"RAG技術的核心組件有哪些？\",\n",
    "        \"向量檢索如何提升搜索準確性？\",\n",
    "        \"中文NLP處理面臨什麼挑戰？\",\n",
    "        \"引用標註在學術寫作中的重要性\",\n",
    "        \"多代理系統的協作機制\",\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for query in test_queries:\n",
    "        print(f\"\\nTesting: {query}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        task = ResearchTask(query=query, max_sources=3)\n",
    "        result = researcher.research(task)\n",
    "        end_time = time.time()\n",
    "\n",
    "        response_time = end_time - start_time\n",
    "        metrics.record_research(result, response_time)\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"query\": query,\n",
    "                \"confidence\": result.confidence,\n",
    "                \"sources\": len(result.sources),\n",
    "                \"time\": response_time,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"  Confidence: {result.confidence:.2f}, Sources: {len(result.sources)}, Time: {response_time:.2f}s\"\n",
    "        )\n",
    "\n",
    "    # Display summary\n",
    "    summary = metrics.get_summary()\n",
    "    print(f\"\\n📊 Benchmark Summary:\")\n",
    "    for key, value in summary.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    return results, summary\n",
    "\n",
    "\n",
    "benchmark_results, benchmark_summary = benchmark_researcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d506a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 9: Error Handling and Robustness\n",
    "# ================================\n",
    "\n",
    "\n",
    "class RobustResearcher(ResearcherAgent):\n",
    "    \"\"\"Researcher with enhanced error handling\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.error_count = 0\n",
    "        self.max_retries = 3\n",
    "\n",
    "    def safe_research(self, task: ResearchTask) -> Optional[ResearchResult]:\n",
    "        \"\"\"Research with error handling and retries\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                print(f\"Research attempt {attempt + 1}/{self.max_retries}\")\n",
    "                return self.research(task)\n",
    "\n",
    "            except torch.cuda.OutOfMemoryError:\n",
    "                print(\"⚠ CUDA OOM - clearing cache and retrying...\")\n",
    "                torch.cuda.empty_cache()\n",
    "                self.error_count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ Research error: {str(e)[:100]}\")\n",
    "                self.error_count += 1\n",
    "\n",
    "                if attempt == self.max_retries - 1:\n",
    "                    # Return fallback result\n",
    "                    return self._create_fallback_result(task, str(e))\n",
    "\n",
    "                time.sleep(1)  # Brief pause before retry\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _create_fallback_result(\n",
    "        self, task: ResearchTask, error_msg: str\n",
    "    ) -> ResearchResult:\n",
    "        \"\"\"Create fallback result when research fails\"\"\"\n",
    "        return ResearchResult(\n",
    "            summary=f\"無法完成對「{task.query}」的研究分析。錯誤：{error_msg[:50]}...\",\n",
    "            key_findings=[\"研究過程中遇到技術問題\", \"建議稍後重試或調整查詢\"],\n",
    "            sources=[],\n",
    "            confidence=0.1,\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            task=task,\n",
    "        )\n",
    "\n",
    "    def get_error_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get error statistics\"\"\"\n",
    "        return {\n",
    "            \"total_errors\": self.error_count,\n",
    "            \"error_rate\": self.error_count / max(1, metrics.metrics[\"total_queries\"]),\n",
    "            \"status\": \"healthy\" if self.error_count < 3 else \"needs_attention\",\n",
    "        }\n",
    "\n",
    "\n",
    "# Test robust researcher\n",
    "robust_researcher = RobustResearcher(rag_retriever=rag_retriever)\n",
    "\n",
    "\n",
    "def test_error_handling():\n",
    "    \"\"\"Test error handling capabilities\"\"\"\n",
    "    print(\"\\n=== Testing Error Handling ===\")\n",
    "\n",
    "    # Test with normal query\n",
    "    normal_task = ResearchTask(query=\"什麼是機器學習？\")\n",
    "    result = robust_researcher.safe_research(normal_task)\n",
    "\n",
    "    print(f\"Normal query result: {result.confidence:.2f}\")\n",
    "\n",
    "    # Test with potentially problematic query (very long)\n",
    "    long_query = \"很長的查詢 \" * 100  # Artificially long query\n",
    "    long_task = ResearchTask(query=long_query)\n",
    "\n",
    "    try:\n",
    "        result = robust_researcher.safe_research(long_task)\n",
    "        print(f\"Long query handled: {result.confidence:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Long query failed: {e}\")\n",
    "\n",
    "    # Get error stats\n",
    "    error_stats = robust_researcher.get_error_stats()\n",
    "    print(f\"Error stats: {error_stats}\")\n",
    "\n",
    "    return error_stats\n",
    "\n",
    "\n",
    "error_stats = test_error_handling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3ce76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 10: Export and Integration Utils\n",
    "# ================================\n",
    "\n",
    "\n",
    "def save_research_result(result: ResearchResult, output_dir: str = \"outs\"):\n",
    "    \"\"\"Save research result to file\"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "\n",
    "    # Create filename from timestamp and query\n",
    "    safe_query = re.sub(r\"[^\\w\\s-]\", \"\", result.task.query)[:50]\n",
    "    safe_query = re.sub(r\"[-\\s]+\", \"-\", safe_query)\n",
    "    filename = f\"research_{safe_query}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "    filepath = output_path / filename\n",
    "\n",
    "    # Convert to serializable format\n",
    "    data = {\n",
    "        \"task\": asdict(result.task),\n",
    "        \"result\": {\n",
    "            \"summary\": result.summary,\n",
    "            \"key_findings\": result.key_findings,\n",
    "            \"sources\": result.sources,\n",
    "            \"confidence\": result.confidence,\n",
    "            \"timestamp\": result.timestamp,\n",
    "        },\n",
    "        \"metadata\": {\n",
    "            \"model\": researcher.model_name,\n",
    "            \"embedding_model\": rag_retriever.embedding_model,\n",
    "            \"total_sources\": len(result.sources),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Research result saved to: {filepath}\")\n",
    "    return str(filepath)\n",
    "\n",
    "\n",
    "def load_research_result(filepath: str) -> ResearchResult:\n",
    "    \"\"\"Load research result from file\"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    task = ResearchTask(**data[\"task\"])\n",
    "    result_data = data[\"result\"]\n",
    "\n",
    "    result = ResearchResult(\n",
    "        summary=result_data[\"summary\"],\n",
    "        key_findings=result_data[\"key_findings\"],\n",
    "        sources=result_data[\"sources\"],\n",
    "        confidence=result_data[\"confidence\"],\n",
    "        timestamp=result_data[\"timestamp\"],\n",
    "        task=task,\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_research_report(results: List[ResearchResult]) -> str:\n",
    "    \"\"\"Create formatted research report from multiple results\"\"\"\n",
    "    report_lines = [\n",
    "        \"# 研究報告\",\n",
    "        f\"生成時間：{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "        f\"總研究數量：{len(results)}\",\n",
    "        \"\",\n",
    "    ]\n",
    "\n",
    "    for i, result in enumerate(results, 1):\n",
    "        report_lines.extend(\n",
    "            [\n",
    "                f\"## 研究 {i}: {result.task.query}\",\n",
    "                f\"**可信度：** {result.confidence:.2f}\",\n",
    "                f\"**來源數量：** {len(result.sources)}\",\n",
    "                \"\",\n",
    "                \"### 摘要\",\n",
    "                result.summary,\n",
    "                \"\",\n",
    "                \"### 關鍵發現\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for j, finding in enumerate(result.key_findings, 1):\n",
    "            report_lines.append(f\"{j}. {finding}\")\n",
    "\n",
    "        report_lines.extend(\n",
    "            [\n",
    "                \"\",\n",
    "                \"### 參考來源\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for j, source in enumerate(result.sources, 1):\n",
    "            meta = source[\"meta\"]\n",
    "            title = meta.get(\"title\", \"Unknown\")\n",
    "            score = source.get(\"score\", 0)\n",
    "            report_lines.append(f\"{j}. {title} (相關度: {score:.3f})\")\n",
    "\n",
    "        report_lines.append(\"\\n---\\n\")\n",
    "\n",
    "    return \"\\n\".join(report_lines)\n",
    "\n",
    "\n",
    "# Test export functionality\n",
    "def test_export_functionality():\n",
    "    \"\"\"Test research result export and reporting\"\"\"\n",
    "    print(\"\\n=== Testing Export Functionality ===\")\n",
    "\n",
    "    # Save a research result\n",
    "    filepath = save_research_result(smoke_result)\n",
    "\n",
    "    # Load it back\n",
    "    loaded_result = load_research_result(filepath)\n",
    "    print(f\"Loaded result confidence: {loaded_result.confidence:.2f}\")\n",
    "\n",
    "    # Create a report with multiple results\n",
    "    sample_results = (\n",
    "        [smoke_result, multi_result] if \"multi_result\" in locals() else [smoke_result]\n",
    "    )\n",
    "    report = create_research_report(sample_results)\n",
    "\n",
    "    # Save report\n",
    "    report_path = (\n",
    "        Path(\"outs\") / f\"research_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "    )\n",
    "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "    print(f\"Report saved to: {report_path}\")\n",
    "    print(f\"Report length: {len(report)} characters\")\n",
    "\n",
    "    return report_path\n",
    "\n",
    "\n",
    "report_path = test_export_functionality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccad3701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 11: Final Integration Test\n",
    "# ================================\n",
    "\n",
    "\n",
    "def comprehensive_integration_test():\n",
    "    \"\"\"Comprehensive test of all researcher features\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"🧪 COMPREHENSIVE RESEARCHER INTEGRATION TEST\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    test_scenarios = [\n",
    "        {\n",
    "            \"name\": \"Basic RAG Research\",\n",
    "            \"query\": \"解釋向量檢索的工作原理\",\n",
    "            \"expected_sources\": 2,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Multi-domain Query\",\n",
    "            \"query\": \"RAG技術在多代理系統中的應用\",\n",
    "            \"expected_sources\": 3,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Technical Deep Dive\",\n",
    "            \"query\": \"FAISS索引的優化策略\",\n",
    "            \"expected_sources\": 2,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    all_results = []\n",
    "    total_start = time.time()\n",
    "\n",
    "    for i, scenario in enumerate(test_scenarios, 1):\n",
    "        print(f\"\\n📋 Test {i}: {scenario['name']}\")\n",
    "        print(f\"Query: {scenario['query']}\")\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Create task\n",
    "        task = ResearchTask(\n",
    "            query=scenario[\"query\"],\n",
    "            max_sources=scenario[\"expected_sources\"],\n",
    "            depth=\"moderate\",\n",
    "        )\n",
    "\n",
    "        # Execute research\n",
    "        result = robust_researcher.safe_research(task)\n",
    "\n",
    "        if result:\n",
    "            elapsed = time.time() - start\n",
    "            all_results.append(result)\n",
    "\n",
    "            print(f\"✅ Completed in {elapsed:.2f}s\")\n",
    "            print(f\"   Confidence: {result.confidence:.2f}\")\n",
    "            print(f\"   Sources: {len(result.sources)}\")\n",
    "            print(f\"   Findings: {len(result.key_findings)}\")\n",
    "\n",
    "            # Validate result quality\n",
    "            if result.confidence > 0.3 and len(result.sources) > 0:\n",
    "                print(\"   ✅ Quality check passed\")\n",
    "            else:\n",
    "                print(\"   ⚠ Quality check warning\")\n",
    "        else:\n",
    "            print(\"   ❌ Research failed\")\n",
    "\n",
    "    total_time = time.time() - total_start\n",
    "\n",
    "    # Generate summary report\n",
    "    if all_results:\n",
    "        print(f\"\\n📊 INTEGRATION TEST SUMMARY\")\n",
    "        print(f\"Total time: {total_time:.2f}s\")\n",
    "        print(\n",
    "            f\"Success rate: {len(all_results)}/{len(test_scenarios)} ({len(all_results)/len(test_scenarios)*100:.1f}%)\"\n",
    "        )\n",
    "        print(f\"Average confidence: {np.mean([r.confidence for r in all_results]):.2f}\")\n",
    "        print(\n",
    "            f\"Average sources per query: {np.mean([len(r.sources) for r in all_results]):.1f}\"\n",
    "        )\n",
    "\n",
    "        # Save comprehensive report\n",
    "        final_report = create_research_report(all_results)\n",
    "        final_report_path = (\n",
    "            Path(\"outs\")\n",
    "            / f\"integration_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "        )\n",
    "        with open(final_report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(final_report)\n",
    "\n",
    "        print(f\"Final report saved: {final_report_path}\")\n",
    "\n",
    "        return all_results, final_report_path\n",
    "    else:\n",
    "        print(\"❌ All tests failed\")\n",
    "        return [], None\n",
    "\n",
    "\n",
    "integration_results, final_report_path = comprehensive_integration_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e52daa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 12: Summary and Next Steps\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"📋 NB31 SUMMARY: RESEARCHER WITH RAG INTEGRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\n",
    "    \"\"\"\n",
    "## 🎯 Goals Completed:\n",
    "✅ Integrated Stage 2 RAG components into Researcher agent role\n",
    "✅ Implemented knowledge retrieval with citation tracking\n",
    "✅ Created research synthesis with source attribution\n",
    "✅ Enabled multi-source evidence gathering and summarization\n",
    "✅ Built foundation for 4-role orchestrator collaboration\n",
    "\n",
    "## 🔧 Core Components Built:\n",
    "• RAGRetriever: 向量檢索與FAISS索引整合\n",
    "• ResearcherAgent: 具備RAG能力的研究助理\n",
    "• AdvancedResearcher: 多查詢與歷史追蹤\n",
    "• RobustResearcher: 錯誤處理與重試機制\n",
    "• Blackboard Integration: 與多代理黑板系統整合\n",
    "• Research Metrics: 效能追蹤與基準測試\n",
    "\n",
    "## 🏗 Key Parameters (Low-VRAM Optimized):\n",
    "• Embedding Model: BAAI/bge-m3 (multilingual, efficient)\n",
    "• LLM: Qwen2.5-7B-Instruct (device_map=\"auto\", fp16)\n",
    "• Max Context: 3072 tokens (留空間給生成)\n",
    "• Top-K Retrieval: 3-5 chunks (平衡品質與速度)\n",
    "• Temperature: 0.3 (研究任務需要一致性)\n",
    "• Batch Processing: 支援批次嵌入以提升效率\n",
    "\n",
    "## 🧪 Smoke Test Results:\"\"\"\n",
    ")\n",
    "\n",
    "if \"smoke_result\" in locals():\n",
    "    print(f\"• Basic Research: ✅ (Confidence: {smoke_result.confidence:.2f})\")\n",
    "if \"multi_result\" in locals():\n",
    "    print(f\"• Multi-Query: ✅ (Confidence: {multi_result.confidence:.2f})\")\n",
    "if \"error_stats\" in locals():\n",
    "    print(f\"• Error Handling: ✅ (Status: {error_stats.get('status', 'unknown')})\")\n",
    "if \"benchmark_summary\" in locals():\n",
    "    print(f\"• Performance: ✅ (Avg: {benchmark_summary.get('avg_confidence', 'N/A')})\")\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "## ⚡ Performance Metrics:\"\"\"\n",
    ")\n",
    "if \"metrics\" in locals():\n",
    "    summary = metrics.get_summary()\n",
    "    if summary.get(\"status\") != \"no_data\":\n",
    "        for key, value in summary.items():\n",
    "            print(f\"• {key}: {value}\")\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "## 🎮 When to Use This:\n",
    "• 需要基於特定知識庫的研究分析\n",
    "• 多代理協作中的資訊收集角色\n",
    "• 學術寫作的文獻調研階段\n",
    "• 技術文檔的背景資料整理\n",
    "• 需要引用來源的內容生成\n",
    "\n",
    "## ⚠ Pitfalls & Solutions:\n",
    "• 索引為空 → 檢查data/目錄與chunks.jsonl\n",
    "• CUDA OOM → 降低batch_size或使用CPU\n",
    "• 引用格式錯亂 → 確保chunks包含正確meta資料\n",
    "• 檢索不相關 → 調整embedding模型或查詢改寫\n",
    "• 生成過長 → 限制max_new_tokens與context長度\n",
    "\n",
    "## 🚀 Next Steps (nb32):\n",
    "• Planner Agent: 基於研究結果生成大綱\n",
    "• Writer-Researcher協作: 共享黑板通信\n",
    "• 結構化輸出: JSON schema驗證\n",
    "• 多輪對話: 追問與深入研究\n",
    "• 領域專精: 技術/法律/教育等專業模式\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎉 NB31 COMPLETED - Ready for nb32_planner_outline.ipynb\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
